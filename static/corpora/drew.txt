#(`*Stanford University*`)#.
Coordinates: 3726N 12210W / 37.43N 122.17W / 37.43; -122.17
The Leland Stanford Junior University, commonly referred to as Stanford University or Stanford, is an American private research university located in Stanford, California on an 8,180-acre (3,310ha) campus near Palo Alto.[note 1] It is situated in the northwestern Silicon Valley, approximately 20 miles (32km) northwest of San Jose and 37 miles (60km) southeast of San Francisco.[6] Today, Stanford stands as one of the most prestigious universities in the United States and the world.[8][9][10][11][12]
Leland Stanford, Governor and Senator of California and leading railroad tycoon, and his wife Jane Lathrop Stanford founded the university in 1891 in memory of their son, Leland Stanford,Jr., who died of typhoid two months before his 16th birthday. The university was established as a coeducational and nondenominational institution. Tuition was free until the 1930s.[13] The university struggled financially after the senior Stanford's 1893 death and after much of the campus was damaged by the 1906 San Francisco earthquake. Following World War II, Provost Frederick Terman supported faculty and graduates' entrepreneurialism to build self-sufficient local industry in what would become known as Silicon Valley. By 1970, Stanford was home to a linear accelerator, and was one of the original four ARPANET nodes (precursor to the internet).
Since 1952, more than 54 Stanford faculty, staff, and alumni have won the Nobel Prize, including 19 current faculty members,[14] and Stanford has the largest number of Turing award winners (dubbed the "Nobel Prize of Computer Science") for a single institution. Stanford is the alma mater of 30 living billionaires, 17 astronauts, and one of the leading producers of members of the United States Congress.[15][16] Faculty and alumni have founded many prominent companies including Google, Hewlett-Packard, Nike, Sun Microsystems, and Yahoo!, and companies founded by Stanford alumni generate more than $2.7 trillion in annual revenue, equivalent to the 10th largest economy in the world.[17] Stanford is also home to the original papers of Martin Luther King Jr., and history professor Clayborne Carson directs the King Papers Project.[18]
The university is organized into seven schools, including academic schools of Humanities and Sciences and Earth Sciences as well as professional schools of Business, Education, Engineering, Law, and Medicine. Stanford has a student body of approximately 6,988 undergraduate and 8,400 graduate students.[5] Stanford is a founding member of the Association of American Universities. For the 20112012 year, the university managed a US$16.5 billion endowment, with $25.1 billion in consolidated net assets.[3]
Stanford competes in 34 varsity sports and is one of two private universities in the Division I FBS Pacific-12 Conference. Stanford has won 103 NCAA championships (the second-most for a university), and Stanford's athletic program has won the NACDA Directors' Cup every year since 1995.[19][20] Stanford athletes have won medals in every Olympic Games since 1912, winning 244 Olympic medals total, 129 of them gold. In the 2008 Beijing Olympic Games, Stanford won more Olympic medals than any other university in the United States and, in terms of medals won, tied with the country of Japan for 11th place.[21][22][23]
Stanford was founded by Leland Stanford, a railroad magnate, United States Senator, and former California governor, and his wife, Jane Stanford. It is named in honor of their only child, Leland Stanford, Jr., who died in 1884 just before his 16th birthday. His parents decided to dedicate a university to their only son, and Leland Stanford told his wife, "The children of California shall be our children." The Stanfords visited Harvard's president, Charles Eliot, and asked how much it would cost to duplicate Harvard in California. Eliot replied that he supposed $15 million (in 1884 dollars) would be enough.
The university's founding Grant of Endowment from the Stanfords was issued in November 1885.[24][25] Besides defining the operational structure of the university, it made several specific stipulations: "The Trustees ... shall have the power and it shall be their duty:
The Stanfords chose their country estate, Palo Alto Stock Farm, in Santa Clara County as the site of the university, so that the University is sometimes called "the Farm" to this day.[note 2] The original "inner quad" buildings (188791) were designed by Frederick Law Olmsted, Francis A. Walker, Charles Allerton Coolidge, and Leland Stanford himself.
Visiting Stanford in 1903, President Theodore Roosevelt said of the campus and the university, "Now I have come to this great institution of learning and I wonder whether you yourselves fully appreciate the mere physical beauty of your surroundings. I was not prepared in the least (and I thought I was prepared for it) for the beauty of your surroundings. You have had these plans of your university made by a great architect, native to our own American soil, who himself had the sense to adaptnot to copy in servile fashionbut to adapt the old Californian architecture to the new university uses, and so we have here a great institution of learning absolutely unique, even in its outward aspect, situated in this beautiful valley with the hills in the background, under this sky, with these buildings, and if this university does not turn out the right kind of citizenship and the right kind of scholarship, I shall be more than disappointed."[27]
In Spring 1891 the Stanfords offered the presidency of their new university to the president of Cornell University, Andrew White, but he declined and recommended David Starr Jordan, the 40-year-old president of Indiana University Bloomington. Jordan's educational philosophy was a good fit with the Stanfords' vision of a non-sectarian, co-educational school with a liberal arts curriculum, and he accepted the offer.[28] Jordan arrived at Stanford in June 1891 and immediately set about recruiting faculty for the university's planned October opening. With such a short time frame he drew heavily on his own acquaintance in academia; of the fifteen original professors, most came either from Indiana University or his alma mater Cornell. The 1891 founding professors included Robert Allardice in mathematics, Douglas Houghton Campbell in botany, Charles Henry Gilbert in zoology, George Elliott Howard in history, Oliver Peebles Jenkins in physiology and histology, Charles David Marx in civil engineering, Fernando Sanford in physics and John Maxson Stillman in chemistry. The total initial teaching staff numbered about 35 including instructors and lecturers.[29] For the second (189293) school year, Jordan was able to add additional professors including Frank Angell (psychology), Leander M. Hoskins (mechanical engineering), Walter Miller (classics), George C. Price (zoology), and Arly B. Show (history). Most of these two founding groups of professors remained at Stanford until their retirement and were referred to as the "Old Guard".[30]
The university officially opened on October 1, 1891 to 559 students. On the university's opening day, Founding President David Starr Jordan said to Stanford's Pioneer Class: "[Stanford] is hallowed by no traditions; it is hampered by none. Its finger posts all point forward."[31] Herbert Hoover and his future wife Lou Henry Hoover were in the first class; the Hoovers maintained close lifetime ties to the school.
The motto of Stanford University, selected by President Jordan, is "Die Luft der Freiheit weht." Translated from the German language, this quotation from Ulrich von Hutten means, "The wind of freedom blows." The motto was controversial during World War I, when anything in German was suspect; at that time the university disavowed that this motto was official.[32]
The school was established as a coeducational institution. However, Jane Stanford soon put a policy in place limiting enrollment of women to 500 per year, because of the large number of female students enrolling. She did not want the school to become "the Vassar of the West" because she felt that would not be an appropriate memorial for her son. In 1933 the policy was modified to specify an undergraduate male:female ratio of 3:1.[33] The "Stanford ratio" of 3:1 remained in place until the early 1960s. By the late 1960s the "ratio" was about 2:1 for undergraduates, but much more skewed at the graduate level, except in the humanities. As of 2005 the school no longer maintains a gender preference policy and undergraduate enrollment is split nearly evenly between the sexes, though males outnumber females about 2:1 at the graduate level, exclusive of Humanities.[34][35]
When Leland Stanford died in 1893, the continued existence of the university was in jeopardy. A $15 million government lawsuit against Stanford's estate, combined with the Panic of 1893, made it extremely difficult to meet expenses. Most of the Board of Trustees advised a temporary closing until finances could be sorted out. However, Jane Stanford insisted that the university remain in operation. Faced with the possibility of financial ruin for the University she took charge of financial, administrative, and development matters at the university 1893-1905; from her experience as a mother and housewife, she ran the institution as a household. For the next several years, she paid salaries out of her personal resources, even pawning her jewelry to keep the university going. When the lawsuit was finally dropped in 1895, a university holiday was declared.[36][37]
Stanford alumnus George E. Crothers became a close adviser to Jane Stanford following his graduation from Stanford's law school in 1896.[38] Working with his brother Thomas (also a Stanford graduate and a lawyer), Crothers identified and corrected numerous major legal defects in the terms of the university's founding grant and successfully lobbied for an amendment to the California state constitution granting Stanford an exemption from taxation on its educational propertya change which allowed Jane Stanford to donate her stock holdings to the university.[39]
Edward Alsworth Ross gained fame as a founding father of American sociology; in 1900 Jane Stanford fired him for radicalism and racism, unleashing a major academic freedom case.[40]
Jane Stanford's actions were sometimes eccentric. In 1897, she directed the board of trustees "that the students be taught that everyone born on earth has a soul germ, and that on its development depends much in life here and everything in Life Eternal".[41] She forbade students from sketching nude models in life-drawing class, banned automobiles from campus, and did not allow a hospital to be constructed so that people would not form an impression that Stanford was unhealthy. Between 1899 and 1905, she spent $3 million on a grand construction scheme building lavish memorials to the Stanford family, while university faculty and self-supporting students were living in poverty.[41] However, in 1901, she transferred $30 million in assets, nearly all her remaining wealth, to the university;[42] upon her death in 1905, she left the university nearly $4 million of her remaining $7 million. In total, the Stanfords donated around $40 million in assets to the university (over $1 billion in 2010 dollars).[43]
The 1906 San Francisco earthquake destroyed parts of the Main Quad (including the original iteration of Memorial Church) as well as the gate that first marked the entrance of the school; rebuilding on a somewhat less grandiose scale began immediately. In 1931, Stanford and Harvard University participated in the first ever intercollegiate radio debate.[44]
From 1906 to 1919, in response to the crisis caused by numerous injuries, intercollegiate football was in jeopardy. While some colleges dropped football entirely, a few, such as the University of California and Stanford University, replaced it with English rugby. From 1906 to 1914, the two schools played rugby as their major sport, but they soon found that the objectionable practices they saw in football were introduced into rugby. Finally, when the football rules were changed, a move developed to return to football, reviving intercollegiate sports and enabling students and alumni to identify with football, an American sport.[45]
The Hoover Institution Library and Archives (official name: Hoover Institution on War, Revolution, and Peace) at Stanford was set up in 1920 by Herbert C. Hoover, one of Stanford's first graduates. He had been in charge of American relief efforts in Europe after World War I before his election as President of the United States in 1928. Hoover's express purpose was to collect the records of contemporary history as it was happening. Hoover's helpers frequently risked their lives to rescue documentary and rare printed material, especially from countries under Nazi or Communist rule. Their many successes included the papers of Rosa Luxemburg, the Goebbels diaries, and the records of the Russian secret police in Paris. Research institutes were also set up under Hoover's influence, though inevitably there were to be clashes between the moving force, Hoover, and the host university. In 1960, W. Glenn Campbell was appointed director and substantial budget increases soon led to corresponding increases in acquisitions and related research projects. Despite student unrest in the 1960s, the institution continued to thrive and develop closer relations with Stanford. In particular, the Chinese and Russian collections grew considerably. From the 1980s forward the Hoover Institution itself evolved into a conservative think tank, functioning independently from the library and archive. It continues as an integral component of the University.[46]
The biological sciences department evolved rapidly from 1946 to 1972 as its research focus changed, due to the Cold War and other historically significant conditions external to academia. Stanford science went through three phases of experimental direction during that time. In the early 1950s the department remained fixed in the classical independent and self-directed research mode, shunning interdisciplinary collaboration and excessive government funding. Between the 1950s and mid-1960s biological research shifted focus to the molecular level. Then, from the late 1960s onward, Stanford's goal became applying research and findings toward humanistic ends. Each phase was preempted by larger social issues, such as the escalation of the Cold War, the launch of Sputnik, and public concern over medical abuses.[47] In 1971 professor Philip Zimbardo conducted the Stanford Prison Experiment.
A powerful sense of regional solidarity accompanied the rise of Silicon Valley. From the 1890s, the university's leaders saw its mission as service to the West and shaped the school accordingly. At the same time, the perceived exploitation of the West at the hands of eastern interests fueled booster-like attempts to build self-sufficient indigenous local industry. Thus, regionalism helped align Stanford's interests with those of the area's high-tech firms for the first fifty years of Silicon Valley's development. The distinctive regional ethos of the West during the first half of the 20th century is an ingredient of Silicon Valley's already prepared environment, an ingredient that would-be replicators ignore at their peril.[48]
During the 1940s and 1950s, Frederick Terman, as dean of engineering and provost, encouraged faculty and graduates to start their own companies. He is credited with nurturing Hewlett-Packard, Varian Associates, and other high-tech firms, until what would become Silicon Valley grew up around the Stanford campus. Terman is often called "the father of Silicon Valley."[49] Terman encouraged William B. Shockley, co-inventor of the transistor, to return to his hometown of Palo Alto. In 1956 he established the Shockley Transistor Laboratory.[50]
The spark that set off the explosive boom of "Silicon startups" in Stanford Industrial Park was a personal dispute in 1957 between employees of Shockley Semiconductor and the company's namesake and founder, Nobel laureate and co-inventor of the transistor William Shockley... (His employees) formed Fairchild Semiconductor immediately following their departure... After several years, Fairchild gained its footing, becoming a formidable presence in this sector. Its founders began to leave to start companies based on their own, latest ideas and were followed on this path by their own former leading employees... The process gained momentum and what had once began in a Stanford's research park became a veritable startup avalanche... Thus, over the course of just 20 years, a mere eight of Shockley's former employees gave forth 65 new enterprises, which then went on to do the same...[51]
In 1962-70 negotiations took place between the Cambridge Electron Accelerator Laboratory (shared by Harvard and the Massachusetts Institute of Technology), the Stanford Linear Accelerator Center, and the US Atomic Energy Commission over the proposed 1970 construction of the Stanford Positron Electron Asymmetric Ring (SPEAR). It would be the first US electron-positron colliding beam storage ring. Paris (2001) explores the competition and cooperation between the two university laboratories and presents diagrams of the proposed facilities, charts detailing location factors, and the parameters of different project proposals between 1967 and 1970. Several rings were built in Europe during the five years that it took to obtain funding for the project, but the extensive project revisions resulted in a superior design that was quickly constructed and paved the way for Nobel Prizes in 1976 for Burton Richter and in 1995 for Martin Perl.[52] During 1955-85, solid state technology research and development at Stanford University followed three waves of industrial innovation made possible by support from private corporations, mainly Bell Telephone Laboratories, Shockley Semiconductor, Fairchild Semiconductor, and Xerox PARC. In 1969 the Stanford Research Institute operated one of the four original nodes that comprised ARPANET, predecessor to the Internet.[53]
Since 2000, Stanford has expanded dramatically. In February 2012, Stanford announced the conclusion of the Stanford Challenge. In a period of five years, Stanford raised $6.2 billion dollars, exceeding its initial goal by $2 billion, making it the most successful university fundraising campaign in history.[54] The funds will go towards 103 new endowed faculty appointments, 360 graduate student research fellowships, scholarships and financial aid, and the construction or renovation of 38 campus buildings. It enabled the construction of the world's largest facility dedicated exclusively to stem cell research, an entirely new campus for the business school, added dramatically to the law school, a brand-new engineering quad, created a new art and art history building, an on-campus concert hall, a new art museum, and a planned expansion of the medical school, among others.[55] In 2012, Stanford opened the Stanford Center at Peking University, a just-under 400,000-square-foot, three-story research center at the heart of Peking University, consistently ranked as the best university in China. The ceremony featured remarks by U.S. Ambassador to China Gary Locke, Stanford President John Hennessy, and Peking University Party Chief Zhu Shanlu. Stanford became the first U.S. university to have its own building on a major Chinese University campus.[56]
Other Stanford programs underwent notable expansion as well, such as the Stanford in Washington Program's creation of the Stanford in Washington Art Gallery in Woodley Park, Washington, D.C., and the Stanford in Florence program's move to Palazzo Capponi, a 15th-century Renaissance palace.[57][58] The university completed the James H. Clark Center for interdisciplinary research in engineering and medicine in 2003, named for benefactor, co-founder of Netscape, Silicon Graphics and WebMD, and former professor of electrical engineering James H. Clark.
In 2011, Stanford created the first PhD program in Stem Cell Science in the United States. The program will be housed at Stanford Medical School.[59]
Undergraduate admission selectivity also increased, with the acceptance rate dropping from 13% for the class of 2004 to 6% for the class of 2016. Stanford's reputation, competitive admissions, and strong legacy of entrepreneurship have contributed to the East-West rivalry between Stanford and such institutions as Harvard University, Princeton University and Yale University.[60][61][62]
Stanford University is located on an 8,180-acre (3,310ha)[6] campus on the San Francisco Peninsula, in the northwest part of the Santa Clara Valley (Silicon Valley) approximately 37 miles (60km) southeast of San Francisco and approximately 20 miles (32km) northwest of San Jose. The main campus is adjacent to Palo Alto, bounded by El Camino Real, Stanford Avenue, Junipero Serra Boulevard, and Sand Hill Road. The university also operates at several more remote locations (see below).
Stanford's main campus is a census-designated place within unincorporated Santa Clara County, although some of the university land (including the Stanford Shopping Center and the Stanford Research Park) is within the city limits of Palo Alto. The campus also includes much land in unincorporated San Mateo County (including the SLAC National Accelerator Laboratory and the Jasper Ridge Biological Preserve), as well as in the city limits of Menlo Park (Stanford Hills neighborhood), Woodside, and Portola Valley.[63] The United States Postal Service has assigned Stanford two ZIP codes: 94305 for campus mail and 94309 for P.O. box mail. It lies within area code 650. The university campus was listed by MSN as one of the most beautiful college campuses in the world.[64]
In the summer of 1886, when the campus was first being planned, Stanford brought the president of Massachusetts Institute of Technology (MIT), Francis Amasa Walker, and prominent Boston landscape architect Frederick Law Olmsted westward for consultations. Olmsted worked out the general concept for the campus and its buildings, rejecting a hillside site in favor of the more practical flatlands. Charles Allerton Coolidge then developed this concept in the style of his late mentor, Henry Hobson Richardson, in the Richardsonian Romanesque style, characterized by rectangular stone buildings linked by arcades of half-circle arches. The original campus was also designed in the Spanish-colonial style common to California known as Mission Revival. The red tile roofs and solid sandstone masonry are distinctly Californian in appearance and famously complementary to the bright blue skies common to the region, and most of the subsequently erected buildings have maintained consistent exteriors.
Much of this first construction was destroyed by the 1906 San Francisco earthquake, but the university retains the Quad, the old Chemistry Building (which is not in use and has been boarded up since the 1989 Loma Prieta earthquake),[65] and Encina Hall (the residence of Herbert Hoover, John Steinbeck, and Anthony Kennedy during their times at Stanford). After the 1989 earthquake inflicted further damage, the university implemented a billion-dollar capital improvement plan to retrofit and renovate older buildings for new, up-to-date uses.[66]

Contemporary campus landmarks include the Main Quad and Memorial Church, the Cantor Center for Visual Arts and art gallery, the Stanford Mausoleum and the Angel of Grief, Hoover Tower, the Rodin sculpture garden, the Papua New Guinea Sculpture Garden, the Arizona Cactus Garden, the Stanford University Arboretum, Green Library and the Dish. Frank Lloyd Wright's 1937 Hanna-Honeycomb House and the 1919 Lou Henry and Herbert Hoover House are both listed on the National Historic Register.
One of the benefits of being a Stanford faculty member is the "Faculty Ghetto", where faculty members can live within walking or biking distance of campus. The Faculty Ghetto is composed of land owned entirely by Stanford. Similar to a condominium, the houses can be bought and sold but the land under the houses is rented on a 99-year lease. Houses in the "Ghetto" appreciate and depreciate, but not as rapidly as overall Silicon Valley values. However, it remains an expensive area in which to own property, and the average price of single-family homes on campus is actually higher than in Palo Alto. Stanford itself enjoys the rapid capital gains of Silicon Valley landowners, although by the terms of its founding the university cannot sell the land.
Stanford currently operates or intends to operate in various locations outside of its main campus.
On the founding grant but away from the main campus:
Off the founding grant:
Locations in development:
The university also has its own golf course and a seasonal lake (Lake Lagunita, actually an irrigation reservoir), both home to the vulnerable California Tiger Salamander. Lake Lagunita is often dry now, but the university has no plans to artificially fill it.[71]
In 2011, the university also participated in the bidding for applied science campus in New York City but finally abandoned the project at the end of the year.[72]
Stanford University is a tax-exempt corporate trust owned and governed by a privately appointed 35-member Board of Trustees.[73] Trustees serve five-year terms (not more than two consecutive terms) and meet five times annually.[74] A new trustee is chosen by the remaining Trustees by ballot.[24] The Stanford trustees also oversee the Stanford Research Park, the Stanford Shopping Center, the Cantor Center for Visual Arts, Stanford University Medical Center, and many associated medical facilities (including the Lucile Packard Children's Hospital).[73]
The Board appoints a President to serve as the chief executive officer of the university and prescribe the duties of professors and course of study, manage financial and business affairs, and appoint nine vice presidents.[75] John L. Hennessy was appointed the 10th President of the University in October 2000.[76] The Provost is the chief academic and budget officer, to whom the deans of each of the seven schools report.[77] John Etchemendy was named the 12th Provost in September 2000.[78]
The university is organized into seven schools: School of Humanities and Sciences, School of Engineering, School of Earth Sciences, School of Education, Graduate School of Business, Stanford Law School and the Stanford University School of Medicine.[77] The powers and authority of the faculty are vested in the Academic Council, which is made up of tenure and non-tenure line faculty, research faculty, senior fellows in some policy centers and institutes, the president of the university, and some other academic administrators, but most matters are handled by the Faculty Senate, made up of 55 elected representatives of the faculty.[79]
The Associated Students of Stanford University (ASSU) is the student government for Stanford University and all registered students are members.[80] Its elected leadership consists of the Undergraduate Senate elected by the undergraduate students, the Graduate Student Council elected by the graduate students, and the President and Vice President elected as a ticket by the entire student body.[80]
Stanford is the beneficiary of a special clause in the California Constitution, which explicitly exempts Stanford property from taxation so long as the property is used for educational purposes.[81]
The university's endowment, managed by the Stanford Management Company, was valued at $17.2 billion in 2008 and had achieved an annualized rate of return of 15.1% since 1998.[73][82] The endowment fell 25% in 2009 as a result of the late-2000s recession, but posted gains of 14.4% in 2010 and 22.4% in 2011, when it was valued at $16.5 billion.[83]
Stanford has been the top fundraising university in the United States for several years. It raised $911 million in 2006,[84] $832 million in 2007,[85] $785 million in 2008,[86] $640 million in 2009,[87] $599 million in 2010,[88] and $709 million in 2011.[89]
In 2006 President Hennessy launched a five-year campaign called the Stanford Challenge, which reached its $4.3 billion fundraising goal in 2009, two years ahead of time, but continued fundraising for the duration of the campaign.[90] It concluded on December 31, 2011, having raised a total of $6.23 billion and breaking the previous campaign fundraising record of $3.88 billion held by Yale.[91] Specifically, the campaign raised $253.7 million for undergraduate financial aid, as well as $2.33 billion for its initiative in "Seeking Solutions" to global problems, $1.61 billion for "Educating Leaders" by improving K-12 education, and $2.11 billion for "Foundation of Excellence" aimed at providing academic support for Stanford students and faculty. Funds supported 366 new fellowships for graduate students, 139 new endowed chairs for faculty, and 38 new or renovated buildings. Over 10,000 volunteers helped in raising 560,000 gifts from more than 166,000 donors.[92]
Stanford University is a large, highly residential research university with a majority of enrollments coming from graduate and professional students.[93] The full-time, four-year undergraduate program is classified as "more selective, lower transfer-in" and has an arts and sciences focus with high graduate student coexistence.[93] Stanford University is accredited by the Western Association of Schools and Colleges.[94] Full-time undergraduate tuition was $38,700 for 2010-2011.[95][96]
The schools of Humanities and Sciences (27 departments), Engineering (9 departments), and Earth Sciences (4 departments) have both graduate and undergraduate programs while the schools of Law, Medicine, and Education and the Graduate School of Business have graduate programs only.[97] Stanford follows a quarter system with Autumn quarter usually starting in late September and Spring Quarter ending in early June.
Stanford's current community of scholars includes:
Stanford's faculty and former faculty includes 27 Nobel laureates, as well as 19 recipients (22 if visiting professors and consulting professors included) of the Turing Award, the so-called "Nobel Prize in computer science", comprising one third of the awards given in its 44-year history. The university has 27 ACM fellows. It is also affiliated with 4 Gdel Prize winners, 4 Knuth Prize recipients, 10 IJCAI Computers and Thought Award winners, and about 15 Grace Murray Hopper Award winners for their work in the foundations of computer science.
As of 2011, 107 Stanford students or alumni have become Rhodes Scholars.[104]
Other Stanford-affiliated institutions include the SLAC National Accelerator Laboratory (originally the Stanford Linear Accelerator Center) and the Stanford Research Institute, a now independent institution which originated at the university, in addition to the Stanford Humanities Center.
Stanford also houses the Hoover Institution on War, Revolution and Peace, a major public policy think tank that attracts visiting scholars from around the world, and the Freeman Spogli Institute for International Studies, which is dedicated to the more specific study of international relations. Unable to locate a copy in any of its libraries, the Soviet Union was obliged to ask the Hoover Institution for a microfilm copy of its original edition of the first issue of Pravda (dated March 5, 1917).[105]
Stanford is home to the John S. Knight Fellowships for Professional Journalist and the Center for Ocean Solutions, which brings together marine science and policy to develop solutions to challenges facing the ocean. It also houses the Hasso Plattner Institute of Design (the "d.school"), a multidisciplinary design school in cooperation with the Hasso Plattner Institute of University of Potsdam that integrates product design, engineering, and business management education.
The Stanford University Libraries and Academic Information Resources (SULAIR) hold a collection of nearly 9 million volumes, 260,000 rare or special books, 1.5 million e-books, 1.5 million audiovisual materials, 75,000 serials, 6 million microform holdings, and thousands of other digital resources, making it one of the largest and most diverse academic library systems in the world.[106]
The main library in the SU library system is Green Library, which also contains various meeting and conference rooms, study spaces, and reading rooms. Meyer Library, a 24-hour library slated for demolition in 2015, holds various student-accessible media resources and houses one of the largest East Asia collections, whose 540,000 volumes are being transported to an interim location while a new library is rebuilt.[107]
In 2011, The Times Higher Education World University Rankings ranked Stanford 1st in the world for both humanities and social sciences, remarking, "Stanford University knocks Harvard University off the top spot in the arts and humanities subject rankings. With Pulitzer prizewinners and MacArthur Fellows leading its liberal-arts programme, the relative newcomer (founded in 1891) has proved more than a match for its illustrious Ivy League rival."[116] THES ranked Stanford 2nd best research university in the world in 2011.[117] In 2010, the Times also ranked Stanford 3rd in engineering and technology, 3rd in life sciences, 5th in physical sciences, 2nd in arts & humanities, 1st in social sciences, and 2nd in clinical, pre-clinical and health sciences; no other university places in the top 5 across all broad disciplines studied.[118]
The Academic Ranking of World Universities (ARWU) ranked Stanford 2nd in the world in 2011.[119] ARWU ranked Stanford 6th in Natural Sciences and Mathematics, 2nd in Engineering/Technology and Computer Sciences, 6th in Life and Agriculture Sciences, 13th in Clinical Medicine and Pharmacy, and 6th in Social Sciences worldwide. In its subject rankings, ARWU placed Stanford 4th in mathematics, 6th in physics, 3rd in chemistry, 1st in computer science, and 6th in economics and business.
The U.S. News and World Report (USNWR) ranks it fifth among large universities for its undergraduate program in 2012.[120] In the 2012 U.S. News graduate school rankings, Stanford also placed in the top 5 for every discipline in which it was ranked, except industrial engineering, where it placed 6th, and fine arts, where it placed 36th. Specifically, Stanford was ranked 1st in Business, 4th in Education, 2nd in Engineering, 4th in Medicine, 2nd in Law, 1st in Biological Sciences, 4th in Chemistry, 1st in Computer Science, 4th in Earth Sciences, 2nd in Mathematics, 1st in Physics, 1st in Statistics, 5th in Economics, 2nd in English, 1st in History, 1st in Political Science, 1st in Psychology, 5th in Sociology. Within engineering, Stanford placed 1st in aerospace, computer, electrical, environmental, and mechanical engineering; 3rd in civil engineering; and 5th in chemical, material, and bioengineering.[121]
The 2011 QS World University Rankings placed Stanford 7th in arts & humanities, 2nd in engineering & technology, 3rd in social sciences, 6th in natural sciences, 4th in life sciences, and 11th overall.[122] In specific disciplines, Stanford was ranked 8th in English (5th in the United States), 11th in modern languages (7), 10th in history (8), 8th in philosophy (4), 7th in geography & area studies (4), 5th in linguistics (3), 2nd in computer science (2), 2nd in civil & structural engineering (2), 5th in chemical engineering (3), 2nd in electrical engineering (2), 4th in mechanical, aeronautical, & manufacturing engineering, 5th in medicine (3), 5th in biological sciences (3), 5th in chemistry (4), 6th in physics and astronomy (4), 5th in metallurgy (4), 4th in mathematics (3), 6th in environmental sciences (4), 8th in earth and marine sciences (6), 3rd in psychology (2), 6th in sociology (4), 1st in statistics, 7th in politics and international studies (4), 5th in law (3), 3rd in economics (3), and 5th in account and finance.[123]
In 2011 Stanford was ranked second in the world according to the Human Resources & Labor Review.[124] Stanford places fourth among national universities by The Washington Monthly,[125] second among "global universities" by Newsweek,[126] and tied for 1st with MIT and Columbia University in the first tier among national universities by the Center for Measuring University Performance.[127] In the MINE ParisTech rankings in 2008 measuring the number of Chief Executive Officers among the Fortune Global 500, Stanford is ranked third in the world.[128][129] According to Forbes, Stanford has produced the second highest number of billionaires of all universities.[130]
Among professional schools, the Stanford Graduate School of Business is ranked 1st, Stanford Law School is ranked 2nd, the Stanford School of Education is ranked 4th, and Stanford Medical School is ranked 4th, according to U.S. News and World Report. Forbes ranked the business school at the top in its 2009 "Best Business Schools" list.[131] In the 2010 QS Global 200 Business Schools Report[132] Stanford placed 4th in North America.
From a 2010 poll done by The Princeton Review, Stanford is the most commonly named "dream college", both for students and for parents, a title it has held in previous years.[133] A 2003 Gallup poll, which asked about the best colleges in the U.S., found that Stanford is the second-most prestigious university (behind Harvard) in the eyes of the general American public and roughly equal in prestige to Harvard among college-educated people.[134]
Stanford ranks #6 among Research Universities by Salary Potential in the United States.[135]
The Center for World University Rankings ranked Stanford third in the world and third nationally in its 2012 CWUR World University Rankings.
Stanford University is home to the Cantor Center for Visual Arts museum with 24 galleries, sculpture gardens, terraces, and a courtyard first established in 1891 by Jane and Leland Stanford as a memorial to their only child. Notably, the Center possesses the largest collection of Rodin works outside of Paris, France. The Thomas Welton Stanford Gallery, built in 1917, serves as a teaching resource for the Department of Art & Art History as well as an exhibition venue. There are also a large number of outdoor art installations throughout the campus, primarily sculptures, but some murals as well. The Papua New Guinea Sculpture Garden near Roble Hall features handmade wood carvings and "totem poles."
Stanford has a thriving artistic and musical community. Extracurricular activities include theater groups such as Ram's Head Theatrical Society and the Stanford Shakespeare Society, award-winning a cappella music groups such as the Mendicants, Counterpoint, the Stanford Fleet Street Singers, Harmonics, Mixed Company, Testimony, Talisman, Everyday People, Raagapella, and a group dedicated to performing the works of Gilbert and Sullivan, the Stanford Savoyards. Beyond these, the music department sponsors many ensembles including five choirs, the Stanford Symphony Orchestra, Stanford Taiko, and the Stanford Wind Ensemble.
Stanford's dance community is one of the most vibrant in the country, with an active dance division in the Drama Department and over 30 different dance-related student groups, including the Stanford Band's Dollie dance troupe.
Perhaps most distinctive of all is its social and vintage dance community, cultivated by dance historian Richard Powers and enjoyed by hundreds of students and thousands of alumni. Stanford hosts monthly informal dances (called Jammix) and large quarterly dance events, including Ragtime Ball (fall), the Stanford Viennese Ball (winter), and Big Dance (spring). Stanford also boasts a student-run swing performance troupe called Swingtime and several alumni performance groups, including Decadance and the Academy of Danse Libre.
The creative writing program brings young writers to campus via the Stegner Fellowships and other graduate scholarship programs. This Boy's Life author Tobias Wolff teaches writing to undergraduates and graduate students. Knight Journalism Fellows are invited to spend a year at the campus taking seminars and courses of their choice. There is also an extracurricular writing and performance group called the Stanford Spoken Word Collective, which also serves as the school's poetry slam team.
Stanford also hosts various publishing courses for professionals. Stanford Professional Publishing Course, which has been offered on campus since the late 1970s, brings together international publishing professionals to discuss changing business models in magazine and book publishing.

Stanford enrolled 6,927 undergraduate[139] and 8,786 graduate students[137] in the 20112012 year. Women comprised 48% of undergraduates and 37% of professional and graduate students.[95] The freshman retention rate for 2010 was 98%, the four-year graduation rate is 78.4%, and the six-year rate is 95%.[95] The relatively low four-year graduation rate is a function of the university's coterminal degree (or "coterm") program, which allows students to earn a Master's degree as an extension of their undergraduate program.[140]
Stanford awarded 1,671 undergraduate degrees, 2,068 Master's degrees, 708 doctoral degrees, and 270 professional degrees in 2010.[95] The most popular Bachelor's degrees were in the social sciences, interdisciplinary studies, and engineering.
For the class of 2016, Stanford received 36,631 applications and accepted 2427 or 6.6%, the lowest in the university's history and the second lowest in the country.[141]
The cost of attendance in 20102011 is $54,947.[139] Stanford's admission process is need-blind for US citizens and permanent residents; while it is not need-blind for international students, 64% are on need-based aid, with an average aid package of $31,411.[95] In 2010, the university awarded $117 million in financial aid to 3,530 students, with an average aid package of $40,593.[95] total external and internal aid (including jobs and optional loans) amounted to $172.3 million to undergraduate students.[139] 80% of students are on some form of financial aid.[139] Stanford's no-loan policy waives tuition, room, and board for families with incomes below $60,000, and families with incomes below $100,000 are not required to pay tuition (those with incomes up to $150,000 will have tuition significantly reduced).[95][142] 17% of students receive Pell Grants,[139] a common measure of low-income students at a college. 15% of the undergraduates are first-generation students.[143]
Eighty-nine percent of undergraduate students live in on-campus university housing. First-year students are required to live on campus, and all undergraduates are guaranteed housing for all four undergraduate years.[95][144] According to the Stanford Housing Assignments Office, undergraduates live in 80 different houses, including dormitories, co-ops, row houses, fraternities and sororities.[145] At Manzanita Park, 118 mobile homes were installed as "temporary" housing from 1969 to 1991, but it is now the site of modern dorms Castano, Kimball, and Lantana.[146] Most student residences are located just outside the campus core, within ten minutes (on foot or bike) of most classrooms and libraries. Some are for freshmen only; others give priority to sophomores, others to both freshmen and sophomores; some are for upperclass students only, and some are open to all four classes. Most residences are co-ed; seven are all-male fraternities, three are all-female sororities, and there is also one all-female non-sorority house, Roth House. In most residences, men and women live on the same floor, but a few dorms are configured for men and women to live on separate floors (single-gender floors), including all Wilbur dorms except for Arroyo and Okada.[147] Beginning in 200910, the University's housing plan anticipates that all freshmen desiring to live in all-freshman dorms will be accommodated. In the 200910 year, almost two-thirds of freshmen will be housed in Stern and Wilbur Halls. The one-third who requested four-class housing will be located in other dormitories throughout campus, including Florence Moore (FloMo).[148] In April 2008, Stanford unveiled a new pilot plan to test out gender-neutral housing in five campus residences, allowing males and females to live in the same room. This was after concerted student pressure, as well as the institution of similar policies at peer institutions such as Wesleyan, Oberlin, Clark, Dartmouth, Brown, and UPenn.[149]
Several residences are considered theme houses. The Academic, Language and Culture Houses include EAST (Education And Society Theme), Hammarskjld (International Theme), Haus Mitteleuropa (Central European Theme), La Casa Italiana (Italian Language and Culture), La Maison Franaise (French Language and Culture House), Slavianskii Dom (Slavic/East European Theme House), Storey (Human Biology Theme House), and Yost (Spanish Language and Culture).Cross-Cultural Theme Houses include Casa Zapata (Chicano/Latino Theme in Stern Hall), Muwekma-tah-ruk (American Indian/Alaska Native, and Native Hawaiian Theme), Okada (Asian-American Theme in Wilbur Hall), and Ujamaa (Black/African-American Theme in Lagunita Court). Focus Houses include Freshman-Sophomore College (Freshman Focus), Branner Hall (Community Service), Kimball (Arts & Performing Arts), Crothers (Global Citizenship), and Toyon (Sophomore Priority).[150] Theme houses predating the current "theme" classification system are Columbae (Social Change Through Nonviolence, since 1970),[151] and Synergy (Exploring Alternatives, since 1972).[152]
Another famous style of housing at Stanford is the co-ops. These houses feature cooperative living, where residents and eating associates each contribute work to keep the house running, such as cooking meals or cleaning shared spaces. The co-ops on campus are Chi Theta Chi, Columbae, Enchanted Broccoli Forest (EBF), Hammarskjld (which is also the International Theme House), Kairos, Terra, and Synergy.[153]
At any time, around 50 percent of the graduate population lives on campus. Now that construction has concluded on the new Munger graduate residence, this percentage has probably increased. First-year graduate students are guaranteed housing.
Former campus traditions include the Big Game bonfire on Lake Lagunita (a seasonal lake usually dry in the fall), which is now inactive because of the presence of endangered salamanders in the lake bed.
Fraternities and sororities have been active on the Stanford campus since 1891, when the University first opened. In 1944, University President Donald Tresidder banned all Stanford sororities due to extreme competition.[162] However, following Title IX, the Board of Trustees lifted the 33-year ban on sororities in 1977.[163] Stanford is now home to 29 Greek organizations, including 13 sororities and 16 fraternities, representing 13% of undergraduates. In contrast to many universities, nine of the ten housed Greek organizations live in University-owned houses, the exception being Sigma Chi, which owns its own house (but not the land) on The Row. Six chapters are members of the African American Fraternal and Sororal Association, 11 chapters are members of the Interfraternity Council, 6 chapters belong to the Intersorority Council, and 6 chapters belong to the Multicultural Greek Council.[164]
Stanford offers its students the opportunity to engage in over 650 groups.[166] Groups are often, though not always, partially funded by the University via allocations directed by the student government organization, the ASSU. These funds include "special fees", which are decided by a Spring Quarter vote by the student body. Groups span from Athletic/Recreational, Careers/Pre-professional, Community Service, Ethnic/Cultural, Fraternities/Sororities, Health/Counseling, Media/Publications, Music/Dance/Creative Arts, Political/Social Awareness to Religious/Philosophical.
Groups include (but are not limited to):
Stanford participates in the NCAA's Division I FBS and is a member of the Pacific-12 Conference. It also participates in the Mountain Pacific Sports Federation for indoor track (men and women), fencing (men and women), water polo (men and women), gymnastics (men and women), women's lacrosse, and men's volleyball. The women's field hockey team is part of the NorPac Conference.[180] Stanford's traditional sports rival is the University of California, Berkeley, its neighbor to the north in the East Bay.
Stanford has had at least one NCAA team champion every year since the 1976-77 school year[181] and has earned 103 NCAA national team titles[182] since its establishment, second most behind the University of California, Los Angeles, and 467 individual National championships, the most by any university.[183] Stanford has won the award for the top-ranked collegiate athletic programthe NACDA Director's Cup, formerly known as the Sears Cupevery year for the past seventeen years.
Stanford offers 36 varsity sports (18 female, 15 male, one coed), 19 club sports and 37 intramural sportsabout 800 students participate in intercollegiate sports. The university offers about 300 athletic scholarships.
The winner of the annual "Big Game" between the Cal and Stanford football teams gains custody of the Stanford Axe. The first "Big Game", played at Haight Street Park in San Francisco on March 19, 1892, established football on the west coast. Stanford won 14 to 10 in front of 8 thousand spectators. Stanford's football team played in the first Rose Bowl in 1902. However, the violence of the sport at the time, coupled with the post-game rioting of drunken spectators, led San Francisco to bar further "Big Games" in the city in 1905. In 1906, David Starr Jordan banned football from Stanford. The 19061914 "Big Game" contests featured rugby instead of football. Stanford football was resumed in 1919.[184] Stanford won back-to-back Rose Bowls in 1971 and 1972. Stanford has played in 12 Rose Bowls, most recently in 2000. Stanford's Jim Plunkett won the Heisman Trophy in 1970.
Club sports, while not officially a part of Stanford athletics, are numerous at Stanford. Sports include archery, badminton, cheerleading, cricket, cycling, equestrian, hurling, ice hockey, judo, kayaking, men's lacrosse, polo, racquetball, rugby union, squash, skiing, taekwondo, tennis, triathlon and Ultimate. The men's Ultimate team won national championships in 1984 and 2002,[185] the women's Ultimate team in 1997, 1998, 1999, 2003, 2005, 2006, and 2007,[186] the women's rugby team in 1999, 2005, 2006 and 2008. The cycling team won the 2007 Division I USA Cycling Collegiate Road National Championships.
Until 1930, Stanford did not have a "mascot" name for its athletic teams. In that year, the athletic department adopted the name "Indians." In 1972, "Indians" was dropped after a complaint of racial insensitivity was lodged by Native American students.
The Stanford sports teams are now officially referred to as the Stanford Cardinal, referring to the deep red color, not the cardinal bird. Stanford is unusual in that it has only had one official color, Cardinal red, not two colors like most colleges.[187] Since the 19th century. The Band's mascot, "The Tree", has become associated with the school in general. Part of the Leland Stanford Junior University Marching Band (LSJUMB), the tree symbol derives from the El Palo Alto redwood tree on the Stanford and City of Palo Alto seals.
Stanford hosts an annual U.S. Open Series tennis tournament, the Bank of the West Classic, at Taube Stadium. Cobb Track, Angell Field, and Avery Stadium Pool are considered world-class athletic facilities. Stanford Stadium hosted Super Bowl XIX on January 20, 1985, in which the San Francisco 49ers defeated the Miami Dolphins by a score of 3816, and several group stage matches in the 1994 FIFA World Cup.
According to the Stanford Daily, "Stanford has been represented in every summer Olympiad since 1908."[188] As of 2004, Stanford athletes had won 182 Olympic medals at the summer games; "in fact, in every Olympiad since 1912, Stanford athletes have won at least one and as many as 17 gold medals."[189] Stanford athletes won 16 medals at the 2012 Summer Games12 gold, 2 silver and 2 bronze.[190]
As of late 2012, Stanford has 1,995 tenure-line faculty, senior fellows, center fellows, and medical center faculty.[191]
Professors who have served in government include Former Secretary of State Condoleezza Rice, Former Secretary of Defense William Perry, Former US Ambassador to Afghanistan Lt. General Karl Eikenberry, current US Ambassador to Russia Michael McFaul, Former Chair of the Council of Economic Advisors Edward Lazear and Former director of policy planning for the US State Dept. Stephen D. Krasner. George Schultz, Former Secretary of State, Secretary of Labor and Secretary of the Treasury, is a fellow at the Hoover Institution and lectures at the Stanford Graduate School of Business. Former President of Peru Alejandro Toledo was a distinguished lecturer from 2007-2009.[192] Siegfried Hecker, director emeritus of Los Alamos National Laboratory, makes frequent visits to North Korea to inspect their nuclear weapons facilities, and co-teaches a class on national security with William Perry. Tenzin Tethong, former prime minister of the Central Tibetan Administration, chairs the university's Tibetan Studies Initiative, and was a candidate for Prime Minister of the Tibetan Government in Exile.[193] Former US President Benjamin Harrison was a founding professor at Stanford Law School.
The Freeman Spogli Institute for International Studies is also home to political theorist Francis Fukuyama, and founding editor of the Journal of Democracy and advisor to the Coalition Provisional Authority in Iraq, Larry Diamond.
Professor Philip Zimbardo is a leading social psychologist, and oversaw the Stanford Prison Experiment, and psychologist Lewis Terman developed the Stanford-Binet IQ Test. Albert Bandura conducted the famed Bobo Doll Experiment, contributing significantly to social learning theory. Tobias Wolff, best known for his memoir This Boy's Life, is a member of the creative writing faculty. Philosophy Professor Joshua Cohen is a widely cited scholar in political science, philosophy, and ethics. History Professor Jack N. Rakove won the pullitzer prize for his book on the history of the constitution, the subject of a course he teaches at Stanford.
In 2012, it was announced that Alexander Nemerov, prominent art historian and chair of the History of Art Department at Yale University, would join the Stanford faculty as part of the University's efforts to increase its presence in the arts.[194]
The economics department and the Hoover Institution have also been home to more than nine Nobel Prize winners in economics, including Kenneth Arrow, Milton Friedman and Gary Becker. Chair of the economics department Jonathan Levin won the 2011 John Bates Clark Medal, awarded to the leading economist under 40. Economist John B. Taylor served as the Under Secretary of the Treasury for International affairs, and developed the Taylor Rule. Professor Caroline Hoxby is a leading education economist and directs of the Economics of Education Program for the National Bureau of Economic Research. She is married to fellow Rhodes Scholar and Stanford English Professor Blair Hoxby.
Stanford alumni have started many companies, with Forbes magazine remarking, "it is almost impossible to name a leading-edge company in Silicon Valley that isn't closely associated with Stanford."[195][196] Companies founded by Stanford alumni include Hewlett-Packard (William Hewlett and David Packard), Cisco Systems (Sandra Lerner and Leonard Bosack), Nvidia (Jen-Hsun Huang), SGI, VMware, MIPS Technologies, Yahoo! (Chih-Yuan Yang and David Filo), Google (Sergey Brin and Lawrence Page), Wipro Technologies (Azim Premji), Nike (Phil Knight), Gap (Doris F. Fisher), Logitech, Instagram, and Sun Microsystems (Vinod Khosla). The Sun in Sun Microsystems originally stood for "Stanford University Network."[197][198][199] Other companies and organizations founded or co-founded by Stanford alumni include the Special Olympics, Tesla Motors, LinkedIn, Netflix, Varian Associates, Pandora Radio, Electronic Arts, Trader Joe's, Dolby Laboratories, Charles Schwab, Capital One, Renren (the Chinese version of Facebook), TechCrunch, IDEO, Kiva.org, Acumen Fund, Victoria's Secret, Firefox, Match.com, Participant Media and PayPal.
Former Japanese Prime Ministers Yukio Hatoyama and Taro Aso,[200] former U.S. President Herbert Hoover, former U.S. Secretary of State Warren Christopher, former Israeli Prime Minister Ehud Barak, former Peruvian President Alejandro Toledo, former President of Guatemala Jorge Serrano Elias, current President of the Maldives Mohammed Waheed Hassan, former Vice President of Iran Mohammad-Reza Aref, former Honduras President Ricardo Maduro, and Prince Philippe, Duke of Brabant, the Crown Prince of Belgium, are alumni. Former Ghanaian President John Atta Mills earned his J.D. as a Fulbright Scholar at Stanford Law School.[201] U.S. Supreme Court Justices Anthony Kennedy and Stephen Breyer and former Justices Sandra Day O'Connor and William Rehnquist are also alumni.
Other alumni in politics include UN Ambassador Susan Rice, former Secretary of Defense and current Stanford professor William Perry, former US Ambassador to Afghanistan Karl Eikenberry, Former US Ambassador to Mexico Carlos Pascual, Eileen Donahoe, United States Ambassador to the United Nations Human Rights Council, William Kennard, U.S. Ambassador to the European Union, Michael McFaul, US Ambassador to Russia, Newark Mayor Cory Booker, and current US Senators Dianne Feinstein, Max Baucus, Jeff Bingaman, Jeff Merkley and Ron Wyden, and Representatives Xavier Becerra, Judy Biggert, Zoe Lofgren, Adam Schiff, Jim Sensenbrenner, and David Wu. Chelsea Clinton attended Stanford while her father was President, and met her future husband while attending.[202][203]
NBA guard Landry Fields, NFL quarterbacks Frankie Albert, John Brodie, Jim Plunkett, Trent Edwards, John Elway and Andrew Luck, NFL receivers Gordon Banks, Ed McCaffrey and Doug Baldwin, NFL Fullback Jon Ritchie, runner Ryan Hall, MLB starting pitcher Mike Mussina, MLB left-fielder Carlos Quentin, MLB infielder Jed Lowrie, Grand Slam winning tennis players John McEnroe (did not graduate) (singles and doubles) and (doubles) Bob and Mike Bryan, professional golfers Tom Watson and Tiger Woods (did not graduate), New Zealand Football and Queens Park Rangers Defender Ryan Nelsen, Olympic swimmers Jenny Thompson, Summer Sanders and Pablo Morales, Olympic figure skater Debi Thomas, Olympic gymnast Amy Chow, Olympic water polo players Tony Azevedo and Brenda Villa, Olympic softball player Jessica Mendoza, Olympic volleyball player Kerri Walsh, Olympic volleyball player Logan Tom, and Heisman finalist Toby Gerhart are alumni.
In the field of entertainment, Jennifer Connelly, Sigourney Weaver, Ted Koppel, Ben Savage, and Rachel Maddow are prominent graduates. Other alumni include Jay Roach, director of the Austin Powers and Meet the Parents films and Game Change, among others. Alexander Payne wrote and directed such films as Sideways, The Descendants, I Now Pronounce you Chuck and Larry, and About Schmidt. David Chase, a seven-time Emmy Award winner, is best known as the creator and writer of The Sopranos.[204]
John Steinbeck, author of Of Mice and Men and The Grapes of Wrath, attended Stanford for five years but did not receive a degree. Ken Kesey studied creative writing at Stanford, and began the manuscript of One Flew Over the Cuckoo's Nest while attending. Larry McMurtry, author of Lonesome Dove, studied for two years at Stanford on the Stegner Fellowship. Michael Cunningham is best known as author of The Hours, while Jeffrey Eugenides wrote Middlesex and The Virgin Suicides. N. Scott Momaday is widely credited as a leader in bringing Native American fiction into mainstream American literature. US Poet Laureates Robert Pinsky and Robert Hass were classmates while attaining their PhDs at Stanford, and another Poet Laureate, Philip Levine, studied poetry at Stanford. Other prominent novelists and poets have been recipients of the renowned Stegner Fellowship in fiction and poetry.
Former Harvard University president Derek Bok, the president of Yale University, Rick Levin, and the provost of Yale Peter Salovey earned a bachelor's degree at Stanford, while the presidents of MIT and Caltech, Rafael Reif and Jean-Lou Chameau, respectively, earned Ph.D.s there. Alan M. Garber, the current Provost of Harvard, earned his M.D. from Stanford Medical School. Other University presidents include William Brody, former president of Johns Hopkins, Vartan Gregorian, former president of Brown University, and William P. Leahy, president of Boston College. At least eight Stanford alumni have won the Nobel prize.[205]
1 History

1.1 Origins

1.1.1 Coeducation
1.1.2 Early finances


1.2 20th century

1.2.1 Football
1.2.2 Hoover Institution


1.3 Post-1945

1.3.1 Biology
1.3.2 High tech
1.3.3 Physics


1.4 Recent history


1.1 Origins

1.1.1 Coeducation
1.1.2 Early finances


1.1.1 Coeducation
1.1.2 Early finances
1.2 20th century

1.2.1 Football
1.2.2 Hoover Institution


1.2.1 Football
1.2.2 Hoover Institution
1.3 Post-1945

1.3.1 Biology
1.3.2 High tech
1.3.3 Physics


1.3.1 Biology
1.3.2 High tech
1.3.3 Physics
1.4 Recent history
2 Campus

2.1 History of campus development
2.2 Landmarks
2.3 Faculty residences
2.4 Non-main campus


2.1 History of campus development
2.2 Landmarks
2.3 Faculty residences
2.4 Non-main campus
3 Administration and organization

3.1 Endowment and fundraising


3.1 Endowment and fundraising
4 Academics

4.1 Faculty and staff
4.2 Student
4.3 Research centers and institutes
4.4 Libraries and digital resources
4.5 Rankings
4.6 Arts


4.1 Faculty and staff
4.2 Student
4.3 Research centers and institutes
4.4 Libraries and digital resources
4.5 Rankings
4.6 Arts
5 Student life

5.1 Student body
5.2 Dormitories and student housing
5.3 Traditions
5.4 Greek life
5.5 Student groups


5.1 Student body
5.2 Dormitories and student housing
5.3 Traditions
5.4 Greek life
5.5 Student groups
6 Athletics
7 Notable people

7.1 Notable faculty

7.1.1 Government and politics
7.1.2 Humanities and social sciences


7.2 Notable alumni


7.1 Notable faculty

7.1.1 Government and politics
7.1.2 Humanities and social sciences


7.1.1 Government and politics
7.1.2 Humanities and social sciences
7.2 Notable alumni
8 Notes
9 References
10 Further reading
11 External links
1.1 Origins

1.1.1 Coeducation
1.1.2 Early finances


1.1.1 Coeducation
1.1.2 Early finances
1.2 20th century

1.2.1 Football
1.2.2 Hoover Institution


1.2.1 Football
1.2.2 Hoover Institution
1.3 Post-1945

1.3.1 Biology
1.3.2 High tech
1.3.3 Physics


1.3.1 Biology
1.3.2 High tech
1.3.3 Physics
1.4 Recent history
1.1.1 Coeducation
1.1.2 Early finances
1.2.1 Football
1.2.2 Hoover Institution
1.3.1 Biology
1.3.2 High tech
1.3.3 Physics
2.1 History of campus development
2.2 Landmarks
2.3 Faculty residences
2.4 Non-main campus
3.1 Endowment and fundraising
4.1 Faculty and staff
4.2 Student
4.3 Research centers and institutes
4.4 Libraries and digital resources
4.5 Rankings
4.6 Arts
5.1 Student body
5.2 Dormitories and student housing
5.3 Traditions
5.4 Greek life
5.5 Student groups
7.1 Notable faculty

7.1.1 Government and politics
7.1.2 Humanities and social sciences


7.1.1 Government and politics
7.1.2 Humanities and social sciences
7.2 Notable alumni
7.1.1 Government and politics
7.1.2 Humanities and social sciences
To establish and maintain at such University an educational system, which will, if followed, fit the graduate for some useful pursuit, and to this end to cause the pupils, as easily as may be, to declare the particular calling, which, in life, they may desire to pursue; ...
To prohibit sectarian instruction, but to have taught in the University the immortality of the soul, the existence of an all-wise and benevolent Creator, and that obedience to His laws is the highest duty of man.
To have taught in the University the right and advantages of association and co-operation.
To afford equal facilities and give equal advantages in the University to both sexes.
To maintain on the Palo Alto estate a farm for instruction in agriculture in all its branches."






Stanford Memorial Church









Lou Henry and Herbert Hoover House









Hoover Tower









The Dish



Jasper Ridge Biological Preserve is a 1,200-acre (490ha) natural reserve owned by the university and used by wildlife biologists for research, located south of the main campus.
SLAC National Accelerator Laboratory is a facility located west of main campus and originally owned by Stanford but now operated by the university for the Department of Energy. It contains the longest linear particle accelerator in the world, 2 miles (3.2km) on 426 acres (172ha) of land.[67]
Hopkins Marine Station, located in Pacific Grove, California, is a marine biology research center owned by the university since 1892.
Study abroad locations: unlike typical study abroad programs, Stanford itself operates in locations around the globe; thus, each location, which ranges from Beijing to Cape Town, has Stanford faculty-in-residence and staff in addition to students, creating a "mini Stanford."[68]
Redwood City: in 2005, the university purchased a small, 35-acre (14ha) campus in Midpoint Technology Park intended for staff offices, although it remains undeveloped.[69]
China: the university is currently building a small campus for researchers and students in collaboration with Peking University.[70]
19 Nobel Prize laureates;[4]
150 members of the National Academy of Sciences;[4]
94 members of National Academy of Engineering;[4][98]
69 members of Institute of Medicine;[99]
263 members of the American Academy of Arts and Sciences;[4]
18 recipients of the National Medal of Science;[4]
2 recipients of the National Medal of Technology;[4]
31 members of the National Academy of Education;[4]
51 members of American Philosophical Society;[4]
56 fellows of the American Physics Society (since 1995);[100]
4 Pulitzer Prize winners;[4]
24 MacArthur Fellows;[4]
7 Wolf Foundation Prize winners;[4]
6 Koret Foundation Prize winners;[4]
2 ACL Lifetime Achievement Award winners;[101]
14 AAAI fellows;[102]
3 Presidential Medal of Freedom winners.[4][103]
Uncommon Man/Uncommon Woman: Stanford does not award honorary degrees,[154][155] but in 1953 the University created the degree of Uncommon Man/Uncommon Woman for individuals who give rare and extraordinary service to the University. The University's highest honor, the degree is not given at prescribed intervals, but only when appropriate to recognize extraordinary service. Recipients include Herbert Hoover, Bill Hewlett, Dave Packard, Lucile Packard, and John Gardner.[156]
Big Game events: The events in the week leading up to the Big Game vs. UC Berkeley, including Gaieties (a musical written, composed, produced, and performed by the students of Ram's Head Theatrical Society), The Bearial (in which the Stanford Band performs a funeral-like procession and pierces a stuffed-animal bear on the tip of the Stanford Claw fountain), and an hourly train whistle that counts down the hours until Big Game, orchestrated by the Stanford Axe Committee.[citation needed]
Viennese Ball: a formal ball with waltzes that was initially started in the 1970s by students returning from the now-closed Stanford in Vienna overseas program.[157] It is now open to all students.
Mausoleum Party: An annual Halloween Party at the Stanford Mausoleum, which contains the corpses of Leland Stanford, Jr. and his parents. A 20-year tradition, the Mausoleum party was on hiatus from 2002 to 2005[158] due to a lack of funding from the alumni,[159] but was revived in 2006. In 2008, it was hosted in Old Union rather than at the actual Mausoleum, because rain prohibited generators from being rented.[160] In 2009, after fundraising efforts by the Junior Class Presidents and the ASSU Executive, the event was able to return to the Mausoleum despite facing budget cuts earlier in the year.[161]
The Game: The Game is a treasure hunt put on by dorm staff usually in the spring and summer quarters.[citation needed]
Stanford is home to three unhoused historically NPHC (National Pan-Hellenic Council or "Divine Nine") three sororities (Alpha Kappa Alpha, Delta Sigma Theta, and Sigma Gamma Rho) and three unhoused NPHC fraternities (Alpha Phi Alpha, Kappa Alpha Psi, and Phi Beta Sigma). These fraternities and sororities operate under the AAFSA (African American Fraternal Sororal Association) at Stanford.
Seven historically NPC (National Panhellenic Conference) sororities, four of which are unhoused (Alpha Phi, Alpha Epsilon Phi, Chi Omega, and Kappa Kappa Gamma) and three of which are housed (Delta Delta Delta, Kappa Alpha Theta, and Pi Beta Phi) call Stanford home. These sororities operate under the Stanford Inter-sorority Council (ISC).
Eleven historically NIC (National Interfraternity Conference) fraternities are also represented at Stanford, including four unhoused fraternities (Alpha Epsilon Pi, Delta Kappa Epsilon, Delta Tau Delta, and Sigma Phi Epsilon), and seven housed fraternities (Kappa Alpha Order, Kappa Sigma, Phi Kappa Psi, Sigma Alpha Epsilon, Sigma Chi, Sigma Nu, and Theta Delta Chi). These fraternities operate under the Stanford Inter-fraternity Council (IFC).
There are also four unhoused MGC (Multicultural Greek Council) sororities on campus (Alpha Kappa Delta Phi, Lambda Theta Nu, Sigma Psi Zeta, and Sigma Theta Psi), as well as two unhoused MGC fraternities (Gamma Zeta Alpha and Lambda Phi Epsilon). Lambda Phi Epsilon is recognized by the National Interfraternity Conference (NIC).[165]
The Stanford Daily is an independent organization located on campus, and is the daily newspaper serving Stanford University. It has been published since the University was founded in 1892.
The Stanford Axe Committee is the official guardian of the Stanford Axe and the rest of the time assists the Stanford Band as a supplementary spirit group.
SUpost.com is an online marketplace for Stanford students and alumni, in partnership with Stanford Student Enterprises (SSE). [167]
The Stanford Pre-Business Association[168] is the largest business-focused undergraduate organization. It plays an instrumental role in establishing an active link between the industry, alumni, and student communities.
The Stanford solar car project, where students build a solar-powered car every 2 years and race it in either the North American Solar Challenge or the World Solar Challenge.
Stanford Astronomical Society organizes viewings of meteor showers, lunar eclipses, and other astronomical events.
The Stanford Kite Flying Society[169] (founded 2008), a group of undergraduates dedicated to flying kites. Society "meetings" are usually on Wilbur Field when it is windy out.
The Pilipino American Student Union (PASU),[170] a culture-oriented community service and social activism group. Also integral to PASU is a traditional performing arts arm called Kayumanggi.
Stanford Finance is a pre-professional organization aimed at mentoring students who want to enter a career in finance, through mentors and internships.
Leland Quarterly[171] is Stanford's literary magazine. It publishes the creative writing, essays, and art of Stanford students on a "Stanford quarterly" basis (autumn, winter, and spring) and also runs features of arts and literary figures and events on campus.
KZSU Stanford 90.1 FM is the student-run radio station which features freeform music programming, sports commentary, and news segments.
BASES, the Business Association of Stanford Entrepreneurial Students is one of the largest professional organizations in Silicon Valley, with over 5,000 members. Its goal is to support the next generation of entrepreneurs.
The Stanford Robber Barons are Stanford's only sketch comedy group, and perform original material for free every quarter on campus. They regularly host events, and have performed at the Laugh Factory and at the SF SketchFest.
A cappella groups include the Mendicants (all male),[172] the Stanford Fleet Street Singers (all male),[173] Counterpoint (all female),[174] Stanford Raagapella[175] and Mixed Company (coed).[176] Because Fleet Street maintains Stanford songs as a regular part of its performing repertoire, Stanford University used the group as ambassadors during the University's centennial celebration and commissioned an album, entitled Up Toward Mountains Higher (1999), of Stanford songs which were sent to alumni around the world.
The Stanford Martial Arts Program (SMAP) is an umbrella organization for 11 different martial arts groups on campus: Aikido, Capoeira, Eskrima, Judo, Jujitsu, Kenpo Karate, Muay Thai, Wing Chun, JKA Shotokan, Taekwondo, and Wushu.
The Stanford Women In Business (SWIB)[177] is an on-campus business organization consisting of over a board of 40 and 100 active members. Each year, SWIB organizes over 25 events and workshops, hosts a winter and spring conference, and provides mentorship and spring quarter internships.
The Claw Magazine,[178] named after the fountain in White Plaza, is an undergraduate publication that features humor, artwork, investigative reporting, and fiction.
Free Stanford forum,[179] Everything about life at Stanford University. Campus, careers, education, visa, faq and more. Interesting insights at Stanford University.
Lee Altenberg, Beyond Capitalism: Leland Stanford's Forgotten Vision (Stanford Historical Society, 1990)
Ronald N. Bracewell, Trees of Stanford and Environs (Stanford Historical Society, 2005)
Ken Fenyo, The Stanford Daily 100 Years of Headlines (2003-10-01) ISBN 0-9743654-0-8
Jean Fetter, Questions and Admissions: Reflections on 100,000 Admissions Decisions at Stanford (1997-07-01) ISBN 0-8047-3158-6
Ricard Joncas, David Neumann, and Paul V. Turner. Stanford University. The Campus Guide. Princeton Architectural Press, 2006. Available online.
Stuart W. Leslie, The Cold War and American Science: The Military-Industrial-Academic Complex at MIT and Stanford, Columbia University Press 1994
Rebecca S. Lowen, R. S. Lowen, Creating the Cold War University: The Transformation of Stanford, University of California Press 1997
Official website
Official athletics website
"Leland Stanford Jr. University". Encyclopdia Britannica (11th ed.). 1911.
.
#(`*Cello*`)#.
The cello (/tlo/ CHEL-oh; plural cellos or celli) is a bowed string instrument with four strings tuned in perfect fifths. It is a member of the violin family of musical instruments, which also includes the violin and viola.
A person who plays a cello is called a cellist. The cello is used as a solo instrument, in chamber music, in a string orchestra, and as a member of the string section of an orchestra. It is the second largest bowed string instrument in the modern symphony orchestra, the double bass being the largest.
Cellos were derived from other mid- to large-sized bowed instruments in the 16th century, such as the viola da gamba, and the generally smaller and squarer viola da braccio, and such instruments made by members of the Amati family of luthiers. The invention of wire-wrapped strings in Bologna gave the cello greater versatility. By the 18th century, the cello had largely replaced other mid-sized bowed instruments.
The name cello is an abbreviation of the Italian violoncello,[1] which means "little violone", referring to the violone ("big viol"), the lowest-pitched instrument of the viol family, the group of string instruments that went out of fashion around the end of the 17th century in most countries except France, where they survived another half-century or so before the louder violin family came into greater favour in that country too. In modern symphonies, it is the second largest stringed instrument after the bass. Thus, the name cello carries both an augmentative "-one" ("big") and a diminutive "-cello" ("little"). By the turn of the 20th century, it had grown customary to abbreviate the name violoncello to 'cello, with the apostrophe indicating the six missing prefix letters.[2] It is now customary to use the name "cello" without the apostrophe and as a full designation.[2] The word derives ultimately from vitula, meaning a stringed instrument.
Cellos are tuned in fifths, starting with A3, followed by D3, G2, and then C2 (two octaves below middle C) as the lowest string. It is tuned in the same intervals as the viola, but an octave lower. Unlike the violin or viola but similar to the double bass, the cello has an endpin resting on the floor in order to support its heavy weight.
The cello is most closely associated with European classical music, and has been described as the closest sounding instrument to the male human voice.[3] The instrument is a part of the standard orchestra and is the bass voice of the string quartet, as well as being part of many other chamber groups. A large number of concertos and sonatas have been written for the cello.
Among the most well-known Baroque works for the cello are Johann Sebastian Bach's six unaccompanied Suites. The Prelude from the First Suite is particularly famous. From the Classical era, the two concertos by Joseph Haydn in C major and D major stand out, as do the five sonatas for cello and pianoforte of Ludwig van Beethoven, which span the important three periods of his compositional evolution. Romantic era repertoire includes the Robert Schumann Concerto, the Antonn Dvok Concerto as well as the two sonatas and the Double Concerto by Johannes Brahms. Compositions from the early 20th century include Edward Elgar's Cello Concerto in E minor, Claude Debussy's Sonata for Cello and Piano and unaccompanied cello sonatas by Zoltn Kodly and Paul Hindemith. The cello's versatility made it popular with composers in the mid- to late 20th century such as Sergei Prokofiev, Dmitri Shostakovich, Benjamin Britten, Gyrgy Ligeti, Witold Lutoslawski and Henri Dutilleux, encouraged by soloists who specialized in contemporary music (such as Siegfried Palm and Mstislav Rostropovich) commissioning from and collaborating with composers.
Today the instrument is less common in popular music, but was commonly used in 1970's pop and disco music.[citation needed] Today it is still sometimes featured in pop and rock recordings, examples of which are noted later in this article. The cello has also recently appeared in major hip-hop and R & B performances, such as singers Rihanna and Ne-Yo's performance at the American Music Awards.[citation needed] The instrument has also been modified for Indian classical music by Nancy Lesh and Saskia Rao-de Haas.[4]
The history of bowed string musical instruments in Europe dates back to the 9th century with the lira (Greek: , Latin: lr), the bowed instrument of the Byzantine Empire, equivalent to the rabb of the Islamic Empires. The Persian geographer Ibn Khurradadhbih (d. 911) of the 9th century, in his lexicographical discussion of instruments, cited the Byzantine lira as a typical instrument of the Byzantines along with the urghun (organ), shilyani (probably a type of harp or lyre) and the salandj.[5] The Byzantine lira spread through Europe westward and in the 11th and 12th centuries European writers use the terms fiddle and lira interchangeably when referring to bowed instruments (Encyclopdia Britannica. 2009). In the meantime the Arab rabb was introduced to Western Europe possibly through the Iberian Peninsula and both bowed instruments spread widely throughout Europe giving birth to various European bowed instruments.
Over the centuries that followed, Europe continued to have two distinct types of bowed instruments: the first type was held with the left arm like a modern violin and was known by the Italian term lira da braccio (or viola da braccio, meaning viol for the arm); the other type, with sloping shoulders and held between the knees like a modern cello, was known by the Italian term lira da gamba (or viola da gamba, meaning viol for the leg), and included the Byzantine lyra.[6] During the Renaissance, the gambas were important and elegant instruments; they eventually lost ground to the louder (and originally less aristocratic)[citation needed] lira da braccio.
The violoncello da spalla (sometimes "violoncello piccolo da spalla" or "violoncello da span") was the first cello referred to in print (by Jambe de Fer in 1556).[2] "Violone" means a larger "viola" (viol), while "-cello" in Italian is a diminutive and spalla means "shoulder" in Italian so that violoncello da spalla suggest a "little big violin" that may be held on the shoulder so that the player could perform while walking or that the early, short-necked instrument was hung across the shoulder by a strap.[2]
Monteverdi referred to the instrument as "basso de viola da braccio" in Orfeo (1607). Although the first bass violin, possibly invented as early as 1538, was most likely inspired by the viol, it was created to be used in consorts with the violin. The bass violin was actually often referred to as a "violone," or "large viola," as were the viols of the same period. Instruments that share features with both the bass violin and the viola da gamba appear in Italian art of the early 16th century.
The invention of wire-wound strings (fine wire around a thin gut core), around 1660 in Bologna, allowed for a finer bass sound than was possible with purely gut strings on such a short body. Bolognese makers exploited this new technology to create the cello, a somewhat smaller instrument suitable for solo repertoire due to both the timbre of the instrument and the fact that the smaller size made it easier to play virtuosic passages. This instrument had disadvantages as well, however. The cello's light sound was not as suitable for church and ensemble playing, so it had to be doubled by basses or violones.
Around 1700, Italian players popularized the cello in northern Europe, although the bass violin (basse de violon) continued to be used for another two decades in France.[citation needed] Many existing bass violins were literally cut down in size to convert them into cellos according to the smaller pattern developed by Stradivarius, who also made a number of old pattern large cellos (the 'Servais').[7] The sizes, names, and tunings of the cello varied widely by geography and time.[7] The size was not standardized until around 1750.
Despite similarities to the viola da gamba, the cello is actually part of the viola da braccio family, meaning "viol of the arm," which includes, among others, the violin and viola. Though paintings like Bruegel's "The Rustic Wedding" and de Fer in his Epitome Musical suggest that the bass violin had alternate playing positions, these were short-lived and the more practical and ergonomic a gamba position eventually replaced them entirely.
Baroque era cellos differed from the modern instrument in several ways. The neck has a different form and angle, which matches the baroque bass-bar and stringing. Modern cellos have an endpin at the bottom to support the instrument (and transmit some of the sound through the floor), while Baroque cellos are held only by the calves of the player. Modern bows curve in and are held at the frog; Baroque bows curve out and are held closer to the bow's point of balance. Modern strings normally have a metal core, although some use a synthetic core; Baroque strings are made of gut, with the G and C strings wire-wound. Modern cellos often have fine-tuners connecting the strings to the tailpiece, which make it much easier to tune the instrument, but such pins are rendered ineffective by the flexibility of the gut strings used on Baroque cellos. Overall, the modern instrument has much higher string tension than the Baroque cello, resulting in a louder, more projecting tone, with fewer overtones.
No educational works specifically devoted to the cello existed before the 18th century, and those that do exist contain little value to the performer beyond simple accounts of instrumental technique. The earliest cello manual is Michel Corrette's Mthode, thorique et pratique pour apprendre en peu de temps le violoncelle dans sa perfection (Paris, 1741).
Cellos are part of the standard symphony orchestra, which usually includes eight to twelve players. The cello section, in standard orchestral seating, is located on stage left (the audience's right) in the front, opposite the first violin section. However, some orchestras and conductors prefer switching the positioning of the viola and cello sections. The principal cellist is the section leader, determining bowings for the section in conjunction with other string principals, and playing solos. Principal players always sit closest to the audience.
The cellos are a critical part of orchestral music; all symphonic works involve the cello section, and many pieces require cello soli or solos. Much of the time, cellos provide part of the harmony for the orchestra. On many occasions, the cello section will play the melody for a brief period of time, before returning to the harmony. There are also cello concertos, which are orchestral pieces in which a featured, solo cellist is accompanied by an entire orchestra.
There are numerous cello concertos - where a solo cello is accompanied by an orchestra - notably 25 by Vivaldi, 12 by Boccherini, at least 3 by Haydn, 3 by C.P.E. Bach, 2 by Saint-Sans, 2 by Dvok, and one each by Schumann, Lalo, and Elgar. Beethoven's Triple Concerto for Cello, Violin and Piano and Brahms' Double Concerto for Cello and Violin are also part of the concertante repertoire although in both cases the cello shares solo duties with at least one other instrument. Moreover, several composers wrote large-scale pieces for cello and orchestra, which are concertos in all but name. Some familiar "concertos" are Richard Strauss' tone poem Don Quixote, Tchaikovsky's Variations on a Rococo Theme, Bloch's Schelomo and Bruch's Kol Nidrei.
In the 20th century, the cello repertoire grew immensely. This was partly due to the influence of virtuoso cellist Mstislav Rostropovich who inspired, commissioned and/or premiered dozens of new works. Among these, Prokofiev's Symphonia Concertante, Britten's Cello Symphony and the concertos of Shostakovich, Lutosawski and Dutilleux have already become part of the standard repertoire. Other major composers who wrote concertante works for him include Messiaen, Berio and Penderecki. In addition, Arnold, Barber, Glass, Hindemith, Honegger, Ligeti, Myaskovsky, Penderecki, Rodrigo, Villa-Lobos and Walton also wrote major concertos for other cellists, notably for Gaspar Cassad, Gregor Piatigorsky, Siegfried Palm and Julian Lloyd Webber.
There are also many sonatas for cello and piano. Those written by Beethoven, Mendelssohn, Chopin, Brahms, Grieg, Rachmaninoff, Debussy, Faur, Shostakovich, Prokofiev, Poulenc, Carter, and Britten are the most famous. Other important pieces for cello and piano include Schumann's five Stcke im Volkston and transcriptions like Schubert's Arpeggione Sonata (originally for arpeggione and piano), Stravinsky's Suite italienne (transcribed by the composer from his ballet Pulcinella) and Bartk's first rhapsody (also transcribed by the composer, originally for violin and piano)
Finally, there are several pieces for cello solo, most importantly J.S. Bach's six Suites for Cello (arguably the most important cello pieces), Kodly's Sonata for Solo Cello and Britten's three Cello Suites. Other notable examples include Hindemith's and Ysae's Sonatas for Solo Cello, Dutilleux's Trois Strophes sur le Nom de Sacher, Berio's Les Mots Sont Alls (both part of a series of twelve compositions for solo cello commissioned by Rostropovich for Swiss conductor Paul Sacher's 70th birthday), Cassad's Suite for Solo Cello, Ligeti's Solo Sonata, Carter's two Figments and Xenakis' Nomos Alpha and Kottos.
The cello is a member of the traditional string quartet as well as string quintets, sextet or trios and other mixed ensembles. There are also pieces written for two, three, four or more cellos; this type of ensemble is also called a "cello choir" and its sound is familiar from the introduction to Rossini's William Tell Overture as well as Zaccharias' prayer scene in Verdi's Nabucco. As a self-sufficient ensemble, its most famous repertoire is Villa-Lobos' first of his Bachianas Brasileiras for cello ensemble (the fifth is for soprano and 8 cellos). Other examples are Offenbach's cello duets, quartet, and sextet, Prt's Fratres for 8 cellos and Boulez' Messagesquisse for 7 cellos, or even Villa-Lobos' rarely played Fantasia Concertante (1958) for 32 cellos. The 12 cellists of the Berlin Philharmonic Orchestra (or "the Twelve" as they have since taken to being called) specialize in this repertoire and have commissioned many works, including arrangements of well-known popular songs.
Though the cello is less common in popular music than in classical music, it is sometimes featured in pop and rock recordings. The cello is rarely part of a group's standard lineup but like its cousin the violin it is becoming more common in mainstream pop (e.g. the baroque rock band Arcade Fire uses the cello in their songs).
In the 1960s, artists such as the Beatles and Cher used the cello in popular music, in songs such as "Bang Bang (My Baby Shot Me Down)", "Eleanor Rigby" and "Strawberry Fields Forever". "Good Vibrations" by the Beach Boys includes the cello in its instrumental ensemble, which includes a number of instruments unusual for this sort of music. Bass guitarist Jack Bruce, who had originally studied music on a performance scholarship for cello, played a prominent cello part in "As You Said" on Cream's Wheels of Fire studio album (1968). In the 1970s, the Electric Light Orchestra enjoyed great commercial success taking inspiration from so-called "Beatlesque" arrangements, adding the cello (and violin) to the standard rock combo line-up and in 1978 the UK based rock band, Colosseum II, collaborated with cellist Julian Lloyd Webber on the recording Variations. Most notably, Pink Floyd included a cello solo in their 1970 epic instrumental "Atom Heart Mother". Bass guitarist Mike Rutherford of Genesis was originally a cellist and included some cello parts in their Foxtrot album.
Established non-traditional cello groups include Apocalyptica, a group of Finnish cellists best known for their versions of Metallica songs, Rasputina, a group of cellists committed to an intricate cello style intermingled with Gothic music, Von Cello, a cello fronted rock power trio, Break of Reality who mix elements of classical music with the more modern rock and metal genre, and Jelloslave ([1]) a Minneapolis based Cello duo with two percussionists. These groups are examples of a style that has become known as cello rock. The crossover string quartet bond also includes a cellist. Silenzium and Cellissimo Quartet are Russian (Novosibirsk) groups playing rock and metal and having more and more popularity in Siberia. Cold Fairyland from Shanghai, China is using a cello along a Pipa as the main solo instrument to create East meets West progressive (folk) rock.
More recent bands using the cello are Aerosmith, The Auteurs, Nirvana, Oasis, Murder by Death, Cursive, A Genuine Freakshow, Ra Ra Riot, Smashing Pumpkins, James, Talk Talk, and OneRepublic. An Atlanta-based trio, King Richard's Sunday Best, also uses a cellist in their lineup. So-called "chamber pop" artists like Kronos Quartet, The Vitamin String Quartet and Margot and the Nuclear So and So's have also recently made cello common in modern alternative rock. Heavy metal band System of a Down has also made use of the cello's rich sound. The indie rock band The Stiletto Formal are known for using a cello as a major staple of their sound, similarly, the indie rock band Canada employs two cello players in their lineup. The orch-rock group, The Polyphonic Spree, which has pioneered the use of stringed and symphonic instruments, employs the cello in very creative ways for many of their "psychedelic-esque" melodies. The first wave screamo band I Would Set Myself On Fire For You featured a cello as well as a viola to create a more folk-oriented sound. The band, Panic! At the Disco uses a cello in their song, "Build God, Then We'll Talk." The lead vocalist of the band, Brendon Urie, also did the recording of the cello solo.
In jazz, bassists Oscar Pettiford and Harry Babasin were among the first to use the cello as a solo instrument; both tuned their instrument in fourths, an octave above the double bass. Fred Katz (who was not a bassist) was one of the first notable jazz cellists to use the instrument's standard tuning and arco technique. Contemporary jazz cellists include Abdul Wadud, Diedre Murray, Ron Carter, Dave Holland, David Darling, Lucio Amanti, Akua Dixon, Ernst Reijseger, Fred Lonberg-Holm, Tom Cora, Vincent Courtois, John O'Keefe, Stephan Braun, Jean-Charles Capon, Erik Friedlander, and James Hinkley of jazz combo Billet-Deux.
Modern musical theatre pieces like Jason Robert Brown's The Last Five Years, Duncan Sheik's Spring Awakening, Adam Guettel's Floyd Collins, and Ricky Ian Gordon's My Life with Albertine use small string ensembles (including solo cellos) to a prominent extent.
In Indian Classical music Saskia Rao-de Haas is a well established soloist as well as playing duets with her sitarist husband Pt. Shubhendra Rao. Other cellists performing Indian classical music are: Nancy Lesh ( Dhrupad) and Anup Biswas. Both Rao and Lesh play the cello sitting cross-legged on the floor.
The cello can also be used in bluegrass and folk music, with notable players including Ben Sollee of the Sparrow Quartet and the "Cajun cellist" Sean Grissom as well as Damien Rice. Lindsay Mac is becoming well known for playing the cello like a guitar, with her cover of The Beatles' "Blackbird" a big hit on The Bob & Tom Show.
The cello is typically made from wood, although other materials such as carbon fiber or aluminum may be used. A traditional cello has a spruce top, with maple for the back, sides, and neck. Other woods, such as poplar or willow, are sometimes used for the back and sides. Less expensive cellos frequently have tops and backs made of laminated wood.
The top and back are traditionally hand-carved, though less expensive cellos are often machine-produced. The sides, or ribs, are made by heating the wood and bending it around forms. The cello body has a wide top bout, narrow middle formed by two C-bouts, and wide bottom bout, with the bridge and F holes just below the middle.
The top and back of the cello has decorative border inlay known as purfling. While purfling is attractive, it is also functional: if the instrument is struck, the purfling can prevent cracking of the wood. A crack may form at the rim of the instrument, but will spread no further. Without purfling, cracks can spread up or down the top or back. Playing, traveling and the weather all affect the cello and can increase a crack if purfling is not in place. Less expensive instruments typically have painted purfling.
Cello manufacturer Luis & Clark constructs cellos from carbon fibre. Carbon fibre instruments are particularly suitable for outdoor playing because of the strength of the material and its resistance to humidity and temperature fluctuations. Luis & Clark has produced over 1000 such cellos, some of which are owned by cellists such as Yo-Yo Ma[8] and Josephine van Lier.[9]
In the late 1920s and early 1930s, the Aluminum Company of America (Alcoa) as well as German luthier G.A. Pfretzschner produced an unknown number of aluminum cellos (in addition to aluminum double basses and violins). An advertisement published in N.Y. Music Service catalogue (1930) reads: "...made entirely of aluminum with the exception of the fingerboard. They have many advantages over the wood basses and violoncellos, as they cannot crack, split or warp and are made to last forever ... possessing a tone quality that is deep, resonant and responsive to the utmost degree. Violoncello $150." Only a handful of aluminum cellos exist today including a Pfretzschner played by modern classical cellist Frances-Marie Uitti, another played by bluegrass cellist Stan Young.
Above the main body is the carved neck, which leads to a pegbox and the scroll. The neck, pegbox, and scroll are normally carved out of a single piece of wood. Attached to the neck and extending over the body of the instrument is the fingerboard. The nut is a raised piece of wood, where the fingerboard meets the pegbox, which the strings rest on. The pegbox houses four tuning pegs, one for each string. The pegs are used to tune the cello by either tightening or loosening the string. The scroll is a traditional part of the cello and all other members of the violin family. Ebony is usually used for the tuning pegs, fingerboard, and nut, but other hard woods, such as boxwood or rosewood, can be used.
Strings on a cello have cores made out of gut (sheep or goat), metal, or synthetic materials, such as Perlon. Most modern strings used today are also wound with metallic materials like aluminum, titanium and chromium. Cellists may mix different types of strings on their instruments. The pitches of the open strings are C, G, D, and A (black note heads in the playing range figure above), unless alternative tuning (scordatura) is used.
The tailpiece and endpin are found in the lower part of the cello. The tailpiece is traditionally made of ebony or another hard wood, but can also be made of plastic or steel. It attaches the strings to the lower end of the cello, and can have one or more fine tuners. The endpin or spike is made of wood, metal or rigid carbon fibre and supports the cello in playing position. In the Baroque period the cello was held between the calves. Around the 1830s, the Belgian cellist Auguste Adrien Servais introduced the endpin and propagated its use. Modern endpins are retractable and adjustable; older ones were removed when not in use. (The word "endpin" sometimes also refers to the button of wood located at this place in all instruments in the violin family, but this is usually called "tailpin".[10]) The sharp tip of the cello's endpin is sometimes capped with a rubber tip that protects the tip from dulling and prevents the cello from slipping on the floor.
The bridge holds the strings above the cello and transfers their vibrations to the top of the instrument and the soundpost inside (see below). The bridge is not glued, but rather held in place by the tension of the strings. The f-holes, named for their shape, are located on either side of the bridge, and allow air to move in and out of the instrument as part of the sound-production process. The f-holes also act as access points to the interior of the cello for repairs or maintenance. Sometimes a small hose containing a water-soaked sponge, called a Dampit, is inserted through the f-holes, and serves as a humidifier.
Internally, the cello has two important features: a bass bar, which is glued to the underside of the top of the instrument, and a round wooden sound post, which is wedged between the top and bottom plates. The bass bar, found under the bass foot of the bridge, serves to support the cello's top and distribute the vibrations. The sound post, found under the treble side of the bridge, connects the back and front of the cello. Like the bridge, the sound post is not glued, but is kept in place by the tensions of the bridge and strings. Together, the bass bar and sound post transfer the strings' vibrations to the top (front) of the instrument (and to a lesser extent the back), acting as a diaphragm to produce the instrument's sound.
Cellos are constructed and repaired using hide glue, which is strong but reversible, allowing for disassembly when needed. Tops may be glued on with diluted glue, since some repairs call for the removal of the top. Theoretically, hide glue is weaker than the body's wood, so as the top or back shrinks side-to-side, the glue holding it will let go, avoiding a crack in the plate.
Traditionally, bows are made from pernambuco or brazilwood. Both come from the same species of tree (Caesalpina echinata), but pernambuco, used for higher-quality bows, is the heartwood of the tree and is darker in color than brazilwood (which is sometimes stained to compensate). Pernambuco is a heavy, resinous wood with great elasticity, which makes it an ideal wood for instrument bows.
Bows are also made from other materials, such as carbon-fibrestronger than woodand fiberglass (often used to make inexpensive, low-quality student bows). An average cello bow is 73cm long (shorter than a violin or viola bow) 3cm high (from the frog to the stick) and 1.5cm wide. The frog of a cello bow typically has a rounded corner like that of a viola bow, but is wider. A cello bow is roughly 10grams heavier than a viola bow, which in turn is roughly 10grams heavier than a violin bow.
Bow hair is traditionally horsehair, though synthetic hair, in varying colors, is also used. Prior to playing, the musician tightens the bow by turning a screw to pull the frog (the part of the bow under the hand) back, and increase the tension of the hair. Rosin is applied by the player to make the hairs sticky. Bows need to be re-haired periodically.
Baroque style (16001750) cello bows were much thicker and were formed with a larger outward arch when compared to modern cello bows. The inward arch of a modern cello bow produces greater tension, which in turn gives off a louder sound.
The cello bow has also been used to play guitars. Jimmy Page pioneered its application on tracks such as "Dazed and Confused." The post-rock Icelandic band Sigur Rs' lead singer often plays a guitar using a cello bow.
When a string is bowed or plucked, it vibrates and moves the air around it, producing sound waves. Because the string is quite thin, not much air is moved, and consequently the sound is weak. In acoustic stringed instruments such as the cello, this lack of volume is solved by mounting the vibrating string on a larger body. The vibrations are transmitted to the larger body, which can move more air and produce a louder sound. Different designs of the instrument produces variations in the instruments vibrational patterns and thus changes the character of the sound produced.[11]
A strings fundamental pitch can be adjusted by changing its stiffness, which depends on tension and length. Tightening a string stiffens it by increasing both the outward forces along its length and the net forces it experiences during a distortion.[12] A cello can be tuned by adjusting the tension of its strings, by turning the tuning pegs mounted on its pegbox, and tension adjusters (fine tuners) on the tail piece.
A strings length also affects its fundamental pitch. Shortening a string stiffens it by increasing its curvature during a distortion and subjecting it to larger net forces. Shortening the string also reduces its mass. Since a stiffer string with a smaller mass vibrates faster, shortening a string increases the pitch. Because of this effect, you can raise and change the pitch of a string by pressing it against the fingerboard in the cellos neck and effectively shortening it.[12]
When a string is bowed or plucked to produce a note, the fundamental note is accompanied by higher frequency overtones. Each sound has a particular recipe of frequencies that combine to make the total sound.[13]
For the cello, the main wood resonance generally appears very close to the note F#2, often with serious consequences. When the cellist plays the note F#2, the main wood resonance vibrates at its frequency as the cello sounds the frequency of the note F#2. A loud beating sound results between these nearby frequencies; this is known as the wolf tone because it is an unpleasant growling sound. The wood resonance appears to be split into two frequencies by the driving force of the sounding string. These two periodic resonances beat with each other. This wolf tone must be eliminated or significantly reduced for the cello to play the nearby notes with a pleasant tone. This can be accomplished by modifying the cello front plate, attaching a wolf eliminator, or moving the sound post.[14]
A vibrating string subdivides itself into many parts vibrating at the same time. Each part produces a pitch of its own, called a partial. A vibrating string has one fundamental and a series of partials. The most pure combination of two pitches is when one is double the frequency of the other.[14]
For a repeating wave, the velocity, v, equals the wavelength, , times the frequency, f. v = f On a cello string, waves reflect from both ends. The superposition of reflecting waves results in a standing wave pattern, but only for wavelengths  = 2L, L, L/2,  = 2L/n, where L is the length of the string and n is a positive whole number. Therefore the only frequencies produced on a single string are f = nv/(2L). Timbre is largely determined by the amount of each of these harmonics in the sound. Different instruments have different harmonic content for the same pitch. A real string vibrates at harmonics that are not perfect multiples of the fundamental. This results in a little in-harmonicity, which gives richness to the tone and covers up slight de-tunings of different notes in a chord.[15] These considerations represent an idealized situation, which pertains to a freely vibrating ideal string with fixed endpoints. The pressure of the bow on the string while it vibrates can cause a noticeable departure from this ideal situation.
Playing the cello is done while seated with the instrument supported on the floor. The left hand fingertips stop the strings on the fingerboard determining the pitch of the fingered note. The right hand plucks or bows the strings to sound the notes.
Standard-sized cellos are referred to as "full-size". However, cellos come in smaller (fractional) sizes, from "7/8" and "3/4" down to "1/16" sized cellos (e.g. 7/8, 3/4, 1/2, 1/4, 1/8, 1/10, 1/16). The smaller-sized cellos are identical to standard cellos in construction, range, and usage, but are simply 'scaled-down' for the benefit of children and shorter adults. Note that a "half-size" cello is half the volume of a full-size, not half the length (i.e., a 'half-length' cello would have ()3 the volume and correspond to 1/8-size).[citation needed] A 1/10-size cello, for example, which is meant to be used by small children, is only slightly larger than a violin (and as such can be played by an adult player like one), but about twice as thick, and the C string tends to be quite slack due to the difficulty for such a small string to produce a sound that low. Many smaller cellists prefer to play a "7/8" cello as the hand stretches in the lower positions are less demanding. Although rare, cellos in sizes larger than 4/4 do exist. Cellists with unusually large hands may play a slightly larger than full-sized cello. Cellos made before approximately 1700 tended to be considerably larger than those made and commonly played today.
Around 1680, string-making technology made lower pitches on shorter strings possible. The cellos of Stradivari, for example, can be clearly divided into two models, the style made before 1702 characterized by larger instruments (of which only three exist in their original size and configuration), and the style made during and after 1702, when Stradivari, presumably in response to the "new" strings, began making smaller cellos. This later model is the one most commonly used by modern luthiers.
There are many accessories for the cello.
Cellos are made by luthiers, specialists in building and repairing stringed instruments, ranging from guitars to violins. The following luthiers are notable for the cellos they have produced:
A person who plays the cello is called a cellist. For a list of notable cellists, see the list of cellists and Category:Cellists.
Specific instruments are, or become, famous, for a variety of reasons. An instrument's notability may arise from its age, the fame of its maker, its physical appearance, its acoustic properties, and its use by notable performers. The most famous instruments are generally known for all of these things. The most highly prized instruments are now collector's items, and are priced beyond the reach of most musicians. These instruments are typically owned by some kind of organization or investment group, which loans the instrument to a performer. (For example, the Davidov Stradivarius, which is currently in the possession of one of the most widely known living cellists, Yo-Yo Ma, is actually owned by the Vuitton Foundation.[17])
Some notable cellos:
  
Violin family (violin, viola)
Viol family (includes double bass)
1 Etymology
2 Description
3 History
4 Current use

4.1 Orchestral
4.2 Solo
4.3 Quartets and other ensembles
4.4 Popular music, jazz, world music and neoclassical


4.1 Orchestral
4.2 Solo
4.3 Quartets and other ensembles
4.4 Popular music, jazz, world music and neoclassical
5 Construction

5.1 Alternative materials
5.2 Neck, pegbox, and scroll
5.3 Strings
5.4 Tailpiece and endpin
5.5 Bridge and f-holes
5.6 Internal features
5.7 Glue
5.8 Bow


5.1 Alternative materials
5.2 Neck, pegbox, and scroll
5.3 Strings
5.4 Tailpiece and endpin
5.5 Bridge and f-holes
5.6 Internal features
5.7 Glue
5.8 Bow
6 Physics

6.1 Physical aspects
6.2 Subjective aspects
6.3 Harmonics


6.1 Physical aspects
6.2 Subjective aspects
6.3 Harmonics
7 Playing technique
8 Sizes
9 Accessories
10 Instrument makers
11 Cellists
12 Famous cellos
13 Cello Organizations
14 Media
15 See also
16 Notes
17 References
18 Further reading
19 External links
4.1 Orchestral
4.2 Solo
4.3 Quartets and other ensembles
4.4 Popular music, jazz, world music and neoclassical
5.1 Alternative materials
5.2 Neck, pegbox, and scroll
5.3 Strings
5.4 Tailpiece and endpin
5.5 Bridge and f-holes
5.6 Internal features
5.7 Glue
5.8 Bow
6.1 Physical aspects
6.2 Subjective aspects
6.3 Harmonics
Cases are used to protect the cello and bow (or multiple bows) when traveling and for safe storage. They are often made of carbon fiber, fiber-glass, and less commonly wood.
Rosin, made from conifer resin, is applied to the bow hairs to increase the effectiveness of the friction, grip or bite, and allow proper sound production. Rosin may have additives to modify the friction such as beeswax, gold, silver or tin. Commonly, rosins are classified as either Dark or Light. Dark rosins increase the friction more than Light rosins.
Endpin stops or straps (tradenames include Rockstop and Black Hole) keep the cello from sliding if the end pin does not have a rubber piece on the end (used on wood floors) though in many cases a rubber piece will not suffice on even a wood floor. Many Cellists often use a square or rectangle of carpet that can be secured under the front two legs of the chair as an endpin stop. This is however less likely to be seen in a professional arena and more used in rehearsal or in private.
Wolf tone eliminators are often placed on cello strings between the tailpiece and the bridge to eliminate acoustic anomalies known as wolf tones or "wolfs".
Mutes are used to change the sound of the cello by reducing overtones. Practice mutes (made of metal) significantly reduce the instrument's volume (they are also referred to as "hotel mutes"). The most common mute is a rubber disc with two holes to fit the two middle strings. It sits just after the bridge and has a flap that can be placed over the top of the bridge to mute the vibrations travelling down it to the sound post inside the cello. These are especially used due to their simplicity and can be taken off or put on very quickly because they can be stored on the strings past the bridge.
Metronomes provide a steady tempo by sounding out a certain number of beats per minute. They are adjustable to fit the tempo of the piece. Many models can also produce a tuning pitch of A4 (440Hz), among others. These can, of course, be used for all instruments.
Humidifiers are used to control and stabilize the humidity around and inside the cello and are popular with traveling cellists. Often, these are placed inside the cello itself or inside the case. Some players will not use humidifiers inside their cellos because they have the potential to drip, which may cause damage to the cello.
Electronic tuners are sometimes used to tune the instrument. A tuner indicates if a played note is sharp or flat.
Nicol Amati and others in the Amati family
William Forster
Nicol Gagliano
Matteo Goffriller
Giovanni Battista Guadagnini
Giuseppe Guarneri
Domenico Montagnana
Giovanni Battista Rogeri
Francesco Ruggieri
Stefano Scarampella
Antonio Stradivari
David Tecchler
Carlo Giuseppe Testore
Jean Baptiste Vuillaume
the "King", by Andrea Amati, is one of the oldest known cellos, built between 1538 and 1560. It is in the collection of the National Music Museum in South Dakota.[18]
Servais Stradivarius is in the collection of the Smithsonian Institution, Washington DC
Davidov Stradivarius, played by Jacqueline du Pr, currently played by Yo-Yo Ma
Barjansky Stradivarius, played by Julian Lloyd Webber
Bonjour Stradivarius, played by Soo Bae
Paganini-Ladenburg Stradivarius, played by Clive Greensmith of the Tokyo String Quartet
Duport Stradivarius, until recently played by the late Mstislav Rostropovich
Piatti Stradivarius, 1720, played by Carlos Prieto
CelloBello
International Cello Society
World Cello Congress
Category:Composers for cello
Brahms guitar
Cello Rock
Double Concerto for Violin and Cello
Electric cello
List of compositions for cello and orchestra
List of compositions for cello and organ
List of compositions for cello and piano
List of solo cello pieces
String Instrument Repertoire
Triple concerto for violin, cello, and piano
tgardon, a percussive Hungarian folk instrument similar in construction to the Cello
Stephen Bonta. "Violoncello", Grove Music Online, ed. L. Macy (accessed January 28, 2006), grovemusic.com (subscription access).
Cyr, Mary. "Basses and basse continue in the Orchestra of the Paris Opra 1700-1764". Early Music XVIII (Apr., 1982): 155170.
Grassineau, James (1740). A Musical Dictionary. London: J. Wilcox. "VIOLONCELLO of the Italians, is properly what we call the Bass Violin with four strings, sometimes even five or six; but those are not common, the first being most used among us."
Holman, Peter (1982). "The English Royal Violin Consort in the Sixteenth Century". Proceedings of the Royal Musical Association 109: 3959. doi:10.1093/jrma/109.1.39.
Jesselson, Robert. "The Etymology of Violoncello: Implications on Literature in the Early History of the Cello". Strings Magazine No. 22 (JAN/FEB 1991). http://www.cello.org/Newsletter/Articles/celloetymology.htm.
"The King Violoncello by Andrea Amati, Cremona, after 1538". National Music Museum. http://www.usd.edu/smm/Cellos/Amati/Amaticello.html. Retrieved 2008-11-02.
Woodfield, Ian (1984) [1984]. Howard Mayer Brown, Peter le Huray, John Stevens. ed. The Early History of the Viol. Cambridge: Cambridge University Press. ISBN0-521-24292-4.
Marcella Ghigi, "Il violoncello. Conoscere la tecnica per esprimere la musica", Milano, Casa Musicale Sonzogno, 1999. ISBN 88-87318-08-5 with a preface by Mario Brunello.
Machover, Tod, "My Cello" in Turkle, Sherry (editor), Evocative objects: things we think with, Cambridge, Mass.: MIT Press, 2007. ISBN 978-0-262-20168-1
CelloBello - Online Cello Resource Center (Educational)
The Internet Cello Society
Sources for the prescribed sheet music for the ABRSM practical Cello exams
cellist.nl: An international register of professional cellists, teachers, and students.
Cello History: A brief history of the cello
Bow Technique, Nature of the String, Harmonics: Information on bow technique and the characteristics of the string by cellist Georg Mertens
.
#(`*Running*`)#.
Running is a means of terrestrial locomotion allowing humans and other animals to move rapidly on foot. It is simply defined in athletics terms as a gait in which at regular points during the running cycle both feet are off the ground. This is in contrast to walking, where one foot is always in contact with the ground, the legs are kept mostly straight and the center of gravity vaults over the legs in an inverted pendulum fashion.[1] A characteristic feature of a running body from the viewpoint of spring-mass mechanics is that changes in kinetic and potential energy within a stride occur simultaneously, with energy storage accomplished by springy tendons and passive muscle elasticity.[2] The term running can refer to any of a variety of speeds ranging from jogging to sprinting.
The ancestors of mankind developed the ability to run for long distances about four and a half million years ago,[citation needed] probably in order to hunt animals. Competitive running grew out of religious festivals in various areas. Records of competitive racing date back to the Tailteann Games in Ireland in 1829 BCE, while the first recorded Olympic Games took place in 776 BCE.
It is thought that human running evolved at least four and a half million years ago out of the ability of the ape-like Australopithecus, an early ancestor of humans, to walk upright on two legs.[3]
The theory proposed considered to be the most likely evolution of running is of early humans' developing as endurance runners from the practice of persistence hunting of animals, the activity of following and chasing until a prey is too exhausted to flee, succumbing to "chase myopathy" (Sears 2001), and that human features such as the nuchal ligament, abundant sweat glands, the Achilles tendons, big knee joints and muscular glutei maximi, were changes caused by this type of activity (Bramble & Lieberman 2004, et al.).[4][5][6] The theory as first proposed used comparitative physiological evidence and the natural habits of animals when running, indicating the likelihood of this activity as a successful hunting method. Further evidence from observation of modern day hunting practice also indicated this likelihood (Carrier et al. 1984). [6][7] According to Sears (p.12) scientific investigation (Walker & Leakey 1993) of the Nariokotome Skeleton provided further evidence for the Carrier theory.[8]
Competitive running grew out of religious festivals in various areas such as Greece, Egypt, Asia, and the East African Rift in Africa. The Tailteann Games, an Irish sporting festival in honour of the goddess Tailtiu, dates back to 1829 BCE, and is one of the earliest records of competitive running.[9] The origins of the Olympics and Marathon running are shrouded by myth and legend, though the first recorded game took place in 776 BCE.[10]
Humans leap from one leg to the other while running. Each leap raises the center of gravity during take-off and lowers it on landing as the knee bends to absorb the shock. At mid arc, both feet are momentarily off the ground. This continual rise and fall of bodyweight expends energy opposing gravity and absorbing shock during take-off and landing. Running on a track requires more energy per unit distance than walking to cover the same distance. As reported by Hall et al. males on a track running at a pace of 2.82m/s use 1.41 times as much energy to travel the same distance as walking at a pace of 1.41m/s, with similar values on a treadmill.[11] Therefore, running is less efficient than walking in terms of calories expended per unit distance, though it is faster.
In 2004, scientists at the University of Utah and Harvard University hypothesized that the ability of humans to sustain long-distance endurance running may have been instrumental in the evolution of the human form.[12]
Running is executed as a sequence of strides, which alternate between the two legs. Each leg's stride can be roughly divided into three phases: support, drive, and recovery. Support and drive occur when the foot is in contact with the ground. Recovery occurs when the foot is off the ground. Since only one foot is on the ground at a time in running, one leg is always in recovery, while the other goes through support and drive. Then, briefly, as the runner leaps through the air, both legs are in recovery. These phases are described in detail below.
During the support phase, the foot is in contact with the ground and supports the body against gravity. The body's centre of mass is typically somewhere in the lower abdominal area between the hips. The supporting foot touches the ground slightly ahead of the point that lies directly below the body's centre of mass. The knee joint is at its greatest extension just prior to the support phase. When contact is made with the ground, the knee joint begins to flex, and the extent it flexes varies with running style. Stiff-legged running styles reduce knee flexion, and looser, or more dynamic, running styles increase it. As the supporting leg bends at the knee, the pelvis dips down on the opposite side. These motions absorb shock and are opposed by the coordinated action of several muscles. The pelvic dip is opposed by the tensor fasciae lataeilio-tibial band of the supporting leg, the hip abductor, and the abdominals and lower back muscles. The knee flexion is opposed by the Muscle contraction[clarification needed] eccentric contraction of the quadriceps muscle. The supporting hip continues to extend, and the body's centre of mass passes over the supporting leg. The knee then begins to extend, and the opposite hip rises from its brief dip. The support phase begins to transition into drive.
The support phase quickly transitions into the drive phase. The drive leg extends at the knee joint, and at the hips, such that the toe maintains contact with the ground as that leg trails behind the body. The foot pushes backward and also down, creating a diagonal force vector, which, in an efficient running style, is aimed squarely at the runner's centre of mass. Since the diagonal vector has a vertical component, the drive phase continues to provide some support against gravity and can be regarded as an extension of the support phase. During the drive, the foot may extend also, by a flexing of the soleus and gastrocnemius muscle in the calf. In some running styles, notably long-distance "shuffles" which keep the feet close to the ground, the ankle remains more or less rigid during drive. Because the knee joint straightens, though not completely, much of the power of the drive comes from the quadriceps muscle group, and in some running styles, additional power comes from the calves as they extend the foot for a longer drive. This motion is most exhibited in sprinting.
There has been much discussion about the exact nature of the drive phase, because it has now been shown scientifically that the quadriceps have no activity after the supporting phase; this has become known as the extensor paradox in running.[13] Essentially, the body automatically turns off the quadriceps after the body weight moves forward of its supporting foot. This has led to a hypothesis that there is no driving phase in running, and that the runner's own body weight is providing the propulsion during this time, essentially falling through a gravitational torque created as the general centres of mass of the runner is in a forward position from the supporting foot.
When the driving toe loses contact with the ground, the recovery phase begins. During recovery, the hip flexes, which rapidly drives the knee forward. Much of the motion of the lower leg is driven by the forces transferred from the upper leg rather than by the action of the muscles. As the knee kicks forward, it exerts torque against the lower leg through the knee joint, causing the leg to snap upward. The degree of leg lift can be consciously adjusted by the runner, with additional muscle power. During the last stage of recovery, the hip achieves maximal flexion, and, as the lower leg rapidly unfolds, which it does in a passive way, the knee joint also reaches its greatest, though not full, extension. During this extension of the leg and flexion of the hip, the hamstring and gluteal muscles are required to stretch rapidly. Muscles which are stretched respond by contracting by a reflex action. Recovery ends when the foot comes into contact with the ground, transitioning again into the support phase.
The motions of the upper body are essential to maintaining balance, and a forward motion for optimal running. They compensate for the motions of the lower body, keeping the body in rotational balance. A leg's recovery is matched by a forward drive of the opposite arm, and a leg's support and drive motions are balanced by backward movement of the opposite arm. The shoulders and torso are also involved. Because the leg drive is slower than the kick of recovery, the arm thrusting backward is slower also. The forward arm drive is more forceful and rapid.
The more force exerted by the lower body, the more exaggerated the upper body motions have to be to absorb the momentum. While it is possible to run without movements of the arms, the spine and shoulders will generally still be recruited. Using the arms to absorb the forces aids in maintaining balance at higher speed. Otherwise, optimal force would be hard to attain for fear of falling over.
Most of the energy expended in running goes to the compensating motions, and so considerable gains in running speed as well as economy can be made by eliminating wasteful or incorrect motions. For instance, if the force vector in the drive phase is aimed too far away from the centre of mass of the body, it will transfer an angular momentum to the body which has to be absorbed.
The faster the running, the more energy has to be dissipated through compensating motions throughout the entire body. This is why elite sprinters have powerful upper body physiques. As the competitive distance increases, there is a rapid drop in the upper body and overall muscle mass typically exhibited by the people who compete at a high level in each respective event. Long distance runners typically have lean muscles.
Leaning forward places a runner's center of mass on the front part of the foot, which avoids landing on the heel and facilitates the use of the spring mechanism of the foot. It also makes it easier for the runner to avoid landing the foot in front of the center of mass and the resultant braking effect. While upright posture is essential, a runner should maintain a relaxed frame and use his/her core to keep posture upright and stable. This helps prevent injury as long as the body is neither rigid nor tense. The most common running mistakes are tilting the chin up and scrunching shoulders.[14]
Exercise physiologists have found that the stride rates are extremely consistent across professional runners, between 185 and 200 steps per minute. The main difference between long- and short-distance runners is the length of stride rather than the rate of stride.[15][16]
During running, the speed at which the runner moves may be calculated by multiplying the cadence (steps per second) by the stride length. Running is often measured in terms of pace[17] in minutes per mile or kilometer. Fast stride rates coincide with the rate one pumps one's arms. The faster one's arms move up and down, parallel with the body, the faster the rate of stride. Different types of stride are necessary for different types of running. When sprinting, runners stay on their toes bringing their legs up, using shorter and faster strides. Long distance runners tend to have more relaxed strides that vary.
Because of its high-impact nature, many injuries are associated with running. They include "runner's knee" (pain in the knee), shin splints, pulled muscles (especially the hamstring), twisted ankles, iliotibial band syndrome, plantar fasciitis, Achilles tendinitis, and stress fractures.[citation needed] Repetitive stress on the same tissues without enough time for recovery or running with improper form can lead to many of the above. Runners generally attempt to minimize these injuries by warming up before exercise,[18] focusing on proper running form, performing strength training exercises, eating a well balanced diet, allowing time for recovery, and "icing" (applying ice to sore muscles or taking an ice bath).[citation needed]
Foot blisters are also common among runners. Specialized socks greatly help to prevent blisters.
Another common, running-related injury is chafing, caused by repetitive rubbing of one piece of skin against another, or against an article of clothing. One common location for chafe to occur is the runner's upper thighs. The skin feels coarse and develops a rash-like look. A variety of deodorants and special anti-chafing creams are available to treat such problems. Chafe is also likely to occur on the nipple.
A cold bath is a popular treatment of subacute injuries or inflammation, muscular strains, and overall muscular soreness, but its efficacy is controversial.[19] Some claim that for runners in particular, ice baths offer two distinct improvements over traditional techniques.[who?] First, immersion allows controlled, even constriction around all muscles, effectively closing microscopic damage that cannot be felt and numbing the pain that can. One may step into the tub to relieve sore calves, quads, hams, and connective tissues from hips to toes will gain the same benefits, making hydrotherapy an attractive preventive regimen. Saint Andrew's cross-country coach John O'Connell, a 2:48 masters marathoner, will hit the ice baths before the ibuprofen. "Pain relievers can disguise injury", he warns. "Ice baths treat both injury and soreness." The second advantage involves a physiological reaction provoked by the large amount of muscle submerged. Assuming one has overcome the mind's initial flight response in those first torturous minutes, the body fights back by invoking a "blood rush". This rapid transmission circulation flushes the damage-inflicting waste from the system, while the cold water on the outside preserves contraction. Like an oil change or a fluid dump, the blood rush revitalizes the very areas that demand fresh nutrients.
Some runners may experience injuries when running on concrete surfaces. The problem with running on concrete is that the body adjusts to this flat surface running and some of the muscles will become weaker, along with the added impact of running on a harder surface. Therefore it is advised to change terrain occasionally  such as trail, beach, or grass running. This is more unstable ground and allows the legs to strengthen different muscles. Runners should be wary of twisting their ankles on such terrain. Running downhill also increases knee stress and should therefore be avoided. Reducing the frequency and duration can also prevent injury.
A common acronym used to help the recovery process is RICE: rest, ice, compression, and elevation.
Another injury prevention method common in the running community is stretching. Stretching is often recommended as a requirement to avoid running injuries, and it is almost uniformly performed by competitive runners of any level. Recent medical literature, however, finds mixed effects of stretching prior to running. One study found insufficient evidence to support the claim that stretching prior to running was effective in injury prevention or soreness reduction.[20] Another, however, has demonstrated that stretching prior to running increases injuries, while stretching afterwards actually decreases them.[21] The American College of Sports Medicine recommends that all stretching be done after exercise because this is when the muscles are most warmed up and capable of increasing flexibility. Recent studies have also shown that stretching will reduce the amount of strength the muscle can produce during that training session.
Proper running technique can dramatically lower the risk of running injuries.[14] Engaging the hips, driving the thigh or knee, pushing off with the ankles and not the hamstrings, pawing your legs back, and erect posture are some of the key actions in proper running technique. Running injuries can be from a lack of strength and stride length and pushing off with the hamstrings and not the ankle. The hamstrings and gluteus maximus are not involved in the push off phase of running, contrary to popular belief.[22]
Barefoot running has been promoted as a means of reducing running related injuries[23] though this position on barefoot running remains controversial and a majority of professionals advocate the wearing of appropriate shoes as the best method for avoiding injury.[24]
Recent studies have shown that runners do not have more osteoarthritis than people who do not run.[25]
While there is the potential for injury in running (just as there is in any sport), there are many benefits. Some of these benefits include potential weight loss, improved cardiovascular and respiratory health (reducing the risk of cardiovascular and respiratory diseases), improved cardiovascular fitness, reduced total blood cholesterol, strengthening of bones (and potentially increased bone density), possible strengthening of the immune system and an improved self-esteem and emotional state.[26] Running, like all forms of regular exercise, can effectively slow[27] or reverse[28] the effects of aging.
Running can assist people in losing weight, staying in shape and improving body composition. Running increases your metabolism. Different speeds and distances are appropriate for different individual health and fitness levels. For new runners, it takes time to get into shape. The key is consistency and a slow increase in speed and distance. While running, it is best to pay attention to how one's body feels. If a runner is gasping for breath or feels exhausted while running, it may be beneficial to slow down or try a shorter distance for a few weeks. If a runner feels that the pace or distance is no longer challenging, then the runner may want to speed up or run farther.[citation needed]
Running can also have psychological benefits, as many participants in the sport report feeling an elated, euphoric state, often referred to as a "runner's high".[29] Running is frequently recommended as therapy for people with clinical depression and people coping with addiction.[30] A possible benefit may be the enjoyment of nature and scenery, which also improves psychological well-being[31] (see Ecopsychology#Practical benefits).
In animal models, running has been shown to increase the number of newly born neurons within the brain.[32] This finding could have significant implications in aging as well as learning and memory.
Running is both a competition and a type of training for sports that have running or endurance components. As a sport, it is split into events divided by distance and sometimes includes permutations such as the obstacles in steeplechase and hurdles. Running races are contests to determine which of the competitors is able to run a certain distance in the shortest time. Today, competitive running events make up the core of the sport of athletics. Events are usually grouped into several classes, each requiring substantially different athletic strengths and involving different tactics, training methods, and types of competitors.
Running competitions have probably existed for most of humanity's history and were a key part of the ancient Olympic Games as well as the modern Olympics. The activity of running went through a period of widespread popularity in the United States during the running boom of the 1970s. Over the next two decades, as many as 25 million Americans were doing some form of running or jogging  accounting for roughly one tenth of the population.[33] Today, road racing is a popular sport among non-professional athletes, who included over 7.7million people in America alone in 2002.[34]
Footspeed, or sprint speed, is the maximum speed at which a human can run. It is affected by many factors, varies greatly throughout the population, and is important in athletics and many sports.
The fastest human footspeed on record is 44.72km/h (27.79mph), seen during a 100-meter sprint (average speed between the 60th and the 80th meter) by Usain Bolt.[35]
Track running events are individual or relay events with athletes racing over specified distances on an oval running track. The events are categorised as sprints, middle and long-distance, and hurdling.
Road running takes place on a measured course over an established road (as opposed to track and cross country running). These events normally range from distances of 5 kilometers to longer distances such as half marathons and marathons, and they may involve large numbers of runners or wheelchair entrants.
Cross country running takes place over open or rough terrain. The courses used at these events may include grass, mud, woodlands, hills, flat ground and water. It is a popular participatory sport, and is one of the events which, along with track and field, road running, and racewalking, makes up the umbrella sport of athletics.
Sprints are short running events in athletics and track and field. Races over short distances are among the oldest running competitions. The first 13 editions of the Ancient Olympic Games featured only one event  the stadion race, which was a race from one end of the stadium to the other.[36] There are three sprinting events which are currently held at the Olympics and outdoor World Championships: the 100 metres, 200 metres, and 400 metres. These events have their roots in races of imperial measurements which were later altered to metric: the 100m evolved from the 100 yard dash,[37] the 200m distances came from the furlong (or 1/8 of a mile),[38] and the 400m was the successor to the 440 yard dash or quarter-mile race.[39]
At the professional level, sprinters begin the race by assuming a crouching position in the starting blocks before leaning forward and gradually moving into an upright position as the race progresses and momentum is gained.[40] Athletes remain in the same lane on the running track throughout all sprinting events,[39] with the sole exception of the 400m indoors. Races up to 100m are largely focused upon acceleration to an athlete's maximum speed.[40] All sprints beyond this distance increasingly incorporate an element of endurance.[41] Human physiology dictates that a runner's near-top speed cannot be maintained for more than thirty seconds or so as lactic acid builds up and leg muscles begin to be deprived of oxygen.[39]
The 60 metres is a common indoor event and it an indoor world championship event. Other less-common events include the 50 metres, 55 metres, 300 metres and 500 metres which are used in some high and collegiate competitions in the United States. The 150 metres, though rarely competed, has a star-studded history: Pietro Mennea set a world best in 1983,[42] Olympic champions Michael Johnson and Donovan Bailey went head-to-head over the distance in 1997,[43] and Usain Bolt improved Mennea's record in 2009.[42]
Middle distance running events are track races longer than sprints up to 3000 metres. The standard middle distances are the 800 metres, 1500 metres and mile run, although the 3000 metres may also be classified as a middle distance event.[44] The 880 yard run, or half mile, was the forebear to the 800m distance and it has its roots in competitions in the United Kingdom in the 1830s.[45] The 1500m came about as a result of running three laps of a 500m track, which was commonplace in continental Europe in the 1900s.[46]


1 History
2 Motion

2.1 Lower body motion

2.1.1 Support
2.1.2 Drive
2.1.3 Recovery


2.2 Upper body motion


2.1 Lower body motion

2.1.1 Support
2.1.2 Drive
2.1.3 Recovery


2.1.1 Support
2.1.2 Drive
2.1.3 Recovery
2.2 Upper body motion
3 Elements of good running technique

3.1 Upright posture and a slight forward lean
3.2 Stride rate and types


3.1 Upright posture and a slight forward lean
3.2 Stride rate and types
4 Running injuries
5 Benefits of running
6 Running events

6.1 Limits of speed
6.2 Running speed over increasing distance
6.3 Events by type
6.4 Events by distance


6.1 Limits of speed
6.2 Running speed over increasing distance
6.3 Events by type
6.4 Events by distance
7 See also
8 References
9 External links
2.1 Lower body motion

2.1.1 Support
2.1.2 Drive
2.1.3 Recovery


2.1.1 Support
2.1.2 Drive
2.1.3 Recovery
2.2 Upper body motion
2.1.1 Support
2.1.2 Drive
2.1.3 Recovery
3.1 Upright posture and a slight forward lean
3.2 Stride rate and types
6.1 Limits of speed
6.2 Running speed over increasing distance
6.3 Events by type
6.4 Events by distance
Long distance
Marathon
Ultrarunning
Multiday running
Level and Incline Running
Outline of running
Running at the Open Directory Project
 Media related to Running at Wikimedia Commons
Chisholm, Hugh, ed. (1911). "Running". Encyclopdia Britannica (11th ed.). Cambridge University Press.
.
#(`*The Hunger Games*`)#.
The Hunger Games is a 2008 young adult novel by American writer Suzanne Collins. It is written in the voice of 16-year-old Katniss Everdeen, who lives in the post-apocalyptic nation of Panem, where the countries of North America once existed. The Capitol, a highly advanced metropolis, exercises political control over the rest of the nation. The Hunger Games are an annual event in which one boy and one girl aged1218 from each of the twelve districts surrounding the Capitol are selected by lottery to compete in a televised battle to the death.
The book received mostly positive feedback from major reviewers and authors, including author Stephen King. It was praised for its storyline and character development, though some reviewers have noted similarities between Collins' book and the Japanese novel Battle Royale (1999), as well as other works. In writing The Hunger Games, Collins drew upon Greek mythology and contemporary reality television for thematic content. The novel won many awards, including the California Young Reader Medal, and was named one of Publishers Weekly's "Best Books of the Year" in 2008.
The Hunger Games was first published in hardcover on September 14, 2008 by Scholastic, featuring a cover designed by Tim O'Brien. It has since been released in paperback and also as an audiobook and ebook. After an initial print of 200,000, the book had sold 800,000 copies by February 2010. Since its release, The Hunger Games has been translated into 26 languages, and publishing rights have been sold in 38territories. The novel is the first in The Hunger Games trilogy, followed by Catching Fire (2009) and Mockingjay (2010). A film adaptation, directed by Gary Ross and co-written and co-produced by Collins herself, was released in 2012.
Collins has said that the inspiration for The Hunger Games came from channel surfing on television. On one channel she observed people competing on a reality show and on another she saw footage of the invasion of Iraq. The two "began to blur in this very unsettling way" and the idea for the book was formed.[1] The Greek myth of Theseus served as a major basis for the story, with Collins describing Katniss as a futuristic Theseus, and Roman gladiatorial games provided the framework. The sense of loss that Collins developed through her father's service in the Vietnam War was also an influence on the story, with Katniss having lost her father at age 11, five years before the story begins.[2] Collins stated that the deaths of young characters and other "dark passages" were the most difficult parts of the book to write, but that she had accepted that passages such as these were necessary to the story.[3] She considered the moments where Katniss reflects on happier moments in her past to be more enjoyable.[3]
The Hunger Games takes place in a nation known as Panem, established in North America after the destruction of the continent's civilization by an unknown apocalyptic event. The nation consists of the wealthy Capitol and twelve surrounding, poorer districts united under the Capitol's control. District12, where the book begins, is located in the coal-rich region that was formerly known as Appalachia.[4]
As punishment for a past rebellion against the Capitol, in which a 13th district was destroyed, one boy and one girl between the ages of 12and 18from each district are selected by an annual lottery to participate in the Hunger Games, an event in which the participants (or "tributes") must fight to the death in an outdoor arena controlled by the Capitol, until only one individual remains. The story is narrated by 16-year-old Katniss Everdeen, a girl from District12 who volunteers for the 74th annual Hunger Games in place of her younger sister, Primrose. The male tribute chosen from District12 is Peeta Mellark, a former schoolmate of Katniss who once gave her bread from his family's bakery when her family was starving.
Katniss and Peeta are taken to the Capitol, where their drunken mentor, Haymitch Abernathy, victor of the 50th Hunger Games, instructs them to watch and determine the strengths and weaknesses of the other tributes. "Stylists" are employed to make each tribute look his or her best; Katniss's stylist, Cinna, is the only person at the Capitol with whom she feels a degree of understanding. The tributes are publicly displayed to the Capitol audience in an interview with television host Caesar Flickerman, and have to attempt to appeal to the television audience in order to obtain "sponsors". During this time, Peeta reveals on-air his longtime unrequited love for Katniss. Katniss believes this to be a ploy to gain sponsors, who can be critical to survival because of their ability to send gifts such as food, medicine, and tools to favored tributes during the Games.
While nearly half the tributes are killed in the first day of the Games, Katniss relies on her well-practiced hunting and survival skills to remain unharmed and concealed from the other tributes. A few days into the games, Katniss develops an alliance with Rue, a 12-year-old girl from the agricultural District11 who reminds Katniss of her own sister. In the meantime, Peeta appears to have joined forces with the tributes from the richer districts. However, when he has the opportunity to kill Katniss, he instead saves her from the others. Katniss's alliance with Rue is brought to an abrupt end when Rue is killed by another tribute, whom Katniss then kills with an arrow. Katniss sings to Rue until she dies, and spreads flowers over her body as a sign of respect for Rue and disgust towards the Capitol.
Apparently because of Katniss and Peeta's image in the minds of the audience as "star-crossed lovers", a rule change is announced midway through the Games, allowing two tributes from the same district to win the Hunger Games as a couple. Upon hearing this, Katniss begins searching for Peeta. She eventually finds him, wounded and in hiding. As she nurses him back to health, she acts the part of a young girl falling in love to gain more favor with the audience and, consequently, gifts from her sponsors. When the couple remain as the last two surviving tributes, the Gamemakers reverse the rule change in an attempt to force them into a dramatic finale, where one must kill the other to win. Katniss, knowing that the Gamemakers would rather have two victors than none, retrieves highly poisonous berries known as "nightlock" from her pouch and offers some to Peeta. Realizing that Katniss and Peeta intend to commit suicide, the Gamemakers announce that both will be the victors of the 74th Hunger Games.
Although she survives the ordeal in the arena and is treated to a hero's welcome in the Capitol, Katniss is warned by Haymitch that she has now become a political target after defying her society's authoritarian leaders so publicly. Afterwards, Peeta is heartbroken when he learns that Katniss's actions in the arena were part of a calculated ploy to earn sympathy from the audience. However, Katniss is unsure of her own feelings and realizes that she is dreading the moment when she and Peeta will go their separate ways.
In an interview with Collins, it was noted that the novel "tackles issues like severe poverty, starvation, oppression, and the effects of war among others."[5] The novel deals with the struggle for self-preservation that the people of Panem face in their districts and the Hunger Games in which they must participate.[1] The citizens' starvation and their need for resources, both in and outside of the arena, create an atmosphere of helplessness that the main characters try to overcome in their fight for survival. Katniss needs to hunt to provide food for her familythis necessity results in the development of skills that are useful to her in the Games (such as her proficiency with the bow and arrow), and represents her rejection of the Capitol's rules in the face of life-threatening situations.[6] On the subject of the Games' parallels with popular culture, Darren Franich of Entertainment Weekly writes that the book "is an incisive satire of reality television shows", and that the character of Cinna "almost seems like a contestant on a fascist version of Project Runway, using Katniss outfits as a vehicle to express potentially dangerous ideas."[7]
The choices the characters make and the strategies they use are often morally complex. The tributes build a personality they want the audience to see throughout the Games.[6] Library journal Voice of Youth Advocates names the major themes of The Hunger Games as "government control, 'big brother', and personal independence."[8] The trilogy's theme of power and downfall, similar to that of Shakespeare's Julius Caesar, was pointed out by its publisher Scholastic.[9] Laura Miller of The New Yorker finds the author's stated premise of the Games  an exercise in propaganda and a "humiliating as well as torturous .... punishment" for a failed uprising against the Capitol many years earlier  to be unconvincing. "You dont demoralize and dehumanize a subject people by turning them into celebrities and coaching them on how to craft an appealing persona for a mass audience." But the story works much better if the theme is vicissitudes of high school and "the adolescent social experience". Miller writes:
"The rules are arbitrary, unfathomable, and subject to sudden change. A brutal social hierarchy prevails, with the rich, the good-looking, and the athletic lording their advantages over everyone else. To survive you have to be totally fake. Adults dont seem to understand how high the stakes are; your whole life could be over, and they act like its just some "phase"! Everyones always watching you, scrutinizing your clothes or your friends and obsessing over whether youre having sex or taking drugs or getting good enough grades, but no one cares who you really are or how you really feel about anything."[10]
Donald Brake from The Washington Times and pastor Andy Langford state that the story has Christian themes, such as that of self-sacrifice, which is found in Katniss' substitution for her younger sister, analogous to the sacrifice of Jesus as a substitute for the atonement of sins.[11][12] Brake, as well as another reviewer, Amy Simpson, both find that the story also revolves around the theme of hope, which is exemplified in the "incorruptible goodness of Katniss' sister, Primrose."[13] Simpson also points to events similar to the Passion of Jesus; in the Games, "Christ figure" Peeta Mellark is stabbed after warning Katniss to flee for her life, and is then buried in the ground and placed in a cave for three days before emerging with a new lease on life.[13] Further, she finds that the Christian image of the Bread of Life is used throughout The Hunger Games; in the story, Peeta gives Katniss a loaf of bread, saving the girl and her family from starvation.[13]
After writing the novel, Collins signed a six-figure deal for three books with Scholastic in 2006. First published as a hardcover in the United States on September 14, 2008, The Hunger Games had a first printing of 50,000copies, which was bumped up twice to 200,000copies.[1] By February 2010, the book had sold 800,000copies,[14] and rights to the novel had been sold in 38 territories worldwide.[14] A few months later, in July, the book was released in paperback.[15] The Hunger Games entered the New York Times Best Seller list in November2008,[16] where it would feature for over 100consecutive weeks.[17] By the time the film adaptation of The Hunger Games was released in March2012, the book had been on USA Today's best-sellers list for 135 consecutive weeks.[18]
The novel is the first in The Hunger Games trilogy; it is followed by sequels Catching Fire (2009) and Mockingjay (2010). In March 2012, during the time of The Hunger Games film's release, Scholastic reported 26 million Hunger Games trilogy books in print, including movie tie-in books.[19] The Hunger Games (and also its sequels) have sold exceptionally well in ebook format. Suzanne Collins is the first children's or young adult author to sell over one million Amazon Kindle ebooks, making her the sixth author to join the "Kindle Million Club".[20] In March 2012, Amazon announced that Collins had become the best-selling Kindle ebook author of all time.[21]
An audiobook version of The Hunger Games was released in December2008. Read by actress Carolyn McCormick, it has a total running time of eleven hours and fourteen minutes.[22] The magazine AudioFile said: "Carolyn McCormick gives a detailed and attentive narration. However, she may rely too much on the strength of the prose without providing the drama young adult listeners often enjoy."[23] School Library Journal also praised the audiobook, stating that "McCormick ably voices the action-packed sequences and Katniss's every fear and strength shines through, along with her doomed growing attraction to one of her fellow Tributes."[24]
The Tim O'Brien-designed cover features a gold "mockingjay"a fictional bird in The Hunger Games born by crossbreeding female mockingbirds and genetically engineered male "jabberjays"with an arrow engraved in a circle. This is a depiction of the pin worn by Katniss into the arena, given to her by the District12 mayor's daughter, Madge Undersee.[25] The image matches the description of the pin that is given in the novel, except for the arrow: "It's as if someone fashioned a small golden bird and then attached a ring around it. The bird is connected to the ring only by its wing tips."[26]
The Hunger Games has been well received by critics. In a review for The New York Times, John Green wrote that the novel was "brilliantly plotted and perfectly paced", and that "the considerable strength of the novel comes in Collins's convincingly detailed world-building and her memorably complex and fascinating heroine." However, he also noted that, while allegorically rich, the book sometimes does not realize the allegorical potential that the plot has to offer and that the writing "described the action and little else."[27] Time magazine's review was also positive, stating that it "is a chilling, bloody and thoroughly horrifying book" and praising what it called the "hypnotic" quality of the violence.[28] In Stephen King's review for Entertainment Weekly, he compared it to "shoot-it-if-it-moves videogames in the lobby of the local eightplex; you know it's not real, but you keep plugging in quarters anyway." However, he stated that there were "displays of authorial laziness that kids will accept more readily than adults" and that the love triangle was standard for the genre. He gave the book an overall B grade.[29] Elizabeth Bird of School Library Journal praised the novel, saying it is "exciting, poignant, thoughtful, and breathtaking by turns". The review also called it one of the best books of 2008.[30] Booklist also gave a positive review, praising the character violence and romance involved in the book.[31] Kirkus Reviews gave a positive review, praising the action and world-building, but pointed out that "poor copyediting in the first printing will distract careful readersa crying shame".[32] Rick Riordan, author of the Percy Jackson & the Olympians series, claims it is the "closest thing to a perfect adventure novel" he has ever read.[33] Stephenie Meyer (author of the Twilight series) endorsed the book on her website, saying, "I was so obsessed with this book The Hunger Games is amazing."[34]
The novel has been criticized for its similarities to the 1999novel Battle Royale, by Koushun Takami. Collins has stated, "I had never heard of that book or that author until my book was turned in. At that point, it was mentioned to me, and I asked my editor if I should read it. He said: 'No, I don't want that world in your head. Just continue with what you're doing.' " Susan Dominus of The New York Times reports that "the parallels are striking enough that Collins's work has been savaged on the blogosphere as a baldfaced ripoff," but argued that "there are enough possible sources for the plot line that the two authors might well have hit on the same basic setup independently."[35] King noted that the reality TV "badlands" were similar to Battle Royale, as well as his own The Running Man and The Long Walk.[29] Eric Eisenberg wrote that The Hunger Games was "not a rip off [of Battle Royale], but simply a different usage of a similar idea", pointing out various differences in both story and themes.[36] Robert Nishimura wrote that "The Hunger Games has an entirely different set of cultural baggage ... Collins just happened to tap in to the creative collective consciousness, drawing on ideas that have played out many times before, in addition to her intentional reference to Greek mythology."[37] The novel has also been controversial with parents;[38] it ranked in fifth place on the American Library Association's list of frequently challenged books for 2010, with "unsuited to age group" and "violence" being among the reasons cited.[39]
The Hunger Games received many awards and honors. It was named one of Publishers Weekly's "Best Books of the Year" in 2008[40] and a The New York Times "Notable Children's Book of 2008".[41] It was the 2009 winner of the Golden Duck Award in the Young Adult Fiction Category.[42] The Hunger Games was also a "2008 Cybil Winner" for fantasy and science-fiction books along with The Graveyard Book.[43] It is also one of School Library Journal's "Best Books 2008"[44] and a "Booklist Editors' Choice" in 2008.[45] In 2011, the book won the California Young Reader Medal.[46] In the 2012edition of Scholastic's Parent and Child magazine, The Hunger Games was listed as the 33rd-best book for children, with the award for "Most Exciting Ending".[47][48] The book is one of the top 5 best selling Kindle books of all time.[49]
In March 2009, Lions Gate Entertainment entered into a co-production agreement for The Hunger Games with Nina Jacobson's production company Color Force, which had acquired worldwide distribution rights to the novel a few weeks earlier.[50][51] The studio, which had not made a profit for five years, raided the budgets of other productions and sold assets to secure a budget of $88,000,000  one of its largest ever[52]  for the film.[53][54] Collins' agent Jason Dravis remarked that "they [Lionsgate] had everyone but the valet call us" to help secure the franchise.[54] Intending the film to have a PG-13 rating,[55] Collins adapted the novel for film herself,[50] in collaboration with screenwriter Billy Ray and director Gary Ross.[56][57] The screenplay remains extremely faithful to the original novel,[58] with Ross saying he "felt the only way to make the film really successful was to be totally subjective" in its presentation of events, echoing Collins' use of first person present in the novel.[59]
Twenty-year-old actress Jennifer Lawrence was chosen to play Katniss Everdeen.[60] Though Lawrence was four years older than the character when filming began,[61] Collins felt the role demanded "a certain maturity and power" and said she would rather the actress be older than younger.[62] She added that Lawrence was the "only one who truly captured the character I wrote in the book" and that she had "every essential quality necessary to play Katniss."[63] Lawrence, a fan of the books, took three days to accept the role, initially intimidated by the size of the production.[64][65] Josh Hutcherson and Liam Hemsworth were later added to the cast, in the roles of Peeta and Gale, respectively.[66][67] Production began in late spring 2011[68] and the film was released on March 23, 2012.[69] The film's opening weekend brought in a non-sequel record $152.5million (USD) in North America.[70] The Hunger Games: Catching Fire, based on the second novel in the series, is due to be released in 2013.[71]
1 Background
2 Plot
3 Themes
4 Publication history
5 Critical reception
6 Film adaptation
7 See also
8 References
9 External links
The Condemned
The Most Dangerous Game
Series 7: The Contenders
The Lottery
Suzanne Collins's official website
Scholastic Official Site
The Hunger Games on Google Books
.
#(`*Intelligence quotient*`)#.
An intelligence quotient, or IQ, is a score derived from one of several standardized tests designed to assess intelligence. The abbreviation "IQ" comes from the German term Intelligenz-Quotient, originally coined by psychologist William Stern. When modern IQ tests are devised, the mean (average) score within an age group is set to 100 and the standard deviation (SD) almost always to 15, although this was not always so historically.[1] Thus, the intention is that approximately 95% of the population scores within two SDs of the mean, i.e. has an IQ between 70 and 130.
IQ scores have been shown to be associated with such factors as morbidity and mortality,[2] parental social status,[3] and, to a substantial degree, biological parental IQ. While the heritability of IQ has been investigated for nearly a century, there is still debate about the significance of heritability estimates[4][5] and the mechanisms of inheritance.[6]
IQ scores are used as predictors of educational achievement, special needs, job performance and income. They are also used to study IQ distributions in populations and the correlations between IQ and other variables. The average IQ scores for many populations have been rising at an average rate of three points per decade since the early 20th century, a phenomenon called the Flynn effect. It is disputed whether these changes in scores reflect real changes in intellectual abilities.
Well-constructed IQ tests are generally accepted as an accurate measure of intelligence by the scientific community.[7]
The first large-scale mental test may have been the imperial examination system in China. Modern mental testing began in France in the 19th century. It contributed to separating mental retardation from mental illness and reducing the neglect, torture, and ridicule heaped on both groups.[8]
Englishman Francis Galton coined the terms psychometrics and eugenics, and developed a method for measuring intelligence based on nonverbal sensory-motor tests. It was initially popular, but was abandoned after the discovery that it had no relationship to outcomes such as college grades.[8][9]
French psychologist Alfred Binet, together with psychologists Victor Henri and Thodore Simon, after about 15 years of development, published the Binet-Simon test in 1905, which focused on verbal abilities. It was intended to identify mental retardation in school children.[8] The score on the Binet-Simon scale would reveal the child's mental age. For example, a six-year-old child who passed all the tasks usually passed by six-year-oldsbut nothing beyondwould have a mental age that exactly matched his chronological age, 6.0. (Fancher, 1985). In Binet's view, there were limitations with the scale and he stressed what he saw as the remarkable diversity of intelligence and the subsequent need to study it using qualitative, as opposed to quantitative, measures (White, 2000). American psychologist Henry H. Goddard published a translation of it in 1910. The eugenics movement in the USA seized on it as a means to give them credibility in diagnosing mental retardation, and thousands of American women, most of them poor African Americans, were forcibly sterilized based on their scores on IQ tests, often without their consent or knowledge.[10] American psychologist Lewis Terman at Stanford University revised the Binet-Simon scale, which resulted in the Stanford-Binet Intelligence Scales (1916). It became the most popular test in the United States for decades.[8][11][12][13]
The many different kinds of IQ tests use a wide variety of methods. Some tests are visual, some are verbal, some tests only use abstract-reasoning problems, and some tests concentrate on arithmetic, spatial imagery, reading, vocabulary, memory or general knowledge. The psychologist Charles Spearman in 1904 made the first formal factor analysis of correlations between the tests. He found a single common factor explained the positive correlations among tests. This is an argument still accepted in principle by many psychometricians. Spearman named it g for "general factor" and labelled the smaller, specific factors or abilities for specific areas s. In any collection of IQ tests, by definition the test that best measures g is the one that has the highest correlations with all the others. Most of these g-loaded tests typically involve some form of abstract reasoning. Therefore, Spearman and others have regarded g as the (perhaps genetically determined) real essence of intelligence. This is still a common but not universally accepted view. Other factor analyses of the data, with different results, are possible. Some psychometricians regard g as a statistical artifact. One of the best measures of g is Raven's Progressive Matrices which is a test of visual reasoning.[1][8]
During World War I, a way was needed to evaluate and assign recruits. This led to the rapid development of several mental tests. The testing generated controversy and much public debate. Nonverbal or "performance" tests were developed for those who could not speak English or were suspected of malingering.[8] After the war, positive publicity on army psychological testing helped to make psychology a respected field.[14] Subsequently, there was an increase in jobs and funding in psychology.[15] Group intelligence tests were developed and became widely used in schools and industry.[16]
L.L. Thurstone argued for a model of intelligence that included seven unrelated factors (verbal comprehension, word fluency, number facility, spatial visualization, associative memory, perceptual speed, reasoning, and induction). While not widely used, it influenced later theories.[8]
David Wechsler produced the first version of his test in 1939. It gradually became more popular and overtook the Binet in the 1960s. It has been revised several times, as is common for IQ tests, to incorporate new research. One explanation is that psychologists and educators wanted more information than the single score from the Binet. Wechsler's 10+ subtests provided this. Another is Binet focused on verbal abilities, while the Wechsler also included nonverbal abilities. The Binet has also been revised several times and is now similar to the Wechsler in several aspects, but the Wechsler continues to be the most popular test in the United States.[8]
Raymond Cattell (1941) proposed two types of cognitive abilities in a revision of Spearman's concept of general intelligence. Fluid intelligence (Gf) was hypothesized as the ability to solve novel problems by using reasoning, and crystallized intelligence (Gc) was hypothesized as a knowledge-based ability that was very dependent on education and experience. In addition, fluid intelligence was hypothesized to decline with age, while crystallized intelligence was largely resistant. The theory was almost forgotten, but was revived by his student John L. Horn (1966) who later argued Gf and Gc were only two among several factors, and he eventually identified 9 or 10 broad abilities. The theory continued to be called Gf-Gc theory.[8]
John B. Carroll (1993), after a comprehensive reanalysis of earlier data, proposed the Three Stratum theory, which is a hierarchical model with three levels. The bottom stratum consists of narrow abilities that are highly specialized (e.g., induction, spelling ability). The second stratum consists of broad abilities. Carroll identified eight second-stratum abilities. Carroll accepted Spearman's concept of general intelligence, for the most part, as a representation of the uppermost, third stratum.[17][18]
More recently (1999), a merging of the Gf-Gc theory of Cattell and Horn with Carroll's Three-Stratum theory has led to the CattellHornCarroll theory. It has greatly influenced many of the current broad IQ tests.[8]
It is argued that this reflects much of what is known about intelligence from research. A hierarchy of factors is used; g is at the top. Under it are 10 broad abilities that in turn are subdivided into 70 narrow abilities. The broad abilities are:[8]
Modern tests do not necessarily measure of all of these broad abilities. For example, Gq and Grw may be seen as measures of school achievement and not IQ.[8] Gt may be difficult to measure without special equipment.
g was earlier often subdivided into only Gf and Gc, which were thought to correspond to the nonverbal or performance subtests and verbal subtests in earlier versions of the popular Wechsler IQ test. More recent research has shown the situation to be more complex.[8]
Modern comprehensive IQ tests no longer give a single score. Although they still give an overall score, they now also give scores for many of these more restricted abilities, identifying particular strengths and weaknesses of an individual.[8]
J.P. Guilford's Structure of Intellect (1967) model used three dimensions which when combined yielded a total of 120 types of intelligence. It was popular in the 1970s and early 1980s, but faded due to both practical problems and theoretical criticisms.[8]
Alexander Luria's earlier work on neuropsychological processes lead to the PASS theory (1997). It argued that only looking at one general factor was inadequate for researchers and clinicians who worked with learning disabilities, attention disorders, mental retardation, and interventions for such disabilities. The PASS model covers four kinds of processes. The planning processes involve decision making, problem solving, and performing activities and requires goal setting and self-monitoring. The attention/arousal process involves selectively attending to a particular stimulus, ignoring distractions, and maintaining vigilance. Simultaneous processing involves the integration of stimuli into a group and requires the observation of relationships. Successive processing involves the integration of stimuli into serial order. The planning and attention/arousal components comes from structures located in the frontal lobe, and the simultaneous and successive processes come from structures located in the posterior region of the cortex.[19][20][21] It has influenced some recent IQ tests, and been seen as a complement to the Cattell-Horn-Carroll theory described above.[8]
Well-known modern IQ tests include Raven's Progressive Matrices, Wechsler Adult Intelligence Scale, Wechsler Intelligence Scale for Children, Stanford-Binet, Woodcock-Johnson Tests of Cognitive Abilities, and Kaufman Assessment Battery for Children.
Approximately 95% of the population have scores within two standard deviations (SD) of the mean. If one SD is 15 points, as is common in almost all modern tests, then 95% of the population are within a range of 70 to 130, and 98% are below 131. Alternatively, two-thirds of the population have IQ scores within one SD of the mean, i.e. within the range 85-115.
IQ scales are ordinally scaled.[22][23][24][25] While one standard deviation is 15 points, and two SDs are 30 points, and so on, this does not imply that mental ability is linearly related to IQ, such that IQ 50 means half the cognitive ability of IQ 100. In particular, IQ points are not percentage points.
The correlation between IQ test results and achievement test results is about 0.7.[8][26]
German psychologist William Stern proposed a method of scoring children's intelligence tests in 1912. He calculated what he called a Intelligenz-Quotient score, or IQ, as the quotient of the 'mental age' (the age group which scored such a result on average) of the test-taker and the 'chronological age' of the test-taker, multiplied by 100. Terman used this system for the first version of the Stanford-Binet Intelligence Scales.[28] This method has several problems such as the fact that it cannot be used to score adults.
Wechsler introduced a different procedure for his test that is now used by almost all IQ tests. When an IQ test is constructed, a standardization sample representative of the general population takes the test. The median result is defined to be equivalent to 100 IQ points. In almost all modern tests, a standard deviation of the results is defined to be equivalent to 15 IQ points. When a subject takes an IQ test, the result is ranked compared to the results of the standardization sample and the subject is given an IQ score equal to those with the same test result in the standardization sample.
The values of 100 and 15 were chosen to get somewhat similar scores as in the older type of test. Likely as a part of the rivalry between the Binet and the Wechsler, the Binet until 2003 chose to have 16 for one SD, causing considerable confusion. Today, almost all tests use 15 for one SD. Modern scores are sometimes referred to as "deviation IQs," while older method age-specific scores are referred to as "ratio IQs."[8][29]
Psychometricians generally regard IQ tests as having high statistical reliability.[citation needed] A high reliability implies that while test-takers can have varying scores on differing occasions when taking the same test and can vary in scores on different IQ tests taken at the same age, the scores generally agree. A test-taker's score on any one IQ test is surrounded by an error band that shows, to a specified degree of confidence, what the test-taker's true score is likely to be. For modern tests, the standard error of measurement is about three points, or in other words, the odds are about two out of three that a person's true IQ is in range from three points above to three points below the test IQ. Another description is there is a 95% chance the true IQ is in range from four to five points above to four to five points below the test IQ, depending on the test in question. Clinical psychologists generally regard them as having sufficient statistical validity for many clinical purposes.[8][30][31]
Since the early 20th century, raw scores on IQ tests have increased in most parts of the world.[32][33][34] When a new version of an IQ test is normed, the standard scoring is set so performance at the population median results in a score of IQ 100. The phenomenon of rising raw score performance means if test-takers are scored by a constant standard scoring rule, IQ test scores have been rising at an average rate of around three IQ points per decade. This phenomenon was named the Flynn effect in the book The Bell Curve after James R. Flynn, the author who did the most to bring this phenomenon to the attention of psychologists.[35][36]
Researchers have been exploring the issue of whether the Flynn effect is equally strong on performance of all kinds of IQ test items, whether the effect may have ended in some developed nations, whether or not there are social subgroup differences in the effect, and what possible causes of the effect might be.[37] Flynn's observation has prompted much new research in psychology and "demolish some long-cherished beliefs, and raise a number of other interesting issues along the way."[33]
IQ can change to some degree over the course of childhood.[38] However, in one longitudinal study, the mean IQ scores of tests at ages 17 and 18 were correlated at r=.86 with the mean scores of tests at ages five, six and seven and at r=.96 with the mean scores of tests at ages 11, 12 and 13.[39]
IQ scores for children are relative to children of a similar age. That is, a child of a certain age does not do as well on the tests as an older child or an adult with the same IQ. But relative to persons of a similar age, or other adults in the case of adults, they do equally well if the IQ scores are the same.[39] To convert a child's IQ score into an adult score the following calculation should be made: child IQ score/100*age/16*100 = adult IQ score. The number 16 is used to indicate the age at which supposedly the IQ reaches its peak.[40]
For decades, practitioners' handbooks and textbooks on IQ testing have reported IQ declines with age after the beginning of adulthood. However, later researchers pointed out this phenomenon is related to the Flynn effect and is in part a cohort effect rather than a true aging effect.
A variety of studies of IQ and aging have been conducted since the norming of the first Wechsler Intelligence Scale drew attention to IQ differences in different age groups of adults. Current consensus is that fluid intelligence generally declines with age after early adulthood, while crystallized intelligence remains intact. Both cohort effects (the birth year of the test-takers) and practice effects (test-takers taking the same form of IQ test more than once) must be controlled to gain accurate data. It is unclear whether any lifestyle intervention can preserve fluid intelligence into older ages.[41]
The peak of capacity for both fluid intelligence and crystallized intelligence occurs at age 26. This is followed by a slow decline.[42]
Environmental and genetic factors play a role in determining IQ. Their relative importance has been the subject of much research and debate.
Heritability is defined as the proportion of variance in a trait which is attributable to genotype within a defined population in a specific environment. A number of points must be considered when interpreting heritability.[43] Heritability measures the proportion of 'variation' in a trait can be attributed to genes, and not the proportion of a trait caused by genes. The value of heritability can change if the impact of environment (or of genes) in the population is substantially altered. A high heritability of a trait does not mean environmental effects, such as learning, are not involved. Since heritability increases during childhood and adolescence, one should be cautious drawing conclusions regarding the role of genetics and environment from studies where the participants are not followed until they are adults.
Studies have found the heritability of IQ in adult twins to be 0.7 to 0.8 and in children twins 0.45 in the Western world.[39][44][45] It may seem reasonable to expect genetic influences on traits like IQ should become less important as one gains experiences with age. However, the opposite occurs. Heritability measures in infancy are as low as 0.2, around 0.4 in middle childhood, and as high as 0.8 in adulthood.[46] One proposed explanation is that people with different genes tend to reinforce the effects of those genes, for example by seeking out different environments.[39] Debate is ongoing about whether these heritability estimates are too high due to not adequately considering various factors, such as that the environment may be relatively more important in families with low socioeconomic status or the effect of the maternal (fetal) environment.
Recent research suggests that molecular genetics of psychology and social science requires approaches that go beyond the examination of candidate genes.[47]
Family members have aspects of environments in common (for example, characteristics of the home). This shared family environment accounts for 0.250.35 of the variation in IQ in childhood. By late adolescence, it is quite low (zero in some studies). The effect for several other psychological traits is similar. These studies have not looked at the effects of extreme environments, such as in abusive families.[39][48][49][50]
Although parents treat their children differently, such differential treatment explains only a small amount of nonshared environmental influence. One suggestion is that children react differently to the same environment due to different genes. More likely influences may be the impact of peers and other experiences outside the family.[39][49]
A number of individual genes have been reported to be associated with IQ. Examples include CHRM2, microcephalin, and ASPM. However, Deary and colleagues (2009) argued almost no evidence has been replicated.[51] About 20,000 genes are thought to have an impact on the development and functionality of the brain.[52]
Dickens and Flynn (2001) argued the "heritability" figure includes both a direct effect of the genotype on IQ and also indirect effects where the genotype changes the environment, in turn affecting IQ. That is, those with a higher IQ tend to seek out stimulating environments that further increase IQ. The direct effect can initially have been very small but feedback loops can create large differences in IQ. In their model, an environmental stimulus can have a very large effect on IQ, even in adults, but this effect also decays over time unless the stimulus continues (the model could be adapted to include possible factors, like nutrition in early childhood, that may cause permanent effects). The Flynn effect can be explained by a generally more stimulating environment for all people. The authors suggest that programs aiming to increase IQ would be most likely to produce long-term IQ gains if they taught children how to replicate outside the program the kinds of cognitively demanding experiences that produce IQ gains while they are in the program and motivate them to persist in that replication long after they have left the program.[53][54]
In general, educational interventions, as those described below, have shown short-term effects on IQ, but long-term follow-up is often missing. For example, in the US some interventive programs such as the Head Start Program have not produced lasting gains in IQ scores, although the more intensive Abecedarian Project have.[39]
A placebo controlled double-blind experiment found that vegetarians who took 5grams of creatine per day for six weeks showed a significant improvement on two separate tests of fluid intelligence, Raven's Progressive Matrices, and the backward digit span test from the WAIS. The treatment group was able to repeat longer sequences of numbers from memory and had higher overall IQ scores than the control group. The researchers concluded that "supplementation with creatine significantly increased intelligence compared with placebo."[55] A subsequent study found that creatine supplements improved cognitive ability in the elderly.[56] However, a study on young adults (0.03 g/kg/day for six weeks, e.g., 2 g/day for 150-pound individual) failed to find any improvements.[57]
Recent studies have shown that training in using one's working memory may increase IQ. A study on young adults published in April 2008 by a team from the Universities of Michigan and Bern supports the possibility of the transfer of fluid intelligence from specifically designed working memory training.[58][59] Further research will be needed to determine nature, extent and duration of the proposed transfer. Among other questions, it remains to be seen whether the results extend to other kinds of fluid intelligence tests than the matrix test used in the study, and if so, whether, after training, fluid intelligence measures retain their correlation with educational and occupational achievement or if the value of fluid intelligence for predicting performance on other tasks changes. It is also unclear whether the training is durable of extended periods of time.[60]
Musical training in childhood has been found to correlate with higher than average IQ.[61] In a 2004 study conducted by E. Glenn Schellenberg, results showed that 6 year old children who received musical training (voice or piano lessons) had an average increase in IQ of 7.0 points while children who received alternative training (i.e. drama) or no training had an average increase in IQ of only 4.3 points (which may be consequence of the children entering grade school) as indicated by full scale IQ. Children were tested using Wechsler Intelligence Scale for ChildrenThird Edition, Kaufman Test of Educational Achievement and Parent Rating Scale of the Behavioral Assessment System for Children.[61]
Listening to classical music has also been found to increase IQ; specifically spatial ability. In 1994 Frances Rauscher and Gorden Shaw found that when college students listened to 10 minutes of Mozart's Sonata for Two Pianos, there is an IQ increase of 8 to 9 points on the spatial subtest on the Standford-Binet Intelligence Scale.[62] However, this effect is a short term effect and usually lasts no longer than 10 to 15 minutes. This phenomenon was coined the Mozart effect.
In 2004, Schellenberg devised an experiment to test his hypothesis that music lessons can enhance the IQ of children. He had 144 samples of 6 year old children which were put into 4 groups; keyboard lessons, vocal lessons, drama lessons or no lessons at all, for 36 weeks. The samples' IQ was measured both before and after the lessons had taken place using the Wechsler Intelligence Scale for ChildrenThird Edition, Kaufman Test of Educational Achievement and Parent Rating Scale of the Behavioral Assessment System for Children. All four groups had increases in IQ, most likely resulted by the entrance of grade school. The notable difference with the two music groups compared to the two controlled groups was a slightly higher increase in IQ. The children in the control groups on average had an increase in IQ of 4.3 points, while the increase in IQ of the music groups was 7.0 points. Though the increases in IQ were not dramatic, one can still conclude that musical lessons does have a positive effect for children, if taken at a young age. It is hypothesized that improvements in IQ occur after musical lessons because the music lessons encourage multiple experiences which generates progression in a wide range of abilities for the children. Testing this hypothesis has however been proven difficult.[63]
Another test also performed by Schellenberg tested the effects of musical training in adulthood. He had two groups of adults, one group whom were musically trained and another group who were not. He administered tests of intelligence quotient and emotional intelligence to the trained and non-trained groups and found that the trained participants had an advantage in IQ over the untrained subjects even with gender, age, environmental issues (e.g. income, parent's education) held constant. The two groups, however, score similarly in the emotional intelligence test. The test results (like the previous results) show that there is a positive correlation between musical training and IQ, but it is not evident that musical training has a positive effect on emotional intelligence.[64]
Several neurophysiological factors have been correlated with intelligence in humans, including the ratio of brain weight to body weight and the size, shape and activity level of different parts of the brain. Specific features that may affect IQ include the size and shape of the frontal lobes, the amount of blood and chemical activity in the frontal lobes, the total amount of gray matter in the brain, the overall thickness of the cortex and the glucose metabolic rate.
Health is important in understanding differences in IQ test scores and other measures of cognitive ability. Several factors can lead to significant cognitive impairment, particularly if they occur during pregnancy and childhood when the brain is growing and the bloodbrain barrier is less effective. Such impairment may sometimes be permanent, sometimes be partially or wholly compensated for by later growth.[citation needed]
Developed nations have implemented several health policies regarding nutrients and toxins known to influence cognitive function. These include laws requiring fortification of certain food products and laws establishing safe levels of pollutants (e.g. lead, mercury, and organochlorides). Improvements in nutrition, and in public policy in general, have been implicated in worldwide IQ increases.[citation needed]
Cognitive epidemiology is a field of research that examines the associations between intelligence test scores and health. Researchers in the field argue that intelligence measured at an early age is an important predictor of later health and mortality differences.
Outside of academic research and medicine, IQ testing is often conducted because of its ability to predict future job performance, social pathologies, or academic achievement. Academic research has also examined these associations, as well as the effect of IQ on other social outcomes, such as income and wealth.
Many of the arguments and criticisms assume that explained variance can be calculated as the square of the correlation coefficient. This way of calculating explained variance has been criticized as inappropriate for most social scientific work.[65] Also, as for the heritability figure, the explained variance only refers to the proportion of variation in an outcome that is explained by a factor, and not the proportion of an outcome that is explained by a factor.
One study found a correlation of 0.82 between g (general intelligence factor) and SAT scores;[66] another has found correlation of 0.81 between g and GCSE scores.[67]
Correlations between IQ scores (general cognitive ability) and achievement test scores are reported to be 0.81 by Deary and colleagues, with the explained variance ranging "from 58.6% in Mathematics and 48% in English to 18.1% in Art and Design".[67]
The American Psychological Association's report "Intelligence: Knowns and Unknowns" states that wherever it has been studied, children with high scores on tests of intelligence tend to learn more of what is taught in school than their lower-scoring peers. The correlation between IQ scores and grades is about .50. This means that the explained variance is 25%. Achieving good grades depends on many factors other than IQ, such as "persistence, interest in school, and willingness to study" (p.81).[39]
It has been found IQ correlation with school performance depends on the IQ measurement used. For undergraduate students, the Verbal IQ as measured by WAIS-R has been found to correlate significantly (0.53) with the GPA of the last 60 hours. In contrast, Performance IQ correlation with the same GPA was only 0.22 in the same study.[68]
According to Frank Schmidt and John Hunter, "for hiring employees without previous experience in the job the most valid predictor of future performance is general mental ability."[69] The validity of IQ as a predictor of job performance is above zero for all work studied to date, but varies with the type of job and across different studies, ranging from 0.2 to 0.6.[70] The correlations were higher when the unreliability of measurement methods were controlled for.[39] While IQ is more strongly correlated with reasoning and less so with motor function,[71] IQ-test scores predict performance ratings in all occupations.[69] That said, for highly qualified activities (research, management) low IQ scores are more likely to be a barrier to adequate performance, whereas for minimally-skilled activities, athletic strength (manual strength, speed, stamina, and coordination) are more likely to influence performance.[69] It is largely through the quicker acquisition of job-relevant knowledge that higher IQ mediates job performance.
In establishing a causal direction to the link between IQ and work performance, longitudinal studies by Watkins and others suggest that IQ exerts a causal influence on future academic achievement, whereas academic achievement does not substantially influence future IQ scores.[72] Treena Eileen Rohde and Lee Anne Thompson write that general cognitive ability, but not specific ability scores, predict academic achievement, with the exception that processing speed and spatial ability predict performance on the SAT math beyond the effect of general cognitive ability.[73]
The US military has minimum enlistment standards at about the IQ 85 level. There have been two experiments with lowering this to 80 but in both cases these men could not master soldiering well enough to justify their costs [74]
Some US police departments have set a maximum IQ score for new officers (for example: 125, in New London, CT), under the argument that those with overly-high IQs will become bored and exhibit high turnover in the job. This policy has been challenged as discriminatory, but upheld by at least one US District court.[75]
The American Psychological Association's report "Intelligence: Knowns and Unknowns" states that since the explained variance is 29%, other individual characteristics such as interpersonal skills, aspects of personality etc. are probably of equal or greater importance, but at this point there are no equally reliable instruments to measure them.[39]
Some researchers claim "in economic terms it appears that the IQ score measures something with decreasing marginal value. It is important to have enough of it, but having lots and lots does not buy you that much."[76][77]
Other studies show that ability and performance for jobs are linearly related, such that at all IQ levels, an increase in IQ translates into a concomitant increase in performance.[78] Charles Murray, coauthor of The Bell Curve, found that IQ has a substantial effect on income independently of family background.[79]
Taking the above two principles together, very high IQ produces very high job performance, but no greater income than slightly high IQ. Studies also show that high IQ is related to higher net worth.[80]
The American Psychological Association's 1995 report Intelligence: Knowns and Unknowns stated that IQ scores accounted for (explained variance) about quarter of the social status variance and one-sixth of the income variance. Statistical controls for parental SES eliminate about a quarter of this predictive power. Psychometric intelligence appears as only one of a great many factors that influence social outcomes.[39]
Some studies claim that IQ only accounts for (explained variance) a sixth of the variation in income because many studies are based on young adults (many of whom have not yet completed their education). On pg 568 of The g Factor, Arthur Jensen claims that although the correlation between IQ and income averages a moderate 0.4 (one sixth or 16% of the variance), the relationship increases with age, and peaks at middle age when people have reached their maximum career potential. In the book, A Question of Intelligence, Daniel Seligman cites an IQ income correlation of 0.5 (25% of the variance).
A 2002 study[81] further examined the impact of non-IQ factors on income and concluded that an individual's location, inherited wealth, race, and schooling are more important as factors in determining income than IQ.
The American Psychological Association's 1995 report Intelligence: Knowns and Unknowns stated that the correlation between IQ and crime was -0.2. It was -0.19 between IQ scores and number of juvenile offenses in a large Danish sample; with social class controlled, the correlation dropped to -0.17. A correlation of 0.20 means that the explained variance is less than 4%. It is important to realize that the causal links between psychometric ability and social outcomes may be indirect. Children with poor scholastic performance may feel alienated. Consequently, they may be more likely to engage in delinquent behavior, compared to other children who do well.[39]
In his book The g Factor (1998), Arthur Jensen cited data which showed that, regardless of race, people with IQs between 70 and 90 have higher crime rates than people with IQs below or above this range, with the peak range being between 80 and 90.
The 2009 Handbook of Crime Correlates stated that reviews have found that around eight IQ points, or 0.5 SD, separate criminals from the general population, especially for persistent serious offenders. It has been suggested that this simply reflects that "only dumb ones get caught" but there is similarly a negative relation between IQ and self-reported offending. That children with conduct disorder have lower IQ than their peers "strongly argues" for the theory.[82]
A study of the relationship between US county-level IQ and US county-level crime rates found that higher average IQs were associated with lower levels of property crime, burglary, larceny rate, motor vehicle theft, violent crime, robbery, and aggravated assault. These results were not "confounded by a measure of concentrated disadvantage that captures the effects of race, poverty, and other social disadvantages of the county."[83]
In addition, IQ and its correlation to health, violent crime, gross state product, and government effectiveness are the subject of a 2006 paper in the publication Intelligence. The paper breaks down IQ averages by U.S. states using the federal government's National Assessment of Educational Progress math and reading test scores as a source.[84]
The American Psychological Association's 1995 report Intelligence: Knowns and Unknowns stated that the correlations for most "negative outcome" variables are typically smaller than 0.20, which means that the explained variance is less than 4%.[39]
Tambs et al.[85][bettersourceneeded] found that occupational status, educational attainment, and IQ are individually heritable; and further found that "genetic variance influencing educational attainment ... contributed approximately one-fourth of the genetic variance for occupational status and nearly half the genetic variance for IQ." In a sample of U.S. siblings, Rowe et al.[86] report that the inequality in education and income was predominantly due to genes, with shared environmental factors playing a subordinate role.
A recent USA study connecting political views and intelligence has shown that the mean adolescent intelligence of young adults who identify themselves as "very liberal" is 106.4, while that of those who identify themselves as "very conservative" is 94.8.[87] Two other studies conducted in the UK reached similar conclusions.[88][89]
There are also other correlations such as those between religiosity and intelligence and fertility and intelligence.
Average adult combined IQs associated with real-life accomplishments by various tests:[90][91]
Average IQ of various occupational groups:[92]
Type of work that can be accomplished:[90]
There is considerable variation within and overlap between these categories. People with high IQs are found at all levels of education and occupational categories. The biggest difference occurs for low IQs with only an occasional college graduate or professional scoring below 90.[8]
Among the most controversial issues related to the study of intelligence is the observation that intelligence measures such as IQ scores vary between ethnic and racial groups and sexes. While there is little scholarly debate about the existence of some of these differences, their causes remain highly controversial both within academia and in the public sphere.
Most IQ tests are constructed so that there are no overall score differences between females and males.[93] Because environmental factors affect brain activity and behavior, where differences are found, it can be difficult for researchers to assess whether or not the differences are innate. Areas where differences have been found include verbal and mathematical ability.
The 1996 Task Force investigation on Intelligence sponsored by the American Psychological Association concluded that there are significant variations in IQ across races.[39] The problem of determining the causes underlying this variation relates to the question of the contributions of "nature and nurture" to IQ. Psychologists such as Alan S. Kaufman[94] and Nathan Brody[95] and statisticians such as Bernie Devlin[96] argue that there are insufficient data to conclude that this is because of genetic influences. One of the most notable researchers arguing for a strong genetic influence on these average score differences is Arthur Jensen. In contrast, other researchers such as Richard Nisbett argue that environmental factors can explain all of the average group differences.[97]
In the United States, certain public policies and laws regarding military service,[98] [99] education, public benefits,[100] capital punishment,[101] and employment incorporate an individual's IQ into their decisions. However, in the case of Griggs v. Duke Power Co. in 1971, for the purpose of minimizing employment practices that disparately impacted racial minorities, the U.S. Supreme Court banned the use of IQ tests in employment, except when linked to job performance via a Job analysis. Internationally, certain public policies, such as improving nutrition and prohibiting neurotoxins, have as one of their goals raising, or preventing a decline in, intelligence.
A diagnosis of mental retardation is in part based on the results of IQ testing. Borderline intellectual functioning is a categorization where a person has below average cognitive ability (an IQ of 7185), but the deficit is not as severe as mental retardation (70 or below).
In the United Kingdom, the eleven plus exam which incorporated an intelligence test has been used from 1945 to decide, at eleven years old, which type of school a child should go to. They have been much less used since the widespread introduction of comprehensive schools.
IQ is the most researched approach to intelligence and by far the most widely used in practical setting. However, although IQ attempts to measure some notion of intelligence, it may fail to act as an accurate measure of "intelligence" in its broadest sense. IQ tests only examine particular areas embodied by the broadest notion of "intelligence", failing to account for certain areas which are also associated with "intelligence" such as creativity or emotional intelligence.
There are critics such as Keith Stanovich who do not dispute the stability of IQ test scores or the fact that they predict certain forms of achievement rather effectively. They do argue, however, that to base a concept of intelligence on IQ test scores alone is to ignore many important aspects of mental ability.[3][102]
Some scientists dispute IQ entirely. In The Mismeasure of Man (1996), paleontologist Stephen Jay Gould criticized IQ tests and argued that that they were used for scientific racism. He argued that g was a mathematical artifact and criticized:
Psychologist Peter Schnemann was also a persistent critic of IQ, calling it "the IQ myth". He argued that g is a flawed theory and that the high heritability estimates of IQ are based on false assumptions.[103][104]
Psychologist Arthur Jensen has rejected the criticism by Gould and also argued that even if g was replaced by a model with several intelligences this would change the situation less than expected. All tests of cognitive ability would continue to be highly correlated with one another and there would still be a black-white gap on cognitive tests.[105]
The American Psychological Association's report Intelligence: Knowns and Unknowns stated that in the United States IQ tests as predictors of social achievement are not biased against African Americans since they predict future performance, such as school achievement, similarly to the way they predict future performance for Caucasians.[39]
However, IQ tests may well be biased when used in other situations. A 2005 study stated that "differential validity in prediction suggests that the WAIS-R test may contain cultural influences that reduce the validity of the WAIS-R as a measure of cognitive ability for Mexican American students,"[106] indicating a weaker positive correlation relative to sampled white students. Other recent studies have questioned the culture-fairness of IQ tests when used in South Africa.[107][108] Standard intelligence tests, such as the Stanford-Binet, are often inappropriate for children with autism; the alternative of using developmental or adaptive skills measures are relatively poor measures of intelligence in autistic children, and may have resulted in incorrect claims that a majority of children with autism are mentally retarded.[109]
A 2006 article stated that contemporary psychologic research often did not reflect substantial recent developments in psychometrics and "bears an uncanny resemblance to the psychometric state of the art as it existed in the 1950s."[110]
In response to the controversy surrounding The Bell Curve, the American Psychological Association's Board of Scientific Affairs established a task force in 1995 to write a report on the state of intelligence research which could be used by all sides as a basis for discussion, "Intelligence: Knowns and Unknowns". The full text of the report is available through several websites.[39][111]
In this paper the representatives of the association regret that IQ-related works are frequently written with a view to their political consequences: "research findings were often assessed not so much on their merits or their scientific standing as on their supposed political implications".
The task force concluded that IQ scores do have high predictive validity for individual differences in school achievement. They confirm the predictive validity of IQ for adult occupational status, even when variables such as education and family background have been statistically controlled. They stated that individual differences in intelligence are substantially influenced by both genetics and environment.
The report stated that a number of biological factors, including malnutrition, exposure to toxic substances, and various prenatal and perinatal stressors, result in lowered psychometric intelligence under at least some conditions. The task force agrees that large differences do exist between the average IQ scores of blacks and whites, saying:
The APA journal that published the statement, American Psychologist, subsequently published eleven critical responses in January 1997, several of them arguing that the report failed to examine adequately the evidence for partly genetic explanations.
Notable and increasingly influential[112][113] alternative to the wide range of standard IQ tests originated in the writings of psychologist Lev Vygotsky (1896-1934) of his most mature and highly productive period of 1932-1934. The notion of the zone of proximal development that he introduced in 1933, roughly a year before his death, served as the banner for his proposal to diagnose development as the level of actual development that can be measured by the child's independent problem solving and, at the same time, the level of proximal, or potential development that is measured in the situation of moderately assisted problem solving by the child.[114] The maximum level of complexity and difficulty of the problem that the child is capable to solve under some guidance indicates the level of potential development. Then, the difference between the higher level of potential and the lower level of actual development indicates the zone of proximal development. Combination of the two indexesthe level of actual and the zone of the proximal developmentaccording to Vygotsky, provides a significantly more informative indicator of psychological development than the assessment of the level of actual development alone.[115][116]
The ideas on the zone of development were later developed in a number of psychological and educational theories and practices. Most notably, they were developed under the banner of dynamic assessment that focuses on the testing of learning and developmental potential[117][118][119] (for instance, in the work of Reuven Feuerstein and his associates,[120] who has criticized standard IQ testing for its putative assumption or acceptance of "fixed and immutable" characteristics of intelligence or cognitive functioning). Grounded in developmental theories of Vygotsky and Feuerstein, who recognized that human beings are not static entities but are always in states of transition and transactional relationships with the world, dynamic assessment received also considerable support in the recent revisions of cognitive developmental theory by Joseph Campione, Ann Brown, and John D. Bransford and in theories of multiple intelligences by Howard Gardner and Robert Sternberg.[121]
There are social organizations, some international, which limit membership to people who have scores as high as or higher than the 98th percentile on some IQ test or equivalent. Mensa International is perhaps the most well known of these. There are other groups requiring a score above the 98th percentile.
IQ reference charts are tables suggested by test publishers to divide intelligence ranges in various categories.
Abstract thought
Communication
Creativity
Emotional intelligence
g factor
Intelligence quotient
Knowledge
Learning
Memory
Problem solving
Reaction time
Reasoning
Understanding
Visual processing
CattellHornCarroll theory
Fluid and crystallized intelligence
Theory of multiple intelligences
Three stratum theory
Triarchic theory of intelligence
PASS theory of intelligence
Cognitive epidemiology
Evolution of human intelligence
Psychometrics
Heritability of IQ
Impact of health on intelligence
Environment and intelligence
Neuroscience and intelligence
Race and intelligence
v
t
e
1 History

1.1 Early history
1.2 General factor (g)
1.3 The War Years
1.4 CattellHornCarroll theory
1.5 Other theories


1.1 Early history
1.2 General factor (g)
1.3 The War Years
1.4 CattellHornCarroll theory
1.5 Other theories
2 Modern tests
3 Mental age vs. modern method
4 Reliability and validity
5 Flynn effect
6 IQ and age
7 Genetics and environment

7.1 Heritability
7.2 Shared family environment
7.3 Non-shared family environment and environment outside the family
7.4 Individual genes
7.5 Gene-environment interaction


7.1 Heritability
7.2 Shared family environment
7.3 Non-shared family environment and environment outside the family
7.4 Individual genes
7.5 Gene-environment interaction
8 Interventions
9 Music and IQ

9.1 Music lessons


9.1 Music lessons
10 IQ and brain anatomy
11 Health and IQ
12 Social outcomes

12.1 Other tests
12.2 School performance
12.3 Job performance
12.4 Income
12.5 IQ and crime
12.6 Other correlations with IQ
12.7 Real-life accomplishments


12.1 Other tests
12.2 School performance
12.3 Job performance
12.4 Income
12.5 IQ and crime
12.6 Other correlations with IQ
12.7 Real-life accomplishments
13 Group differences

13.1 Sex
13.2 Race


13.1 Sex
13.2 Race
14 Public policy
15 Criticism and views

15.1 Relation between IQ and intelligence
15.2 Criticism of g
15.3 Test bias
15.4 Outdated methodology
15.5 "Intelligence: Knowns and Unknowns"
15.6 Dynamic assessment


15.1 Relation between IQ and intelligence
15.2 Criticism of g
15.3 Test bias
15.4 Outdated methodology
15.5 "Intelligence: Knowns and Unknowns"
15.6 Dynamic assessment
16 High IQ societies
17 Reference charts
18 See also
19 References
20 Further reading
21 External links
1.1 Early history
1.2 General factor (g)
1.3 The War Years
1.4 CattellHornCarroll theory
1.5 Other theories
7.1 Heritability
7.2 Shared family environment
7.3 Non-shared family environment and environment outside the family
7.4 Individual genes
7.5 Gene-environment interaction
9.1 Music lessons
12.1 Other tests
12.2 School performance
12.3 Job performance
12.4 Income
12.5 IQ and crime
12.6 Other correlations with IQ
12.7 Real-life accomplishments
13.1 Sex
13.2 Race
15.1 Relation between IQ and intelligence
15.2 Criticism of g
15.3 Test bias
15.4 Outdated methodology
15.5 "Intelligence: Knowns and Unknowns"
15.6 Dynamic assessment
Fluid intelligence (Gf) includes the broad ability to reason, form concepts, and solve problems using unfamiliar information or novel procedures.
Crystallized intelligence (Gc) includes the breadth and depth of a person's acquired knowledge, the ability to communicate one's knowledge, and the ability to reason using previously learned experiences or procedures.
Quantitative reasoning (Gq) is the ability to comprehend quantitative concepts and relationships and to manipulate numerical symbols.
Reading and writing ability (Grw) includes basic reading and writing skills.
Short-term memory (Gsm) is the ability to apprehend and hold information in immediate awareness, and then use it within a few seconds.
Long-term storage and retrieval (Glr) is the ability to store information and fluently retrieve it later in the process of thinking.
Visual processing (Gv) is the ability to perceive, analyze, synthesize, and think with visual patterns, including the ability to store and recall visual representations.
Auditory processing (Ga) is the ability to analyze, synthesize, and discriminate auditory stimuli, including the ability to process and discriminate speech sounds that may be presented under distorted conditions.
Processing speed (Gs) is the ability to perform automatic cognitive tasks, particularly when measured under pressure to maintain focused attention.
Decision/reaction time/speed (Gt)reflects the immediacy with which an individual can react to stimuli or a task (typically measured in seconds or fractions of seconds; it is not to be confused with Gs, which typically is measured in intervals of 23 minutes). See Mental chronometry.
MDs, JDs, or PhDs 125+ (WAIS-R, 1987)
College graduates 112 (KAIT, 2000; K-BIT, 1992), 115 (WAIS-R)
13 years of college 104 (KAIT, K-BIT), 105-110 (WAIS-R)
Clerical and sales workers 100-105
High school graduates, skilled workers (e.g., electricians, cabinetmakers) 100 (KAIT, WAIS-R), 97 (K-BIT)
13 years of high school (completed 911 years of school) 94 (KAIT), 90 (K-BIT), 95 (WAIS-R)
Semi-skilled workers (e.g., truck drivers, factory workers) 90-95
Elementary school graduates (completed eighth grade) 90
Elementary school dropouts (completed 07 years of school) 80-85
Have 50/50 chance of reaching high school 75
Professional and technical 112
Managers and administrators 104
Clerical workers; sales workers; skilled workers, craftsmen, and foremen 101
Semi-skilled workers (operatives, service workers, including private household) 92
Unskilled workers 87
Adults can harvest vegetables, repair furniture 60
Adults can do domestic work 50
Child prodigy
Cultural intelligence
Curiosity quotient
Developmental disability
Educational quotient
Emotional quotient
Genius
Idiot
IQ and the Wealth of Nations
Intellectual giftedness
Late bloomer
Learning disability
Malleable intelligence
Nature versus nurture
Savant syndrome
Sentience quotient
Social IQ
Spiritual intelligence
Personality test
Carroll, J.B. (1993). Human cognitive abilities: A survey of factor-analytical studies. New York: Cambridge University Press. ISBN0-521-38275-0.
Lahn, Bruce T.; Ebenstein, Lanny (2009). "Let's celebrate human genetic diversity". Nature 461 (7265): 7268. doi:10.1038/461726a. PMID19812654.
Coward, W. Mark; Sackett, Paul R. (1990). "Linearity of ability^performance relationships: A reconfirmation". Journal of Applied Psychology 75 (3): 297300. doi:10.1037/0021-9010.75.3.297.
Duncan, J.; Seitz, RJ; Kolodny, J; Bor, D; Herzog, H; Ahmed, A; Newell, FN; Emslie, H (2000). "A Neural Basis for General Intelligence". Science 289 (5478): 45760. doi:10.1126/science.289.5478.457. PMID10903207.
Duncan, John; Burgess, Paul; Emslie, Hazel (1995). "Fluid intelligence after frontal lobe lesions". Neuropsychologia 33 (3): 2618. doi:10.1016/0028-3932(94)00124-8. PMID7791994.
Flynn, James R. (1999). "Searching for justice: The discovery of IQ gains over time". American Psychologist 54 (1): 520. doi:10.1037/0003-066X.54.1.5. http://www.stat.columbia.edu/~gelman/stuff_for_blog/flynn.pdf.
Frey, Meredith C.; Detterman, Douglas K. (2004). "Scholastic Assessment org?". Psychological Science 15 (6): 3738. doi:10.1111/j.0956-7976.2004.00687.x. PMID15147489.
Gale, C. R; Deary, I. J; Schoon, I.; Batty, G D.; Batty, G D. (2006). "IQ in childhood and vegetarianism in adulthood: 1970 British cohort study". BMJ 334 (7587): 245. doi:10.1136/bmj.39030.675069.55. PMC1790759. PMID17175567. //www.ncbi.nlm.nih.gov/pmc/articles/PMC1790759/.
Gottfredson, L (1997). "Why g matters: The complexity of everyday life". Intelligence 24 (1): 79132. doi:10.1016/S0160-2896(97)90014-3. http://www.udel.edu/educ/gottfredson/reprints/1997whygmatters.pdf.
Gottfredson, Linda S. (1998). "The general intelligence factor" (PDF). Scientific American Presents 9 (4): 2429. http://www.udel.edu/educ/gottfredson/reprints/1998generalintelligencefactor.pdf.
Gottfredson, L.S. (2005). "Suppressing intelligence research: Hurting those we intend to help.". In Wright, R.H. and Cummings, N.A (Eds.) (PDF). Destructive trends in mental health: The well-intentioned path to harm. New York: Taylor and Francis. pp.155186. ISBN0-415-95086-4. http://www.udel.edu/educ/gottfredson/reprints/2005suppressingintelligence.pdf.
Gottfredson, L.S. (2006). "Social consequences of group differences in cognitive ability (Consequencias sociais das diferencas de grupo em habilidade cognitiva)". In Flores-Mendoza, C.E. and Colom, R. (Eds.) (PDF). Introduo  psicologia das diferenas individuais. Porto Alegre, Brazil: ArtMed Publishers. pp.155186. ISBN85-363-0621-1. http://www.udel.edu/educ/gottfredson/reprints/2004socialconsequences.pdf.
Gould, S.J. (1996). W. W. Norton & Co.. ed. The Mismeasure of Man: Revised and Expanded Edition. New-York: Penguin. ISBN0-14-025824-8.
Gray, Jeremy R.; Chabris, Christopher F.; Braver, Todd S. (2003). "Neural mechanisms of general fluid intelligence". Nature Neuroscience 6 (3): 31622. doi:10.1038/nn1014. PMID12592404.
Gray, Jeremy R.; Thompson, Paul M. (2004). "Neurobiology of intelligence: science and ethics". Nature Reviews Neuroscience 5 (6): 47182. doi:10.1038/nrn1405. PMID15152197.
Haier, R; Jung, R; Yeo, R; Head, K; Alkire, M (2005). "The neuroanatomy of general intelligence: sex matters". NeuroImage 25 (1): 3207. doi:10.1016/j.neuroimage.2004.11.019. PMID15734366.
Harris, J.R. (1998). The Nurture Assumption: why children turn out the way they do. New York (NY): Free Press. ISBN0-684-84409-5.
Hunt, Earl (2001). "Multiple Views of Multiple Intelligence". PsycCRITIQUES 46 (1): 57. doi:10.1037/002513.
Jensen, A.R. (1979). Bias in mental testing. New York (NY): Free Press. ISBN0-02-916430-3.
Jensen, A.R. (1979). The g Factor: The Science of Mental Ability. Wesport (CT): Praeger Publishers. ISBN0-275-96103-6.
Jensen, A.R. (2006). Clocking the Mind: Mental Chronometry and Individual Differences.. Elsevier. ISBN0-08-044939-5.
Kaufman, Alan S. (2009). IQ Testing 101. New York (NY): Springer Publishing. ISBN978-0-8261-0629-2.
Klingberg, Torkel; Forssberg, Hans; Westerberg, Helena (2002). "Training of Working Memory in Children With ADHD". Journal of Clinical and Experimental Neuropsychology (Neuropsychology, Development and Cognition: Section A) 24 (6): 78191. doi:10.1076/jcen.24.6.781.8395. PMID12424652.
McClearn, G. E.; Johansson, B; Berg, S; Pedersen, NL; Ahern, F; Petrill, SA; Plomin, R (1997). "Substantial Genetic Influence on Cognitive Abilities in Twins 80or More Years Old". Science 276 (5318): 15603. doi:10.1126/science.276.5318.1560. PMID9171059.
Mingroni, M (2004). "The secular rise in IQ: Giving heterosis a closer look". Intelligence 32 (1): 6583. doi:10.1016/S0160-2896(03)00058-8.
Murray, C. (1998) (PDF). Income Inequality and IQ. Washington (DC): AEI Press. ISBN0-8447-7094-9. http://www.aei.org/docLib/20040302_book443.pdf.
Noguera, P.A (2001). "Racial politics and the elusive quest for excellence and equity in education". Motion Magazine. Article # ER010930002. http://www.inmotionmagazine.com/er/pnrp1.html.
Plomin, R.; DeFries, J.C.; Craig, I.W.; McGuffin, P (2003). Behavioral genetics in the postgenomic era. Washington (DC): American Psychological Association. ISBN1-55798-926-5.
Plomin, R.; DeFries, J.C.; McClearn, G.E.; McGuffin, P (2000). Behavioral genetics (4th ed.). New York (NY): Worth Publishers. ISBN0-7167-5159-3.
Rowe, D.C.; Vesterdal, W.J.; Rodgers, J.L. (1997). The Bell Curve Revisited: How Genes and Shared Environment Mediate IQ-SES Associations.[verification needed]
Schoenemann, P Thomas; Sheehan, Michael J; Glotzer, L Daniel (2005). "Prefrontal white matter volume is disproportionately larger in humans than in other primates". Nature Neuroscience 8 (2): 24252. doi:10.1038/nn1394. PMID15665874.
Shaw, P.; Greenstein, D.; Lerch, J.; Clasen, L.; Lenroot, R.; Gogtay, N.; Evans, A.; Rapoport, J. et al. (2006). "Intellectual ability and cortical development in children and adolescents". Nature 440 (7084): 6769. doi:10.1038/nature04513. PMID16572172.
Tambs, Kristian; Sundet, Jon Martin; Magnus, Per; Berg, Kre (1989). "Genetic and environmental contributions to the covariance between occupational status, educational attainment, and IQ: A study of twins". Behavior Genetics 19 (2): 20922. doi:10.1007/BF01065905. PMID2719624.
Thompson, Paul M.; Cannon, Tyrone D.; Narr, Katherine L.; Van Erp, Theo; Poutanen, Veli-Pekka; Huttunen, Matti; Lnnqvist, Jouko; Standertskjld-Nordenstam, Carl-Gustaf et al. (2001). "Genetic influences on brain structure". Nature Neuroscience 4 (12): 12538. doi:10.1038/nn758. PMID11694885.
Wechsler, D. (1997). Wechsler Adult Intelligence Scale (3rd ed.). San Antonia (TX): The Psychological Corporation.
Wechsler, D. (2003). Wechsler Intelligence Scale for Children (4th ed.). San Antonia (TX): The Psychological Corporation.
Weiss, Volkmar (2009). "National IQ means transformed from Programme for International Student Assessment (PISA) Scores". The Journal of Social, Political and Economic Studies 31 (1): 7194. http://mpra.ub.uni-muenchen.de/14600/.
Human Intelligence: biographical profiles, current controversies, resources for teachers
Classics in the History of Psychology
Free online IQ test
.
#(`*Thinking, Fast and Slow*`)#.
Thinking, Fast and Slow is a 2011 book by Nobel Prize winner in Economics Daniel Kahneman which summarizes research that he conducted over decades, often in collaboration with Amos Tversky.[1][2] It covers all three phases of his career: his early days working on cognitive bias, his work on prospect theory, and his later work on happiness.
The book's central thesis is a dichotomy between two modes of thought: System 1 is fast, instinctive and emotional; System 2 is slower, more deliberative, and more logical. The book delineates cognitive biases associated with each type of thinking, starting with Kahneman's own research on loss aversion. From framing choices to substitution, the book highlights several decades of academic research to suggest that we place too much confidence in human judgment.
The basis for his Nobel, Kahneman developed prospect theory to account for experimental errors he noticed in Daniel Bernoulli's traditional Utility theory. This theory makes logical assumptions that do not reflect peoples actual choices because it doesnt take into account behavioral biases.
For example, one might reasonably assume that an individual would place twice as much value on a 20% chance of winning a prize as opposed to a 10% chance, but experiments show otherwise. As shown in the figure below, humans are more likely to act to avoid loss than to achieve a gain. For example, people generally ascribe a different absolute change in value to a 10% chance of a loss as opposed to a 10% chance of a gain. They also consider the reference point in deciding how much to value either one. Thus, a 10% change in probability has a greater value to most people if it changes the probability from 0% to 10% than if it changes the probability from 90% to 100%.
In the book's first section, Kahneman describes the two different ways the brain forms thoughts:
Kahneman covers a number of experiments which purport to highlight the differences between these two thought processes, and how they arrive at different results even given the same inputs. Terms and concepts include coherence, attention, laziness, association, jumping to conclusions and how one forms judgements.
The second section offers explanations for why humans struggle to think statistically. It begins by documenting a variety of situations in which we either arrive at binary decisions or fail to precisely associate reasonable probabilities to outcomes. Kahneman explains this phenomenon using the theory of Heuristics.
Kahneman uses Heuristics to assert that System 1 thinking involves associating new information with existing patterns, or thoughts, rather than creating new patterns for each new experience. For example, a child who has only seen shapes with straight edges would experience an octagon rather than a triangle when first viewing a circle. In a legal metaphor, a judge limited to heuristic thinking would only be able to think of similar historical cases when presented with a new dispute, rather than seeing the unique aspects of that case. In addition to offering an explanation for the statistical problem, the theory also offers an explanation for human biases.
The anchoring effect names our tendency to be influenced by irrelevant numbers. Shown higher/lower numbers, experimental subjects gave higher/lower responses. Experiment: experienced German judges proposed longer sentences if they had just rolled a pair of dice loaded to give a high number.[1]
System 1 is prone to substituting a simple question for a more difficult one. In what Kahneman calls their best-known and most controversial experiment, the Linda problem. Experiment: Subjects were told about an imaginary Linda, young, single, outspoken and very bright, who, as a student, was deeply concerned with discrimination and social justice. They asked whether it was more probable that Linda is a bank teller or that she is a bank teller and an active feminist. The overwhelming response was that feminist bank teller was more likely than bank teller, violating the laws of probability. (Every feminist bank teller is a bank teller.) In this case System 1 substituted the easier question, "Is Linda a feminist?" dropping the occupation qualifier. An alternative view is that the subjects added an unstated implicature to the effect that the other answer implied that Linda was not a feminist.[1]
Kahneman writes of a "pervasive optimistic bias", which may well be the most significant of the cognitive biases. This bias generates the illusion of control, that we have substantial control of our lives. This bias may be usefully adaptive. Optimists are more psychologically resilient, have stronger immune systems, and live longer on average than more reality-based opposites. Optimism protects from loss aversion: our tendency to fear losses more than we value gains.[1]
A natural experiment reveals the prevalence of one kind of unwarranted optimism. The planning fallacy is the tendency to overestimate benefits and underestimate costs, impelling people to take on risky projects. In 2002, American kitchen remodeling was expected to average $18,658 on average, but cost $38,769.[1]
To explain overconfidence, Kahneman introduces the concept he labels What You See Is All There Is (WYSIATI). This theory states that when the mind makes decisions, it deals primarily with Known Knowns, phenomena it has already observed. It rarely considers Known Unknowns, phenomena that it knows to be relevant but about which it has no information. Finally it appears oblivious to the possibility of Unknown Unknowns, unknown phenomena of unknown relevance.
He explains that humans fail to take into account complexity and that their understanding of the world consists of a small and not necessarily representative set of observations. Furthermore, the mind generally does not account for the role of chance and therefore falsely assumes that a future event will mirror a past event.
Framing is the context in which choices are presented. Experiment: subjects were asked whether they would opt for surgery if the survival rate is 90 percent, while others were told that the mortality rate is 10 percent. The first framing increased acceptance, even though the situation was no different.[3]
Rather than consider the odds that an incremental investment would produce a positive return, people tend to "throw good money after bad" and continue investing in projects with poor prospects that have already consumed significant resources. In part this is to avoid feelings of regret.[3]
In this section Kahneman returns to economics and expands his seminal work on Prospect Theory. He discusses the tendency for problems to be addressed in isolation and how, when other reference points are considered, the choice of that reference point (called a frame) has a disproportionate impact on the outcome. In what appears to be his area of greatest intellectual comfort, he also offers advice on how some of the shortcomings of System 1 thinking can be avoided.
Evolution teaches that traits persist and develop because they increase fitness. One possible hypothesis is that our conceptual biases are adaptive, as are our rational faculties. Kahneman offers happiness as one quality that our thinking process nurtures. Kahneman first took up this question in the 1990s. At the time most happiness research relied on polls about life satisfaction.
Kahneman proposed an alternate measure that assessed pleasure or pain sampled from moment to moment, and then summed over time. Kahneman called this experienced well-being and attached it to a separate "self". He distinguished this from the remembered well-being that the polls had attempted to measure. He found that these two measures of happiness diverged. His major discovery was that the remembering self does not care about the duration of a pleasant or unpleasant experience. Rather, it retrospectively rates an experience by the peak (valley) of the experience, and by the way it ends. Further, the remembering self dominated the patient's ultimate conclusion.
Kahneman demonstrated the principle using two groups of patients undergoing painful colonoscopies. Group A got the normal procedure. Group B, unknowingly received a few extra minutes of less painful discomfort after the end of the examination, i.e., more total discomfort. However, since Group Bs procedure ended less painfully, the patients in this group retrospectively minded the whole affair less.
1 Prospect Theory
2 Two systems
3 Heuristics and biases

3.1 Anchoring
3.2 Substitution
3.3 Optimism and loss aversion
3.4 Framing
3.5 Sunk-cost


3.1 Anchoring
3.2 Substitution
3.3 Optimism and loss aversion
3.4 Framing
3.5 Sunk-cost
4 Choices
5 Rationality and happiness

5.1 Two selves


5.1 Two selves
6 Awards and honors
7 References
3.1 Anchoring
3.2 Substitution
3.3 Optimism and loss aversion
3.4 Framing
3.5 Sunk-cost
5.1 Two selves
System 1: Fast, automatic, frequent, emotional, stereotypic, subconscious
System 2: Slow, effortful, infrequent, logical, calculating, conscious
2011 Los Angeles Times Book Prize (Current Interest)
Kahneman, D., Tversky, A. (1979)."Prospect Theory: An Analysis of Decision Under Risk". Economitrica, 47 (2), pp.263-291.
.
#(`*Albert Einstein*`)#.
Albert Einstein (/lbrt anstan/; German: [albt antan]( listen); 14 March 1879  18 April 1955) was a German-born theoretical physicist who developed the general theory of relativity, effecting a revolution in physics. For this achievement, Einstein is often regarded as the father of modern physics[2][3] and the most influential physicist of the 20th century. While best known for his massenergy equivalence formula E = mc2 (which has been dubbed "the world's most famous equation"),[4] he received the 1921 Nobel Prize in Physics "for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect".[5] The latter was pivotal in establishing quantum theory.
Near the beginning of his career, Einstein thought that Newtonian mechanics was no longer enough to reconcile the laws of classical mechanics with the laws of the electromagnetic field. This led to the development of his special theory of relativity. He realized, however, that the principle of relativity could also be extended to gravitational fields, and with his subsequent theory of gravitation in 1916, he published a paper on the general theory of relativity. He continued to deal with problems of statistical mechanics and quantum theory, which led to his explanations of particle theory and the motion of molecules. He also investigated the thermal properties of light which laid the foundation of the photon theory of light. In 1917, Einstein applied the general theory of relativity to model the structure of the universe as a whole.[6]
He was visiting the United States when Adolf Hitler came to power in 1933, and did not go back to Germany, where he had been a professor at the Berlin Academy of Sciences. He settled in the U.S., becoming a citizen in 1940.[7] On the eve of World War II, he helped alert President Franklin D. Roosevelt that Germany might be developing an atomic weapon, and recommended that the U.S. begin similar research; this eventually led to what would become the Manhattan Project. Einstein was in support of defending the Allied forces, but largely denounced using the new discovery of nuclear fission as a weapon. Later, with the British philosopher Bertrand Russell, Einstein signed the RussellEinstein Manifesto, which highlighted the danger of nuclear weapons. Einstein was affiliated with the Institute for Advanced Study in Princeton, New Jersey, until his death in 1955.
Einstein published more than 300 scientific papers along with over 150 non-scientific works.[6][8] His great intellectual achievements and originality have made the word "Einstein" synonymous with genius.[9]
Albert Einstein was born in Ulm, in the Kingdom of Wrttemberg in the German Empire on 14March 1879.[10] His father was Hermann Einstein, a salesman and engineer. His mother was Pauline Einstein (ne Koch). In 1880, the family moved to Munich, where his father and his uncle founded Elektrotechnische Fabrik J. Einstein & Cie, a company that manufactured electrical equipment based on direct current.[10]
The Einsteins were non-observant Jews. Albert attended a Catholic elementary school from the age of five for three years. Later, at the age of eight, Einstein was transferred to the Luitpold Gymnasium where he received advanced primary and secondary school education until he left Germany seven years later.[11] Although it has been thought that Einstein had early speech difficulties, this is disputed by the Albert Einstein Archives, and he excelled at the first school that he attended.[12] He was right handed;[12][13] there appears to be no evidence for the widespread popular belief[14] that he was left handed.
His father once showed him a pocket compass; Einstein realized that there must be something causing the needle to move, despite the apparent "empty space".[15] As he grew, Einstein built models and mechanical devices for fun and began to show a talent for mathematics.[10] When Einstein was ten years old, Max Talmud (later changed to Max Talmey), a poor Jewish medical student from Poland, was introduced to the Einstein family by his brother, and during weekly visits over the next five years, he gave the boy popular books on science, mathematical texts and philosophical writings. These included Immanuel Kant's Critique of Pure Reason, and Euclid's Elements (which Einstein called the "holy little geometry book").[16][17][fn 1]
In 1894, his father's company failed: direct current (DC) lost the War of Currents to alternating current (AC). In search of business, the Einstein family moved to Italy, first to Milan and then, a few months later, to Pavia. When the family moved to Pavia, Einstein stayed in Munich to finish his studies at the Luitpold Gymnasium. His father intended for him to pursue electrical engineering, but Einstein clashed with authorities and resented the school's regimen and teaching method. He later wrote that the spirit of learning and creative thought were lost in strict rote learning. At the end of December 1894, he travelled to Italy to join his family in Pavia, convincing the school to let him go by using a doctor's note.[19] It was during his time in Italy that he wrote a short essay with the title "On the Investigation of the State of the Ether in a Magnetic Field."[20][21]
In late summer 1895, at the age of sixteen, Einstein sat the entrance examinations for the Swiss Federal Polytechnic in Zurich (later the Eidgenssische Polytechnische Schule). He failed to reach the required standard in several subjects, but obtained exceptional grades in physics and mathematics.[22] On the advice of the Principal of the Polytechnic, he attended the Aargau Cantonal School in Aarau, Switzerland, in 1895-96 to complete his secondary schooling. While lodging with the family of Professor Jost Winteler, he fell in love with Winteler's daughter, Marie. (His sister Maja later married the Wintelers' son, Paul.)[23] In January 1896, with his father's approval, he renounced his citizenship in the German Kingdom of Wrttemberg to avoid military service.[24] (He acquired Swiss citizenship five years later, in February 1901.)[25] In September 1896, he passed the Swiss Matura with mostly good grades (including a top grade of 6 in physics and mathematical subjects, on a scale of 1-6),[26] and, though only seventeen, enrolled in the four-year mathematics and physics teaching diploma program at the ETH Zurich. Marie Winteler moved to Olsberg, Switzerland for a teaching post.
Einstein's future wife, Mileva Mari, also enrolled at the Polytechnic that same year, the only woman among the six students in the mathematics and physics section of the teaching diploma course. Over the next few years, Einstein and Mari's friendship developed into romance, and they read books together on extra-curricular physics in which Einstein was taking an increasing interest. In 1900, Einstein was awarded the Zurich Polytechnic teaching diploma, but Mari failed the examination with a poor grade in the mathematics component, theory of functions.[27] There have been claims that Mari collaborated with Einstein on his celebrated 1905 papers,[28][29] but historians of physics who have studied the issue find no evidence that she made any substantive contributions.[30][31][32][33]
In early 1902, Einstein and Mari had a daughter they named Lieserl, born in Novi Sad where Mari was staying with her parents. Her fate is unknown, but the contents of a letter Einstein wrote to Mari in September 1903 suggest that she was either adopted or died of scarlet fever in infancy.[34][35]
Einstein and Mari married in January 1903. In May 1904, the couple's first son, Hans Albert Einstein, was born in Bern, Switzerland. Their second son, Eduard, was born in Zurich in July 1910. In 1914, Einstein moved to Berlin, while his wife remained in Zurich with their sons. They divorced on 14 February 1919, having lived apart for five years.
Einstein married Elsa Lwenthal (ne Einstein) on 2 June 1919, after having had a relationship with her since 1912. She was his first cousin maternally and his second cousin paternally. In 1933, they emigrated to the United States. In 1935, Elsa Einstein was diagnosed with heart and kidney problems and died in December 1936.[36]
After graduating, Einstein spent almost two frustrating years searching for a teaching post, but a former classmate's father helped him secure a job in Bern, at the Federal Office for Intellectual Property, the patent office, as an assistant examiner.[37] He evaluated patent applications for electromagnetic devices. In 1903, Einstein's position at the Swiss Patent Office became permanent, although he was passed over for promotion until he "fully mastered machine technology".[38]
Much of his work at the patent office related to questions about transmission of electric signals and electrical-mechanical synchronization of time, two technical problems that show up conspicuously in the thought experiments that eventually led Einstein to his radical conclusions about the nature of light and the fundamental connection between space and time.[39]
With a few friends he met in Bern, Einstein started a small discussion group, self-mockingly named "The Olympia Academy", which met regularly to discuss science and philosophy. Their readings included the works of Henri Poincar, Ernst Mach, and David Hume, which influenced his scientific and philosophical outlook.
In 1901, his paper "Folgerungen aus den Kapillaritt Erscheinungen" ("Conclusions from the Capillarity Phenomena") was published in the prestigious Annalen der Physik.[40] On 30 April 1905, Einstein completed his thesis, with Alfred Kleiner, Professor of Experimental Physics, serving as pro-forma advisor. Einstein was awarded a PhD by the University of Zurich. His dissertation was entitled "A New Determination of Molecular Dimensions".[41][42] That same year, which has been called Einstein's annus mirabilis (miracle year), he published four groundbreaking papers, on the photoelectric effect, Brownian motion, special relativity, and the equivalence of mass and energy, which were to bring him to the notice of the academic world.
By 1908, he was recognized as a leading scientist, and he was appointed lecturer at the University of Bern. The following year, he quit the patent office and the lectureship to take the position of physics docent [43] at the University of Zurich. He became a full professor at Karl-Ferdinand University in Prague in 1911. In 1914, he returned to Germany after being appointed director of the Kaiser Wilhelm Institute for Physics (19141932)[44] and a professor at the Humboldt University of Berlin, with a special clause in his contract that freed him from most teaching obligations. He became a member of the Prussian Academy of Sciences. In 1916, Einstein was appointed president of the German Physical Society (19161918).[45][46]
During 1911, he had calculated that, based on his new theory of general relativity, light from another star would be bent by the Sun's gravity. That prediction was claimed confirmed by observations made by a British expedition led by Sir Arthur Eddington during the solar eclipse of 29 May 1919. International media reports of this made Einstein world famous. On 7 November 1919, the leading British newspaper The Times printed a banner headline that read: "Revolution in Science New Theory of the Universe Newtonian Ideas Overthrown".[47] Much later, questions were raised whether the measurements had been accurate enough to support Einstein's theory. In 1980 historians John Earman and Clark Glymour published an analysis suggesting that Eddington had suppressed unfavorable results.[48] The two reviewers found possible flaws in Eddington's selection of data, but their doubts, although widely quoted and, indeed, now with a "mythical" status almost equivalent to the status of the original observations, have not been confirmed.[49][50] Eddington's selection from the data seems valid and his team indeed made astronomical measurements verifying the theory.[51]
In 1921, Einstein was awarded the Nobel Prize in Physics for his explanation of the photoelectric effect, as relativity was considered still somewhat controversial. He also received the Copley Medal from the Royal Society in 1925.
Einstein visited New York City for the first time on 2 April 1921, where he received an official welcome by the Mayor, followed by three weeks of lectures and receptions. He went on to deliver several lectures at Columbia University and Princeton University, and in Washington he accompanied representatives of the National Academy of Science on a visit to the White House. On his return to Europe he was the guest of the British statesman and philosopher Viscount Haldane in London, where he met several renowned scientific, intellectual and political figures, and delivered a lecture at King's College.[52]
In 1922, he traveled throughout Asia and later to Palestine, as part of a six-month excursion and speaking tour. His travels included Singapore, Ceylon, and Japan, where he gave a series of lectures to thousands of Japanese. His first lecture in Tokyo lasted four hours, after which he met the emperor and empress at the Imperial Palace where thousands came to watch. Einstein later gave his impressions of the Japanese in a letter to his sons:[53]:307 "Of all the people I have met, I like the Japanese most, as they are modest, intelligent, considerate, and have a feel for art."[53]:308
On his return voyage, he also visited Palestine for 12 days in what would become his only visit to that region. "He was greeted with great British pomp, as if he were a head of state rather than a theoretical physicist", writes Isaacson. This included a cannon salute upon his arrival at the residence of the British high commissioner, Sir Herbert Samuel. During one reception given to him, the building was "stormed by throngs who wanted to hear him". In Einstein's talk to the audience, he expressed his happiness over the event:
I consider this the greatest day of my life. Before, I have always found something to regret in the Jewish soul, and that is the forgetfulness of its own people. Today, I have been made happy by the sight of the Jewish people learning to recognize themselves and to make themselves recognized as a force in the world.[54]:308
In February 1933 while on a visit to the United States, Einstein decided not to return to Germany due to the rise to power of the Nazis under Germany's new chancellor.[55][56] He visited American universities in early 1933 where he undertook his third two-month visiting professorship at the California Institute of Technology in Pasadena. He and his wife Elsa returned by ship to Belgium at the end of March. During the voyage they were informed that their cottage was raided by the Nazis and his personal sailboat had been confiscated. Upon landing in Antwerp on 28 March, he immediately went to the German consulate where he turned in his passport and formally renounced his German citizenship.[54]
In early April, he learned that the new German government had passed laws barring Jews from holding any official positions, including teaching at universities.[54] A month later, Einstein's works were among those targeted by Nazi book burnings, and Nazi propaganda minister Joseph Goebbels proclaimed, "Jewish intellectualism is dead."[54] Einstein also learned that his name was on a list of assassination targets, with a "$5,000 bounty on his head."[54] One German magazine included him in a list of enemies of the German regime with the phrase, "not yet hanged".[54]
He resided in Belgium for some months, before temporarily living in England.[57][58] In a letter to his friend, physicist Max Born, who also emigrated from Germany and lived in England, Einstein wrote, ". . . I must confess that the degree of their brutality and cowardice came as something of a surprise."[54]
In October 1933 he returned to the U.S. and took up a position at the Institute for Advanced Study at Princeton, New Jersey, that required his presence for six months each year.[59][60] He was still undecided on his future (he had offers from European universities, including Oxford), but in 1935 he arrived at the decision to remain permanently in the United States and apply for citizenship.[61][62] His affiliation with the Institute for Advance Studies would last until his death in 1955.[63] He was one of the four first selected (two of the others being John von Neumann and Kurt Gdel) at the new Institute, where he soon developed a close friendship with Gdel. The two would take long walks together discussing their work. His last assistant was Bruria Kaufman, who later became a renowned physicist. During this period, Einstein tried to develop a unified field theory and to refute the accepted interpretation of quantum physics, both unsuccessfully.
Other scientists also fled to America. Among them were Nobel laureates and professors of theoretical physics. With so many other Jewish scientists now forced by circumstances to live in America, often working side by side, Einstein wrote to a friend, "For me the most beautiful thing is to be in contact with a few fine Jewsa few millennia of a civilized past do mean something after all." In another letter he writes, "In my whole life I have never felt so Jewish as now."[54]
In 1939, a group of Hungarian scientists that included emigre physicist Le Szilrd attempted to alert Washington of ongoing Nazi atomic bomb research. The group's warnings were discounted.[64] Einstein and Szilrd, along with other refugees such as Edward Teller and Eugene Wigner, "regarded it as their responsibility to alert Americans to the possibility that German scientists might win the race to build an atomic bomb, and to warn that Hitler would be more than willing to resort to such a weapon."[53]:630[65] In the summer of 1939, a few months before the beginning of World War II in Europe, Einstein was persuaded to lend his prestige by writing a letter with Szilrd to President Franklin D. Roosevelt to alert him of the possibility. The letter also recommended that the U.S. government pay attention to and become directly involved in uranium research and associated chain reaction research.
The letter is believed to be "arguably the key stimulus for the U.S. adoption of serious investigations into nuclear weapons on the eve of the U.S. entry into World War II".[66] President Roosevelt could not take the risk of allowing Hitler to possess atomic bombs first. As a result of Einstein's letter and his meetings with Roosevelt, the U.S. entered the "race" to develop the bomb, drawing on its "immense material, financial, and scientific resources" to initiate the Manhattan Project. It became the only country to successfully develop an atomic bomb during World War II.
For Einstein, "war was a disease . . . [and] he called for resistance to war." But in 1933, after Hitler assumed full power in Germany, "he renounced pacifism altogether . . . In fact, he urged the Western powers to prepare themselves against another German onslaught."[67]:110 In 1954, a year before his death, Einstein said to his old friend, Linus Pauling, "I made one great mistake in my life when I signed the letter to President Roosevelt recommending that atom bombs be made; but there was some justification the danger that the Germans would make them..."[68]
Einstein became an American citizen in 1940. Not long after settling into his career at Princeton, he expressed his appreciation of the "meritocracy" in American culture when compared to Europe. According to Isaacson, he recognized the "right of individuals to say and think what they pleased", without social barriers, and as result, the individual was "encouraged" to be more creative, a trait he valued from his own early education. Einstein writes:
What makes the new arrival devoted to this country is the democratic trait among the people. No one humbles himself before another person or class. . . American youth has the good fortune not to have its outlook troubled by outworn traditions.[54]:432
As a member of the National Association for the Advancement of Colored People (NAACP) at Princeton who campaigned for the civil rights of African Americans, Einstein corresponded with civil rights activist W. E. B. Du Bois, and in 1946 Einstein called racism America's "worst disease".[69] He later stated, "Race prejudice has unfortunately become an American tradition which is uncritically handed down from one generation to the next. The only remedies are enlightenment and education".[70]
During the final stage of his life, Einstein transitioned to a vegetarian lifestyle,[71] arguing that "the vegetarian manner of living by its purely physical effect on the human temperament would most beneficially influence the lot of mankind".[72]
After the death of Israel's first president, Chaim Weizmann, in November 1952, Prime Minister David Ben-Gurion offered Einstein the position of President of Israel, a mostly ceremonial post.[73] The offer was presented by Israel's ambassador in Washington, Abba Eban, who explained that the offer "embodies the deepest respect which the Jewish people can repose in any of its sons".[53]:522 However, Einstein declined, and wrote in his response that he was "deeply moved", and "at once saddened and ashamed" that he could not accept it:
All my life I have dealt with objective matters, hence I lack both the natural aptitude and the experience to deal properly with people and to exercise official function. I am the more distressed over these circumstances because my relationship with the Jewish people became my strongest human tie once I achieved complete clarity about our precarious position among the nations of the world.[53]:522[73][74]
On 17 April 1955, Albert Einstein experienced internal bleeding caused by the rupture of an abdominal aortic aneurysm, which had previously been reinforced surgically by Dr. Rudolph Nissen in 1948.[75] He took the draft of a speech he was preparing for a television appearance commemorating the State of Israel's seventh anniversary with him to the hospital, but he did not live long enough to complete it.[76] Einstein refused surgery, saying: "I want to go when I want. It is tasteless to prolong life artificially. I have done my share, it is time to go. I will do it elegantly."[77] He died in Princeton Hospital early the next morning at the age of 76, having continued to work until near the end.
During the autopsy, the pathologist of Princeton Hospital, Thomas Stoltz Harvey, removed Einstein's brain for preservation without the permission of his family, in the hope that the neuroscience of the future would be able to discover what made Einstein so intelligent.[78] Einstein's remains were cremated and his ashes were scattered at an undisclosed location.[79][80]
In his lecture at Einstein's memorial, nuclear physicist Robert Oppenheimer summarized his impression of him as a person: "He was almost wholly without sophistication and wholly without worldliness . . . There was always with him a wonderful purity at once childlike and profoundly stubborn."[67]
Throughout his life, Einstein published hundreds of books and articles.[8][10] In addition to the work he did by himself he also collaborated with other scientists on additional projects including the BoseEinstein statistics, the Einstein refrigerator and others.[81]
The Annus Mirabilis papers are four articles pertaining to the photoelectric effect (which gave rise to quantum theory), Brownian motion, the special theory of relativity, and E = mc2 that Albert Einstein published in the Annalen der Physik scientific journal in 1905. These four works contributed substantially to the foundation of modern physics and changed views on space, time, and matter. The four papers are:
Albert Einstein's first paper[86] submitted in 1900 to Annalen der Physik was on capillary attraction. It was published in 1901 with the title "Folgerungen aus den Kapillaritt Erscheinungen," which translates as "Conclusions from the capillarity phenomena". Two papers he published in 19021903 (thermodynamics) attempted to interpret atomic phenomena from a statistical point of view. These papers were the foundation for the 1905 paper on Brownian motion, which showed that Brownian movement can be construed as firm evidence that molecules exist. His research in 1903 and 1904 was mainly concerned with the effect of finite atomic size on diffusion phenomena.[86]
He articulated the principle of relativity. This was understood by Hermann Minkowski to be a generalization of rotational invariance from space to space-time. Other principles postulated by Einstein and later vindicated are the principle of equivalence and the principle of adiabatic invariance of the quantum number.
Einstein's "Zur Elektrodynamik bewegter Krper" ("On the Electrodynamics of Moving Bodies") was received on 30 June 1905 and published 26 September of that same year. It reconciles Maxwell's equations for electricity and magnetism with the laws of mechanics, by introducing major changes to mechanics close to the speed of light. This later became known as Einstein's special theory of relativity.
Consequences of this include the time-space frame of a moving body appearing to slow down and contract (in the direction of motion) when measured in the frame of the observer. This paper also argued that the idea of a luminiferous aether one of the leading theoretical entities in physics at the time was superfluous.[87]
In his paper on massenergy equivalence Einstein produced E=mc2 from his special relativity equations.[88] Einstein's 1905 work on relativity remained controversial for many years, but was accepted by leading physicists, starting with Max Planck.[89][90]
In a 1905 paper,[91] Einstein postulated that light itself consists of localized particles (quanta). Einstein's light quanta were nearly universally rejected by all physicists, including Max Planck and Niels Bohr. This idea only became universally accepted in 1919, with Robert Millikan's detailed experiments on the photoelectric effect, and with the measurement of Compton scattering.
Einstein concluded that each wave of frequency f is associated with a collection of photons with energy hf each, where h is Planck's constant. He does not say much more, because he is not sure how the particles are related to the wave. But he does suggest that this idea would explain certain experimental results, notably the photoelectric effect.[92]
In 1907 Einstein proposed a model of matter where each atom in a lattice structure is an independent harmonic oscillator. In the Einstein model, each atom oscillates independently a series of equally spaced quantized states for each oscillator. Einstein was aware that getting the frequency of the actual oscillations would be different, but he nevertheless proposed this theory because it was a particularly clear demonstration that quantum mechanics could solve the specific heat problem in classical mechanics. Peter Debye refined this model.[93]
Throughout the 1910s, quantum mechanics expanded in scope to cover many different systems. After Ernest Rutherford discovered the nucleus and proposed that electrons orbit like planets, Niels Bohr was able to show that the same quantum mechanical postulates introduced by Planck and developed by Einstein would explain the discrete motion of electrons in atoms, and the periodic table of the elements.
Einstein contributed to these developments by linking them with the 1898 arguments Wilhelm Wien had made. Wien had shown that the hypothesis of adiabatic invariance of a thermal equilibrium state allows all the blackbody curves at different temperature to be derived from one another by a simple shifting process. Einstein noted in 1911 that the same adiabatic principle shows that the quantity which is quantized in any mechanical motion must be an adiabatic invariant. Arnold Sommerfeld identified this adiabatic invariant as the action variable of classical mechanics. The law that the action variable is quantized was a basic principle of the quantum theory as it was known between 1900 and 1925.[citation needed]
Although the patent office promoted Einstein to Technical Examiner Second Class in 1906, he had not given up on academia. In 1908, he became a privatdozent at the University of Bern.[94] In "ber die Entwicklung unserer Anschauungen ber das Wesen und die Konstitution der Strahlung" ("The Development of Our Views on the Composition and Essence of Radiation"), on the quantization of light, and in an earlier 1909 paper, Einstein showed that Max Planck's energy quanta must have well-defined momenta and act in some respects as independent, point-like particles. This paper introduced the photon concept (although the name photon was introduced later by Gilbert N. Lewis in 1926) and inspired the notion of waveparticle duality in quantum mechanics.
Einstein returned to the problem of thermodynamic fluctuations, giving a treatment of the density variations in a fluid at its critical point. Ordinarily the density fluctuations are controlled by the second derivative of the free energy with respect to the density. At the critical point, this derivative is zero, leading to large fluctuations. The effect of density fluctuations is that light of all wavelengths is scattered, making the fluid look milky white. Einstein relates this to Raleigh scattering, which is what happens when the fluctuation size is much smaller than the wavelength, and which explains why the sky is blue.[95] Einstein quantitatively derived critical opalescence from a treatment of density fluctuations, and demonstrated how both the effect and Rayleigh scattering originate from the atomistic constitution of matter.
Einstein's physical intuition led him to note that Planck's oscillator energies had an incorrect zero point. He modified Planck's hypothesis by stating that the lowest energy state of an oscillator is equal to 12hf, to half the energy spacing between levels. This argument, which was made in 1913 in collaboration with Otto Stern, was based on the thermodynamics of a diatomic molecule which can split apart into two free atoms.
General relativity (GR) is a theory of gravitation that was developed by Albert Einstein between 1907 and 1915. According to general relativity, the observed gravitational attraction between masses results from the warping of space and time by those masses. General relativity has developed into an essential tool in modern astrophysics. It provides the foundation for the current understanding of black holes, regions of space where gravitational attraction is so strong that not even light can escape.
As Albert Einstein later said, the reason for the development of general relativity was that the preference of inertial motions within special relativity was unsatisfactory, while a theory which from the outset prefers no state of motion (even accelerated ones) should appear more satisfactory.[96] So in 1908 he published an article on acceleration under special relativity. In that article, he argued that free fall is really inertial motion, and that for a freefalling observer the rules of special relativity must apply. This argument is called the Equivalence principle. In the same article, Einstein also predicted the phenomenon of gravitational time dilation. In 1911, Einstein published another article expanding on the 1907 article, in which additional effects such as the deflection of light by massive bodies were predicted.
While developing general relativity, Einstein became confused about the gauge invariance in the theory. He formulated an argument that led him to conclude that a general relativistic field theory is impossible. He gave up looking for fully generally covariant tensor equations, and searched for equations that would be invariant under general linear transformations only.
In June 1913 the Entwurf ("draft") theory was the result of these investigations. As its name suggests, it was a sketch of a theory, with the equations of motion supplemented by additional gauge fixing conditions. Simultaneously less elegant and more difficult than general relativity, after more than two years of intensive work Einstein abandoned the theory in November 1915 after realizing that the hole argument was mistaken.[97]
In 1917, Einstein applied the General theory of relativity to model the structure of the universe as a whole. He wanted the universe to be eternal and unchanging, but this type of universe is not consistent with relativity. To fix this, Einstein modified the general theory by introducing a new notion, the cosmological constant. With a positive cosmological constant, the universe could be an eternal static sphere.[98]
Einstein believed a spherical static universe is philosophically preferred, because it would obey Mach's principle. He had shown that general relativity incorporates Mach's principle to a certain extent in frame dragging by gravitomagnetic fields, but he knew that Mach's idea would not work if space goes on forever. In a closed universe, he believed that Mach's principle would hold. Mach's principle has generated much controversy over the years.
Einstein was displeased with quantum theory and mechanics, despite its acceptance by other physicists, stating "God doesn't play with dice." As Einstein passed away at the age of 76 he still would not accept quantum theory.[99] In 1917, at the height of his work on relativity, Einstein published an article in Physikalische Zeitschrift that proposed the possibility of stimulated emission, the physical process that makes possible the maser and the laser.[100] This article showed that the statistics of absorption and emission of light would only be consistent with Planck's distribution law if the emission of light into a mode with n photons would be enhanced statistically compared to the emission of light into an empty mode. This paper was enormously influential in the later development of quantum mechanics, because it was the first paper to show that the statistics of atomic transitions had simple laws. Einstein discovered Louis de Broglie's work, and supported his ideas, which were received skeptically at first. In another major paper from this era, Einstein gave a wave equation for de Broglie waves, which Einstein suggested was the HamiltonJacobi equation of mechanics. This paper would inspire Schrdinger's work of 1926.
In 1924, Einstein received a description of a statistical model from Indian physicist Satyendra Nath Bose, based on a counting method that assumed that light could be understood as a gas of indistinguishable particles. Einstein noted that Bose's statistics applied to some atoms as well as to the proposed light particles, and submitted his translation of Bose's paper to the Zeitschrift fr Physik. Einstein also published his own articles describing the model and its implications, among them the BoseEinstein condensate phenomenon that some particulates should appear at very low temperatures.[101] It was not until 1995 that the first such condensate was produced experimentally by Eric Allin Cornell and Carl Wieman using ultra-cooling equipment built at the NISTJILA laboratory at the University of Colorado at Boulder.[102] BoseEinstein statistics are now used to describe the behaviors of any assembly of bosons. Einstein's sketches for this project may be seen in the Einstein Archive in the library of the Leiden University.[81]
General relativity includes a dynamical spacetime, so it is difficult to see how to identify the conserved energy and momentum. Noether's theorem allows these quantities to be determined from a Lagrangian with translation invariance, but general covariance makes translation invariance into something of a gauge symmetry. The energy and momentum derived within general relativity by Noether's presecriptions do not make a real tensor for this reason.
Einstein argued that this is true for fundamental reasons, because the gravitational field could be made to vanish by a choice of coordinates. He maintained that the non-covariant energy momentum pseudotensor was in fact the best description of the energy momentum distribution in a gravitational field. This approach has been echoed by Lev Landau and Evgeny Lifshitz, and others, and has become standard.
The use of non-covariant objects like pseudotensors was heavily criticized in 1917 by Erwin Schrdinger and others.
Following his research on general relativity, Einstein entered into a series of attempts to generalize his geometric theory of gravitation to include electromagnetism as another aspect of a single entity. In 1950, he described his "unified field theory" in a Scientific American article entitled "On the Generalized Theory of Gravitation".[103] Although he continued to be lauded for his work, Einstein became increasingly isolated in his research, and his efforts were ultimately unsuccessful. In his pursuit of a unification of the fundamental forces, Einstein ignored some mainstream developments in physics, most notably the strong and weak nuclear forces, which were not well understood until many years after his death. Mainstream physics, in turn, largely ignored Einstein's approaches to unification. Einstein's dream of unifying other laws of physics with gravity motivates modern quests for a theory of everything and in particular string theory, where geometrical fields emerge in a unified quantum-mechanical setting.
Einstein collaborated with others to produce a model of a wormhole. His motivation was to model elementary particles with charge as a solution of gravitational field equations, in line with the program outlined in the paper "Do Gravitational Fields play an Important Role in the Constitution of the Elementary Particles?". These solutions cut and pasted Schwarzschild black holes to make a bridge between two patches.
If one end of a wormhole was positively charged, the other end would be negatively charged. These properties led Einstein to believe that pairs of particles and antiparticles could be described in this way.
In order to incorporate spinning point particles into general relativity, the affine connection needed to be generalized to include an antisymmetric part, called the torsion. This modification was made by Einstein and Cartan in the 1920s.
The theory of general relativity has a fundamental law  the Einstein equations which describe how space curves, the geodesic equation which describes how particles move may be derived from the Einstein equations.
Since the equations of general relativity are non-linear, a lump of energy made out of pure gravitational fields, like a black hole, would move on a trajectory which is determined by the Einstein equations themselves, not by a new law. So Einstein proposed that the path of a singular solution, like a black hole, would be determined to be a geodesic from general relativity itself.
This was established by Einstein, Infeld, and Hoffmann for pointlike objects without angular momentum, and by Roy Kerr for spinning objects.
Einstein conducted other investigations that were unsuccessful and abandoned. These pertain to force, superconductivity, gravitational waves, and other research. Please see the main article for details.
In addition to longtime collaborators Leopold Infeld, Nathan Rosen, Peter Bergmann and others, Einstein also had some one-shot collaborations with various scientists.
Einstein and De Haas demonstrated that magnetization is due to the motion of electrons, nowadays known to be the spin. In order to show this, they reversed the magnetization in an iron bar suspended on a torsion pendulum. They confirmed that this leads the bar to rotate, because the electron's angular momentum changes as the magnetization changes. This experiment needed to be sensitive, because the angular momentum associated with electrons is small, but it definitively established that electron motion of some kind is responsible for magnetization.
Einstein suggested to Erwin Schrdinger that he might be able to reproduce the statistics of a BoseEinstein gas by considering a box. Then to each possible quantum motion of a particle in a box associate an independent harmonic oscillator. Quantizing these oscillators, each level will have an integer occupation number, which will be the number of particles in it.
This formulation is a form of second quantization, but it predates modern quantum mechanics. Erwin Schrdinger applied this to derive the thermodynamic properties of a semiclassical ideal gas. Schrdinger urged Einstein to add his name as co-author, although Einstein declined the invitation.[104]
In 1926, Einstein and his former student Le Szilrd co-invented (and in 1930, patented) the Einstein refrigerator. This absorption refrigerator was then revolutionary for having no moving parts and using only heat as an input.[105] On 11 November 1930, U.S. Patent 1,781,541 was awarded to Albert Einstein and Le Szilrd for the refrigerator. Their invention was not immediately put into commercial production, as the most promising of their patents were quickly bought up by the Swedish company Electrolux to protect its refrigeration technology from competition.[106]
The BohrEinstein debates were a series of public disputes about quantum mechanics between Albert Einstein and Niels Bohr who were two of its founders. Their debates are remembered because of their importance to the philosophy of science.[107][108][109]
In 1935, Einstein returned to the question of quantum mechanics. He considered how a measurement on one of two entangled particles would affect the other. He noted, along with his collaborators, that by performing different measurements on the distant particle, either of position or momentum, different properties of the entangled partner could be discovered without disturbing it in any way.
He then used a hypothesis of local realism to conclude that the other particle had these properties already determined. The principle he proposed is that if it is possible to determine what the answer to a position or momentum measurement would be, without in any way disturbing the particle, then the particle actually has values of position or momentum.
This principle distilled the essence of Einstein's objection to quantum mechanics. As a physical principle, it was shown to be incorrect when the Aspect experiment of 1982 confirmed Bell's theorem, which had been promulgated in 1964.
Albert Einstein's political view was in favor of socialism;[110][111] his political views emerged publicly in the middle of the 20th century due to his fame and reputation for genius. Einstein offered to and was called on to give judgments and opinions on matters often unrelated to theoretical physics or mathematics.[112]
Einstein's views about religious belief have been collected from interviews and original writings. These views covered Judaism, theological determinism, agnosticism, and humanism. He also wrote much about ethical culture, opting for Spinoza's god over belief in a personal god.[113]
Einstein developed an appreciation of music at an early age. His mother played the piano reasonably well and wanted her son to learn the violin, not only to instill in him a love of music but also to help him assimilate German culture. According to conductor Leon Botstein, Einstein is said to have begun playing when he was five, but did not enjoy it at that age.[114]
When he turned thirteen, however, he discovered the violin sonatas of Mozart. "Einstein fell in love" with Mozart's music, notes Botstein, and learned to play music more willingly. According to Einstein, he taught himself to play by "ever practicing systematically," adding that "Love is a better teacher than a sense of duty."[114] At age seventeen, he was heard by a school examiner in Aarau as he played Beethoven's violin sonatas, the examiner stating afterward that his playing was "remarkable and revealing of 'great insight.'" What struck the examiner, writes Botstein, was that Einstein "displayed a deep love of the music, a quality that was and remains in short supply. Music possessed an unusual meaning for this student."[114]
Botstein notes that music assumed a pivotal and permanent role in Einstein's life from that period on. Although the idea of becoming a professional himself was not on his mind at any time, among those with whom Einstein played chamber music were a few professionals, and he performed for private audiences and friends. Chamber music also became a regular part of his social life while living in Bern, Zurich, and Berlin, where he played with Max Planck and his son, among others. In 1931, while engaged in research at California Institute of Technology, he visited the Zoellner family conservatory in Los Angeles and played some of Beethoven and Mozart's works with members of the Zoellner Quartet, recently retired from two decades of acclaimed touring all across the United States; Einstein later presented the family patriarch with an autographed photograph as a memento.[115][116] Near the end of his life, when the young Juilliard Quartet visited him in Princeton, he played his violin with them; although they slowed the tempo to accommodate his lesser technical abilities, Botstein notes the quartet was "impressed by Einstein's level of coordination and intonation."[114]
While travelling, Einstein wrote daily to his wife Elsa and adopted stepdaughters Margot and Ilse. The letters were included in the papers bequeathed to The Hebrew University. Margot Einstein permitted the personal letters to be made available to the public, but requested that it not be done until twenty years after her death (she died in 1986[117]). Barbara Wolff, of The Hebrew University's Albert Einstein Archives, told the BBC that there are about 3,500 pages of private correspondence written between 1912 and 1955.[118]
Einstein bequeathed the royalties from use of his image to The Hebrew University of Jerusalem. Corbis, successor to The Roger Richman Agency, licenses the use of his name and associated imagery, as agent for the university.[119]
In the period before World War II, Einstein was so well known in America that he would be stopped on the street by people wanting him to explain "that theory". He finally figured out a way to handle the incessant inquiries. He told his inquirers "Pardon me, sorry! Always I am mistaken for Professor Einstein."[120]
Einstein has been the subject of or inspiration for many novels, films, plays, and works of music.[121] He is a favorite model for depictions of mad scientists and absent-minded professors; his expressive face and distinctive hairstyle have been widely copied and exaggerated. Time magazine's Frederic Golden wrote that Einstein was "a cartoonist's dream come true".[122]
Einstein received numerous awards and honors, including the Nobel Prize in Physics.
         
Wrttemberg/Germany (18791896)
Stateless (18961901)
Switzerland (19011955)
Austria (19111912)
Germany (19141933)
United States (19401955)
Swiss Patent Office (Bern)
University of Zurich
Charles University in Prague
ETH Zurich
Caltech
Prussian Academy of Sciences
Kaiser Wilhelm Institute
University of Leiden
Institute for Advanced Study
ETH Zurich
University of Zurich
Ernst G. Straus
Nathan Rosen
Le Szilrd
Raziuddin Siddiqui[1]
General relativity and special relativity
Photoelectric effect
Mass-energy equivalence
Theory of Brownian Motion
Einstein field equations
BoseEinstein statistics
Bose-Einstein condensate
BoseEinstein correlations
Unified Field Theory
EPR paradox
Nobel Prize in Physics (1921)
Matteucci Medal (1921)
Copley Medal (1925)
Max Planck Medal (1929)
Time Person of the Century (1999)
1 Biography

1.1 Early life and education
1.2 Marriages and children
1.3 Patent office
1.4 Academic career
1.5 Travels abroad
1.6 Emigration to U.S. in 1933

1.6.1 World War II and the Manhattan Project
1.6.2 U.S. citizenship


1.7 Death


1.1 Early life and education
1.2 Marriages and children
1.3 Patent office
1.4 Academic career
1.5 Travels abroad
1.6 Emigration to U.S. in 1933

1.6.1 World War II and the Manhattan Project
1.6.2 U.S. citizenship


1.6.1 World War II and the Manhattan Project
1.6.2 U.S. citizenship
1.7 Death
2 Scientific career

2.1 1905  Annus Mirabilis papers
2.2 Thermodynamic fluctuations and statistical physics
2.3 General principles
2.4 Theory of relativity and E = mc
2.5 Photons and energy quanta
2.6 Quantized atomic vibrations
2.7 Adiabatic principle and action-angle variables
2.8 Waveparticle duality
2.9 Theory of critical opalescence
2.10 Zero-point energy
2.11 General relativity and the Equivalence Principle
2.12 Hole argument and Entwurf theory
2.13 Cosmology
2.14 Modern quantum theory
2.15 BoseEinstein statistics
2.16 Energy momentum pseudotensor
2.17 Unified field theory
2.18 Wormholes
2.19 EinsteinCartan theory
2.20 Equations of motion
2.21 Other investigations
2.22 Collaboration with other scientists

2.22.1 Einsteinde Haas experiment
2.22.2 Schrdinger gas model
2.22.3 Einstein refrigerator


2.23 Bohr versus Einstein
2.24 EinsteinPodolskyRosen paradox


2.1 1905  Annus Mirabilis papers
2.2 Thermodynamic fluctuations and statistical physics
2.3 General principles
2.4 Theory of relativity and E = mc
2.5 Photons and energy quanta
2.6 Quantized atomic vibrations
2.7 Adiabatic principle and action-angle variables
2.8 Waveparticle duality
2.9 Theory of critical opalescence
2.10 Zero-point energy
2.11 General relativity and the Equivalence Principle
2.12 Hole argument and Entwurf theory
2.13 Cosmology
2.14 Modern quantum theory
2.15 BoseEinstein statistics
2.16 Energy momentum pseudotensor
2.17 Unified field theory
2.18 Wormholes
2.19 EinsteinCartan theory
2.20 Equations of motion
2.21 Other investigations
2.22 Collaboration with other scientists

2.22.1 Einsteinde Haas experiment
2.22.2 Schrdinger gas model
2.22.3 Einstein refrigerator


2.22.1 Einsteinde Haas experiment
2.22.2 Schrdinger gas model
2.22.3 Einstein refrigerator
2.23 Bohr versus Einstein
2.24 EinsteinPodolskyRosen paradox
3 Political and religious views
4 Love of music
5 Non-scientific legacy
6 In popular culture
7 Awards and honors
8 Publications
9 See also
10 Notes
11 References
12 Further reading
13 External links
1.1 Early life and education
1.2 Marriages and children
1.3 Patent office
1.4 Academic career
1.5 Travels abroad
1.6 Emigration to U.S. in 1933

1.6.1 World War II and the Manhattan Project
1.6.2 U.S. citizenship


1.6.1 World War II and the Manhattan Project
1.6.2 U.S. citizenship
1.7 Death
1.6.1 World War II and the Manhattan Project
1.6.2 U.S. citizenship
2.1 1905  Annus Mirabilis papers
2.2 Thermodynamic fluctuations and statistical physics
2.3 General principles
2.4 Theory of relativity and E = mc
2.5 Photons and energy quanta
2.6 Quantized atomic vibrations
2.7 Adiabatic principle and action-angle variables
2.8 Waveparticle duality
2.9 Theory of critical opalescence
2.10 Zero-point energy
2.11 General relativity and the Equivalence Principle
2.12 Hole argument and Entwurf theory
2.13 Cosmology
2.14 Modern quantum theory
2.15 BoseEinstein statistics
2.16 Energy momentum pseudotensor
2.17 Unified field theory
2.18 Wormholes
2.19 EinsteinCartan theory
2.20 Equations of motion
2.21 Other investigations
2.22 Collaboration with other scientists

2.22.1 Einsteinde Haas experiment
2.22.2 Schrdinger gas model
2.22.3 Einstein refrigerator


2.22.1 Einsteinde Haas experiment
2.22.2 Schrdinger gas model
2.22.3 Einstein refrigerator
2.23 Bohr versus Einstein
2.24 EinsteinPodolskyRosen paradox
2.22.1 Einsteinde Haas experiment
2.22.2 Schrdinger gas model
2.22.3 Einstein refrigerator
Einstein, Albert (1901), "Folgerungen aus den Capillarittserscheinungen (Conclusions Drawn from the Phenomena of Capillarity)", Annalen der Physik 4 (3): 513, Bibcode 1901AnP...309..513E, doi:10.1002/andp.19013090306
Einstein, Albert (1905a), "ber einen die Erzeugung und Verwandlung des Lichtes betreffenden heuristischen Gesichtspunkt (On a Heuristic Viewpoint Concerning the Production and Transformation of Light)", Annalen der Physik 17 (6): 132148, Bibcode 1905AnP...322..132E, doi:10.1002/andp.19053220607, http://www.physik.uni-augsburg.de/annalen/history/einstein-papers/1905_17_132-148.pdf This annus mirabilis paper on the photoelectric effect was received by Annalen der Physik 18 March.
Einstein, Albert (1905b), A new determination of molecular dimensions. This PhD thesis was completed 30 April and submitted 20 July.
Einstein, Albert (1905c), "On the Motion Required by the Molecular Kinetic Theory of Heat of Small Particles Suspended in a Stationary Liquid", Annalen der Physik 17 (8): 549560, Bibcode 1905AnP...322..549E, doi:10.1002/andp.19053220806. This annus mirabilis paper on Brownian motion was received 11 May.
Einstein, Albert (1905d), "On the Electrodynamics of Moving Bodies", Annalen der Physik 17 (10): 891921, Bibcode 1905AnP...322..891E, doi:10.1002/andp.19053221004. This annus mirabilis paper on special relativity was received 30 June.
Einstein, Albert (1905e), "Does the Inertia of a Body Depend Upon Its Energy Content?", Annalen der Physik 18 (13): 639641, Bibcode 1905AnP...323..639E, doi:10.1002/andp.19053231314. This annus mirabilis paper on mass-energy equivalence was received 27 September.
Einstein, Albert (1915), "Die Feldgleichungen der Gravitation (The Field Equations of Gravitation)", Kniglich Preussische Akademie der Wissenschaften: 844847
Einstein, Albert (1917a), "Kosmologische Betrachtungen zur allgemeinen Relativittstheorie (Cosmological Considerations in the General Theory of Relativity)", Kniglich Preussische Akademie der Wissenschaften
Einstein, Albert (1917b), "Zur Quantentheorie der Strahlung (On the Quantum Mechanics of Radiation)", Physikalische Zeitschrift 18: 121128, Bibcode 1917PhyZ...18..121E
Einstein, Albert (11 July 1923), "Fundamental Ideas and Problems of the Theory of Relativity", Nobel Lectures, Physics 19011921, Amsterdam: Elsevier Publishing Company, archived from the original on 10 February 2007, http://nobelprize.org/nobel_prizes/physics/laureates/1921/einstein-lecture.pdf, retrieved 25 March 2007
Einstein, Albert (1924), "Quantentheorie des einatomigen idealen Gases (Quantum theory of monatomic ideal gases)", Sitzungsberichte der Preussichen Akademie der Wissenschaften Physikalisch-Mathematische Klasse: 261267. First of a series of papers on this topic.
Einstein, Albert (1926), "Die Ursache der Manderbildung der Flusslufe und des sogenannten Baerschen Gesetzes", Die Naturwissenschaften 14 (11): 223224, Bibcode 1926NW.....14..223E, doi:10.1007/BF01510300. On Baer's law and meanders in the courses of rivers.
Einstein, Albert; Podolsky, Boris; Rosen, Nathan (15 May 1935), "Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?", Physical Review 47 (10): 777780, Bibcode 1935PhRv...47..777E, doi:10.1103/PhysRev.47.777
Einstein, Albert (1940), "On Science and Religion", Nature (Edinburgh: Scottish Academic) 146 (3706): 605, Bibcode 1940Natur.146..605E, doi:10.1038/146605a0, ISBN0-7073-0453-9
Einstein, Albert etal. (4 December 1948), "To the editors", New York Times (Melville, NY: AIP, American Inst. of Physics), ISBN0-7354-0359-7, http://phys4.harvard.edu/~wilson/NYTimes1948.html
Einstein, Albert (May 1949), "Why Socialism?", Monthly Review, archived from the original on 11 January 2006, http://www.monthlyreview.org/598einst.htm, retrieved 16 January 2006
Einstein, Albert (1950), "On the Generalized Theory of Gravitation", Scientific American CLXXXII (4): 1317
Einstein, Albert (1954), Ideas and Opinions, New York: Random House, ISBN0-517-00393-7
Einstein, Albert (1969) (in German), Albert Einstein, Hedwig und Max Born: Briefwechsel 19161955, Munich: Nymphenburger Verlagshandlung, ISBN3-88682-005-X
Einstein, Albert (1979), Autobiographical Notes, Paul Arthur Schilpp (Centennial ed.), Chicago: Open Court, ISBN0-87548-352-6. The chasing a light beam thought experiment is described on pages 4851.
Collected Papers: Stachel, John, Martin J. Klein, a.J. Kox, Michel Janssen, R. Schulmann, Diana Komos Buchwald and others (Eds.) (19872006), The Collected Papers of Albert Einstein, Vol. 110, Princeton University Press, http://press.princeton.edu/einstein/writings.html#papers Further information about the volumes published so far can be found on the webpages of the Einstein Papers Project and on the Princeton University Press Einstein Page
Book: Albert Einstein
The Einstein Theory of Relativity (educationalfilm
about the theory of relativity)
German inventors and discoverers
Heinrich Burkhardt
Hermann Einstein
Historical Museum of Bern (Einstein museum)
History of gravitational theory
Introduction to special relativity
List of coupled cousins
Relativity priority dispute
Sticky bead argument
Summation convention
Jewish Nobel laureates
Brian, Denis (1996). Einstein: A Life. New York: John Wiley.
Clark, Ronald (1971). Einstein: The Life and Times. New York: Avon Books.
Flsing, Albrecht (1997): Albert Einstein: A Biography. New York: Penguin Viking. (Translated and abridged from the German by Ewald Osers.) ISBN 978-0670855452
Highfield, Roger; Carter, Paul (1993). The Private Lives of Albert Einstein. London: Faber and Faber. ISBN978-0-571-16744-9.
Hoffmann, Banesh, with the collaboration of Helen Dukas (1972): Albert Einstein: Creator and Rebel. London: Hart-Davis, MacGibbon Ltd. ISBN 978-0670111817
Isaacson, Walter (2007): Einstein: His Life and Universe. Simon & Schuster Paperbacks, New York. ISBN 978-0-7432-6473-0
Moring, Gary (2004): The complete idiot's guide to understanding Einstein ( 1st ed. 2000). Indianapolis IN: Alpha books (Macmillan USA). ISBN 0-02-863180-3
Pais, Abraham (1982): Subtle is the Lord: The science and the life of Albert Einstein. Oxford University Press. ISBN 978-0198539070. The definitive biography to date.
Pais, Abraham (1994): Einstein Lived Here. Oxford University Press. ISBN 0-192-80672-6
Parker, Barry (2000): Einstein's Brainchild: Relativity Made Relatively Easy!. Prometheus Books. Illustrated by Lori Scoffield-Beer. A review of Einstein's career and accomplishments, written for the lay public. ISBN 978-1591025221
Schweber, Sylvan S. (2008): Einstein and Oppenheimer: The Meaning of Genius. Harvard University Press. ISBN 978-0-674-02828-9.
Oppenheimer, J.R. (1971): "On Albert Einstein," p.812 in Science and synthesis: an international colloquium organized by Unesco on the tenth anniversary of the death of Albert Einstein and Teilhard de Chardin, Springer-Verlag, 1971, 208 pp. (Lecture delivered at the UNESCO House in Paris on 13 December 1965.) Also published in The New York Review of Books, 17 March 1966, On Albert Einstein by Robert Oppenheimer
Ideas and Opinions, Einstein's letters and speeches, Full text, Crown Publishers (1954) 384 pages
Einstein's Scholar Google profile
Works by Albert Einstein (public domain in Canada)
The MacTutor History of Mathematics archive, School of Mathematics and Statistics, University of St Andrews, Scotland, April 1997, http://www-history.mcs.st-andrews.ac.uk/Biographies/Einstein.html, retrieved 14 June 2009
Why Socialism? by Albert Einstein, Monthly Review, May 1949
Einstein's Personal Correspondence: Religion, Politics, The Holocaust, and Philosophy Shapell Manuscript Foundation
FBI file on Albert Einstein
Nobelprize.org Biography:Albert Einstein
The Einstein You Never Knew slideshow by Life magazine
Albert Einstein videos
Science Odyssey People And Discoveries
MIT OpenCourseWare STS.042J/8.225J: Einstein, Oppenheimer, Feynman: Physics in the 20th century free study course that explores the changing roles of physics and physicists during the 20th century
List of publications from Google Scholar.
Albert Einstein Archives Online (80,000+ Documents) (MSNBC - 19 March 2012)
.
#(`*Elon Musk*`)#.
Justine Musk (m.2000d.2008) start:(2000)end+1:(2009)"Marriage: Justine Musk to Elon Musk" Location: (linkback://en.wikipedia.org/wiki/Elon_Musk)
Talulah Riley (m.2010d.2012) start:(2010)end+1:(2013)"Marriage: Talulah Riley to Elon Musk" Location: (linkback://en.wikipedia.org/wiki/Elon_Musk)
Elon Musk (born 28 June 1971) is a South African American entrepreneur and inventor best known for founding SpaceX, and co-founding Tesla Motors and PayPal (initially known as X.com). While at those companies, he oversaw the construction of the first electric car of the modern era, the Tesla Roadster, a private rocket and spaceship successor to the Space Shuttle known as Falcon 9/Dragon, and the Internet payment system PayPal. He is currently the CEO and Chief Designer of SpaceX, CEO and Product Architect of Tesla Motors and Chairman of SolarCity. Mr. Musk holds a bachelor's degree in Business from the The Wharton School,[2], and a second bachelor's degree in Physics from University of Pennsylvania, School of Arts and Sciences,[2].
Musk was born and raised in Pretoria, South Africa, the son of a Canadian mother, Maye (ne Haldeman), and a South African father, Errol Musk.[5][6] His mother's heritage include Pennsylvania Dutch;[7] his maternal grandfather was from Minnesota, and had moved to Saskatchewan, where Musk's mother was born. His father is an engineer and his mother is an author, nutritionist and model.
Musk bought his first computer at the age of 10 and taught himself how to program;[8] by the age of 12 he sold his first commercial software for about $500, a space game called Blastar.[8]
After spending 3 years (Grade 8 - 10) at Bryanston High School, Musk matriculated at Pretoria Boys High School. He left home in 1988 at the age of 17, without his parents' support and in part because of the prospect of compulsory service in the South African military: "I don't have an issue with serving in the military per se, but serving in the South African army suppressing black people just didn't seem like a really good way to spend time".[8] He wanted to move to the US, saying: "It is where great things are possible".[9]
In 1992, after spending two years at Queen's University, Kingston, Ontario, Elon left Canada, pursuing business and physics at the University of Pennsylvania, Philadelphia on a full scholarship. From the The Wharton School,[2] he received an undergraduate degree in Business, and from University of Pennsylvania, School of Arts and Sciences,[2] he received a second bachelor's degree in Physics.[10]
His undergraduate degrees behind him, and drawing inspiration from innovators such as Nikola Tesla,[11] Musk then considered three areas he wanted to get into that were "important problems that would most affect the future of humanity", as he said later, "One was the Internet, one was clean energy, and one was space".[8]
Musk went on to a graduate program in both applied physics and materials science at Stanford in 1995. He stayed two days before dropping out to start Zip2, which provided online content publishing software for news organizations, with his brother Kimbal Musk.[12] In 1999, Compaq's AltaVista division acquired Zip2 for US$307 million in cash and US$34 million in stock options.[13]
Musk co-founded X.com, an online financial services and e-mail payment company, in March 1999. One year later, in a 50/50 merger,[14] X.com acquired Confinity,[15] which operated an auction payment system similar in size to X.com, namely PayPal. Musk was a principal architect behind the purchase, which hinged on his belief in the emerging online-transfer, or "P2P" technology.[15] Musk believed that the Confinity sub-brand would become the necessary vehicle to incorporate and develop a person-to-person payment platform within X.com.[15] The combined company at first adopted X.com as the corporate name, but in February 2001, X.com changed its legal name to PayPal Inc. Musk was instrumental in the new PayPals focus on a global payment system and departure from the core financial offerings of X.com.[16]
PayPals early growth was due in large part to a successful viral growth campaign created by Musk.[17] In October 2002, PayPal was acquired by eBay for US$1.5 billion in stock.[18] Before its sale, Musk, the company's largest shareholder, owned 11.7% of PayPal's shares.[19]
Musk founded his third company, Space Exploration Technologies (SpaceX), in June 2002[20] of which he is currently the CEO and CTO. SpaceX develops and manufactures space launch vehicles with a focus on advancing the state of rocket technology. The company's first two launch vehicles are the Falcon 1 and Falcon 9 rockets and its first spacecraft is Dragon.[21]
SpaceX was awarded a $1.6 billion NASA contract on 23 December 2008, for 12 flights of their Falcon 9 rocket and Dragon spacecraft to the International Space Station, replacing the Space Shuttle after it retired in 2011. Initially, Falcon 9/Dragon will replace the cargo transport function of the Shuttle and astronaut transport will be handled by the Soyuz. However, SpaceX has designed Falcon 9/Dragon with astronaut transport in mind and the Augustine commission has recommended that astronaut transport be handled by commercial companies like SpaceX.[22]
Musk views space exploration as an important step in expandingif not preservingthe consciousness of human life.[23] Musk has said that multiplanetary life may serve as a hedge against threats to the survival of the human species. "An asteroid or a super volcano could destroy us, and we face risks the dinosaurs never saw: An engineered virus, inadvertent creation of a micro black hole, catastrophic global warming or some as-yet-unknown technology could spell the end of us. Humankind evolved over millions of years, but in the last sixty years atomic weaponry created the potential to extinguish ourselves. Sooner or later, we must expand life beyond this green and blue ballor go extinct." Musk's goal is to reduce the cost of human spaceflight by a factor of 100. He founded SpaceX with $100 million of his early fortune. He remains chief executive officer and chief technology officer of the Hawthorne, Calif.-based company.[24]
In seven years, SpaceX designed the family of Falcon launch vehicles and the Dragon multi-purpose spacecraft from the ground-up. In September 2009, SpaceX's Falcon 1 rocket became the first privately funded liquid-fueled vehicle to put a satellite into Earth orbit. NASA selected SpaceX to be part of the first program that entrusts private companies to deliver cargo to the International Space Station. This contract, which has a minimum value of $1.6 billion and a maximum value of $3.1 billion, has become a cornerstone of the Space Station's continued access to cargo delivery and return. In addition to these services, SpaceX's goals include simultaneously lowering the price of orbital spaceflight and improving reliability, both by an order of magnitude, while creating the first fully reusable orbital launch vehicle. In the coming years, Musk will focus on delivering astronauts to the International Space Station, but has stated his personal goal of eventually enabling human exploration and settlement of Mars. In a 2011 interview, he said he hopes to send humans to Mars' surface within 1020 years.[25] On 25 May 2012, the SpaceX Dragon vehicle docked with the ISS, making history as the first commercial company to launch and dock a vehicle to the International Space Station.
Musk is a co-founder and currently head of product design at Tesla Motors. He oversaw development of the Tesla Roadster, the first production electric car of the modern era. Musk's interest in electric vehicles extends long before the creation of Tesla. He originally went to Silicon Valley to do a PhD in Applied Physics and Materials Science at Stanford, where his goal was to create ultracapacitors with enough energy to power electric cars.
Musk began by hiring Martin Eberhard as CEO and a management team and provided almost all of the capital for Tesla's first two funding rounds, giving him a controlling interest from the start. As a result of the financial crisis in 2008 and a forced layoff at Tesla,[26] Musk was forced to assume the additional responsibility of CEO.
Tesla Motors first built an electric sports car, the Tesla Roadster, which has shipped over 2,300 vehicles to 31 countries. Tesla began delivery of its four-door ModelS sedan on 22 June 2012 and unveiled its third product the Model X, aimed at the SUV/minivan market, on 9 February 2012. Model X is scheduled to begin production in 2014. [27] In addition to its own cars, Tesla sells electric powertrain systems to Daimler for the Smart EV and Mercedes A Class, and to Toyota for the upcoming electric RAV4. Musk was also able to bring in both companies as long term investors in Tesla.
Musk is principally responsible for an overarching business strategy that aims to deliver affordable electric vehicles to mass-market consumers. His vision was to create the Tesla Roadster as a means to that enda car aimed specifically at affluent early adopters, whose purchase of the sports car would subsidize the research and development costs of lower priced models of electric vehicles. From the start of Tesla, Musk has been a champion of the Model S, a four-door family sedan with an anticipated base price of half that of the Roadster. Musk has also favored building a sub-$30,000 subcompact and building and selling electric vehicle powertrain components so that other automakers can produce electric vehicles at affordable prices without having to develop the products in house.[28] Several mainstream publications have compared him with Henry Ford for his revolutionary work on advanced vehicle powertrains.[29]
He is reported to have a 32% stake in Tesla, which is currently valued above $1 billion, as of March 2012.[30][31]
Musk provided the initial concept for SolarCity, where he remains the largest shareholder and chairman of the board. SolarCity is the largest provider of solar power systems in the United States. His cousin Lyndon Rive is the CEO and co-founder.[32][33] The underlying motivation for funding both SolarCity and Tesla is to help combat global warming.[34] In 2012, Musk announced that SolarCity and Tesla Motors are collaborating to use electric vehicle batteries to smooth the impact of rooftop solar on the power grid.[35]
Musk is Chairman of the Musk Foundation, which focuses its philanthropic efforts on science education, pediatric health and clean energy. He is a trustee of the X Prize Foundation, promoting renewable energy technologies. He sits on the boards of The Space Foundation, The National Academies Aeronautics and Space Engineering Board, The Planetary Society, and Stanford Engineering Advisory Board. Musk is also a member of the board of trustees of the California Institute of Technology (Caltech).
He began a multi-million dollar program through his foundation in 2010 to donate solar power systems for critical needs in disaster areas. The first such solar power installation occurred on a hurricane response center in Alabama that had been neglected by state and federal aid. To make it clear that this was not serving Musk's commercial interests, SolarCity noted that it had no present or planned business activity in Alabama.[36]
Musk had plans for a "Mars Oasis" project in 2001, which would land a miniature experimental greenhouse on Mars, containing food crops growing on Martian regolith.[37][38] He put this project on hold when he came to the conclusion that the fundamental problem preventing humanity from becoming a true spacefaring civilization was the lack of advancement in rocket technology. He has sought to address this by founding SpaceX to create revolutionary new interplanetary rockets.
His long term goal is to help humanity through SpaceX by creating a true spacefaring civilization.[39] Musk's philosophy and description of what is needed to solve the problem are provided in the IEEE podcast "Elon Musk: a founder of Paypal, Tesla Motors, and SpaceX"[40] and article "Risky Business."[38]
Musk joined The Giving Pledge in April 2012, offering a moral commitment to donate the majority of his fortune to philanthropy.[41][42] Musk became a member of the campaign first popularized by Warren Buffett and Bill Gates with a class of 12 of Americas wealthiest families and individuals, which included Arthur Blank and Michael Moritz.[41]
Car blog Jalopnik reported on 16 August 2012 that Musk was supporting an effort by Matthew Inman of The Oatmeal to preserve the site of Nikola Tesla's lab and turn it into a museum, the Tesla Science Center at Wardenclyffe.[43]
Listed as one of Time Magazine's 100 people who most affected the world in 2010. Jon Favreau, director of the Iron Man movies, describes in his article how Musk was the inspiration for Favreau's film depiction of genius billionaire Tony Stark.[44] The world governing body for aerospace records, Fdration Aronautique Internationale, presented Musk in 2010 with the highest award in air & space, the FAI Gold Space Medal, for designing the first privately developed rocket to reach orbit. Prior recipients include Neil Armstrong, Burt Rutan of Scaled Composites and John Glenn. Named as one of the 75 most influential people of the 21st century by Esquire magazine.[23] In June 2011, Musk was awarded the $500,000 Heinlein Prize for Advances in Space Commercialization[45] In February 2011, Forbes listed Musk as one of "America's 20 Most Powerful CEOs 40 And Under".[46] Recognized as a Living Legend in Aviation in 2010 by the Kitty Hawk Foundation for creating the successor to the Space Shuttle (Falcon 9 rocket and Dragon spacecraft). Other recipients include Buzz Aldrin and Richard Branson.[47] American Institute of Aeronautics and Astronautics George Low award for the most outstanding contribution in the field of space transportation in 2007/2008. Musk was recognized for his design of the Falcon 1, the first privately developed liquid fuel rocket to reach orbit.[48] National Space Society's Von Braun Trophy in 2008/2009, given for leadership of the most significant achievement in space. Prior recipients include Burt Rutan and Steve Squyres.[49] National Wildlife Federation 2008 National Conservation Achievement award for Tesla Motors and SolarCity. Other 2008 recipients include journalist Thomas Friedman, U.S. Senator Patrick Leahy and Florida Governor Charlie Crist.[50] The Aviation Week 2008 Laureate for the most significant achievement worldwide in the space industry.[51] R&D Magazine Innovator of the Year for 2007 for SpaceX, Tesla and SolarCity.[52] Automotive Executive of the Year (worldwide) in 2010 for demonstrating technology leadership and innovation via Tesla Motors. Prior awardees include Bill Ford Jr, Bob Lutz, Dieter Zetsche and Lee Iacocca. Musk is the youngest ever recipient of this award.[53] Inc Magazine Entrepreneur of the Year award for 2007 for his work on Tesla and SpaceX.[54] 2007 Index Design award for his design of the Tesla Roadster.[55] Global Green 2006 product design award for his design of the Tesla Roadster, presented by Mikhail Gorbachev.[56] Musk is a Director of the Planetary Society, a Trustee of The X-Prize Foundation and a member of the Stanford University Engineering Advisory Board. He has previously served as a member of the United States National Academy of Sciences Aeronautics and Space Engineering Board.[57] In a 2010 Space Foundation survey, Musk was ranked as the #10 (tied with rocketry pioneer and scientist Wernher von Braun) most popular space hero.[58] In 2010, Musk was elected to the board of trustees of the California Institute of Technology.[59] In 2011, Musk was honored as a Legendary Leader at the Churchill Club Awards.[60] In 2012, Musk was awarded with the Royal Aeronauticals Societys highest award  a Gold Medal.[61]
Musk has described himself as a workaholic who routinely invests 100 hours per week running Tesla Motors and SpaceX, often flying in a fuel-efficient corporate jet.[64]
The SpaceX factory was used as a filming location for Iron Man 2 and Musk has a cameo in the movie.[65]
Musk previously owned and later sold a McLaren F1 sports car and a Czech made jet trainer aircraft Aero L-39.[66] The 1994 model Dassault Falcon 900 aircraft used in the film Thank You for Smoking is registered to Musk (N900SX)[67] and Musk had a cameo as the pilot of his plane, opening the door for Robert Duvall and escorting Aaron Eckhart aboard.[68] Musk is an attendee of the Burning Man festival, and says that he first thought up the idea for SolarCity at the 2004 festival.[35] Recently, he proposed a solar-powered jet tunnel system known as the Hyperloop that would enable individuals to make trips from San Francisco to Los Angeles in less than 30 minutes.[69]
Musk lives in Bel-Air, California. Musk met his first wife, the Canadian-born author Justine Musk, while they were both students at Queen's University. They were married in 2000 and together had five sons.[70] They announced their separation in September 2008. Musk announced in January 2012 that he had recently ended a four-year relationship with his second wife, British actress Talulah Riley.[71]
Tosca Musk, Elon's sister, is the founder of Musk Entertainment and has produced various movies.[72][73] Elon himself was the executive producer of her first movie, called Puzzled.[74] His brother Kimbal is the CEO of a social search company OneRiot and owner of The Kitchen restaurant with locations in Boulder, Colorado and Denver, Colorado.[75]
1 Early life
2 Career

2.1 PayPal
2.2 SpaceX
2.3 Tesla Motors
2.4 SolarCity


2.1 PayPal
2.2 SpaceX
2.3 Tesla Motors
2.4 SolarCity
3 Philanthropy
4 Awards and recognition

4.1 Honorary Doctorates


4.1 Honorary Doctorates
5 Interests
6 Family
7 Quotes
8 References
9 External links

9.1 Interviews


9.1 Interviews
2.1 PayPal
2.2 SpaceX
2.3 Tesla Motors
2.4 SolarCity
4.1 Honorary Doctorates
9.1 Interviews
Honorary doctorate in design from the Art Center College of Design[62]
Honorary Doctorate (DUniv) in aerospace engineering from the University of Surrey[63]
"We have planes, trains, automobiles and boats," ... "What if there was a fifth mode?," on the Hyperloop.[69]
"In terms of the Internet, it's like humanity acquiring a collective nervous system. Whereas previously we were more like a... collection of cells that communicated by diffusion. With the advent of the Internet, it was suddenly like we got a nervous system. It's a hugely impactful thing."[76]
"Sooner or later, we must expand life beyond this green and blue ball-or go extinct."[77]
"I would like to die on Mars; just not on impact."[78]
Elon Musk Twitter
The Musk Foundation web site
Viral Marketing, MBAs and pesky governments, Elon talks at Stanford (8 October 2003)
Statement of Elon Musk at House Space and Aeronautics Subcommittee Hearings on the Future Market for Commercial Space (20 April 2005)
Gimien, Mark (17 August 1999). "Fast Track". Salon.com. http://www.salon.com/tech/feature/1999/08/17/elon_musk/index.html.
Hoffman, Carl (June 2007). "Elon Musk Is Betting His Fortune on a Mission Beyond Earth's Orbit". Wired Magazine. http://www.wired.com/science/space/magazine/15-06/ff_space_musk.
Carlson, Nicholas (19 February 2008). "Elon Musk's Tesla caught on video smoking Scoble and Calacanis". Valleywag. http://valleywag.com/358015/elon-musks-tesla-caught-on-video-smoking-scoble-and-calacanis.
Kanellos, Michael (15 February 2008). "Elon Musk on rockets, sports cars, and solar power". CNET News.com. http://news.cnet.com/Elon-Musk-on-rockets%2C-sports-cars%2C-and-solar-power/2008-11389_3-6230661.html.
Chafkin, Max (December 2007). "Entrepreneur of the Year: Elon Musk". Inc. Magazine. http://www.inc.com/magazine/20071201/entrepreneur-of-the-year-elon-musk.html.
Bailey, Brandon (May 2010). "Elon Musk: Will his Silicon Valley story have a Hollywood ending?". San Jose Mercury News. http://www.mercurynews.com/business/ci_15138666.
History of PayPal
"An interview with Elon Musk". HobbySpace. 5 August 2003. http://www.hobbyspace.com/AAdmin/archive/Interviews/Systems/ElonMusk.html.
"Lift off with Elon Musk". Carte Blanche. 4 September 2005. http://www.carteblanche.co.za/Display/Display.asp?Id=2879.
Bergin, Chris (20 January 2006). "SpaceX's Musk and Thompson Q and A". nasaspaceflight.com. http://www.nasaspaceflight.com/2006/01/spacexs-musk-and-thompson-q-and-a/.
Video interview of Elon Musk by Zadi Diaz of EPIC FU, weekly web show that covers online pop culture (17 June 2008)
Gray, Sadie (4 January 2009). "Forget the bungalow, retire to Mars". Sunday Times (London). http://www.timesonline.co.uk/tol/life_and_style/men/article5433496.ece.
"Uber Entrepreneur: An Evening with Elon Musk" video at the Churchill Club via FORA.TV (4 January 2009)
onInnovation
An interview at the Founders Showcase. (5 August 2010)
An interview on the Kevin Pollak Chat Show. (17 September 2009)
Elon Musk: 'I'm planning to retire to Mars' a video interview for the Guardian. (1 August 2010)
An interview about commercial space flights on 60 minutes. (18 March 2012)
A 20 minute interview about sending humans to Mars with BBC's Jonathan Amos. (20 March 2012)
Elon Musk on The Daily Show with Jon Stewart on 10 April 2012
.
#(`*Peter Thiel*`)#.
Peter Andreas Thiel (German: [pet andeas til]; born October 11, 1967)[1] is a German-born American entrepreneur, venture capitalist, and hedge fund manager. Thiel co-founded PayPal with Max Levchin and served as its CEO. He currently serves as president of Clarium Capital, a global macro hedge fund with under $700million in assets under management; a managing partner in The Founders Fund, a $275million venture capital fund that he launched with Ken Howery and Luke Nosek in2005; and co-founder and investment committee chair of Mithril Capital Management.[4][5] He was the first outside investor in Facebook, the popular social-networking site, with a 10.2% stake acquired in 2004 for $500,000, and sits on the company's board of directors. Thiel was ranked #293 on the Forbes400 in 2011, with a net worth of $1.5billion as of March2012.[2] Thiel lives in San Francisco, California.[6]
Born to German parents in Frankfurt am Main, West Germany, Thiel moved to America with his parents when he was a toddler, and was raised in Foster City, California.[7] Thiel was a US-rated Chess Master and one of the highest ranked under-21 players in the country.
Thiel studied 20th-century philosophy as an undergraduate at Stanford University. He received his B.A. in Philosophy from Stanford in1989 and acquired a J.D. from Stanford Law School in1992.[8]
An avowed libertarian, he founded The Stanford Review in 1987 along with Norman Book. The Stanford Review became famous for challenging campus mores including political correctness and laws against hate speech. The Stanford Review is now the university's main conservative/libertarian newspaper.
Thiel formed friendships with other students at Stanford, many of whom contributed to the Stanford Review. These include Keith Rabois, David O. Sacks, and Reid Hoffman. Some of these friends later took up jobs at PayPal (co-founded by Thiel) and became part of the PayPal Mafia.
While studying at Stanford, Thiel also encountered Ren Girard, whose mimetic theory influenced him.
Thiel clerked for Judge J.L. Edmondson of the United States Court of Appeals for the 11thCircuit.[citation needed] From 1993 to 1996, he traded derivatives for Credit Suisse Group.[9] He founded Thiel Capital Management, a multistrategy fund, in 1996.
In 1998 Thiel co-founded PayPal, an online payments system, with Max Levchin. The company later merged with X.com, then headed by Elon Musk. PayPal went public on February15,2002, and was sold to eBay for $1.5billion later that year.[10] Thiel's 3.7percent stake in PayPal was worth approximately $55million at the time of the acquisition.[11]
According to Eric Jackson's account of PayPal in his book The PayPal Wars, Thiel viewed PayPal's mission as liberating people throughout the world from the erosion of the value of their currencies due to inflation. Jackson recalls an inspirational speech by Thiel in1999:
"We're definitely onto something big. The need PayPal answers is monumental. Everyone in the world needs money - to get paid, to trade, to live. Paper money is an ancient technology and an inconvenient means of payment. You can run out of it. It wears out. It can get lost or stolen. In the twenty-first century, people need a form of money that's more convenient and secure, something that can be accessed from anywhere with a PDA or an Internet connection. Of course, what we're calling 'convenient' for American users will be revolutionary for the developing world. Many of these countries' governments play fast and loose with their currencies," the former derivatives trader [referring to Thiel] noted, before continuing, "They use inflation and sometimes wholesale currency devaluations, like we saw in Russia and several Southeast Asian countries last year [referring to the 1998Russian financial crisis and 1997Asian financial crisis], to take wealth away from their citizens. Most of the ordinary people there never have an opportunity to open an offshore account or to get their hands on more than a few bills of a stable currency like U.S.dollars. Eventually PayPal will be able to change this. In the future, when we make our service available outside the U.S. and as Internet penetration continues to expand to all economic tiers of people, PayPal will give citizens worldwide more direct control over their currencies than they ever had before. It will be nearly impossible for corrupt governments to steal wealth from their people through their old means because if they try the people will switch to dollars or Pounds or Yen, in effect dumping the worthless local currency for something more secure."[12]
Immediately after selling PayPal, Thiel launched a global macro hedge fund, Clarium Capital, pursuing a global macro strategy. In 2005 Clarium was honored as global macro fund of the year by both MarHedge and Absolute Return, two trade magazines. Thiels approach to investing became the subject of a chapter in Steve Drobnys book, Inside the House of Money. Thiel successfully bet that the U.S.dollar would weaken in2003, and gained significant returns betting that the dollar and energy would rally in 2005. After significant losses starting in2009, Clarium dropped from $7billion dollars in assets in2008 to around $350million in2011.[13]
In 2004, well before the financial crisis of 20072010 bore him out in general terms, Thiel spoke of the dot-com bubble of 2000 having migrated, in effect, into a growing bubble in the financial sector. He specified General Electric, with its large financing arm, and WalMart as vulnerable. At the same time, he was "always talking about a real estate bubble". To illustrate, in2004, he reported having backed away from buying Martha Stewart's Manhattan duplex for $7million in the winter of 2003-2004.[9] While the apartment did sell in 2004 for $6.65million to another buyer, it was on the market but unsold in early 2010 at $15.9million,[14] and later at the reduced price of $13.9million.[15]
In August2004, Thiel made a $500,000 angel investment in the social network Facebook for 10.2% of the company and joined Facebook's board. This was the first outside investment in Facebook,[3][16] and Thiel went on to be portrayed in The Social Network (2010) by actor Wallace Langham.
In his book The Facebook Effect, David Kirkpatrick outlines the story of how Thiel came to make his investment: former Napster and Plaxo employee Sean Parker, who at the time had assumed the title of "President" of Facebook, was seeking investors for Facebook. Parker approached Reid Hoffman, the CEO of work-based social network LinkedIn. Hoffman liked Facebook but declined to be the lead investor because of the potential for conflict of interest with his duties as LinkedIn CEO. He redirected Parker to Thiel, whom he knew from their PayPal days (both Hoffman and Thiel are considered members of the PayPal Mafia). Thiel met Parker and Mark Zuckerberg, the Harvard college student who had founded Facebook and controlled it. Thiel and Zuckerberg got along well and Thiel agreed to lead Facebook's seed round with $500,000 for 10.2% of the company. Hoffman and Mark Pincus also participated in the round. The investment was originally in the form of a convertible note, to be converted to equity if Facebook reached 1.5million users by the end of 2004. Although Facebook narrowly missed the target, Thiel allowed the loan to be converted to equity anyway.[17] Thiel said of his investment:
"I was comfortable with them pursuing their original vision. And it was a very reasonable valuation. I thought it was going to be a pretty safe investment."[17]
As a board member, Thiel was not actively involved in Facebook's day-to-day decision making. According to Sarah Lacy, Thiel's main advice to Zuckerberg in their initial years was "Just dont fuck it up."[18] However, he did provide help with timing the various rounds of funding. Zuckerberg credited Thiel with helping him time Facebook's 2007 Series D to close before the 2007-2010 financial crisis.[19]
In September2010, Thiel, while expressing skepticism about the potential for growth in the consumer Internet sector, argued that relative to other Internet companies, Facebook (which then had a secondary market valuation of $30billion) was comparatively undervalued.[20] Facebook's IPO was in May 2012, with a market cap of nearly $100 billion ($38 a share), at which time Thiel sold 16.8 million shares for $638 million.[21] In August 2012, immediately upon the conclusion of the early investor lock out period, Thiel sold almost all of his remaining stake for between $19.27 and $20.69 per share, or $395.8 million, for a total of more than $1 billion.[21][22] He still retained 5 million shares and a seat on the board of directors.[23]
In 2005 Thiel created the Founders Fund, a fund for angel investments and venture capital investments. Other partners in the fund include Sean Parker, Ken Howery, and Luke Nosek.
In addition to Facebook, Thiel has made early-stage investments in numerous startups (personally or through his venture capital fund), including Booktrack, Slide, LinkedIn, Friendster, Rapleaf, Geni.com, Yammer, Yelp, Inc., Powerset, Practice Fusion, Vator, Palantir Technologies, IronPort, Votizen, Asana, Big Think, and Quora. Slide, LinkedIn, Yelp, Geni.com, and Yammer were founded by Thiel's former colleagues at PayPal: Slide by Levchin, Linkedin by Reid Hoffman, Yelp by Jeremy Stoppelman, Geni.com, Yammer by David Sacks and Xero by Rod Drury. Fortune magazine reports that PayPal alumni have founded or invested in dozens of startups with an aggregate value of around $30billion. In Silicon Valley circles, Thiel is colloquially referred to as the "Don of the PayPal Mafia", as noted in the Fortune magazine article.[24] Thiel's views on management are highly regarded, especially his famous observation that start-up success is highly correlated with low CEO pay.
Thiel founded Palantir Technologies funded by the CIA's venture capital arm In-Q-Tel.[25]
In June 2012, Peter Thiel launched Mithril, a late-stage investment fund with $402 million at the time of launch, intended for companies that were at the cusp between being private and going public.[4][5] Other partners in the fund include Jim O'Neill, co-founder of the Thiel Fellowship, and Ajay Royan, a former managing director at Clarium Capital, a hedge fund started by Thiel.
Thiel carries out most of his philanthropic activities through a nonprofit foundation created by him called the Thiel Foundation.[26]
Thiel concentrates the bulk of his philanthropic efforts on what he sees as potential breakthrough technologies. In November2010, Thiel organized a Breakthrough Philanthropy conference that showcased eight nonprofits that he believed were working on radical new ideas in technology, government, and human affairs.[27] A similar conference was organized in December2011 with the name "Fast Forward".[28]
Thiel believes in the importance and desirability of a technological singularity.[29] In February2006, Thiel provided $100,000 of matching funds to back the Singularity Challenge donation drive of the Singularity Institute for Artificial Intelligence. Additionally, he joined the Institute's advisory board and participated in the May2006 Singularity Summit at Stanford as well as at the 2011Summit held in New York City.
In May2007, Thiel provided half of the $400,000 matching funds for the annual Singularity Challenge donation drive.
The Singularity Institute was a participant in the Breakthrough Philanthropy conference (November2010) and the Fast Forward conference (December2011).
In September2006, Thiel announced that he would donate $3.5million to foster anti-aging research through the Methuselah Mouse Prize foundation.[30] He gave the following reasons for his pledge: "Rapid advances in biological science foretell of a treasure trove of discoveries this century, including dramatically improved health and longevity for all. Im backing Dr. [Aubrey] de Grey, because I believe that his revolutionary approach to aging research will accelerate this process, allowing many people alive today to enjoy radically longer and healthier lives for themselves and their loved ones."
The Thiel Foundation supports the research of the SENSFoundation, currently headed by Dr.deGrey, that is working to achieve the reversal of biological aging. The Thiel Foundation also supports the work of anti-aging researcher Cynthia Kenyon.
The SENSFoundation was a participant in the Breakthrough Philanthropy conference (November2010) and the Fast Forward conference (December2011).
On April15,2008, Thiel pledged $500,000 to the new Seasteading Institute, directed by Patri Friedman, whose mission is "to establish permanent, autonomous ocean communities to enable experimentation and innovation with diverse social, political, and legal systems".[31] This was followed in February 2010 by a subsequent grant of $250,000, and an additional $100,000 in matching funds.[32]
In a talk at the Seasteading Institute conference in November2009, Thiel explained why he believed that seasteading was necessary for the future of humanity.[33]
In2011 Thiel was reported as having given a total of $1.25million to the Seasteading Institute.[34]
The Seasteading Institute was a participant in the Breakthrough Philanthropy conference (November 2010) and the Fast Forward conference (December 2011).
On September29,2010, Thiel said he had created a new fellowship called the Thiel Fellowship, which will award $100,000 each to 20 people under 20 years old,[35] in order to spur them to quit college and create their own ventures.[36]
In October 2011, the Thiel Foundation announced the creation of Breakout Labs, a grant-making program intended to fund early-stage scientific research that may be too radical or innovative for traditional scientific funding bodies but also too long-term and speculative for venture investors.[37] In April 2012, Breakout Labs announced its first set of grantees.[38]
The Thiel Foundation is a supporter of the Committee to Protect Journalists, which promotes the right of journalists to report the news freely without fear of reprisal.[39]
The Thiel Foundation is also a supporter of the Human Rights Foundation which organizes the Oslo Freedom Forum.[40]
Thiel is listed as a member of the Steering Committee of The Bilderberg Group, a controversial group of influential business and government leaders who meet annually behind closed doors under a media blackout to discuss world issues.[41]
Thiel, who is openly gay, has supported gay-rights causes such as the American Foundation for Equal Rights and GOProud.[42]
In 2009, it was reported that Thiel helped fund college student James O'Keefe's "Taxpayers Clearing House" video - a satirical look at the politics behind the Wall Street bailout.[43] O'Keefe went on to produce the ACORN undercover sting videos.[44]
Thiel was the largest contributor to the conservative Club for Growth a political action committee in July 2012, donating 1 million dollars in a single donation.[45]
In December 2007, Thiel endorsed Ron Paul for President.[46]
In 2010, Thiel supported Meg Whitman, who as CEO of eBay had purchased PayPal from Thiel and his co-founders and investors, in her unsuccessful bid for the governorship of California. He contributed the maximum allowable $25,900 to the Whitman campaign.[47]
In 2012, Thiel, along with PayPal co-founder Luke Nosek and Scott Banister, an early adviser and board member, put their support behind the Endorse Liberty Super PAC, alongside Internet advertising veteran Stephen Oskoui and entrepreneur Jeffrey Harmon, who founded Endorse Liberty in November 2011. Collectively Thiel et al. gave $3.9 million to Endorse Liberty, whose purpose was to promote Texas congressman Ron Paul for president in 2012. As of January 31, 2012(2012 -01-31)[update], Endorse Liberty reported spending about $3.3 million promoting Paul by setting up two YouTube channels, buying ads from Google and Facebook and StumbleUpon, and building a presence on the Web.[48]
Thiel is an occasional commentator on CNBC, having appeared numerous times on both Closing Bell with Maria Bartiromo, and Squawk Box with Becky Quick.[49] He has been interviewed twice by Charlie Rose on PBS.[50]
Thiel has contributed articles to The Wall Street Journal, First Things, Forbes, and Policy Review, the journal published by the Hoover Institution, on whose board he sits.
In 2006, Thiel won the Herman Lay Award for Entrepreneurship.[51] In 2007, he was honored as a Young Global leader by the World Economic Forum as one of the 250 most distinguished leaders age 40 and under.[52] On November 7, 2009, Thiel was awarded an honorary degree from Universidad Francisco Marroquin[53] In 2012, Students For Liberty, an organization dedicated to spreading libertarian ideals on college campuses, awarded Thiel its "Alumnus of the Year" award, and Thiel delivered the keynote address at the 2012 International Students For Liberty Conference.[54]
Thiel is the co-author, with David O. Sacks, of the 1995 book The Diversity Myth: 'Multiculturalism' and the Politics of Intolerance at Stanford. The book was critical of what it perceived as political correctness and a dilution of academic rigor. It "drew a sharp rebuttal from then-Stanford Provost (and later President George W. Bush's National Security Advisor) Condoleezza Rice."[3] According to his 2011 New Yorker profile, Thiel has backtracked somewhat from his assertions in the book:
All of the identity-related things are in my mind much more nuanced, he said. I think there is a gay experience, I think there is a black experience, I think there is a womans experience that is meaningfully different. I also think there was a tendency to exaggerate it and turn it into an ideological category. But his reaction against political correctness, he said, was just as narrowly ideological.
Thiel was the co-producer of Thank You for Smoking, a 2005 feature film based on Christopher Buckley's 1994 novel of the same name.[citation needed]
Thiel occasionally teaches classes at Stanford University, his alma mater. For instance, in the Spring of 2012, he taught a class on startups using a social network made by Lore, one of his portfolio companies.[55][56]
Epub version of the notes from the lectures of Peter Thiel in Stanford University, CS183: Startup:
Managing partner in The Founders Fund
President of Clarium Capital
Former CEO and Co-founder of PayPal
Financial backer in Facebook
1 Early life

1.1 Childhood
1.2 College and law school


1.1 Childhood
1.2 College and law school
2 Career

2.1 Early career
2.2 PayPal
2.3 Clarium Capital
2.4 Facebook
2.5 Angel investor and venture capitalist
2.6 Mithril: a late-stage investment fund


2.1 Early career
2.2 PayPal
2.3 Clarium Capital
2.4 Facebook
2.5 Angel investor and venture capitalist
2.6 Mithril: a late-stage investment fund
3 Philanthropy

3.1 Theory of philanthropy
3.2 Singularity Institute
3.3 Anti-aging research
3.4 Seasteading
3.5 Thiel Fellowship
3.6 Breakout Labs
3.7 Other causes


3.1 Theory of philanthropy
3.2 Singularity Institute
3.3 Anti-aging research
3.4 Seasteading
3.5 Thiel Fellowship
3.6 Breakout Labs
3.7 Other causes
4 Political activities

4.1 Bilderberg Group
4.2 Support for political activism
4.3 Support for political candidates


4.1 Bilderberg Group
4.2 Support for political activism
4.3 Support for political candidates
5 Other pursuits

5.1 Media appearances and commentary
5.2 Awards and honors
5.3 The Diversity Myth
5.4 Thank You For Smoking
5.5 Teaching


5.1 Media appearances and commentary
5.2 Awards and honors
5.3 The Diversity Myth
5.4 Thank You For Smoking
5.5 Teaching
6 References
7 Further reading
8 External links
1.1 Childhood
1.2 College and law school
2.1 Early career
2.2 PayPal
2.3 Clarium Capital
2.4 Facebook
2.5 Angel investor and venture capitalist
2.6 Mithril: a late-stage investment fund
3.1 Theory of philanthropy
3.2 Singularity Institute
3.3 Anti-aging research
3.4 Seasteading
3.5 Thiel Fellowship
3.6 Breakout Labs
3.7 Other causes
4.1 Bilderberg Group
4.2 Support for political activism
4.3 Support for political candidates
5.1 Media appearances and commentary
5.2 Awards and honors
5.3 The Diversity Myth
5.4 Thank You For Smoking
5.5 Teaching
Drobny, Steven (2006). Inside the House of Money. The Dot-Commer: Wiley. ISBN0-471-79447-3. http://www.amazon.com/dp/0471794473.
Thiel Foundation profile
The Diversity Myth: Multiculturalism and Political Intolerance on Campus book by David O. Sacks and Peter A. Thiel, published by The Independent Institute
Honorary Doctoral Degrees, Universidad Francisco Marroqun
Peter Thiel compares Silicon Valley bloggers to Al Qaeda
Virtual Money on ITConversations.com - November 2004
The Optimistic Thought Experiment essay by Peter Thiel published in Policy Review
Bloomberg Markets profile of Thiel
[http://newmedia.ufm.edu/gsm/index.php/Listados:Peter_Thiel Audiovisual Media, NewMedia Universidad Francisco Marroqun
Can Entrepreneurs change the world?: Peter Thiel interviewed Luis Figueroa
Crunchbase profile of Peter Thiel
http://stanfy.com/blog/2012/07/09/epub-peter-thiel/
.
#(`*Salman Khan (educator)*`)#.
Salman Amin 'Sal' Khan[2] born October 11, 1976) is an American educator, entrepreneur, and former hedge fund analyst. He is the founder of the Khan Academy, a free online education platform and nonprofit organization. From a small office in his home, Khan has produced over 3,000 videos teaching a wide spectrum of academic subjects, mainly focusing on mathematics and the sciences. As of September 2012, the Khan Academy channel on YouTube attracted 400,000 subscribers.[3] In 2012, Time named Salman Khan in its annual list of the 100 most influential people in the world.[4] Forbes magazine put Salman Khan on its cover with the story "$1 Trillion Opportunity".[5]
Khan was born in New Orleans, Louisiana. His father is from Barisal, Bangladesh and mother was from Kolkata, India.[6] He was raised by his mother in New Orleans, where she had a series of jobs and businesses.[7] He went to public schools, where, as he recalls, "a few classmates were fresh out of jail and others were bound for top universities". Salman Khan holds four degrees: a BS in mathematics, a BS in electrical engineering and computer science, as well as an MS in electrical engineering and computer science from Massachusetts Institute of Technology, and an MBA from Harvard Business School.[8][9] He is married to Umaima Marvi, who is a physician.[10]
Salman Khan worked as a hedge fund analyst before quitting in late 2009.[11][12]
In late 2004, Khan began tutoring his cousin, Nadia, in mathematics over the internet using Yahoo!'s Doodle notepad.[13] When other relatives and friends sought his tutoring, he decided it would be more practical and beneficial to distribute the tutorials on YouTube where he created an account on 16 November 2006.[14] Their popularity on the video sharing website and the testimonials of appreciative students prompted Khan to quit his job as a hedge fund analyst in late 2009 to focus on developing his YouTube channel, Khan Academy, full-time with the aid of his long time friend Josh Gefner.[12]
His videos received more than 200 million views in just a few years. Students from around the world have been attracted to Khan's concise, practical, and relaxed teaching method.[14]
Khan outlined his mission as to "accelerate learning for students of all ages. With this in mind, we want to share our content with whoever may find it useful." Khan also plans to extend his "free school" to cover topics such as English. Programs are being undertaken to use Khan's videos to teach those in isolated areas of Africa and Asia. He delineated his motives: "With so little effort on my own part, I can empower an unlimited amount of people for all time. I can't imagine a better use of my time."[15]
Salman Khan has been featured on the The Colbert Report,[11] PBS NewsHour,[16][17][18] CNN,[19] and National Public Radio.[20] In 2009, the Khan Academy received the Microsoft Tech Award for education.[21] In 2010, Google provided $2 million to support the creation of more courses and to enable the Khan Academy to translate its core library into the worlds most widely spoken languages.[22] In October 2010, Khan was tied for #34 in Fortune's annual "40 under 40", a list recognizing business's hottest rising stars.[23] In March 2011, Salman Khan was invited to speak at TED by Bill Gates who says he uses the Khan Academy Exercise Software to teach his own children. In May 2011, Salman Khan appeared on The Colbert Report to talk in an interview about his teachings. He told the audience how he planned to use his software to revolutionize the field of education. Included in his interview was a montage of notable teaching moments from Khan's YouTube channel.
Khan has also been interviewed by journalist Charlie Rose, and news anchor Tom Brokaw.[24]
Khan spoke at Bellarmine College Prep during TEDxSanJoseCA about the importance of education and the founding of Khan Academy. Khan was also the commencement speaker for Rice University's commencement exercises on May 12, 2012 as well as MIT's commencement on June 8, 2012.[25][26]
He lives with his wife, Umaima Marvi, who is a medical specialist in rheumatology and internal medicine, with their son and daughter in Mountain View, California.[27]
1 Early life and education
2 Career

2.1 Khan Academy


2.1 Khan Academy
3 Recognition
4 Personal life
5 References
6 External links
2.1 Khan Academy
Khan Academy web site
Salman Khan on Charlie Rose
Salman Khan at TED
.
#(`*Khan Academy*`)#.
The Khan Academy is a non-profit[2] educational organization and a website created in 2006 by Bangladeshi-Indian-American [3] educator Salman Khan, a graduate of MIT and Harvard Business School. With the stated mission of "providing a high quality education to anyone, anywhere", the website supplies a free online collection of more than 3,600 micro lectures via video tutorials stored on YouTube teaching mathematics, history, healthcare and medicine, finance, physics, chemistry, biology, astronomy, economics, cosmology, organic chemistry, American civics, art history, macroeconomics, microeconomics, and computer science.[4] Khan Academy launched a computer science module in September 2012.[5][6] Khan Academy has delivered over 200 million lessons.[7]
The founder of the organization, Salman Khan, was born to a Bangladeshi father and Indian mother from Kolkata living in New Orleans, Louisiana, United States.[8] His father is from Barisal, Bangladesh.[9] After earning three degrees from the Massachusetts Institute of Technology (a BS in mathematics, a BS in electrical engineering and computer science, and an MS in electrical engineering and computer science), he pursued an MBA from Harvard Business School. In late 2004, Khan began tutoring his cousin Nadia in mathematics using Yahoo!'s Doodle notepad. When other relatives and friends sought similar help, he decided it would be more practical to distribute the tutorials on YouTube.[8][10] Their popularity there and the testimonials of appreciative students prompted Khan to quit his job in finance as a hedge fund analyst at Connective Capital Management in 2009, and focus on the tutorials (then released under the moniker "Khan Academy") full-time.[10] Bill Gates once said that "I'd say we've moved about 160 IQ points from the hedge fund category to the teaching-many-people-in-a-leveraged-way category. It was a good day his wife let him quit his job".[11]
The project is funded by donations. Khan Academy can be a 501(c)(3) not-for-profit organization,[2] now with significant backing from the Bill & Melinda Gates Foundation and Google. Several people have made US$10,000 contributions; Ann and John Doerr gave $100,000; total revenue is about $150,000 in donations. Additionally, it earned $2,000 per month from ads on the website in 2010 until Khan Academy ceased to accept advertising.[12] In 2010, Google announced it would give the Khan Academy $2 million for creating more courses and for translating the core library into the worlds most widely spoken languages, as part of their Project 10100.[13]
Khan Academy has eclipsed MIT's OpenCourseWare (OCW) in terms of videos viewed. Its YouTube channel has more than 202 million total views, compared to MIT's 43 million. It also has twice as many subscribers, at more than 429,000.[14][15][16]
Khan Academy also had a language release in mid-2012. It was supported by volunteers[17] from Amara and included Indonesian, German, Spanish, French, Italian, Swahili, Norwegian, Polish, Portuguese, Russian, Turkish, Xhosa, Greek, Bulgarian, Ukrainian, Urdu, Arabic, Persian, Bengali, Hindi, Chinese.
The Khan Academy started with Khan remotely tutoring one of his cousins interactively using Yahoo Doodle images. Based on feedback from his cousin, additional cousins began to take advantage of the interactive, remote tutoring. In order to make better use of his and their time, Khan transitioned to making YouTube video tutorials.[18] Drawings are now made with a Wacom tablet and the free natural drawing application SmoothDraw 3, and recorded with screen capture software from Camtasia Studio.[19]
All videos (hosted via YouTube) are available through Khan Academy's own website, which also contains many other features such as progress tracking, practice exercises, and a variety of tools for teachers in public schools. Logging into the site can be done via a Google or a Facebook account for those who do not want to create a separate Khan Academy account.
Khan chose to avoid the standard format of a person standing by a whiteboard, deciding instead to present the learning concepts as if "popping out of a darkened universe and into one's mind with a voice out of nowhere" in a way akin to sitting next to someone and working out a problem on a sheet of paper: "If you're watching a guy do a problem [while] thinking out loud, I think people find that more valuable and not as daunting".[20] Offline versions of the videos have been distributed by not-for-profit groups to rural areas in Asia, Latin America, and Africa.[8][21] While the current content is mainly concerned with pre-college mathematics and physics, Khan's long-term goal is to provide "tens of thousands of videos in pretty much every subject" and to create "the world's first free, world-class virtual school where anyone can learn anything".[8]
Khan Academy also provides a web-based exercise system that generates problems for students based on skill level and performance. The exercise software is available as open source under the MIT license.[22] Khan believes his academy points an opportunity to overhaul the traditional classroom by using software to create tests, grade assignments, highlight the challenges of certain students, and encourage those doing well to help struggling classmates.[10] The tutorials are touted as helpful because, among other factors, they can be paused by students, while a classroom lecture cannot be.[23]
The success of his low-tech, conversational tutorialsKhan's face never appears, and viewers see only his unadorned step-by-step doodles and diagrams on an electronic blackboardsuggests an educational transformation that de-emphasizes lecture-based classroom interactions.[24]
The major components of Khan Academy include:[25]
Not-for-profit partner organizations are making the content available outside YouTube. The Lewis Center for Educational Research, which is affiliated with NASA, is bringing the content into community colleges and charter schools around the United States. World Possible is creating offline snapshots of the content to distribute in rural, developing regions with limited or no access to the Internet.[8][29]
Khan has stated a vision of turning the academy into a charter school:
This could be the DNA for a physical school where students spend 20 percent of their day watching videos and doing self-paced exercises and the rest of the day building robots or painting pictures or composing music or whatever.[12]
A November 2011 grant of $5 million from Ireland-based The O'Sullivan Foundation, founded by Avego MD and cloud computing pioneer Sean O'Sullivan, will be directed to three initiatives:
Recent teaching appointees as a result of the grant include Dr. Steven Zucker, formerly of Pratt Institute, and Dr. Beth Harris, from the SmARThistory project at the Museum of Modern Art in New York, to produce art and history content. YouTube video creators Vi Hart and Brit Cruise have also joined the teaching faculty.[30]
A series of summer school camps are planned to start in Northern California from June 2012 to test curricula for real-world schools.[31]
There are both critics and proponents of Khan Academy, disagreeing on the actual effectiveness and potential that it has. Some critics have stated that the Khan videos are insubstantial, "repetitive," and "leave kids staring at screens" rather than promoting interaction and active engagement.[41] Although Salman Khan himself has addressed the fact that there is no "consistent, comprehensive plan" for revamping school courses, critics are unsure of the reliability and wide-spread application of Khan Academy.[42]
There are many however, that support Khan Academy for its technological ingenuity and its ability to introduce different educational dynamics. The Khan Academy's ability to freely distribute lessons has demonstrated technology's ability to eliminate economic barriers that prevent effective education. Since Benjamin Bloom's 1984 study on the effectiveness of "one-on-one tutoring," close student-teacher interaction has been aggressively sought after.[43] However, both the cost and the realistic implementation of this ideal has been an issue. Critics promote that the Khan Academy has addressed these issues, through both cost-free nature of the site and its wide accessibility via the internet.
1 History
2 Technical format
3 Services and vision
4 Educational Impact
5 See also
6 References
7 External links
a video library with over 3600 videos in various topic areas and over 200 million lessons delivered.[26][27] These videos are licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License.[28]
automated exercises with continuous assessment; as of December 2012[update], there are 381 practice exercises, mainly in math.
peer-to-peer tutoring based on objective data collected by the system, a process that will be projected in the future.
Salman Khan has been featured in the San Francisco Chronicle,[10] on the Public Broadcasting Service (PBS),[4] National Public Radio, CNN,[32] and CNN Money.[24]
In 2009, the Khan Academy received the Microsoft Tech Award for education.[21]
In 2010 at the Aspen Ideas Festival, Bill Gates endorsed the learning resource, calling it "unbelievable" and saying "I've been using [Khan Academy] with my kids".[11][33]
In 2010, Google's Project 10100 provided $2 million to support the creation of more courses, to allow for translation of the Khan Academy's content, and to allow for the hiring of additional staff.[34]
In 2011, Salman Khan delivered a TED talk.[35]
On 4 May 2011, Salman Khan appeared on Charlie Rose.[36]
Salman Khan appeared on The Colbert Report on 2 June 2011.
An article featuring Khan Academy and Salman Khan appeared in the August 2011 issue of Wired Magazine.[37]
Salman Khan was featured as a "Big Thinker" on Edutopia discussing flip teaching.[38]
In November 2011, the Khan Academy received a $5 million grant from the O'Sullivan Foundation.[39]
In March 2012, Khan Academy was featured on CBS 60 Minutes.[40]
In April 2012, Salman Khan was listed among the Time 100 Most Influential People for 2012
The Khan Academy app features in many 'best educational apps' lists including this one from BBC Active: BBC Active's list of the best educational apps
Coursera
Udacity
OpenCourseWare
Open educational resources
Open textbook
Bookboon
China Open Resources for Education
Connexions
Curriki
Flat World Knowledge
Flexbook
Free High School Science Texts South Africa
Lesson Planet
MIT OpenCourseWare
National Programme on Technology Enhanced Learning India
Open.Michigan
Tufts OpenCourseWare
Khan Academy Homepage
Khan Academy's channel on YouTube
Khan Academy on Facebook
Browse Khan - Watch a Video/Jump to Playlist drop-down Menu
Khan Academy videos(Physics,Chemistry and Biology) in Bangla
Khan Academy videos overdubbed in Italian
In the Media: Khan Academy-Related Talks and Interviews
The 10 Most Innovative Companies in Education (Khan Academy #3)
Notes on Khan Academy videos
http://www.cbsnews.com/video/watch/?id=7401696n, as seen on 60 Minutes, Sunday March 8, 2012.
.
#(`*The Great Gatsby*`)#.
The Great Gatsby is a novel by American author F. Scott Fitzgerald. The book takes place from spring to autumn 1922, during a prosperous time in the United States known as the Roaring Twenties. Although it did not receive widespread attention until after Fitzgerald's death in 1940, today the book is widely regarded as a "Great American Novel" and a literary classic. The Modern Library named it the second best English-language novel of the 20th Century.[1]
With The Great Gatsby, Fitzgerald made a conscious departure from the writing process of his previous novels. He started planning it in June 1922,[citation needed] after completing his play The Vegetable and began composing The Great Gatsby [2] in 1923. He ended up discarding most of it as a false start, some of which resurfaced in the story "Absolution".[3] Unlike his previous works, Fitzgerald intended to edit and reshape Gatsby thoroughly, believing that it held the potential to launch him toward literary acclaim. He told his editor Maxwell Perkins that the novel was a "consciously artistic achievement" and a "purely creative work not trashy imaginings as in my stories but the sustained imagination of a sincere and yet radiant world". He added later, during editing, that he felt "an enormous power in me now, more than I've ever had".[4]
After the birth of their child, the Fitzgeralds moved to Great Neck, Long Island in October 1922, a setting used as the scene for The Great Gatsby.[6] Fitzgerald's neighbors in Great Neck included such prominent and newly wealthy New Yorkers as writer Ring Lardner, actor Lew Fields and comedian Ed Wynn.[3] These figures were all considered to be 'new' money, unlike those who came from Manhasset Neck or Cow Neck Peninsula, places which were home to many of New York's wealthiest established families, and which sat across a bay from Great Neck. This real-life juxtaposition gave Fitzgerald his idea for "West Egg" and "East Egg." In this novel, Great Neck became the new-money peninsula of "West Egg" and Manhasset the old-money peninsula of "East Egg".[7]
Progress on the novel was slow. In May 1923, the Fitzgeralds moved to the French Riviera, where the novel was finished. In November 1923 he sent the draft to his editor Maxwell Perkins and his agent Harold Ober. The Fitzgeralds then moved to Rome for the winter. Fitzgerald made revisions through the winter after Perkins informed him that the novel was too vague and Gatsby's biographical section too long. Content after a few rounds of revision, Fitzgerald returned the final batch of revised galleys in the middle of February 1925.[8]
The cover of The Great Gatsby is among the most celebrated pieces of jacket art in American literature.[9] A little-known artist named Francis Cugat was commissioned to illustrate the book while Fitzgerald was in the midst of writing it. The cover was completed before the novel, with Fitzgerald so enamored of it that he told his publisher he had "written it into" the novel.[9]
Fitzgerald's remarks about incorporating the painting into the novel led to the interpretation that the eyes are reminiscent of those of Dr. T. J. Eckleburg (the novel's erstwhile optometrist on a faded commercial billboard near George Wilson's auto repair shop) which Fitzgerald described as "blue and gigantic their retinas are one yard high. They look out of no face, but instead, from a pair of enormous yellow spectacles which pass over a non-existent nose." Although this passage has some resemblance to the painting, a closer explanation can be found in the description of Daisy Buchanan as the "girl whose disembodied face floated along the dark cornices and blinding signs".[9]
Ernest Hemingway recorded in A Moveable Feast that when Fitzgerald lent him a copy of The Great Gatsby to read, he immediately disliked the cover, but "Scott told me not to be put off by it, that it had to do with a billboard along a highway in Long Island that was important in the story. He said he had liked the jacket and now he didn't like it."[10]
Fitzgerald was ambivalent about the title, making it hard for him to choose. He entertained many choices before settling on The Great Gatsby. Fitzgerald shifted between Gatsby; Among Ash-Heaps and Millionaires; Trimalchio; Trimalchio in West Egg; On the Road to West Egg; Under the Red, White, and Blue; Gold-Hatted Gatsby and The High-Bouncing Lover. Initially, he preferred Trimalchio, after the crude parvenu in Petronius's Satyricon. Unlike Fitzgerald's protagonist, Trimalchio participated in the audacious and libidinous orgies that he hosted. That Fitzgerald refers to Gatsby by the proposed title once in the novel reinforces the view that it would have been a misnomer. As Tony Tanner observed, there are subtle similarities between the two.[11]
On November 7, 1924, Fitzgerald wrote to Perkins. "I have now decided to stick to the title I put on the book [...] Trimalchio in West Egg" but was eventually persuaded that the reference was too obscure and that people would not be able to pronounce it. His wife and Perkins both expressed their preference for The Great Gatsby and the next month Fitzgerald agreed.[12] A month before publication, after a final review of the proofs, he asked if it would be possible to re-title it Trimalchio or Gold-Hatted Gatsby but Perkins advised against it. On March 19, Fitzgerald asked if the book could be renamed Under the Red, White and Blue but it was at that stage too late to change. The Great Gatsby was published on April 10, 1925. Fitzgerald remarked that "the title is only fair, rather bad than good".[13]
The time is the summer of 1922 and the narrator is Nick Carraway, a Yale graduate and World War I veteran who takes a job in New York. He rents a small house on Long Island, next door to the mansion of Jay Gatsby, a mysterious millionaire who holds extravagant parties.
Across the bay lives his attractive second cousin Daisy with her rich husband Tom Buchanan, who was at Yale with Nick. They ask him to lunch, where he meets a girl called Jordan Baker, but the atmosphere is spoiled when Tom answers a telephone call from his mistress Myrtle.
She is the unhappy wife of George Wilson, who owns an unsuccessful garage in the Valley of Ashes on the outskirts of the city. Tom takes Nick to the flat in New York where he meets Myrtle and holds parties, but once again Tom ruins the occasion upon hitting Myrtle and breaking her nose.
Nick gets an invitation to one of Gatsbys huge parties, which he attends with Jordan. Most guests seem to be uninvited and not to know their host, who keeps aloof. However Gatsby befriends Nick, taking him to lunch in New York with a business associate, a notorious gangster called Meyer Wolfsheim. Gatsby then asks Nick, through Jordan, to arrange a meeting with Daisy. In 1917, though from a modest family and penniless, he had hoped to marry her but was sent to Europe to fight. Now he is rich, has bought a house near her and throws enormous parties in the hope she will attend.
Nick asks them both to tea, after which Gatsby shows them round his opulent mansion. Daisy, unhappy with the unpleasant Tom, is ready to revive her relationship with Gatsby. Daisy asks Gatsby to lunch at her house, together with Nick and Jordan. She then suggests that they all go into New York. Tom, Jordan and Nick get into Gatsby's car while Daisy and Gatsby follow in Tom's car. At Wilsons garage, Tom stops to fill up and is told by an unhappy Wilson that he knows Myrtle has a lover.
The group goes to the Plaza Hotel, where Tom angrily confronts Gatsby over his relationship with Daisy and his criminal activities. Gatsby challenges Daisy to choose him, her first love, and to deny she ever loved Tom. She avoids both and, overwrought, begs to go home. Daisy sets off with Gatsby in his car, followed by the rest in Tom's car.
As Daisy passes Wilsons garage, Myrtle runs into the road and, hit by the car, is killed. Daisy in panic drives on but Tom stops and finds the corpse. Back home, Tom and Daisy achieve a reconciliation, pack up and hastily leave. Having been told by Tom that Gatsby was to blame, Wilson finds Gatsby in his swimming pool, shoots him dead and then kills himself.
Nick is the only one left. He arranges Gatsbys funeral, which is avoided by all his former friends and attended only by his father. The old man tells Nick that his dead friend was a poor boy from North Dakota called James Gatz. Disgusted by the whole set-up and no longer interested in the unreliable Jordan, Nick gives up both his job and his house to return to his native Midwest, acknowledging that the five main characters of the novel  Gatsby, Tom, Daisy, Jordan and Nick  were all Westerners who, in some fundamental way, failed to adapt to the standards of the East.

The Great Gatsby received mostly positive reviews when it was first published [17] and many of Fitzgerald's literary friends wrote him letters praising the novel. However, Gatsby did not experience the commercial success of Fitzgerald's previous two novels, This Side of Paradise and The Beautiful and Damned, and although the novel went through two initial printings, some of these copies remained unsold years later.[18]
When Fitzgerald died in 1940, he had been largely forgotten. His obituary in The New York Times mentioned Gatsby as evidence of great potential that was never reached.[19] Gatsby gained readers when Armed Services Editions gave away around 150,000 copies of the novel to the American military in World War II.[20]
In 1951 Arthur Mizener published The Far Side of Paradise, a biography of Fitzgerald. By the 1960s, Gatsby's reputation was established, and it is frequently mentioned as one of the great American novels.
Gatsby has been adapted numerous times, in various media. In addition, an early draft of the novel is now available as Trimalchio: An Early Version of "The Great Gatsby".
The Great Gatsby has been filmed five times and is being filmed for the sixth time:
The Great Catsby, a South Korean TV drama adaptation, based on a South Korean web comic adaptation of The Great Gatsby, was filmed in 2007, starring Kang Kyeong-joon, Park Ye-jin and MC Mong.
The second season of the Showtime television series Californication, starting with its second episode "The Great Ashby", is partly a modern take on the novel, with the characters Lew Ashby, Janie Jones and Hank Moody as modern versions of Jay Gatsby, Daisy Buchanan and Nick Carraway.[22][23]
In the HBO series Entourage the show's main character Vincent Chase stars in a fictional film based off the book entitled Gatsby playing the role of Nick Carraway with the film directed by Martin Scorsese.
An operatic treatment of the novel was commissioned from John Harbison by the New York Metropolitan Opera to commemorate the 25th anniversary of the debut of James Levine. The work, which is also called The Great Gatsby, premiered on December 20, 1999.[24]
1 Writing and publication

1.1 Original cover art
1.2 Title


1.1 Original cover art
1.2 Title
2 Plot

2.1 Major characters
2.2 Secondary characters


2.1 Major characters
2.2 Secondary characters
3 Reception
4 Adaptations

4.1 Film
4.2 Television
4.3 Opera
4.4 Books
4.5 Radio
4.6 Music
4.7 Theater
4.8 Computer games


4.1 Film
4.2 Television
4.3 Opera
4.4 Books
4.5 Radio
4.6 Music
4.7 Theater
4.8 Computer games
5 See also
6 Notes
7 References
8 External links
1.1 Original cover art
1.2 Title
2.1 Major characters
2.2 Secondary characters
4.1 Film
4.2 Television
4.3 Opera
4.4 Books
4.5 Radio
4.6 Music
4.7 Theater
4.8 Computer games
Nick Carraway (narrator) a man from the Midwest, a Yale graduate, a World War I veteran, and a resident of West Egg. He is Gatsby's next-door neighbor and a bond salesman.
Jay Gatsby (originally James Gatz) a young, mysterious millionaire with shady business connections (later revealed to be a bootlegger), originally from North Dakota. He is obsessed with Daisy Buchanan, whom he had met when he was a young officer stationed in the south during World War I. Based on the bootlegger and former World War I officer Max Gerlach, according to Some Sort of Epic Grandeur, Matthew J Bruccoli's biography of F. Scott Fitzgerald.
Daisy Buchanan ne Fay an attractive and effervescent, if shallow, young woman, identified as a flapper.[14] She's Nick's second cousin, once removed; and the wife of Tom Buchanan. Daisy is believed to have been inspired by Fitzgerald's own youthful romances with Ginevra King and Zelda Sayre. Daisy once had a romantic relationship with Gatsby, before she married Tom. Her choice between Gatsby and Tom is one of the central conflicts in the novel.
Tom Buchanan a millionaire who lives on East Egg, and Daisy's husband. Buchanan has parallels with William Mitchell, the Chicagoan who married Ginevra King. Buchanan and Mitchell were both Chicagoans with an interest in polo. Like Ginevra's father, whom Fitzgerald resented, Buchanan attended Yale and is a white supremacist.[15]
Jordan Baker Daisy Buchanan's long-time friend, Nick Carraway's "girlfriend", and a professional golfer with a slightly shady reputation. Fitzgerald told Maxwell Perkins that Jordan was based on the golfer Edith Cummings, a friend of Ginevra King.[15] Her name is a play on the two then-popular automobile brands, the Jordan Motor Car Company and the Baker Motor Vehicle, alluding to Jordan's "fast" reputation and the freedom now presented to Americans, especially women, in the 1920s.[citation needed]
George B. Wilson a mechanic and owner of a garage. Wilson is disliked by Tom Buchanan and his own wife, Myrtle Wilson, who describes him as "so dumb he doesn't know he's alive." When he learns of the death of his wife, he shoots and kills Gatsby, wrongly believing he had been driving the car that killed Myrtle, and then kills himself.
Myrtle Wilson George's wife, and Tom Buchanan's mistress. She is accidentally killed after being hit by a car driven by Daisy, though Gatsby takes the blame for it.
Meyer Wolfshiem a Jewish man Gatsby describes as a gangster/gambler who had fixed the World Series. Wolfshiem is a clear allusion to Arnold Rothstein, a New York crime kingpin who was notoriously blamed for the Black Sox Scandal which tainted the 1919 World Series.[16]
Catherine Myrtle Wilson's sister.
Chester and Lucille McKee Myrtle's New York friends.
"Owl-eyes"a drunken party-goer whom Nick meets in Gatsby's library. One of the few characters in the novel to attend Gatsby's funeral and he represents the Lost Generation itself.
Ewing "The Boarder" Klipspringer a sponger who virtually lives at Gatsby's mansion.
Pammy Buchanan the Buchanans' three-year-old daughter, who is a corrupted child.
Henry C. Gatz Gatsby's somewhat estranged father in North Dakota. One of the only characters in the book to attend Gatsbys funeral.
Mr. and Mrs. Sloane- a couple that visits Gatsby's house with Tom.
Michaelis George Wilson's Greek neighbor.
Dan Cody a wealthy adventurer who was Gatsby's mentor as a youth.
Ernesto Quionez's Bodega Dreams adapted The Great Gatsby to Spanish Harlem
The Great Gatsby, a graphic novel adaptation by Australian cartoonist Nicki Greenberg
The Double Bind by Chris Bohjalian imagines the later years of Daisy and Tom Buchanan's marriage as a social worker in 2007 investigates the possibility that a deceased elderly homeless person is Daisy's son.
The young adult novel Jake Reinvented by Gordon Korman is a modern version of The Great Gatsby in which the characters are high school students.
Daisy Buchanan's Daughter (2011) by Tom Carson is the purported autobiography of Tom and Daisy Buchanan's daughter
The Late Gatsby (2012) is a mashup of The Great Gatsby and a vampire narrative.[25]
In October 2008, the BBC World Service commissioned and broadcast an abridged 10-part reading of the story, read from the view of Nick Carraway by Trevor White.[26]
In May 2012 BBC Radio 4 broadcast The Great Gatsby, a Classic Serial dramatisation by Robert Forrest.[27]
In April 2010, the folk duo Reg & Phil released a song entitled "Daisy Buchanan" on their self-titled album. The song, told by an anonymous narrator, directly addresses the novel's title character.[28]
Ballad group 2AM released an EP in 2012 titled 'F.Scott Fitzgerald's Way of Love', which draws inspiration from and narrates the story of Jay Gatsby's love for Daisy.[29]
The Great Gatsby Musical was due to open at the Kings Head Theatre, London, on August 7, 2012. A Ruby In The Dust production, it is adapted by Joe Evans and Linnie Reedman with music and lyrics by Joe Evans, directed by Linnie Reedman, with Matilda Sturridge as Daisy Buchanan.
Simon Levy's stage adaptation,[30] the only one authorized and granted exclusive rights by the Fitzgerald Estate, had its world premiere at The Guthrie Theater to commemorate the opening of its new theatre in July 2006, directed by David Esbjornson. It was subsequently produced by Seattle Repertory Theatre. In 2012 a revised/reworked version was produced at Arizona Theatre Company[31] and Grand Theatre in London, Ontario, Canada. www.TheGreatGatsbyPlay.com [32]
Elevator Repair Service, an experimental theater group, produced a theater version of The Great Gatsby, entitled "Gatz."[33] It is set in an office and read and performed by actor Scott Shepherd along with a cast of 12 other actors.[34]
In 2010 a casual Hidden Object game called Classic Adventures: The Great Gatsby was released by Oberon Media.[35]
As a tribute to old NES games, developer Charlie Hoey and editor Pete Smith created an 8-bit version of The Great Gatsby that is playable online.[36] Ian Crouch of The New Yorker compared it to The Adventures of Tom Sawyer (1989) for the NES.[37]
In 2012 original PC game Classic Adventures: The Great Gatsby was released for iPad.
Le Monde's 100 Books of the Century
Bruccoli, Matthew Joseph (ed.) (2000). F. Scott Fitzgerald's The Great Gatsby: A Literary Reference. New York: Carroll & Graf Publishers. ISBN0-7867-0996-0
Bruccoli, Matthew Joseph (2002). Some Sort of Epic Grandeur: The Life of F. Scott Fitzgerald (2nd rev. ed.). Columbia, SC: University of South Carolina Press. ISBN1-57003-455-9
Curnutt, Kirk (ed.) (2004). A Historical Guide to F. Scott Fitzgerald. Oxford: Oxford University Press. ISBN0-19-515302-2
Mizener, Arthur (1951). The Far Side of Paradise: A Biography of F. Scott Fitzgerald. Boston: Houghton Mifflin Harcourt.
Prigozy, Ruth (ed.) (2002). The Cambridge Companion to F. Scott Fitzgerald. Cambridge: Cambridge University Press. ISBN0-521-62447-9
The Great Gatsby overview
The Great Gatsby, from Project Gutenberg Australia, plain text.
In Gatsby's Tracks Locating the Valley of Ashes
The Great Gatsby Play Authorized and Granted Exclusive Rights by the Fitzgerald Estate
.
#(`*1984*`)#.
1984 (MCMLXXXIV) was a leap year that started on a Sunday, in accordance with the Gregorian calendar. It is the 1984th year of the Common Era or A.D.; the 984th year of the 2nd millennium; the 84th year of the 20th century; and the 5th year of the 1980s decade.

Archaeology
Architecture
Art
Aviation
Awards
Comics
Film
Home video
Literature(Poetry)
Meteorology
Music(Country, Heavymetal)
Rail transport
Radio
Science
Spaceflight
Sports
Television
Video gaming
Australia
Canada
People's Republic of China
Ecuador
France
Germany
Greece
India
Ireland
Israel
Italy
Japan
Luxembourg
Malaysia
Mexico
New Zealand
Norway
Pakistan
Philippines
Singapore
South Africa
Soviet Union
United Kingdom
United States
Zimbabwe
Sovereignstates
Stateleaders
Religiousleaders
Law
Births
Deaths
Establishments
Disestablishments
Works
Introductions
v
t
e
view
talk
edit
January 1

US Bell System is broken up.
Brunei becomes a fully independent state.


US Bell System is broken up.
Brunei becomes a fully independent state.
January 3  U.S. President Ronald Reagan meets with Navy Lieutenant Robert Goodman and the Reverend Jesse Jackson at the White House, following Lieutenant Goodman's release from Syrian captivity.
January 7  Brunei becomes the 6th member of the Association of Southeast Asian Nations (ASEAN).
January 10

The United States and the Vatican establish full diplomatic relations.[1]
Victoria Agreement signed.


The United States and the Vatican establish full diplomatic relations.[1]
Victoria Agreement signed.
January 18  The Mitsui Miike coal mine explosion at muta, Fukuoka, Japan kills 83.
January 24  The Apple Macintosh is introduced.
January 27  Michael Jackson is accidentally severely burned at the filming of a Pepsi commercial.
US Bell System is broken up.
Brunei becomes a fully independent state.
The United States and the Vatican establish full diplomatic relations.[1]
Victoria Agreement signed.
February 1  Medicare comes into effect in Australia.
February 3

Dr. John Buster and the research team at Harbor-UCLA Medical Center announce history's first embryo transfer, from one woman to another resulting in a live birth.
STS-41-B: Space Shuttle Challenger is launched on the 10th space shuttle mission.


Dr. John Buster and the research team at Harbor-UCLA Medical Center announce history's first embryo transfer, from one woman to another resulting in a live birth.
STS-41-B: Space Shuttle Challenger is launched on the 10th space shuttle mission.
February 7  Astronauts Bruce McCandless II and Robert L. Stewart make the first untethered space walk.
February 8  The 1984 Winter Olympics open in Sarajevo, SFRY.
February 13  Konstantin Chernenko succeeds the late Yuri Andropov as General Secretary of the Communist Party of the Soviet Union.
February 19  The 1984 Winter Olympics close in Sarajevo.
February 26  United States Marines pull out of Beirut, Lebanon.
February 29  Canadian Prime Minister Pierre Trudeau announces his retirement.
Dr. John Buster and the research team at Harbor-UCLA Medical Center announce history's first embryo transfer, from one woman to another resulting in a live birth.
STS-41-B: Space Shuttle Challenger is launched on the 10th space shuttle mission.
March 5  Iran accuses Iraq of using chemical weapons; the United Nations condemns their use on March 30.
March 6  A year-long strike action begins in the British coal industry (see UK Miners' Strike (1984-1985)).
March 14  Sinn Fin's Gerry Adams and 3 others are seriously injured in a gun attack by the UVF.
March 16  The CIA station chief in Beirut, William Francis Buckley, is kidnapped by Islamic Jihad and later dies in captivity.
March 22  Teachers at the McMartin Preschool in Manhattan Beach, California are charged with Satanic ritual abuse of the schoolchildren (the charges are later dropped as completely unfounded).
March 23  General Rahimuddin Khan becomes the first man in Pakistan's history to rule over 2 of its provinces, after becoming interim Governor of Sindh.
April 2  Indian Squadron Leader Rakesh Sharma is launched into space, aboard the Soyuz T-11.
April 4  U.S. President Ronald Reagan calls for an international ban on chemical weapons.
April 9  The 56th Academy Awards, hosted by Johnny Carson, are held at the Dorothy Chandler Pavilion.
April 12  Palestinian gunmen take Israeli Bus Number 300 hostage. Israeli special forces storm the bus, freeing the hostages (1 hostage, 2 hijackers killed).
April 13  India launches Operation Meghdoot, as most of the Siachen Glacier in Kashmir comes under Indian control.
April 15  British comedian Tommy Cooper suffers a massive heart attack and dies while live on TV.
April 17  WPC Yvonne Fletcher is shot and killed by a secluded gunman leading to a police siege of the Libyan Embassy in London.
April 19  Advance Australia Fair is proclaimed as Australia's national anthem, and green and gold as the national colours.
April 25  The term of Sultan Ahmad Shah as the 7th Yang di-Pertuan Agong of Malaysia ends.
April 26  Sultan Iskandar, Sultan of Johor, becomes the 8th Yang di-Pertuan Agong of Malaysia.
May 2  The Liverpool International Garden Festival opens in Liverpool.
May 5  The Herreys win the Eurovision Song Contest 1984 for Sweden, with the song Diggi-Loo, Diggi-Ley.
May 8

The Soviet Union announces that it will boycott the 1984 Summer Olympics in Los Angeles, California.
Denis Lortie kills 3 government employees in the National Assembly of Quebec building.
The Chicago White Sox defeat the Milwaukee Brewers 7-6 in the longest game in Major League Baseball history: 25 innings totalling 8 hours, 6 minutes.


The Soviet Union announces that it will boycott the 1984 Summer Olympics in Los Angeles, California.
Denis Lortie kills 3 government employees in the National Assembly of Quebec building.
The Chicago White Sox defeat the Milwaukee Brewers 7-6 in the longest game in Major League Baseball history: 25 innings totalling 8 hours, 6 minutes.
May 11  A transit of Earth from Mars takes place.
May 12  The Louisiana World's Fair opens.
May 14  The one dollar coin is introduced in Australia.
May 23  Methane gas explosion at Abbeystead water treatment works in Lancashire, England kills 16 people.
May 27  An overnight flash flood rages through neighborhoods in Tulsa, Oklahoma. Nearly 15inches of rain falls in some areas over a four-hour period. 14 people are killed.
May 31  Mecklenburg Correctional Center - 6 inmates - including James and Linwood Briley escape from a death row facility, the first and only occasion this has ever happened in the US.
The Soviet Union announces that it will boycott the 1984 Summer Olympics in Los Angeles, California.
Denis Lortie kills 3 government employees in the National Assembly of Quebec building.
The Chicago White Sox defeat the Milwaukee Brewers 7-6 in the longest game in Major League Baseball history: 25 innings totalling 8 hours, 6 minutes.
June 1  William M. Gibbons is released as receiver and trustee of the Chicago, Rock Island & Pacific railroad, after all of its debts and creditors are paid off by order of a federal bankruptcy court.
June 5  The Indian government begins Operation Blue Star, the planned attack on the Golden Temple in Amritsar.
June 6  Indian troops storm the Golden Temple at Amritsar, the Sikhs' holiest shrine, killing an estimated 2,000 people.
June 8  A deadly F5 tornado nearly destroys the town of Barneveld, Wisconsin, killing 9 people, injuring nearly 200, and causing over $25,000,000 in damage.
June 20  The biggest exam shake-up in the British education system in over 10 years is announced, with O-level and CSE exams to be replaced by a new exam, the GCSE.
June 22

The official name of the Turkish city Urfa is changed into anlurfa.
Virgin Atlantic Airways makes its inaugural flight.


The official name of the Turkish city Urfa is changed into anlurfa.
Virgin Atlantic Airways makes its inaugural flight.
June 27  France beats Spain 20 to win the Euro 84.
June 28  Richard Ramrez, aka the "Night Stalker," murders his first confirmed victim.
June 30  John Napier Turner becomes Canada's 17th Prime Minister.
The official name of the Turkish city Urfa is changed into anlurfa.
Virgin Atlantic Airways makes its inaugural flight.
July 13  Terry Wallis, a nineteen-year old living in the Ozark Mountains of Arkansas falls into a deep coma after a severe automobile accident. He would eventually awaken nineteen years later on June 13, 2003.
July 14  New Zealand Prime Minister Robert Muldoon calls a snap election and is heavily defeated by opposition Labour leader David Lange.
July 18

Beverly Lynn Burns becomes the first woman Boeing 747 captain in the world.
In San Ysidro, California, 41-year-old James Oliver Huberty sprays a McDonald's restaurant with gunfire, killing 21 people before being shot and killed.


Beverly Lynn Burns becomes the first woman Boeing 747 captain in the world.
In San Ysidro, California, 41-year-old James Oliver Huberty sprays a McDonald's restaurant with gunfire, killing 21 people before being shot and killed.
Beverly Lynn Burns becomes the first woman Boeing 747 captain in the world.
In San Ysidro, California, 41-year-old James Oliver Huberty sprays a McDonald's restaurant with gunfire, killing 21 people before being shot and killed.
July 23  Vanessa Lynn Williams becomes the first Miss America to resign when she surrenders her crown, after nude photos of her appear in Penthouse magazine.
July 25  Salyut 7: Cosmonaut Svetlana Savitskaya becomes the first woman to perform a space walk.
July 28August 12  The 1984 Summer Olympics are held in Los Angeles, California.
August 1  Australian banks are deregulated.
August 4  The African republic Upper Volta changes its name to Burkina Faso.
August 11  United States President Ronald Reagan, during a voice check for a radio broadcast remarks, "My fellow Americans, I'm pleased to tell you today that I've signed legislation that will outlaw Russia forever. We begin bombing in five minutes".
August 16  John De Lorean is acquitted of all 8 charges of possessing and distributing cocaine.
August 17  Peru recognizes the Sahrawi Arab Democratic Republic (SADR).
August 21  Half a million people in Manila demonstrate against the regime of Ferdinand Marcos.
August 28  Josef Fritzl drugs and incarcerates his daughter Elisabeth in a secret cellar, in Amstetten, Austria; she was released only after 24 years of sexual abuse and mental ordeal.
August 30  STS-41-D: The Space Shuttle Discovery takes off on its maiden voyage.
September 2  Seven people are shot and killed and 12 wounded in the Milperra massacre, a shootout between the rival motorcycle gangs Bandidos and Comancheros in Sydney.
September 4

The Progressive Conservative Party of Canada, led by Brian Mulroney, wins 211 seats in the House of Commons, forming the largest majority government in Canadian history.
The Sandinista Front wins the Nicaraguan general elections.


The Progressive Conservative Party of Canada, led by Brian Mulroney, wins 211 seats in the House of Commons, forming the largest majority government in Canadian history.
The Sandinista Front wins the Nicaraguan general elections.
September 5

STS-41-D: The Space Shuttle Discovery lands after its maiden voyage.
Western Australia becomes the last Australian state to abolish capital punishment.


STS-41-D: The Space Shuttle Discovery lands after its maiden voyage.
Western Australia becomes the last Australian state to abolish capital punishment.
September 14  P. W. Botha is inaugurated as the first executive State President of South Africa.
September 17  Brian Mulroney is sworn in as Prime Minister of Canada.
September 18  Joe Kittinger becomes the first person to cross the Atlantic, solo, in a hot air balloon.
September 20  Hezbollah car-bombs the U.S. Embassy annex in Beirut, killing 22 people.
September 26  The United Kingdom and the People's Republic of China sign the initial agreement to return Hong Kong to China in 1997.
The Progressive Conservative Party of Canada, led by Brian Mulroney, wins 211 seats in the House of Commons, forming the largest majority government in Canadian history.
The Sandinista Front wins the Nicaraguan general elections.
STS-41-D: The Space Shuttle Discovery lands after its maiden voyage.
Western Australia becomes the last Australian state to abolish capital punishment.
October 4  Tim Macartney-Snape and Greg Mortimer become the first Australians to summit Mount Everest.
October 5  STS-41-G: Marc Garneau becomes the first Canadian in space, aboard the Space Shuttle Challenger.
October 11  Aboard the Space Shuttle Challenger, astronaut Kathryn D. Sullivan becomes the first American woman to perform a space walk.
October 12  The Provisional Irish Republican Army (PIRA) attempts to assassinate Prime Minister Margaret Thatcher and the British Cabinet in the Brighton hotel bombing.
October 19  Polish secret police kidnap Jerzy Popieuszko, a Catholic priest who supports the Solidarity movement. His dead body is found in a reservoir 11 days later on October 30.
October 23  The world learns from moving BBC News TV reports that a famine is plaguing Ethiopia, where thousands of people have already died of starvation due to a famine and as many as 10,000,000 more lives are at risk. [1]
October 25  The European Economic Community makes 1.8 million available to help combat the Ethiopian famine. [2]
October 31  Indian Prime Minister Indira Gandhi is assassinated by her 2 Sikh security guards. Anti-Sikh riots break out, leaving 10,000 to 20,000 sikhs dead in New Delhi and surrounding areas with majority populations of Hindus. Rajiv Gandhi becomes prime minister of India.
November 6  United States presidential election, 1984: Ronald Reagan defeats Walter F. Mondale with 59% of the popular vote, the highest since Richard Nixon's 61% victory in 1972. Reagan carries 49 states in the electoral college; Mondale wins only his home state of Minnesota by a mere 3,761 vote margin and the District of Columbia.
November 9  Cesar Chavez delivers his speech, "What The Future Holds For Farm Workers And Hispanics", at the Commonwealth Club in San Francisco.
November 14  Zamboanga City mayor Cesar Climaco, a prominent critic of the government of Philippine President Ferdinand Marcos, is assassinated in his home city.
November 19  A series of explosions at the PEMEX Petroleum Storage Facility at San Juan Ixhuatepec, in Mexico City, ignites a major fire and kills about 500 people.
November 25

An East Rail train derails between Sheung Shui and Fanling stations, Hong Kong.
Band Aid records the charity single "Do They Know It's Christmas?" to raise money to combat the famine in Ethiopia. It is released December 3, 1984.[2]


An East Rail train derails between Sheung Shui and Fanling stations, Hong Kong.
Band Aid records the charity single "Do They Know It's Christmas?" to raise money to combat the famine in Ethiopia. It is released December 3, 1984.[2]
November 28  Over 250 years after their deaths, William Penn and his wife Hannah Callowhill Penn are made Honorary Citizens of the United States.
November 30  The Tamil Tigers begin the purge of the Sinhalese from North and East Sri Lanka; 127 are killed.
An East Rail train derails between Sheung Shui and Fanling stations, Hong Kong.
Band Aid records the charity single "Do They Know It's Christmas?" to raise money to combat the famine in Ethiopia. It is released December 3, 1984.[2]
December  A peace agreement between Kenya and Somalia is signed in the Egyptian capital Cairo. With this agreement, in which Somalia officially renounces its historical territorial claims, relations between the two countries began to improve.
December 1  Controlled Impact Demonstration: NASA crashes a remote controlled Boeing 720.
December 2  Bob Hawke's government is re-elected in Australia with a reduced majority.
December 3

Bhopal Disaster: A methyl isocyanate leak from a Union Carbide pesticide plant in Bhopal, Madhya Pradesh, India, kills more than 8,000 people outright and injures over half a million (with more later dying from their injuries the death toll is now 23,000+) in the worst industrial disaster in history.
British Telecom is privatised.


Bhopal Disaster: A methyl isocyanate leak from a Union Carbide pesticide plant in Bhopal, Madhya Pradesh, India, kills more than 8,000 people outright and injures over half a million (with more later dying from their injuries the death toll is now 23,000+) in the worst industrial disaster in history.
British Telecom is privatised.
December 4  Hezbollah militants hijack a Kuwait Airlines plane and kill 4 passengers.
December 14  Nigeria recognizes the Sahrawi Arab Democratic Republic (SADR).
December 19  The People's Republic of China and United Kingdom sign the Sino-British Joint Declaration on the future of Hong Kong.
December 22

Four African-American youths (Barry Allen, Troy Canty, James Ramseur, and Darrell Cabey) board an express train in The Bronx borough of New York City. They attempt to rob Bernhard Goetz, who shoots them. The event starts a national debate about urban crime in the United States.
In Malta, Prime Minister Dom Mintoff resigns.


Four African-American youths (Barry Allen, Troy Canty, James Ramseur, and Darrell Cabey) board an express train in The Bronx borough of New York City. They attempt to rob Bernhard Goetz, who shoots them. The event starts a national debate about urban crime in the United States.
In Malta, Prime Minister Dom Mintoff resigns.
December 28  A Soviet cruise missile plunges into Inarinjrvi lake in Finnish Lapland. Finnish authorities announce the fact in public on January 3, 1985.
Bhopal Disaster: A methyl isocyanate leak from a Union Carbide pesticide plant in Bhopal, Madhya Pradesh, India, kills more than 8,000 people outright and injures over half a million (with more later dying from their injuries the death toll is now 23,000+) in the worst industrial disaster in history.
British Telecom is privatised.
Four African-American youths (Barry Allen, Troy Canty, James Ramseur, and Darrell Cabey) board an express train in The Bronx borough of New York City. They attempt to rob Bernhard Goetz, who shoots them. The event starts a national debate about urban crime in the United States.
In Malta, Prime Minister Dom Mintoff resigns.
Famine in Ethiopia begins and kills a million people by the end of 1984. (1984 Famine in Ethiopia).
Crack, a smokeable form of cocaine, is first introduced into the Los Angeles area and soon spreads across the United States in what becomes known as the Crack Epidemic.
January 1  Michael Witt, Australian rugby league player
January 2 - Kristen Hager, Canadian film and television actress
January 8

Jeff Francoeur, American baseball player
Steven Kanumba, Tanzanian actor and director (d. 2012)


Jeff Francoeur, American baseball player
Steven Kanumba, Tanzanian actor and director (d. 2012)
January 10  Marouane Chamakh, Moroccan football player
January 12  Scott Olsen, American baseball player
January 13

Eleni Ioannou, Greek martial artist (d. 2004)
Nathaniel Motte, American songwriter, performer, singer, music producer, film composer, instrumentalist, and playwright (3OH!3)


Eleni Ioannou, Greek martial artist (d. 2004)
Nathaniel Motte, American songwriter, performer, singer, music producer, film composer, instrumentalist, and playwright (3OH!3)
January 15

Megan Quann, American swimmer
Victor Rasuk, American actor


Megan Quann, American swimmer
Victor Rasuk, American actor
January 16  Craig Beattie, Scottish footballer
January 17

Sophie Dee, American pornographic actress
Cassie Hager, American basketball player
Calvin Harris, Scottish singer-songwriter


Sophie Dee, American pornographic actress
Cassie Hager, American basketball player
Calvin Harris, Scottish singer-songwriter
January 18  Seung-hui Cho, Korean-born American Virginia Tech massacre gunman (d. 2007)
January 19

Thomas Vanek, Austrian Hockey player
Trent Cutler, Australian rugby league player
Zakia Mrisho Mohamed, Tanzanian long distance runner


Thomas Vanek, Austrian Hockey player
Trent Cutler, Australian rugby league player
Zakia Mrisho Mohamed, Tanzanian long distance runner
January 20

Toni Gonzaga, Filipina actress and singer
Olivia Hallinan, English actress


Toni Gonzaga, Filipina actress and singer
Olivia Hallinan, English actress
January 21  Richard Gutierrez, Filipino actor
January 22  Raica Oliveira, Brazilian supermodel
January 23  Arjen Robben, Dutch footballer
January 24  Witold Kietyka, Polish musician (d. 2007)
January 25

Robinho, Brazilian footballer
Stefan Kiessling, German football player


Robinho, Brazilian footballer
Stefan Kiessling, German football player
January 26  Luo Xuejuan, Chinese swimmer
January 27 - Davetta Sherwood, American actress and musician
January 28  Andre Iguodala, American basketball player
January 29

Nuno Morais, Portuguese footballer
Natalie du Toit, South African swimmer


Nuno Morais, Portuguese footballer
Natalie du Toit, South African swimmer
January 30

Chad Power, American actor
Kid Cudi, American hip hop artist
Xi Zhang, Chinese contemporary artist


Chad Power, American actor
Kid Cudi, American hip hop artist
Xi Zhang, Chinese contemporary artist
Jeff Francoeur, American baseball player
Steven Kanumba, Tanzanian actor and director (d. 2012)
Eleni Ioannou, Greek martial artist (d. 2004)
Nathaniel Motte, American songwriter, performer, singer, music producer, film composer, instrumentalist, and playwright (3OH!3)
Megan Quann, American swimmer
Victor Rasuk, American actor
Sophie Dee, American pornographic actress
Cassie Hager, American basketball player
Calvin Harris, Scottish singer-songwriter
Thomas Vanek, Austrian Hockey player
Trent Cutler, Australian rugby league player
Zakia Mrisho Mohamed, Tanzanian long distance runner
Toni Gonzaga, Filipina actress and singer
Olivia Hallinan, English actress
Robinho, Brazilian footballer
Stefan Kiessling, German football player
Nuno Morais, Portuguese footballer
Natalie du Toit, South African swimmer
Chad Power, American actor
Kid Cudi, American hip hop artist
Xi Zhang, Chinese contemporary artist
February 1

Lee Thompson Young, American actor
Darren Fletcher, Scottish football player


Lee Thompson Young, American actor
Darren Fletcher, Scottish football player
February 3  Kim Joon, South Korean rapper, actor, and model
February 4  Mauricio Pinilla, Chilean footballer
February 5

Nate Salley, American football player
Carlos Tvez, Argentinian football player


Nate Salley, American football player
Carlos Tvez, Argentinian football player
February 6  Darren Bent, English footballer
February 9

Han Geng, Chinese singer in Korea (Super Junior)
Logan Bartholomew, American actor


Han Geng, Chinese singer in Korea (Super Junior)
Logan Bartholomew, American actor
February 10  Kim Hyo Jin, Korean actress
February 11 - Aubrey O'Day, American singer and actress
February 12 - Jennie McAlpine, English television actress and comedienne
February 14 - Robbie Jones, American television and film actor
February 15  Dorota Rabczewska, Polish singer and model
February 16  Oussama Mellouli, Tunisian swimmer
February 17  AB de Villiers, South African Cricketer
February 18  Genelle Williams, Canadian actress
February 20  Ben Lovejoy, American hockey player
February 21

Karina, Japanese model and actress
Damien Molony, Irish television actor


Karina, Japanese model and actress
Damien Molony, Irish television actor
February 22 - Tommy Bowe, Irish rugby union footballer
February 25

Filip ebo, Slovak footballer
Xing Huina, Chinese athlete


Filip ebo, Slovak footballer
Xing Huina, Chinese athlete
February 26

Beren Saat, Turkish actress
Emmanuel Adebayor, Togoleise footballer


Beren Saat, Turkish actress
Emmanuel Adebayor, Togoleise footballer
February 28  Karolna Kurkov, Czech model
February 29

Alicia Hollowell, American softball pitcher
Cullen Jones, American swimmer


Alicia Hollowell, American softball pitcher
Cullen Jones, American swimmer
Lee Thompson Young, American actor
Darren Fletcher, Scottish football player
Nate Salley, American football player
Carlos Tvez, Argentinian football player
Han Geng, Chinese singer in Korea (Super Junior)
Logan Bartholomew, American actor
Karina, Japanese model and actress
Damien Molony, Irish television actor
Filip ebo, Slovak footballer
Xing Huina, Chinese athlete
Beren Saat, Turkish actress
Emmanuel Adebayor, Togoleise footballer
Alicia Hollowell, American softball pitcher
Cullen Jones, American swimmer
March 1

Naima Mora, winner of America's Next Top Model cycle 4
Claudio Bieler, Argentinian football player


Naima Mora, winner of America's Next Top Model cycle 4
Claudio Bieler, Argentinian football player
March 3  Hayley Marie Norman, American actress and model
March 4

Tamir Cohen, Israeli footballer
Ai Iwamura, Japanese actress
Zak Whitbread, American soccer player


Tamir Cohen, Israeli footballer
Ai Iwamura, Japanese actress
Zak Whitbread, American soccer player
March 7

Dani Woodward, American porn star
Brandon T. Jackson, American stand-up comedian, actor and rapper


Dani Woodward, American porn star
Brandon T. Jackson, American stand-up comedian, actor and rapper
March 8

Ross Taylor, New Zealand cricketer
Nora Jane Noone, Irish actress


Ross Taylor, New Zealand cricketer
Nora Jane Noone, Irish actress
March 9  Julia Mancuso, US Olympic medalist
March 10  Olivia Wilde, American actress
March 12 - Jaimie Alexander, American actress
March 16

Michael Ennis, Australian rugby league player
Hosea Gear, New Zealand Rugby Union player


Michael Ennis, Australian rugby league player
Hosea Gear, New Zealand Rugby Union player
March 20

Fernando Torres, Spanish football player
Nomura Yuka, Japanese actress
Christy Carlson Romano, American stage and film actress


Fernando Torres, Spanish football player
Nomura Yuka, Japanese actress
Christy Carlson Romano, American stage and film actress
March 21 - Kerry Bish, American model, actress and reality television star
March 24

Chris Bosh, American basketball player
Park Bom, Korean singer


Chris Bosh, American basketball player
Park Bom, Korean singer
March 25  Katharine McPhee, American Idol finalist
March 26

Stphanie Lapointe, Canadian singer
Sara Jean Underwood, American model


Stphanie Lapointe, Canadian singer
Sara Jean Underwood, American model
March 28  Nikki Sanderson, English actress
March 30

Anna Nalick, American singer
Samantha Stosur, Australian tennis player


Anna Nalick, American singer
Samantha Stosur, Australian tennis player
Naima Mora, winner of America's Next Top Model cycle 4
Claudio Bieler, Argentinian football player
Tamir Cohen, Israeli footballer
Ai Iwamura, Japanese actress
Zak Whitbread, American soccer player
Dani Woodward, American porn star
Brandon T. Jackson, American stand-up comedian, actor and rapper
Ross Taylor, New Zealand cricketer
Nora Jane Noone, Irish actress
Michael Ennis, Australian rugby league player
Hosea Gear, New Zealand Rugby Union player
Fernando Torres, Spanish football player
Nomura Yuka, Japanese actress
Christy Carlson Romano, American stage and film actress
Chris Bosh, American basketball player
Park Bom, Korean singer
Stphanie Lapointe, Canadian singer
Sara Jean Underwood, American model
Anna Nalick, American singer
Samantha Stosur, Australian tennis player
April 2 - Shawn Roberts, Canadian actor
April 3  Allana Slater, Australian gymnast
April 4  Sean May, American basketball player
April 5  Marshall Allman, American actor
April 7  Alex Smith, American football player
April 8

Jlia Liptkov, Slovak model
Kirsten Storms, American actress


Jlia Liptkov, Slovak model
Kirsten Storms, American actress
April 9

Adam Loewen, Canadian pitcher
Linda Chung, Canadian TVB actress and singer


Adam Loewen, Canadian pitcher
Linda Chung, Canadian TVB actress and singer
April 10

Mandy Moore, American singer and actress
Cara DeLizia, American actress
Natasha Melnick, American television and film actress


Mandy Moore, American singer and actress
Cara DeLizia, American actress
Natasha Melnick, American television and film actress
April 11 - Kelli Garner, American actress
April 13  Kris Britt, Australian cricketer
April 14  Kyle Coetzer, Scottish cricketer
April 16 - Claire Foy, English actress
April 17 - Rosanna Davison, Irish model
April 18  America Ferrera, American actress
April 19  Lee Da Hae, South Korean actress
April 20

John Jairo Castillo, Colombian football player
Tyson Griffin, American MMA fighter
Nelson Evora, Portuguese athlete


John Jairo Castillo, Colombian football player
Tyson Griffin, American MMA fighter
Nelson Evora, Portuguese athlete
April 22

Michelle Ryan, English actress
Amelle Berrabah, British singer (Sugababes)
Breanne Benson, Albanian pornographic actress


Michelle Ryan, English actress
Amelle Berrabah, British singer (Sugababes)
Breanne Benson, Albanian pornographic actress
April 23  Alexandra Kosteniuk, Russian chess player
April 24  Tyson Ritter, American singer/songwriter (The All-American Rejects)
April 25  Melonie Diaz, American actress
April 26 - Brett Novek, American male fashion model and actor
April 27  Patrick Stump, American singer (Fall Out Boy)
April 29

Taylor Cole, American actress and model
Lina Krasnoroutskaya, Russian tennis player and commentator
Pham Van Quyen, Vietnamese footballer


Taylor Cole, American actress and model
Lina Krasnoroutskaya, Russian tennis player and commentator
Pham Van Quyen, Vietnamese footballer
Jlia Liptkov, Slovak model
Kirsten Storms, American actress
Adam Loewen, Canadian pitcher
Linda Chung, Canadian TVB actress and singer
Mandy Moore, American singer and actress
Cara DeLizia, American actress
Natasha Melnick, American television and film actress
John Jairo Castillo, Colombian football player
Tyson Griffin, American MMA fighter
Nelson Evora, Portuguese athlete
Michelle Ryan, English actress
Amelle Berrabah, British singer (Sugababes)
Breanne Benson, Albanian pornographic actress
Taylor Cole, American actress and model
Lina Krasnoroutskaya, Russian tennis player and commentator
Pham Van Quyen, Vietnamese footballer
May 1

Alexander Farnerud, Swedish footballer
Keiichiro Koyama, Japanese singer (NEWS) and actor
Kerry Bish, American actress


Alexander Farnerud, Swedish footballer
Keiichiro Koyama, Japanese singer (NEWS) and actor
Kerry Bish, American actress
May 3  Cheryl Burke, American professional dancer
May 4  Little Boots, English pop singer
May 8  Martin Compston, Scottish actor and former professional footballer
May 11  Andrs Iniesta, Spanish footballer
May 12  Junie Browning, American MMA fighter
May 14

Michael Rensing, German footballer
Mark Zuckerberg, Founder and CEO of Facebook
Gary Ablett Jr., Australian rules footballer


Michael Rensing, German footballer
Mark Zuckerberg, Founder and CEO of Facebook
Gary Ablett Jr., Australian rules footballer
May 17

Christine Robinson, Canadian water polo player
Andreas Kofler, Austrian ski jumper


Christine Robinson, Canadian water polo player
Andreas Kofler, Austrian ski jumper
May 20  Naturi Naughton, American singer and actress
May 21 - Jackson Pearce, American novelist
May 23  Sam Milby, Filipino-American actor, host, singer and model
May 24  Sarah Hagan, American actress
May 25

Unnur Birna Vilhjlmsdttir, "Miss Iceland", crowned Miss World in 2005
Kyle Brodziak, Canadian ice hockey player
Nikolai Pokotylo, Russian singer
Marion Raven, Norwegian singer-songwriter, former child actress


Unnur Birna Vilhjlmsdttir, "Miss Iceland", crowned Miss World in 2005
Kyle Brodziak, Canadian ice hockey player
Nikolai Pokotylo, Russian singer
Marion Raven, Norwegian singer-songwriter, former child actress
May 27  Darin Brooks, American actor
May 29

Kaycee Stroh, American actress
Carmelo Anthony, American basketball player


Kaycee Stroh, American actress
Carmelo Anthony, American basketball player
May 31

Jason Smith, Australian actor
Milorad avi, Serbian swimmer


Jason Smith, Australian actor
Milorad avi, Serbian swimmer
Alexander Farnerud, Swedish footballer
Keiichiro Koyama, Japanese singer (NEWS) and actor
Kerry Bish, American actress
Michael Rensing, German footballer
Mark Zuckerberg, Founder and CEO of Facebook
Gary Ablett Jr., Australian rules footballer
Christine Robinson, Canadian water polo player
Andreas Kofler, Austrian ski jumper
Unnur Birna Vilhjlmsdttir, "Miss Iceland", crowned Miss World in 2005
Kyle Brodziak, Canadian ice hockey player
Nikolai Pokotylo, Russian singer
Marion Raven, Norwegian singer-songwriter, former child actress
Kaycee Stroh, American actress
Carmelo Anthony, American basketball player
Jason Smith, Australian actor
Milorad avi, Serbian swimmer
June 1

Oliver Tielemans, Dutch race-car driver
Naidangiin Tvshinbayar, Mongolian judoka
Taylor Handley, American actor


Oliver Tielemans, Dutch race-car driver
Naidangiin Tvshinbayar, Mongolian judoka
Taylor Handley, American actor
June 4

Rainie Yang, Taiwanese singer
Jillian Murray, American Actress


Rainie Yang, Taiwanese singer
Jillian Murray, American Actress
June 7  Ari Koivunen, Finnish singer
June 8

Todd Boeckman, American football player
Andrea Casiraghi, Prince of Monaco
Javier Mascherano, Argentinian footballer
Torrey DeVitto, American actress and former fashion model


Todd Boeckman, American football player
Andrea Casiraghi, Prince of Monaco
Javier Mascherano, Argentinian footballer
Torrey DeVitto, American actress and former fashion model
June 9  Wesley Sneijder, Dutch footballer
June 11  Vgner Love, Brazilian footballer
June 13  Brengre Schuh, French archer
June 14 - Yury Prilukov, Russian swimmer
June 15  Tim Lincecum, American baseball player
June 16

Rick Nash, Canadian hockey player
Emiri Miyasaka, Japanese model


Rick Nash, Canadian hockey player
Emiri Miyasaka, Japanese model
June 17  John Gallagher Jr., American actor, singer and dancer
June 19 - Paul Dano, American actor and producer
June 23  Duffy, Welsh singer
June 24  J. J. Redick, American basketball player
June 25

Lauren Bush-Lauren, American model
Indigo (actress), American television and voice actress


Lauren Bush-Lauren, American model
Indigo (actress), American television and voice actress
June 26

Raymond Felton, American basketball player
Lauren Harris, British vocalist
Aubrey Plaza, American actress


Raymond Felton, American basketball player
Lauren Harris, British vocalist
Aubrey Plaza, American actress
June 27

Emma Lahana, New Zealand actress
Khlo Kardashian, Television personality


Emma Lahana, New Zealand actress
Khlo Kardashian, Television personality
June 29  Chris Egan, Australian actor
June 30  Fantasia Barrino, American singer
Oliver Tielemans, Dutch race-car driver
Naidangiin Tvshinbayar, Mongolian judoka
Taylor Handley, American actor
Rainie Yang, Taiwanese singer
Jillian Murray, American Actress
Todd Boeckman, American football player
Andrea Casiraghi, Prince of Monaco
Javier Mascherano, Argentinian footballer
Torrey DeVitto, American actress and former fashion model
Rick Nash, Canadian hockey player
Emiri Miyasaka, Japanese model
Lauren Bush-Lauren, American model
Indigo (actress), American television and voice actress
Raymond Felton, American basketball player
Lauren Harris, British vocalist
Aubrey Plaza, American actress
Emma Lahana, New Zealand actress
Khlo Kardashian, Television personality
July 1  Donald Thomas, Bahamian high jumper
July 2  Vanessa Lee Chester, American television and film actress
July 3

Corey Sevier, Canadian actor
Syed Rasel, Bangladeshi cricketer
Manny Lawson, American Football player


Corey Sevier, Canadian actor
Syed Rasel, Bangladeshi cricketer
Manny Lawson, American Football player
July 4  Jin Akanishi, Japanese singer (KAT-TUN) and actor
July 5 - Danay Garca, Cuban film actress
July 6

Lauren Harris, English singer
D. Woods, American R&B singer, dancer, and actress


Lauren Harris, English singer
D. Woods, American R&B singer, dancer, and actress
July 7 - Mohammad Ashraful, Bangladeshi cricketer
July 9

LA Tenorio, Filipino professional basketball player
Hanna R. Hall, American actress
Caroline D'Amore, American model, and actress


LA Tenorio, Filipino professional basketball player
Hanna R. Hall, American actress
Caroline D'Amore, American model, and actress
July 11

Tanith Belbin, Canadian figure skater
Joe Pavelski, American hockey player
Rachael Taylor, Australian actress


Tanith Belbin, Canadian figure skater
Joe Pavelski, American hockey player
Rachael Taylor, Australian actress
July 12

Gareth Gates, English singer
Amanda Hocking, American fantasy novelist
Michael McGovern, Northern Irish footballer


Gareth Gates, English singer
Amanda Hocking, American fantasy novelist
Michael McGovern, Northern Irish footballer
July 16  Katrina Kaif, Indian actress and model
July 17  Asami Kimura, Japanese singer
July 18

Lee Barnard, English footballer
Josh Harding, Canadian hockey player


Lee Barnard, English footballer
Josh Harding, Canadian hockey player
July 19

Lasse Gjertsen, Norwegian videographer
Diana Mocanu, Romanian swimmer
Alessandra De Rossi, Italian-Filipina actress


Lasse Gjertsen, Norwegian videographer
Diana Mocanu, Romanian swimmer
Alessandra De Rossi, Italian-Filipina actress
July 21  Paul Davis, American basketball player
July 23

Brandon Roy, National Basketball Association player
Celeste Thorson, Asian American actress, model


Brandon Roy, National Basketball Association player
Celeste Thorson, Asian American actress, model
July 24  Tyler Kyte, Canadian actor/singer
July 26  Kyriakos Ioannou, Cypriot high jumper
July 27

Taylor Schilling, American actress
Antoine Bethea, professional American football


Taylor Schilling, American actress
Antoine Bethea, professional American football
July 28 - Zach Parise, American hockey player
July 29  Todd Bosley, American actor
July 30

Anna Bessonova, Ukrainian rhythmic gymnast
Gabrielle Christian, American actress


Anna Bessonova, Ukrainian rhythmic gymnast
Gabrielle Christian, American actress
Corey Sevier, Canadian actor
Syed Rasel, Bangladeshi cricketer
Manny Lawson, American Football player
Lauren Harris, English singer
D. Woods, American R&B singer, dancer, and actress
LA Tenorio, Filipino professional basketball player
Hanna R. Hall, American actress
Caroline D'Amore, American model, and actress
Tanith Belbin, Canadian figure skater
Joe Pavelski, American hockey player
Rachael Taylor, Australian actress
Gareth Gates, English singer
Amanda Hocking, American fantasy novelist
Michael McGovern, Northern Irish footballer
Lee Barnard, English footballer
Josh Harding, Canadian hockey player
Lasse Gjertsen, Norwegian videographer
Diana Mocanu, Romanian swimmer
Alessandra De Rossi, Italian-Filipina actress
Brandon Roy, National Basketball Association player
Celeste Thorson, Asian American actress, model
Taylor Schilling, American actress
Antoine Bethea, professional American football
Anna Bessonova, Ukrainian rhythmic gymnast
Gabrielle Christian, American actress
August 1  Bastian Schweinsteiger, German football player
August 2  Brandon Browner, National Football League player
August 3

Carah Faye Charnow, American singer for the band Shiny Toy Guns
Ryan Lochte, American swimmer
Kyle Schmid, Canadian actor


Carah Faye Charnow, American singer for the band Shiny Toy Guns
Ryan Lochte, American swimmer
Kyle Schmid, Canadian actor
August 6  Marco Airosa, Angolan footballer
August 10

Ryan Eggold, American film and television actor
Mariel Rodriguez, Filipina actress & model


Ryan Eggold, American film and television actor
Mariel Rodriguez, Filipina actress & model
August 11 - Melky Cabrera, Major League Baseball outfielder for the Kansas City Royals
August 12

Yua Aida, Japanese model and pornographic actress
Marian Rivera, Filipina actress
Sherone Simpson, Jamaican athlete


Yua Aida, Japanese model and pornographic actress
Marian Rivera, Filipina actress
Sherone Simpson, Jamaican athlete
August 13 - James Morrison, English singer/songwriter and guitarist
August 14

Clay Buchholz, Major League Baseball Pitcher
Robin Sderling, Swedish tennis player


Clay Buchholz, Major League Baseball Pitcher
Robin Sderling, Swedish tennis player
August 15 - Quinton Aaron, American actor
August 17  Garrett Wolfe, American pro football player
August 19  Micah Alberti, American model and actor
August 20

Mirai Moriyama, Japanese actor
Tsokye Karchung, Bhutanese beauty queen, Miss Bhutan 2008


Mirai Moriyama, Japanese actor
Tsokye Karchung, Bhutanese beauty queen, Miss Bhutan 2008
August 21

Alize Jacotey, French singer
Melissa Schuman, American singer and actress


Alize Jacotey, French singer
Melissa Schuman, American singer and actress
August 22  Lee Camp, English footballer
August 23 - Glen Johnson, English footballer
August 24

Kyle Schmid, Canadian actor
Charlie Villanueva, American basketball player
Yesung, Korean singer (Super Junior)


Kyle Schmid, Canadian actor
Charlie Villanueva, American basketball player
Yesung, Korean singer (Super Junior)
August 25  Alvin Banks, American football player
August 27  Josh Duhon, American actor
August 28

Him Law, Hong Kong actor
Sarah Roemer, American model and actress


Him Law, Hong Kong actor
Sarah Roemer, American model and actress
August 31

Ryan Kesler, American ice hockey player
Charl Schwartzel, South African golfer


Ryan Kesler, American ice hockey player
Charl Schwartzel, South African golfer
Carah Faye Charnow, American singer for the band Shiny Toy Guns
Ryan Lochte, American swimmer
Kyle Schmid, Canadian actor
Ryan Eggold, American film and television actor
Mariel Rodriguez, Filipina actress & model
Yua Aida, Japanese model and pornographic actress
Marian Rivera, Filipina actress
Sherone Simpson, Jamaican athlete
Clay Buchholz, Major League Baseball Pitcher
Robin Sderling, Swedish tennis player
Mirai Moriyama, Japanese actor
Tsokye Karchung, Bhutanese beauty queen, Miss Bhutan 2008
Alize Jacotey, French singer
Melissa Schuman, American singer and actress
Kyle Schmid, Canadian actor
Charlie Villanueva, American basketball player
Yesung, Korean singer (Super Junior)
Him Law, Hong Kong actor
Sarah Roemer, American model and actress
Ryan Kesler, American ice hockey player
Charl Schwartzel, South African golfer
September 2  Danson Tang, Taiwanese actor, model, and singer
September 3  Garrett Hedlund, American actor
September 6  Orsi Kocsis, Hungarian model
September 7

Farveez Maharoof, Sri Lankan cricketer
Vera Zvonareva, Russian tennis player
Kate Lang Johnson, American actress and model


Farveez Maharoof, Sri Lankan cricketer
Vera Zvonareva, Russian tennis player
Kate Lang Johnson, American actress and model
September 10  Luke Treadaway, English actor
September 12  September, Swedish singer and songwriter
September 14

Adam Lamberg, American actor
Ayushmann Khurrana, Indian actor


Adam Lamberg, American actor
Ayushmann Khurrana, Indian actor
September 15  Prince Harry of Wales, British Prince and son of Charles, Prince of Wales and Diana, Princess of Wales
September 16

Sabrina Bryan, American actress and singer
Katie Melua, Georgian singer


Sabrina Bryan, American actress and singer
Katie Melua, Georgian singer
September 18

Jack Carpenter, American actor
Dizzee Rascal, English rapper


Jack Carpenter, American actor
Dizzee Rascal, English rapper
September 19  Kevin Zegers, Canadian actor
September 20

Brian Joubert, French figure skater
Holly Weber, American actress and model


Brian Joubert, French figure skater
Holly Weber, American actress and model
September 21 - Dwayne Bowe, American football
September 22

Theresa Fu, Hong Kong singer and actress
Laura Vandervoort, Canadian actress
MyAnna Buring, Swedish actress


Theresa Fu, Hong Kong singer and actress
Laura Vandervoort, Canadian actress
MyAnna Buring, Swedish actress
September 23

Anneliese van der Pol, Dutch-born actress
Gabrielle Christian, American television and film actress and model
Kate French, American television and film actress and model


Anneliese van der Pol, Dutch-born actress
Gabrielle Christian, American television and film actress and model
Kate French, American television and film actress and model
September 25

Rashad McCants, National Basketball Association player
Annabelle Wallis, English actress
Zach Woods, American actor and comedian


Rashad McCants, National Basketball Association player
Annabelle Wallis, English actress
Zach Woods, American actor and comedian
September 26  Keisha Buchanan, British singer (Sugababes)
September 27

Avril Lavigne, Canadian singer
Abhinav Shukla, Indian Television Actor


Avril Lavigne, Canadian singer
Abhinav Shukla, Indian Television Actor
September 28

Melody Thornton, American singer (Pussycat Dolls)
Helen Oyeyemi, British novelist


Melody Thornton, American singer (Pussycat Dolls)
Helen Oyeyemi, British novelist
September 30  Megan Ewing, American model
Farveez Maharoof, Sri Lankan cricketer
Vera Zvonareva, Russian tennis player
Kate Lang Johnson, American actress and model
Adam Lamberg, American actor
Ayushmann Khurrana, Indian actor
Sabrina Bryan, American actress and singer
Katie Melua, Georgian singer
Jack Carpenter, American actor
Dizzee Rascal, English rapper
Brian Joubert, French figure skater
Holly Weber, American actress and model
Theresa Fu, Hong Kong singer and actress
Laura Vandervoort, Canadian actress
MyAnna Buring, Swedish actress
Anneliese van der Pol, Dutch-born actress
Gabrielle Christian, American television and film actress and model
Kate French, American television and film actress and model
Rashad McCants, National Basketball Association player
Annabelle Wallis, English actress
Zach Woods, American actor and comedian
Avril Lavigne, Canadian singer
Abhinav Shukla, Indian Television Actor
Melody Thornton, American singer (Pussycat Dolls)
Helen Oyeyemi, British novelist
October 1  Matt Cain, American baseball player
October 2

John Morris, American actor
Marion Bartoli, French professional tennis player


John Morris, American actor
Marion Bartoli, French professional tennis player
October 3

Chris Marquette, American actor
Ashlee Simpson, American singer and actress
Anthony Le Tallec, French footballer
Yoon Eun Hye, Korean singer, model, actress and entertainer
Jessica Parker Kennedy, Canadian actress


Chris Marquette, American actor
Ashlee Simpson, American singer and actress
Anthony Le Tallec, French footballer
Yoon Eun Hye, Korean singer, model, actress and entertainer
Jessica Parker Kennedy, Canadian actress
October 4

Lena Katina, Russian singer
lvaro Parente, Portuguese racing driver
Dana Davis, American actress


Lena Katina, Russian singer
lvaro Parente, Portuguese racing driver
Dana Davis, American actress
October 5  Glenn McMillan, Brazilian-Australian actor
October 6  Joanna Pacitti, American singer
October 7  Ikuta Toma, Japanese drama actor
October 10

Chiaki Kuriyama, Japanese actress
Steve Turner, Australian rugby league player


Chiaki Kuriyama, Japanese actress
Steve Turner, Australian rugby league player
October 11  Jane Zhang,Chinese singer
October 12  Emmanuel Kipchirchir Mutai, Kenyan long-distance runner
October 13  Kathrin Fricke, German web- and video-artist, known as Coldmirror
October 14  Santino Quaranta, American soccer player
October 15  Chris Olivero, American actor
October 16

Ben Smith, Australian rugby league player
Shayne Ward, British singer


Ben Smith, Australian rugby league player
Shayne Ward, British singer
October 17

Chris Lowell, American actor
Randall Munroe, American programmer and webcomic artist


Chris Lowell, American actor
Randall Munroe, American programmer and webcomic artist
October 18

Holly Dunaway, American female boxer
Lindsey Vonn, American alpine skier


Holly Dunaway, American female boxer
Lindsey Vonn, American alpine skier
October 19  Kaio Almeida, Brazilian swimmer
October 21  Marvin Mitchell, American football player
October 23  Meghan McCain, Author and Daughter of Senator John McCain
October 24 - Emily Barclay, English-born New Zealand AFI award winning actress
October 25

Sara Lumholdt, Swedish singer
Katy Perry, American singer and actress


Sara Lumholdt, Swedish singer
Katy Perry, American singer and actress
October 26  Sasha Cohen, American figure skater
October 27

Kelly Osbourne, English singer
Brady Quinn, American football player
Irfan Pathan, Indian cricketer


Kelly Osbourne, English singer
Brady Quinn, American football player
Irfan Pathan, Indian cricketer
October 28  Obafemi Martins, Nigerian footballer
October 29  Eric Staal, Canadian hockey player
October 30  Eva Marcille, American model
October 31

Scott Clifton, American actor, musician, and video blogger
Nicole Rash, American model


Scott Clifton, American actor, musician, and video blogger
Nicole Rash, American model
John Morris, American actor
Marion Bartoli, French professional tennis player
Chris Marquette, American actor
Ashlee Simpson, American singer and actress
Anthony Le Tallec, French footballer
Yoon Eun Hye, Korean singer, model, actress and entertainer
Jessica Parker Kennedy, Canadian actress
Lena Katina, Russian singer
lvaro Parente, Portuguese racing driver
Dana Davis, American actress
Chiaki Kuriyama, Japanese actress
Steve Turner, Australian rugby league player
Ben Smith, Australian rugby league player
Shayne Ward, British singer
Chris Lowell, American actor
Randall Munroe, American programmer and webcomic artist
Holly Dunaway, American female boxer
Lindsey Vonn, American alpine skier
Sara Lumholdt, Swedish singer
Katy Perry, American singer and actress
Kelly Osbourne, English singer
Brady Quinn, American football player
Irfan Pathan, Indian cricketer
Scott Clifton, American actor, musician, and video blogger
Nicole Rash, American model
November 4  Dustin Brown, American hockey player
November 7  Amelia Vega, Miss Universe 2003, from the Dominican Republic
November 8  Steven Webb, English actor
November 9

Beatrice Bofia, Cameroonian-American basketball player
Delta Goodrem, Australian actress and singer


Beatrice Bofia, Cameroonian-American basketball player
Delta Goodrem, Australian actress and singer
November 12

Yan Zi, Chinese tennis player
Sandara Park, South Korean girl-group member


Yan Zi, Chinese tennis player
Sandara Park, South Korean girl-group member
November 17  Park Han-byul, South Korean actress
November 18 - Johnny Christ, Avenged Sevenfold bassist
November 21

Lindsey Haun, American actress
Jena Malone, American actress


Lindsey Haun, American actress
Jena Malone, American actress
November 22  Scarlett Johansson, American actress
November 24

Koo Hye Sun, South Korean actress
Maria Riesch, German alpine skier
Trey Songz, American singer-songwriter, rapper, record producer and actor


Koo Hye Sun, South Korean actress
Maria Riesch, German alpine skier
Trey Songz, American singer-songwriter, rapper, record producer and actor
November 25

Ian Lacey, Australian rugby league player
Gaspard Ulliel, French actor


Ian Lacey, Australian rugby league player
Gaspard Ulliel, French actor
November 28

Joross Gamboa, Filipino actor
Mary Elizabeth Winstead, American actress
Andrew Bogut, Australian basketball player
Marc-Andr Fleury, Canadian hockey player
Trey Songz, American singer-songwriter, rapper, record producer and actor


Joross Gamboa, Filipino actor
Mary Elizabeth Winstead, American actress
Andrew Bogut, Australian basketball player
Marc-Andr Fleury, Canadian hockey player
Trey Songz, American singer-songwriter, rapper, record producer and actor
November 29  Sitti Navarro, Philippine bossa nova artist
November 30  Alan Hutton, Scottish professional footballer
Beatrice Bofia, Cameroonian-American basketball player
Delta Goodrem, Australian actress and singer
Yan Zi, Chinese tennis player
Sandara Park, South Korean girl-group member
Lindsey Haun, American actress
Jena Malone, American actress
Koo Hye Sun, South Korean actress
Maria Riesch, German alpine skier
Trey Songz, American singer-songwriter, rapper, record producer and actor
Ian Lacey, Australian rugby league player
Gaspard Ulliel, French actor
Joross Gamboa, Filipino actor
Mary Elizabeth Winstead, American actress
Andrew Bogut, Australian basketball player
Marc-Andr Fleury, Canadian hockey player
Trey Songz, American singer-songwriter, rapper, record producer and actor
December 4

Lauren London, American actress and model
Lindsay Felton, American actress


Lauren London, American actress and model
Lindsay Felton, American actress
December 7  Robert Kubica, Polish Formula One racing driver
December 8  Jennifer Grassman, American recording artist and journalist
December 11 - Xosha Roquemore, American actress
December 12  Daniel Agger, Danish football (soccer) player
December 14

Chris Brunt, Northern Irish footballer
Jackson Rathbone, American actor and singer


Chris Brunt, Northern Irish footballer
Jackson Rathbone, American actor and singer
December 15

Yu Fengtong, Chinese speed skater
Martin krtel, Slovakian footballer


Yu Fengtong, Chinese speed skater
Martin krtel, Slovakian footballer
December 16  Laura More (Muncey), British singer
December 17

Asuka Fukuda, Japanese singer
Shannon Marie Woodward, American actress
Christina DeRosa, American actress
Tennessee Thomas, British-born American drummer and actor


Asuka Fukuda, Japanese singer
Shannon Marie Woodward, American actress
Christina DeRosa, American actress
Tennessee Thomas, British-born American drummer and actor
December 18  Tiffany Mulheron, Scottish actress
December 20  David Tavar, Spanish singer
December 21  Jackson Rathbone, American actor
December 22  Jonas Altberg, Swedish Eurodance/Techno singer
December 23  Alison Sudol, American singer/songwriter and pianist known professionally as A Fine Frenzy
December 25

Jessica Origliasso, Australian singer-songwriter, actress and fashion designer
Lisa Origliasso, Australian singer-songwriter, actress and fashion designer


Jessica Origliasso, Australian singer-songwriter, actress and fashion designer
Lisa Origliasso, Australian singer-songwriter, actress and fashion designer
December 26 - Jennifer Sipes, American actress and model
December 27

Tye'sha Fluker, American basketball player
Roco Guirao Daz, Argentinian model


Tye'sha Fluker, American basketball player
Roco Guirao Daz, Argentinian model
December 28

Martin Kaymer, German golfer
Festus, American professional wrestler


Martin Kaymer, German golfer
Festus, American professional wrestler
December 30  LeBron James, American basketball player
Lauren London, American actress and model
Lindsay Felton, American actress
Chris Brunt, Northern Irish footballer
Jackson Rathbone, American actor and singer
Yu Fengtong, Chinese speed skater
Martin krtel, Slovakian footballer
Asuka Fukuda, Japanese singer
Shannon Marie Woodward, American actress
Christina DeRosa, American actress
Tennessee Thomas, British-born American drummer and actor
Jessica Origliasso, Australian singer-songwriter, actress and fashion designer
Lisa Origliasso, Australian singer-songwriter, actress and fashion designer
Tye'sha Fluker, American basketball player
Roco Guirao Daz, Argentinian model
Martin Kaymer, German golfer
Festus, American professional wrestler
January 1  Alexis Korner, British blues musician and broadcaster (b. 1928)
January 6  Ernest Laszlo, Hungarian-American cinematographer (b. 1898)
January 7  Alfred Kastler, French physicist, Nobel Prize laureate (b. 1902)
January 11  Jack La Rue, American actor (b. 1902)
January 14

Brooks Atkinson, American theater critic (b. 1894)
Ray Kroc, American entrepreneur (b. 1902)


Brooks Atkinson, American theater critic (b. 1894)
Ray Kroc, American entrepreneur (b. 1902)
January 17 - George Rigaud, Argentinian actor (b. 1905)
January 20  Johnny Weissmuller, Austrian-born swimmer and actor (b. 1904)
January 21  Jackie Wilson, American singer (b. 1934)
January 22  Sir Count Michael Gonzi, Archbishop of Malta and past politician (b. 1885)
January 29  Frances Goodrich, American screenwriter (b. 1890)
January 30  Luke Kelly, Irish folk singer (b. 1940)
Brooks Atkinson, American theater critic (b. 1894)
Ray Kroc, American entrepreneur (b. 1902)
February 8  Karel Miljon, Dutch boxer (b. 1903)
February 9  Yuri Andropov, General Secretary of the Communist Party of the Soviet Union (b. 1914)
February 10  David Von Erich, American professional wrestler (b. 1958)
February 11  John Comer, English actor (b. 1924)
February 12

Anna Anderson, Pretender to the Russian throne (b. 1896)
Julio Cortzar, Argentine writer (b. 1914)


Anna Anderson, Pretender to the Russian throne (b. 1896)
Julio Cortzar, Argentine writer (b. 1914)
February 13  Naomi Uemura, Japanese adventurer (b. 1941)
February 15  Ethel Merman, American singer and actress (b. 1908)
February 21  Michail Aleksandrovich Sholokhov, Russian writer, Nobel Prize laureate (b. 1905)
February 22

Jessamyn West, American writer. (b. 1902)
Syed Faiz-ul Hassan Shah a Pakistani religious leader,of Allo Mahar Shrif. (b. 1911)


Jessamyn West, American writer. (b. 1902)
Syed Faiz-ul Hassan Shah a Pakistani religious leader,of Allo Mahar Shrif. (b. 1911)
Anna Anderson, Pretender to the Russian throne (b. 1896)
Julio Cortzar, Argentine writer (b. 1914)
Jessamyn West, American writer. (b. 1902)
Syed Faiz-ul Hassan Shah a Pakistani religious leader,of Allo Mahar Shrif. (b. 1911)
March 1  Jackie Coogan, American actor (b. 1914)
March 5

Tito Gobbi, Italian operatic baritone (b. 1915)
William Powell, American actor (b. 1892)


Tito Gobbi, Italian operatic baritone (b. 1915)
William Powell, American actor (b. 1892)
March 6  Henry Wilcoxon, British actor (b. 1905)
March 10  June Marlowe, American actress (b. 1903)
March 12  Arnold Ridley, English playwright and actor (b. 1896)
March 16  John Hoagland, American photographer (b. 1947)
March 18

Charlie Lau, American baseball player (b. 1933)
Paul Francis Webster, American lyricist (b. 1907)


Charlie Lau, American baseball player (b. 1933)
Paul Francis Webster, American lyricist (b. 1907)
March 24  Sam Jaffe, American actor (b. 1891)
March 26  Ahmed Skou Tour, president of Guinea (b. 1922)
March 31  Jack Howarth, English actor (b. 1896)
Tito Gobbi, Italian operatic baritone (b. 1915)
William Powell, American actor (b. 1892)
Charlie Lau, American baseball player (b. 1933)
Paul Francis Webster, American lyricist (b. 1907)
April 1

Douglas Cooper, British art historian, critic and collector (b. 1911)
Marvin Gaye, American singer (b. 1939)
George Glass, American film producer and publicist (b. 1910)
Elizabeth Goudge, English writer (b. 1900)


Douglas Cooper, British art historian, critic and collector (b. 1911)
Marvin Gaye, American singer (b. 1939)
George Glass, American film producer and publicist (b. 1910)
Elizabeth Goudge, English writer (b. 1900)
April 5

Arthur "Bomber" Harris, British air marshall (b. 1892)
Giuseppe Tucci, Italian scholar of oriental cultures, (b. 1894)


Arthur "Bomber" Harris, British air marshall (b. 1892)
Giuseppe Tucci, Italian scholar of oriental cultures, (b. 1894)
April 8  Pyotr Leonidovich Kapitsa, Russian physicist, Nobel Prize laureate (b. 1894)
April 9  Willem Sandberg, Dutch typographer (b. 1897)
April 11  Edgar V. Saks, Estonian statesman and historian (b. 1910)
April 15

Alexander Trocchi, Scottish writer (b. 1925)
Machito, Cuban singer and musician (b. 1908)
Tommy Cooper, Welsh comedian and magician (b. 1921)


Alexander Trocchi, Scottish writer (b. 1925)
Machito, Cuban singer and musician (b. 1908)
Tommy Cooper, Welsh comedian and magician (b. 1921)
April 16  Byron Haskin, American film and television director (b. 1899)
April 17  Mark W. Clark, American general (b. 1896)
April 20

Hristo Prodanov, Bulgarian mountaineer (b. 1943)
Mabel Mercer, English cabaret singer (b. 1900)


Hristo Prodanov, Bulgarian mountaineer (b. 1943)
Mabel Mercer, English cabaret singer (b. 1900)
April 21  Marcel Janco, Romanian-Israeli artist (b. 1895)
April 22  Ansel Adams, American photographer (b. 1902)
April 23

Roland Penrose, English artist, historian and poet (b. 1900)
Vicente Solano Lima, Argentinian journalist and politician (b. 1901)


Roland Penrose, English artist, historian and poet (b. 1900)
Vicente Solano Lima, Argentinian journalist and politician (b. 1901)
April 26

Count Basie, American musician and composer (b. 1904)
May McAvoy, American actress (b. 1899)


Count Basie, American musician and composer (b. 1904)
May McAvoy, American actress (b. 1899)
April 30  Rodrigo Lara Bonilla, Colombian lawyer and politician (b. 1946)
Douglas Cooper, British art historian, critic and collector (b. 1911)
Marvin Gaye, American singer (b. 1939)
George Glass, American film producer and publicist (b. 1910)
Elizabeth Goudge, English writer (b. 1900)
Arthur "Bomber" Harris, British air marshall (b. 1892)
Giuseppe Tucci, Italian scholar of oriental cultures, (b. 1894)
Alexander Trocchi, Scottish writer (b. 1925)
Machito, Cuban singer and musician (b. 1908)
Tommy Cooper, Welsh comedian and magician (b. 1921)
Hristo Prodanov, Bulgarian mountaineer (b. 1943)
Mabel Mercer, English cabaret singer (b. 1900)
Roland Penrose, English artist, historian and poet (b. 1900)
Vicente Solano Lima, Argentinian journalist and politician (b. 1901)
Count Basie, American musician and composer (b. 1904)
May McAvoy, American actress (b. 1899)
May 2  Jack Barry, American television host and producer (b. 1918)
May 4  Diana Dors, English actress (b. 1931)
May 6  Mary Cain, Mississippi newspaper editor and politician (b. 1904)
May 8  Lila Wallace, American publisher (b. 1889)
May 16

Andy Kaufman, American comedian (b. 1949)
Irwin Shaw, American author (b. 1913)


Andy Kaufman, American comedian (b. 1949)
Irwin Shaw, American author (b. 1913)
May 19  John Betjeman, English poet (b. 1906)
May 21

Andrea Leeds, American actress (b. 1914)
Ann Little, American actress (b. 1891)


Andrea Leeds, American actress (b. 1914)
Ann Little, American actress (b. 1891)
May 22  John Marley, American actor (b. 1907)
May 24  Vincent J. McMahon, professional wrestling promoter WWF (b. 1914)
May 28  Eric Morecambe, British comedian (b. 1926)
Andy Kaufman, American comedian (b. 1949)
Irwin Shaw, American author (b. 1913)
Andrea Leeds, American actress (b. 1914)
Ann Little, American actress (b. 1891)
June 6  Jarnail Singh Bhindranwale, Sikh theologian, Most powerful Sikh leader of the 20th century (b. 1947)
June 11  Enrico Berlinguer, Italian Communist politician (b. 1922)
June 13  Antnio Variaes, Portuguese singer (b. 1944)
June 15  Meredith Willson, American composer (b. 1902)
June 17  Chet Allen, American actor (b. 1939)
June 18  Alan Berg, American talk radio host (murdered) (b. 1934)
June 19  Lee Krasner, American painter (b. 1908)
June 20  Estelle Winwood, English actress (b. 1883)
June 22  Joseph Losey, American film director (b. 1909)
June 24  William Keighley, American film director (b. 1889)
June 25  Michel Foucault, French philosopher (b. 1926)
June 26  Carl Foreman, American screenwriter (b. 1914)
June 28  Yigael Yadin, Israeli archeologist, politician and Military Chief of Staff (b. 1917)
June 30

Henri Fabre, pioneer French aviator & inventor (b. 1882)
Lillian Hellman, American playwright (b. 1905)


Henri Fabre, pioneer French aviator & inventor (b. 1882)
Lillian Hellman, American playwright (b. 1905)
Henri Fabre, pioneer French aviator & inventor (b. 1882)
Lillian Hellman, American playwright (b. 1905)
July 1  Mosh Feldenkrais, Ukrainian founder of the Feldenkrais Method (b. 1904)
July 4  Jimmie Spheeris, American singer-songwriter (b. 1949)
July 7  Flora Robson, English actress (b. 1902)
July 8  Brassa, Hungarian-born photographer (b. 1899)
July 14  Philippe Wynne, American musician (b. 1941)
July 17  Karl Wolff, German Nazi SS Officer (b. 1900)
July 19 - Harry Stockwell, American actor and singer (b. 1902)
July 24  Armando Morales Barillas, Nicaraguan guitarist (b. 1936)
July 25  Big Mama Thornton, American singer (b. 1926)
July 26

George Gallup, American statistician and opinion pollster (b. 1901)
Ed Gein, American serial killer (b. 1906)


George Gallup, American statistician and opinion pollster (b. 1901)
Ed Gein, American serial killer (b. 1906)
July 27  James Mason, British actor (b. 1909)
July 28  Bess Flowers, American actress (b. 1898)
July 29  Fred Waring, American bandleader (b. 1900)
George Gallup, American statistician and opinion pollster (b. 1901)
Ed Gein, American serial killer (b. 1906)
August 2  Quirino Cristiani, Argentine animated film director (b. 1896)
August 4  Mary Miles Minter, American actress (b. 1902)
August 5  Richard Burton, Welsh actor (b. 1925)
August 8  Richard Deacon, American actor (b. 1921)
August 11

Alfred A. Knopf, American publisher (b. 1892)
Paul Felix Schmidt, EstonianGerman chess player (b. 1916)


Alfred A. Knopf, American publisher (b. 1892)
Paul Felix Schmidt, EstonianGerman chess player (b. 1916)
August 12  Christine Hargreaves, English actress (b. 1939)
August 13

Clyde Cook, Australian actor (b. 1891)
Tigran Petrosian, Georgian chess player (b. 1929)


Clyde Cook, Australian actor (b. 1891)
Tigran Petrosian, Georgian chess player (b. 1929)
August 14  J. B. Priestley, English novelist and playwright (b. 1894)
August 25

Truman Capote, American writer (b. 1924)
Viktor Chukarin, Russian Olympic gymnast (b. 1921)
Waite Hoyt, American baseball player (b. 1899)


Truman Capote, American writer (b. 1924)
Viktor Chukarin, Russian Olympic gymnast (b. 1921)
Waite Hoyt, American baseball player (b. 1899)
August 27  Bernard Youens, English actor (b. 1914)
August 29  Mohammed Naguib, 1st President of Egypt (b. 1901)
Alfred A. Knopf, American publisher (b. 1892)
Paul Felix Schmidt, EstonianGerman chess player (b. 1916)
Clyde Cook, Australian actor (b. 1891)
Tigran Petrosian, Georgian chess player (b. 1929)
Truman Capote, American writer (b. 1924)
Viktor Chukarin, Russian Olympic gymnast (b. 1921)
Waite Hoyt, American baseball player (b. 1899)
September 3  Arthur Schwartz, American composer (b. 1900)
September 5  Adam Malik, 3rd Vice President of Indonesia (b. 1917)
September 6  Ernest Tubb, American singer (b. 1914)
September 8  Frank Lowson English Test Cricketer 19511955 (b.1925)
September 9  Ylmaz Gney Turkish film director (b.1937)
September 10  Ismael Merlo, Spanish actor (b. 1918)
September 14

Richard Brautigan, American counter-culture author (suicide) (b. 1935)
Janet Gaynor, American Academy Award-winning actress (b. 1906)


Richard Brautigan, American counter-culture author (suicide) (b. 1935)
Janet Gaynor, American Academy Award-winning actress (b. 1906)
September 17  Richard Basehart, American actor (b. 1914)
September 20  Steve Goodman, American folk musician and songwriter (b. 1948)
September 24  Neil Hamilton, American actor (b. 1899)
September 25  Walter Pidgeon, Canadian actor (b. 1897)
September 27  Toke Townley, English actor (b. 1912)
Richard Brautigan, American counter-culture author (suicide) (b. 1935)
Janet Gaynor, American Academy Award-winning actress (b. 1906)
October 5  Leonard Rossiter, British actor (b. 1926)
October 12  Sir Anthony Berry, British politician (bombing) (b. 1925)
October 14  Martin Ryle, English radio astronomer, recipient of the Nobel Prize in Physics (b. 1918)
October 16  Peggy Ann Garner, American actress (b. 1932)
October 18  Jon-Erik Hexum, American actor (b. 1957)
October 20

Carl Ferdinand Cori, Austrian-born biochemist, recipient of the Nobel Prize in Physiology or Medicine (b. 1896)
Paul Dirac, English physicist, Nobel Prize laureate (b. 1902)


Carl Ferdinand Cori, Austrian-born biochemist, recipient of the Nobel Prize in Physiology or Medicine (b. 1896)
Paul Dirac, English physicist, Nobel Prize laureate (b. 1902)
October 21  Franois Truffaut, French film director (b. 1932)
October 23

David Gorcey, American actor (b. 1921)
Oskar Werner, Austrian actor (b. 1922)


David Gorcey, American actor (b. 1921)
Oskar Werner, Austrian actor (b. 1922)
October 24  Walter Woolf King, American singer and actor (b. 1899)
October 30  June Duprez, English actress (b. 1918)
October 31  Indira Gandhi, Prime Minister of India (assassinated) (b. 1917)
Carl Ferdinand Cori, Austrian-born biochemist, recipient of the Nobel Prize in Physiology or Medicine (b. 1896)
Paul Dirac, English physicist, Nobel Prize laureate (b. 1902)
David Gorcey, American actor (b. 1921)
Oskar Werner, Austrian actor (b. 1922)
November 6  Gastn Surez, Bolivian novelist and dramatist (b. 1929)
November 14  Cesar Climaco, Filipino politician (assassinated) (b. 1916)
November 16

Vic Dickenson, American trombonist (b. 1906)
Leonard Rose, American cellist (leukemia) (b. 1918)


Vic Dickenson, American trombonist (b. 1906)
Leonard Rose, American cellist (leukemia) (b. 1918)
November 18  Mary Hamman, American writer and editor, modern living editor LIFE and editor in chief Bride & Home (b. 1907)
Vic Dickenson, American trombonist (b. 1906)
Leonard Rose, American cellist (leukemia) (b. 1918)
December 1  Roelof Frankot, Dutch painter (b. 1911)
December 8  Luther Adler, American actor (b. 1903)
December 11

Oskar Seidlin, Silesian-born Jewish-American literary scholar (b. 1911)
George Waggner, American film director (b. 1894)


Oskar Seidlin, Silesian-born Jewish-American literary scholar (b. 1911)
George Waggner, American film director (b. 1894)
December 14  Vicente Aleixandre, Spanish writer, Nobel Prize laureate (b. 1898)
December 15  Jan Peerce, American tenor (b. 1904)
December 16  J. Roderick MacArthur, American businessman and philanthropist (b. 1920)
December 20  Gonzalo Mrquez, Venezuelan Major League Baseball player (b. 1946)
December 24  Peter Lawford, English actor (b. 1923)
December 28  Sam Peckinpah, American film director (b. 1925)
December 29  Leo Robin, American composer (b. 1900)
Oskar Seidlin, Silesian-born Jewish-American literary scholar (b. 1911)
George Waggner, American film director (b. 1894)
Physics  Carlo Rubbia, Simon van der Meer
Chemistry  Robert Bruce Merrifield
Medicine  Niels Kaj Jerne, Georges J.F. Khler, Csar Milstein
Literature  Jaroslav Seifert
Peace  Bishop Desmond Mpilo Tutu
Bank of Sweden Prize in Economic Sciences in Memory of Alfred Nobel  Richard Stone
Rev. Michael Bourdeaux, founder of the Keston Institute[3]
.
#(`*Death of a Salesman*`)#.
Death of a Salesman is a 1949 play written by American playwright Arthur Miller. It was the recipient of the 1949 Pulitzer Prize for Drama and Tony Award for Best Play. The play premiered on Broadway in February 1949, running for 742 performances, and has been revived on Broadway four times,[1] winning three Tony Awards for Best Revival.
Willy Loman returns home exhausted after a cancelled business trip. Worried over Willy's state of mind and recent car "crash," his wife Linda suggests that he ask his boss Howard Wagner to allow him to work in his home city so he will not have to travel. Willy complains to Linda that their son, Biff, has yet to make good on his life. Despite Biff's promise as an athlete in high school, he flunked senior year math and never went to college.
Biff and his brother, Happy, who is also visiting, reminisce about their childhood together. They discuss their father's mental degeneration, which they have witnessed by his constant vacillations and talking to himself. When Willy walks in, angry that the two boys have never amounted to anything, Biff and Happy tell Willy that Biff plans to make a business proposition the next day in an effort to pacify their father.
The next day, Willy goes to ask his boss for a job in town while Biff goes to make a business proposition, but neither is successful. Willy gets angry and ends up getting fired when the boss tells him he needs a rest and can no longer represent the company. Biff waits hours to see a former employer who does not remember him and turns him down. Biff impulsively steals a fountain pen. Willy then goes to the office of his neighbor Charley, where he runs into Charley's son Bernard (now a successful lawyer); Bernard tells him that Biff originally wanted to do well in summer school, but something happened in Boston when Biff went to visit Willy that changed his mind.
Happy, Biff, and Willy meet for dinner at a restaurant, but Willy refuses to hear bad news from Biff. Happy tries to get Biff to lie to their father. Biff tries to tell him what happened as Willy gets angry and slips into a flashback of what happened in Boston the day Biff came to see him. Willy had been in a hotel on a sales trip with a young woman when Biff arrived. From that moment, Biff's view of his father changed and set Biff adrift.
Biff leaves the restaurant in frustration, followed by Happy and two girls that Happy has picked up. They leave a confused and upset Willy behind in the restaurant. When they later return home, their mother angrily confronts them for abandoning their father while Willy remains talking to himself outside. Biff goes outside to try to reconcile with Willy. The discussion quickly escalates into another argument, at which point Biff forcefully tries to convey to his father that he is not meant for anything great, that he is simply ordinary, insisting that they both are. The feud culminates with Biff hugging Willy and crying as he tries to get him to let go of the unrealistic dreams he still carries for Biff and wants instead for Willy to accept him for who he really is. He tells his father he loves him.
Rather than listen to what Biff actually says, Willy realizes his son has forgiven him and thinks Biff will now pursue a career as a businessman. Willy kills himself, intentionally crashing his car so that Biff can use the life insurance money to start his business. However, at the funeral Biff retains his belief that he does not want to become a businessman. Happy, on the other hand, chooses to follow in his father's footsteps.
The play is mostly told from the point of view of the protagonist, Willy, and the previous parts of Willy's life are revealed in the analepsis, sometimes during a present day scene. It does this by having a scene begin in the present time, and adding characters onto the stage whom only Willy can see and hear, representing characters and conversations from other times and places.
Many dramatic techniques are also used to represent these time shifts. For example, leaves often appear around the current setting (representing the leaves of the two elm trees which were situated next to the house, prior to the development of the apartment blocks). Biff and Happy are dressed in high school football sweaters and are accompanied with the "gay music of the boys". The characters will also be allowed to pass through the walls that are impassable in the present, as told in Miller's stage directions in the opening of ACT 1:
Whenever the action is in the present the actors observe the imaginary wall-lines, entering the house only through its door at the left. But in the scenes of the past these boundaries are broken and characters enter or leave a room by stepping 'through' a wall onto the fore-stage.
However some of these time shifts/imaginings occur when there are present characters onstage; one example of this is during a conversation between Willy and his neighbor Charley. During the conversation, Willy's brother Ben comes on stage and begins talking to Willy while Charley speaks to Willy. When Willy begins talking to his brother, the other characters do not understand to whom he is talking, and some of them even begin to suspect that he has "lost it". However, at times it breaks away from Willy's point of view and focuses on the other characters: Linda, Biff, and Happy. During these parts of the play, the time and place stay constant without any abrupt flashbacks that usually happen while the play takes Willy's point of view.
The play's structure resembles a stream of consciousness account: Willy drifts between his living room, downstage, to the apron and flashbacks of an idyllic past, and also to fantasized conversations with Ben. When we are in the present the characters abide by the rules of the set, entering only through the stage door to the left; however, when we visit Willy's "past" these rules are removed, with characters openly moving through walls. Whereas the term "flashback" as a form of cinematography for these scenes is often heard, Miller himself rather speaks of "mobile concurrences". In fact, flashbacks would show an objective image of the past. Miller's mobile concurrences, however, rather show highly subjective memories. Furthermore, as Willy's mental state deteriorates, the boundaries between past and present are destroyed, and the two start to exist in parallel.
The original Broadway production was produced by Kermit Bloomgarden and opened at the Morosco Theatre on February 10, 1949, closing on November 18, 1950 after 742 performances. The play starred Lee J. Cobb as Willy Loman, Mildred Dunnock as Linda, Arthur Kennedy as Biff and Cameron Mitchell as Happy. Albert Dekker and Gene Lockhart later played Willy Loman during the original Broadway run. It won the Tony Award for Best Play, Best Supporting or Featured Actor (Arthur Kennedy), Best Scenic Design (Jo Mielziner), Producer (Dramatic), Author (Arthur Miller), and Director (Elia Kazan), as well as the 1949 Pulitzer Prize for Drama and the New York Drama Critics' Circle Award for Best Play. Jayne Mansfield performed in a production of the play in Dallas, Texas in October 1953. Her performance in the play attracted Paramount Pictures to hire her for the studio's film productions.[2]
The play has been revived on Broadway four times:
It was also part of the inaugural season of the Guthrie Theater in Minneapolis, Minnesota.
Christopher Lloyd portrayed Willy Loman in a 2010 production by the Weston Playhouse in Weston, Vermont, which toured several New England venues.[4]
2012 Broadway revival
"Death of a Salesman" at Playbill Vault:
1 Plot
2 Characters
3 Style
4 Productions
5 Film and television adaptations
6 Awards and nominations
7 References
8 Further reading
9 External links
William "Willy" Loman: The salesman. He is 60 years old and very unstable, tending to imagine events from the past as if they are real. He vacillates between different perceptions of his life. Willy seems childlike and relies on others for support. His first name, Willy, reflects this childlike aspect as well as sounding like the question "Will he?" His last name gives the feel of Willy's being a "low man," someone low on the social ladder and unlikely to succeed; however, this popular interpretation of his last name has been dismissed by Miller.
Linda Loman: Willy's wife. Linda is passively supportive and docile when Willy talks unrealistically about hopes for the future, although she seems to have a good knowledge of what is really going on. She chides her sons, particularly Happy, for not helping Willy more, and supports Willy lovingly, despite the fact that Willy sometimes treats her poorly, ignoring her opinions over those of others. She is the first to realize Willy is contemplating suicide at the beginning of the play, and urges Biff to make something of himself, while expecting Happy to help Biff do so.
Biff Loman: Willy's older son. Biff was a football star with lots of potential in high school, but failed math his senior year and dropped out of summer school due to seeing Willy with another woman while visiting him in Boston. He goes between going home to try to fulfill Willy's dream for him to be a businessman and ignoring his father and going out West to be a farmhand where he is happiest. He likes being outdoors and working with his hands yet wants to do something worthwhile so Willy will be proud. Biff steals because he wants evidence of success, even if it is false evidence, but overall Biff remains a realist, and informs Willy that he is just a normal guy, and will not be a great man.
Harold "Happy" Loman: Willy's younger son. He's lived in the shadow of his older brother Biff most of his life and seems to be almost ignored, but he still tries to be supportive towards his family. He has a very restless lifestyle as a womanizer and dreams of moving beyond his current job as an assistant to the assistant buyer at the local store, but is unfortunately willing to cheat a little in order to do so, by taking bribes. He is always looking for approval from his parents, but rarely gets any, and he even goes as far as to make things up just for attention, such as telling his parents he is going to get married. He tries often to keep his family's perceptions of each other positive or "happy" by defending each of them during their many arguments, but still has the most turbulent relationship with Linda, who looks down on him for his lifestyle and apparent cheapness, despite him giving them money.
Charley: Willy's wisecracking yet understanding neighbor. He pities Willy and frequently lends him money and comes over to play cards with Willy, although Willy often treats him poorly. Willy is jealous of him because his son is more successful than Willy's. Charley offers Willy a job many times during visits to his office, yet Willy declines every time, even after he loses his job as a salesman.
Bernard: Charley's son. In Willy's flashbacks, he is a nerd, and Willy forces him to give Biff test answers. He worships Biff and does anything for him. Later, he is a very successful lawyer, married, and expecting a second son. These successes are of the very kind that Willy wants for his sons, and in particular, Biff, making him contemplate where he had gone wrong as a father.
Uncle Ben: Willy's older brother who became a diamond tycoon after a detour to Africa. He is dead but Willy frequently speaks to him in his hallucinations of the past. Ben frequently boasts, "when I was seventeen I walked into the jungle, and when I was twenty-one I walked out. And by God I was rich." He is Willy's role model, although he is much older and has no real relationship with Willy, preferring to assert his superiority over his younger brother. He represents Willy's idea of the American Dream success story, and is shown coming by the Lomans' house while on business trips to share stories.
Ms. Francis: A woman with whom Willy cheated on Linda.
Howard Wagner: Willy's boss. He was named by Willy, and yet he sees Willy as a liability for the company and lets him go, ignoring all the years that Willy has given to the company. Howard is extremely proud of his wealth, which is manifested in his recording machine, and his family.
Jenny: Charley's secretary.
Stanley: A waiter at the restaurant who seems to be friends or acquainted with Happy.
Miss Forsythe: A call girl (prostitute) whom Happy picks up at the restaurant. She is very pretty and claims she was on several magazine covers. Happy lies to her, making himself and Biff look like they are important and successful. (Happy claims that he attended West Point and that Biff is a star football player.)
Letta: Miss Forsythe's friend; also a call girl.
June 26, 1975 at the Circle in the Square Theatre, running for 71 performances. George C. Scott starred as Willy.
March 29, 1984 at the Broadhurst Theatre, running for 97 performances. Dustin Hoffman played Willy. In a return engagement, this production re-opened on September 14, 1984 and ran for 88 performances. The production won the Tony Award for Best Revival and the Drama Desk Award for Outstanding Revival.
February 10, 1999 at the Eugene O'Neill Theatre, running for 274 performances, with Brian Dennehy as Willy. The production won the Tony Award for: Best Revival of a Play; Best Actor in Play; Best Featured Actress in a Play (Elizabeth Franz); Best Direction of a Play (Robert Falls). This production was filmed.
February 13, 2012 at the Ethel Barrymore Theatre, in a limited run of 16 weeks. Directed by Mike Nichols, Philip Seymour Hoffman played Willy, Andrew Garfield played Biff, and Linda Emond played Linda.[3]
1951: Adapted by Stanley Roberts and directed by Lszl Benedek who won the Golden Globe Award for Best Director. The film was nominated for Academy Awards for Best Actor in a Leading Role (Fredric March), Best Actor in a Supporting Role (Kevin McCarthy), Best Actress in a Supporting Role (Mildred Dunnock), Best Cinematography, Black-and-White and Best Music, Scoring of a Dramatic or Comedy Picture. - * See main article Death of a Salesman (1951 film)
1961: En Handelsresandes dd starring Kolbjrn Knudsen and directed by Hans Abramson (in Swedish)
1968: Der Tod eines Handlungsreisenden starring Heinz Rhmann and directed by Gerhard Klingenberg
1966 (CBS): Starring Lee J. Cobb, Gene Wilder, Mildred Dunnock, James Farentino, Karen Steele and George Segal and directed by Alex Segal. - * See main article Death of a Salesman (1966 TV movie)
1966 (BBC): Starring Rod Steiger, Betsy Blair, Tony Bill, Brian Davies and Joss Ackland and directed by Alan Cooke.
1985: Starring Dustin Hoffman, Kate Reid, John Malkovich, Stephen Lang and Charles Durning and directed by Volker Schlndorff. - * See main article Death of a Salesman (1985 film)
1996: Starring Warren Mitchell, Rosemary Harris, Iain Glen and Owen Teale and directed by David Thacker.
2000: Starring Brian Dennehy, Elizabeth Franz, Ron Eldard, Ted Koch, Howard Witt and Richard Thompson and directed by Kirk Browning. - * See main article Death of a Salesman (2000 film)
New York Drama Critics' Circle Best Play (win)
Pulitzer Prize for Drama (win)
Tony Award for Best Play (win)
Tony Award, Best Supporting or Featured Actor (Dramatic)- Arthur Kennedy (win)
Tony Award, Best Scenic Design - Jo Mielziner (win)
Tony Award Author - Arthur Miller (win)
Tony Award Best Director - Elia Kazan (win)
Tony Award Best Actor in Play - George C. Scott (nominee)
Drama Desk Award Outstanding Revival (win)
Drama Desk Award Outstanding Actor in a Play - Dustin Hoffman (win)
Drama Desk Award Outstanding Featured Actor in a Play - John Malkovich (win); David Huddleston (nominee)
Tony Award for Best Reproduction (win)
Tony Award for Best Revival of a Play (win)
Drama Desk Award Outstanding Revival of a Play (win)
Tony Award Best Actor in Play - Brian Dennehy (win)
Tony Award Best Featured Actor in a Play - Kevin Anderson (nominee); Howard Witt (nominee)
Tony Award Best Featured Actress in a Play - Elizabeth Franz (win)
Tony Award Best Direction of a Play - Robert Falls (win)
Drama Desk Award Outstanding Revival of a Play (win)
Drama Desk Award Outstanding Actor in a Play - Brian Dennehy (win)
Drama Desk Award Outstanding Actress in a Play - Elizabeth Franz (nominee)
Drama Desk Award Outstanding Featured Actor in a Play - Kevin Anderson (win); Howard Witt (nominee)
Drama Desk Award Outstanding Director of a Play - Robert Falls (nominee)
Drama Desk Award Outstanding Music in a Play - Incidental music by Richard Woodbury (nominee)
Tony Award for Best Revival of a Play (win)
Tony Award for Best Direction of a Play - Mike Nichols (win)
Tony Award for Best Actor in a Play - Philip Seymour Hoffman (nominee)
Tony Award for Best Featured Actor in a Play - Andrew Garfield (nominee)
Tony Award for Best Actress in a Play - Linda Emond (nominee)
Tony Award for Best Lighting Design of a Play - Brian MacDevitt (nominee)
Tony Award for Best Sound Design of a Play - Scott Lehrer (nominee)
Hurell, John D. (1961). Two Modern American Tragedies: Reviews and Criticism of Death of a Salesman and A streetcar Named Desire. New York: Scribner. pp.828. OCLC249094.
Sandage, Scott A. (2005). Born Losers: A History of Failure in America. Cambridge: Harvard University Press. ISBN0-674-01510-X.
Death of a Salesman at the Internet Broadway Database
Character Analysis of Willy Loman
Character Analysis of Linda Loman
Death of a Salesman: A Celebration by Joyce Carol Oates
Death of a Salesman Reviews
Director's Notes for "Death of a Salesman"
First Revival -1975
Second Revival - 1984
Third Revival - 1999
Fourth Revival - 2012
.
#(`*The Scarlet Letter*`)#.
The Scarlet Letter is an 1850 romantic work of fiction in a historical setting, written by Nathaniel Hawthorne. It is considered to be his magnum opus.[1] Set in 17th-century Puritan Boston, Massachusetts during the years 1642 to 1649, it tells the story of Hester Prynne, who conceives a daughter through an adulterous affair and struggles to create a new life of repentance and dignity. Throughout the book, Hawthorne explores themes of legalism, sin, and guilt.
The story starts in seventeenth-century Boston in a Puritan settlement. A young woman, named Hester Prynne, has been led from the town prison with her infant child in her arms, and on the breast of her gown "a rag of scarlet cloth" that "assumed the shape of a letter". It is the uppercase letter "A." The Scarlet Letter "A" represents the act of adultery that she has committed and it is to be a symbol of her sina badge of shamefor all to see. A man, who is elderly and a stranger to the town, enters the crowd and asks another onlooker what's happening. The second man responds by explaining that Hester is being punished for adultery. Hester's husband, who is much older than she, and whose real name is unknown, has sent her ahead to America whilst settling affairs in Europe. However, her husband does not arrive in Boston and the consensus is that he has been lost at sea. It is apparent that, while waiting for her husband, Hester has had an affair, leading to the birth of her daughter. She will not reveal her lover's identity, however, and the scarlet letter, along with her subsequent public shaming, is the punishment for her sin and secrecy. On this day, Hester is led to the town scaffold and harangued by the town fathers, but she again refuses to identify her child's father.[2]
The elderly onlooker is Hester's missing husband, who is now practicing medicine and calling himself Roger Chillingworth. He reveals his true identity to Hester and medicates her daughter. They have a frank discussion where Chillingworth states that it was foolish and wrong for a cold, old intellectual like him to marry a young lively woman like Hester. He expressly states that he thinks that they have wronged each other and that he is even with her  her lover is a completely different matter. Hester refuses to divulge the name of her lover and Chillingworth does not press her stating that he will find out anyway. He does elicit a promise from her to keep his true identity as Hester's husband secret, though. He settles in Boston to practice medicine there. Several years pass. Hester supports herself by working as a seamstress, and her daughter, Pearl, grows into a willful, impish child, and is said to be the scarlet letter come to life as both Hester's love and her punishment. Shunned by the community, they live in a small cottage on the outskirts of Boston. Community officials attempt to take Pearl away from Hester, but with the help of Arthur Dimmesdale, an eloquent minister, the mother and daughter manage to stay together. Dimmesdale, however, appears to be wasting away and suffers from mysterious heart trouble, seemingly caused by psychological distress. Chillingworth attaches himself to the ailing minister and eventually moves in with him so that he can provide his patient with round-the-clock care. Chillingworth also suspects that there may be a connection between the minister's torments and Hester's secret, and he begins to test Dimmesdale to see what he can learn. One afternoon, while the minister sleeps, Chillingworth discovers something undescribed to the reader, supposedly an "A" burned into Dimmesdale's chest, which convinces him that his suspicions are correct.[2]
Dimmesdale's psychological anguish deepens, and he invents new tortures for himself. In the meantime, Hester's charitable deeds and quiet humility have earned her a reprieve from the scorn of the community. One night, when she is about seven years old, Pearl and her mother are returning home from a visit to the deathbed of John Winthrop when they encounter Dimmesdale atop the town scaffold, trying to punish himself for his sins. Hester and Pearl join him, and the three link hands. Dimmesdale refuses Pearl's request that he acknowledge her publicly the next day, and a meteor marks a dull red "A" in the night sky as Dimmesdale sees Chillingworth in the distance. It is interpreted by the townsfolk to mean Angel, as a prominent figure in the community had died that night, but Dimmesdale sees it as meaning adultery. Hester can see that the minister's condition is worsening, and she resolves to intervene. She goes to Chillingworth and asks him to stop adding to Dimmesdale's self-torment. Chillingworth refuses. She suggests that she may reveal his true identity to Dimmesdale.[2]
As Hester walks through the forest, she is unable to feel the sunshine. Pearl, on the other hand, basks in it. They coincide with Dimmesdale, also on a stroll through the woods. Hester informs him of the true identity of Chillingworth. The former lovers decide to flee to Europe, where they can live with Pearl as a family. They will take a ship sailing from Boston in four days. Both feel a sense of relief, and Hester removes her scarlet letter and lets down her hair. The sun immediately breaks through the clouds and trees to illuminate her release and joy. Pearl, playing nearby, does not recognize her mother without the letter. She is unnerved and expels a shriek until her mother points out the letter on the ground. Hester beckons Pearl to come to her, but Pearl will not go to her mother until Hester buttons the letter back onto her dress. Pearl then goes to her mother. Dimmesdale gives Pearl a kiss on the forehead, which Pearl immediately tries to wash off in the brook, because he again refuses to make known publicly their relationship. However, he clearly feels a release from the pretense of his former life, and the laws and sins he has lived with.
The day before the ship is to sail, the townspeople gather for a holiday in honor of an election and Dimmesdale preaches his most eloquent sermon ever. Meanwhile, Hester has learned that Chillingworth knows of their plan and has booked passage on the same ship. Dimmesdale, leaving the church after his sermon, sees Hester and Pearl standing before the town scaffold. He looks ill. Knowing his life is about to end, he mounts the scaffold with his lover and his daughter, and confesses publicly, exposing the mark supposedly seared into the flesh of his chest. He dies in Hester's arms after Pearl kisses him.[2]
Frustrated in his revenge, Chillingworth dies within the year. Hester and Pearl leave Boston, and no one knows what has happened to them. Many years later, Hester returns alone, still wearing the scarlet letter, to live in her old cottage and resumes her charitable work. She receives occasional letters from Pearl, who was rumored to have married a European aristocrat and established a family of her own. Pearl also inherits all of Chillingworth's money even though he knows she is not his daughter. There is a sense of liberation in her and the townspeople, especially the women, who had finally begun to forgive Hester of her tragic indiscretion. When Hester dies, she is buried in "a new grave near an old and sunken one, in that burial ground beside which King's Chapel has since been built. It was near that old and sunken grave, yet with a space between, as if the dust of the two sleepers had no right to mingle. Yet one tombstone served for both". The tombstone was decorated with a letter "A", for Hester and Dimmesdale.
The experience of Hester and Dimmesdale recalls the story of Adam and Eve because, in both cases, sin results in expulsion and suffering. But it also results in knowledgespecifically, in knowledge of what it means to be immoral. For Hester, the scarlet letter functions as "her passport into regions where other women dared not tread", leading her to "speculate" about her society and herself more "boldly" than anyone else in New England.
As for Dimmesdale, the "cheating minister", his sin gives him "sympathies so intimate with the sinful brotherhood of mankind, so that his chest vibrate[s] in unison with theirs." His eloquent and powerful sermons derive from this sense of empathy.[3] The narrative of the Reverend Arthur Dimmesdale is quite in keeping with the oldest and most fully authorized principles in Christian thought. His "Fall" is a descent from apparent grace to his own damnation; he appears to begin in purity but he ends in corruption. The subtlety is that the minister's belief is his own cheating, convincing himself at every stage of his spiritual pilgrimage that he is saved.[4]
The rose bush, its beauty a striking contrast to all that surrounds itas later the beautifully embroidered scarlet A will beis held out in part as an invitation to find "some sweet moral blossom" in the ensuing, tragic tale and in part as an image that "the deep heart of nature" (perhaps God) may look more kind on the errant Hester and her child than her Puritan neighbors do. Throughout the work, the nature images contrast with the stark darkness of the Puritans and their systems.[5]
Chillingworth's misshapen body reflects (or symbolizes) the anger in his soul, which builds as the novel progresses, similar to the way Dimmesdale's illness reveals his inner turmoil. The outward man reflects the condition of the heart; an observation thought to be inspired by the deterioration of Edgar Allan Poe, whom Hawthorne "much admired".[5]
Although Pearl is a complex character, her primary function within the novel is as a symbol. Pearl herself is the embodiment of the scarlet letter, and Hester rightly clothes her in a beautiful dress of scarlet, embroidered with gold thread, just like the scarlet letter upon Hester's bosom.[3]
The clash of past and present is explored in various ways. For example, the character of the old General, whose heroic qualities include a distinguished name, perseverance, integrity, compassion, and moral inner strength, is said to be "the soul and spirit of New England hardihood". Sometimes he presides over the Custom House run by corrupt public servants, who skip work to sleep, allow or overlook smuggling, and are supervised by an inspector with "no power of thought, nor depth of feeling, no troublesome sensibilities", who is honest enough but without a spiritual compass.[5]
Hawthorne himself had ambivalent feelings about the role of his ancestors in his life. In his autobiographical sketch, Hawthorne described his ancestors as "dim and dusky", "grave, bearded, sable-cloaked, and steel crowned", "bitter persecutors" whose "better deeds" would be diminished by their bad ones. There can be little doubt of Hawthorne's disdain for the stern morality and rigidity of the Puritans, and he imagined his predecessors' disdainful view of him: unsuccessful in their eyes, worthless and disgraceful. "A writer of story books!" But even as he disagrees with his ancestors' viewpoint, he also feels an instinctual connection to them and, more importantly, a "sense of place" in Salem. Their blood remains in his veins, but their intolerance and lack of humanity becomes the subject of his novel.[5]
Another theme is the extreme legalism of the Puritans and how Hester chooses not to conform to their rules and beliefs. Hester was rejected by the villagers even though she spent her life doing what she could to help the sick and the poor. Because they rejected her, she spent her life mostly in solitude, and wouldn't go to church. As a result, she retreats into her own mind and her own thinking. Her thoughts begin to stretch and go beyond what would be considered by the Puritans as safe or even Christian. She still sees her sin, but begins to look on it differently than the villagers ever have. She begins to believe that a person's earthly sins don't necessarily condemn them. She even goes so far as to tell Dimmesdale that their sin has been paid for by their daily penance and that their sin won't keep them from getting to heaven, however, the Puritans believed that such a sin surely condemns. But Hester had been alienated from the Puritan society, both in her physical life and spiritual life. When Dimmesdale dies, she knows she has to move on because she can no longer conform to the Puritans' strictness. Her thinking is free from religious bounds and she has established her own different moral standards and beliefs.[3] .
It was long thought that Hawthorne originally planned The Scarlet Letter to be a shorter novelette which was part of a collection to be named Old Time Legends and that his publisher, James Thomas Fields, convinced him to expand the work to a full-length novel.[6] This is not true: Fields persuaded Hawthorne to publish The Scarlet Letter alone (along with the earlier-completed "Custom House" essay) but he had nothing to do with the length of the story.[7] Hawthorne's wife Sophia later challenged Fields' claims a little inexactly: "he has made the absurd boast that he was the sole cause of the Scarlet Letter being published!" She noted that her husband's friend Edwin Percy Whipple, a critic, approached Fields to consider its publication.[8] The manuscript was written at the Peter Edgerley House in Salem, Massachusetts, still standing as a private residence at 14 Mall Street. It was the last Salem home where the Hawthorne family lived.[9]
The Scarlet Letter was published as a novel in the spring of 1850 by Ticknor & Fields, beginning Hawthorne's most lucrative period.[10] When he delivered the final pages to Fields in February 1850, Hawthorne said that "some portions of the book are powerfully written" but doubted it would be popular.[11] In fact, the book was an instant best-seller[12] though, over fourteen years, it brought its author only $1,500.[10] Its initial publication brought wide protest from natives of Salem, who did not approve of how Hawthorne had depicted them in his introduction "The Custom-House". A 2,500-copy second edition of The Scarlet Letter included a preface by Hawthorne dated March 30, 1850, that stated he had decided to reprint his introduction "without the change of a word... The only remarkable features of the sketch are its frank and genuine good-humor... As to enmity, or ill-feeling of any kind, personal or political, he utterly disclaims such motives".[13]
The Scarlet Letter was also one of the first mass-produced books in America. Into the mid-nineteenth century, bookbinders of home-grown literature typically hand-made their books and sold them in small quantities. The first mechanized printing of The Scarlet Letter, 2,500 volumes, sold out within ten days,[10] and was widely read and discussed to an extent not much experienced in the young country up until that time. Copies of the first edition are often sought by collectors as rare books, and may fetch up to around $18,000 USD.
On its publication, critic Evert Augustus Duyckinck, a friend of Hawthorne's, said he preferred the author's Washington Irving-like tales. Another friend, critic Edwin Percy Whipple, objected to the novel's "morbid intensity" with dense psychological details, writing that the book "is therefore apt to become, like Hawthorne, too painfully anatomical in his exhibition of them".[14] Most literary critics praised the book but religious leaders took issue with the novel's subject matter.[15] Orestes Brownson complained that Hawthorne did not understand Christianity, confession, and remorse.[citation needed] A review in The Church Review and Ecclesiastical Register concluded the author "perpetrates bad morals."[16]
[17] Henry James once said of the novel, "It is beautiful, admirable, extraordinary; it has in the highest degree that merit which I have spoken of as the mark of Hawthorne's best things--an indefinable purity and lightness of conception...One can often return to it; it supports familiarity and has the inexhaustible charm and mystery of great works of art."[18]
The book's immediate and lasting success are due to the way it addresses spiritual and moral issues from a uniquely American standpoint.[citation needed] In 1850, adultery was an extremely risqu subject, but because Hawthorne had the support of the New England literary establishment, it passed easily into the realm of appropriate reading. It has been said that this work represents the height of Hawthorne's literary genius, dense with terse descriptions. It remains relevant for its philosophical and psychological depth, and continues to be read as a classic tale on a universal theme.[19]
The defeat of the revolutions of 1848 and 1849 in Europe appears to have unleashed a veritable epidemic of treatments of the theme of adultery. The rebellion of a wife against the fetters of her marriage may be seen as a code for the artist's rebellion against political and legal authority. In the same year in which The Scarlet Letter was published, for instance Verdi's opera Stiffelio was premiered, in which the title character is also a minister; it is he who commits the act of adultery. From 1854 to 1859 Richard Wagner portrayed adulteresses in Die Walkre and Tristan und Isolde; at the same time, Gustave Flaubert was working on Madame Bovary.[20]
The following are historical and Biblical references that appear in The Scarlet Letter.
The Scarlet Letter has been adapted to numerous films, plays and operas and remains frequently referenced in modern popular culture. The Scarlet Letter was also the basis for the 2010 film Easy A, the story of Olive Penderghast (Emma Stone) who experiences the same isolation Hester Prynne undergoes in the novel, but in a reversed way. In the film, she was originally singled out for being the only virgin girl on campus, and becomes popular after she tells the school that she had sex with a homosexual male classmate, who was also picked on for his sexuality.
1 Plot summary
2 Major themes

2.1 Sin
2.2 Past and present
2.3 Puritan Legalism


2.1 Sin
2.2 Past and present
2.3 Puritan Legalism
3 Publication history
4 Critical response
5 Contemporaneous treatments of the theme of adultery
6 Allusions
7 In popular culture
8 See also
9 References

9.1 Notes
9.2 Bibliography


9.1 Notes
9.2 Bibliography
10 External links
2.1 Sin
2.2 Past and present
2.3 Puritan Legalism
9.1 Notes
9.2 Bibliography
Anne Hutchinson, mentioned in Chapter 1, The Prison Door, was a religious dissenter (15911643). In the 1630s she was excommunicated by the Puritans and exiled from Boston and moved to Rhode Island.[5]
Ann Hibbins, who historically was executed for witchcraft in Boston in 1656, is depicted in The Scarlet Letter as a witch who tries to tempt Prynne to the practice of witchcraft.[21][22]
Richard Bellingham, who historically was the governor of Massachusetts and deputy governor at the time of Hibbins's execution, was depicted in The Scarlet Letter as the brother of Ann Hibbins.
Martin Luther (14831545) was a leader of the Protestant Reformation in Germany.
Sir Thomas Overbury and Dr. Forman were the subjects of an adultery scandal in 1615 in England. Dr. Forman was charged with trying to poison his adulterous wife and her lover. Overbury was a friend of the lover and was perhaps poisoned.
John Winthrop (15881649), second governor of the Massachusetts Bay Colony.
King's Chapel Burying Ground, mentioned in the final paragraph, exists; the Elizabeth Pain gravestone is traditionally considered an inspiration for the protagonists' grave.
Hester Prynne was loosely based on Hawthorne's wife, Sofia Peabody.[citation needed]
The story of King David and Bathsheba is depicted in the tapestry in Mr. Dimmesdale's room (chapter 9). (See II Samuel 11-12 for the Biblical story.)
Boston in fiction
Colonial history of the United States
Illegitimacy in fiction
Brodhead, Richard H. Hawthorne, Melville, and the Novel. Chicago and London: The University of Chicago Press, 1973.
Brown, Gillian. "'Hawthorne, Inheritance, and Women's Property", Studies in the Novel 23.1 (Spring 1991): 107-18.
Caadas, Ivan. "A New Source for the Title and Some Themes in The Scarlet Letter". Nathaniel Hawthorne Review 32.1 (Spring 2006): 4351.
Korobkin, Laura Haft. "The Scarlet Letter of the Law: Hawthorne and Criminal Justice". Novel: a Forum on Fiction 30.2 (Winter 1997): 193217.
Gartner, Matthew. "The Scarlet Letter and the Book of Esther: Scriptural Letter and Narrative Life". Studies in American Fiction 23.2 (Fall 1995): 131-51.
Newberry, Frederick. Tradition and Disinheritance in The Scarlet Letter". ESQ: A Journal of the American Renaissance 23 (1977), 126; repr. in: The Scarlet Letter. W. W. Norton, 1988: pp. 231-48.
Reid, Alfred S. Sir Thomas Overbury's Vision (1616) and Other English Sources of Nathaniel Hawthorne's 'The Scarlet Letter. Gainesville, FL: Scholar's Facsimiles and Reprints, 1957.
Reid, Bethany. "Narrative of the Captivity and Redemption of Roger Prynne: Rereading The Scarlet Letter". Studies in the Novel 33.3 (Fall 2001): 247-67.
Ryskamp, Charles. "The New England Sources of The Scarlet Letter". American Literature 31 (1959): 25772; repr. in: "The Scarlet Letter", 3rd edn. Norton, 1988: 191204.
Savoy, Eric. "'Filial Duty': Reading the Patriarchal Body in 'The Custom House'". Studies in the Novel 25.4 (Winter 1993): 397427.
Sohn, Jeonghee. Rereading Hawthorne's Romance: The Problematics of Happy Endings. American Studies Monograph Series, 26. Seoul: American Studies Institute, Seoul National University, 2001; 2002.
Stewart, Randall (Ed.) The American Notebooks of Nathaniel Hawthorne: Based upon the original Manuscripts in the Piermont Morgan Library. New Haven: Yale University Press, 1932.
Waggoner, Hyatt H. Hawthorne: A Critical Study, 3rd edn. Cambridge, MA: Belknap Press of Harvard University Press, 1971.
Hawthorne in Salem Website Page on Hester and Pearl in The Scarlet Letter
The Scarlet Letter at Project Gutenberg
D. H. Lawrence - Studies in Classic American Literature - Nathaniel Hawthorne and The Scarlet Letter
LibriVox Recording of The Scarlet Letter
The Scarlet Letter Review
.
#(`*Crucible*`)#.
A crucible is a container that can withstand very high temperatures and is used for metal, glass, and pigment production as well as a number of modern laboratory processes. While crucibles historically were usually made from clay,[1] they can be made from any material that withstands temperatures high enough to melt or otherwise alter its contents.
The form of the crucibles has varied through time, with designs reflecting the process for which they are used, as well as regional variation.The crucible helps to prevent the heat from affecting the solution. The earliest crucible forms derive from the sixth/fifth millennium B.C. in Eastern Europe and Iran.[2]
Crucibles used for copper smelting were generally wide shallow vessels made from clay that lacks refractory properties which is similar to the types of clay used in other ceramics of the time.[3] During the Chalcolithic period, crucibles were heated from the top by using blow pipes.[4] Ceramic crucibles from this time had slight modifications to their designs such as handles, knobs or pouring spouts (Bayley & Rehren 2007: p47) allowing them to be more easily handled and poured. Early examples of this practice can be seen in Feinan, Jordan.[4] These crucibles have added handles to allow for better manipulation, however due to the poor preservation of the crucibles there is no evidence of a pouring spout. The main purpose of the crucible during this period was to keep the ore in the area where the heat was concentrated to separate it from impurities before shaping.[5]
The use of crucibles in the Iron Age remains very similar to that of the Bronze Age with copper and tin smelting was being used to produce bronze. The Iron Age crucible designs remain the same as the Bronze Age.
The Roman period shows technical innovations, with crucibles for new methods used to produce new alloys. The smelting and melting process also changed with both the heating technique and the crucible design. The crucible changed into rounded or pointed bottom vessels with a more conical shape; these were heated from below, unlike prehistoric types which were irregular in shape and were heated from above. These designs gave greater stability within the charcoal (Bayley & Rehren 2007: p49). These crucibles in some cases have thinner walls and have more refractory properties (Tylecote 1976: p20).
During the Roman period a new process of metalworking started, cementation, used in the production of brass. This process involves the combination of a metal and a gas to produce an alloy (Zwicker et al. 1985: p107). Brass is made by mixing solid copper metal in with zinc oxide or carbonate which comes in the form of calamine or smithsonite (Rehren 2003: p209). This is heated to about 900C and the zinc oxide vaporises into a gas and the zinc gas bonds with the solid copper (Rehren 1999: p1085). This reaction has to take place in a part-closed or closed container otherwise the zinc vapour would escape before it can react with the copper. Cementation crucibles therefore have a lid or cap which limits the amount of gas loss from the crucible. The crucible design is similar to the smelting and melting crucibles of the period utilizing the same material as the smelting and melting crucibles. The conical shape and small mouth allowed the lid to be added. These small crucibles are seen in Colonia Ulpia Trajana (modern day Xanten), Germany, where the crucibles are around 4cm in size, however these are small examples.[6] There are examples of larger vessels such as cooking pots and amphorae being used for cementation to process larger amounts of brass; since the reaction takes place at low temperatures lower fired ceramics could be used.[5] The ceramic vessels which are used are important as the vessel must be able to lose gas through the walls otherwise the pressure would break the vessel. Cementation vessels are mass produced due to crucibles having to be broken open to remove the brass once the reaction has finished as in most cases the lid would have baked hard to the vessel or the brass might have adhered to the vessel walls.
Smelting and melting of copper and its alloys such as leaded bronze were smelted in crucibles similar to those of the roman period which have thinner walls and flat bases to sit within the furnaces. The technology for this type of smelting started to change at the end of the Medieval period with the introduction of new tempering material for the ceramic crucibles. Some of these copper alloy crucibles were used in the making of bells. Bell foundry crucibles had to be larger at about 60cm (Tylecote 1976: p73). These later medieval crucibles were a more mass produced product.
The cementation process, which was lost from the end of the Roman to the early Medieval period, continued in the same way with brass. Brass production increased during the medieval period due to a better understanding of the technology behind it. Furthermore, the process for carrying out cementation for brass did not change greatly until the 19th century.[7] However, during this period a vast and highly important technological innovation happened using the cementation process, the production of steel. Steel production using iron and carbon works in the same way as brass with the iron metal being mixed with carbon to produce steel. The first examples of cementation steel is wootz steel from India (Craddock 1995: p276), where the crucibles were filled with the good quality wrought iron and carbon in the form of organics such as leaves, wood etc. However, no charcoal was used within the crucible. These early crucibles would only produce a small amount of steel as they would have to be broken once the process has finished.
By the late Medieval period steel production had move from India to modern day Uzbekistan where new materials were being used in the production of steel crucibles, for example Mullite crucibles were introduced.[8] These were sandy clay crucibles which had been formed around a fabric tube.[8] These crucibles are used in the same way as other cementation vessels but with a hole in the top of the vessel to allow pressure to escape.
At the end of the Medieval and into the Post Medieval new types of crucible designs and processes started. Smelting and melting crucibles types started to become more limited in designs which are produced by a few specialists. The main types used during the Post Medieval period are the Hessian Crucibles which were made in the Hesse region in Germany. These are triangular vessels made on a wheel or within a mould using high alumina clay and tempered with pure quartz sand.[9] Furthermore another specialised crucible which was made at the same time was that of a graphite crucible from southern Germany. These had a very similar design to that of the triangular crucibles from Hesse but they also occur in conical forms. These crucibles were traded all across Europe and the New World.
The refining of methods during the Medieval and Post Medieval periods led to the invention of the cupel which resembles a small egg cup, made of ceramic or bone ash which was used to separate base metals from noble metals. This process is known as cupellation. Cupellation started long before the Post Medieval period, however the first vessels made to carry out this process started in the 16th Century (Rehren 2003: p208). Another vessel used for the same process is a scorifier which is similar to a cupel but slightly larger and removes the lead and leaves the noble metals behind. Cupels and scorifiers were mass produced as after each reduction the vessels would have absorbed all of the lead and become fully saturated. These vessels were also used in the process of assaying where the noble metals are removed from a coin or a weight of metal to determine the amount of the noble metals within the object.
A crucible is a cup-shaped piece of laboratory equipment used to contain chemical compounds when heated to extremely high temperatures. Crucibles are available in several sizes and typically come with a correspondingly-sized crucible cover (or lid).
Crucibles and their covers are made of high temperature-resistant materials, usually porcelain or an inert metal. One of the earliest uses of platinum was to make crucibles. Ceramics such as alumina, zirconia, and especially magnesia will tolerate the highest temperatures. More recently, metals such as nickel and zirconium have been used. The lids are typically loose-fitting to allow gases to escape during heating of a sample inside. Crucibles and their lids can come in high form and low form shapes and in various sizes, but rather small 1015 ml size porcelain crucibles are commonly used for gravimetric chemical analysis. These small size crucibles and their covers made of porcelain are quite cheap when sold in quantity to laboratories, and the crucibles are sometimes disposed of after use in precise quantitative chemical analysis. There is usually a large mark-up when they are sold individually in hobby shops.
In the area of chemical analysis, crucibles are used in quantitative gravimetric chemical analysis (analysis by measuring mass of an analyte or its derivative). Common crucible use may be as follows. A residue or precipitate in a chemical analysis method can be collected or filtered from some sample or solution on special "ashless" filter paper. The crucible and lid to be used are pre-weighed very accurately on an analytical balance. After some possible washing and/or pre-drying of this filtrate, the residue on the filter paper can be placed in the crucible and fired (heated at very high temperature) until all the volatiles and moisture are driven out of the sample residue in the crucible. The "ashless" filter paper is completely burned up in this process. The crucible with the sample and lid is allowed to cool in a desiccator. The crucible and lid with the sample inside is weighed very accurately again only after it has completely cooled to room temperature (higher temperature would cause air currents around the balance giving inaccurate results). The mass of the empty, pre-weighed crucible and lid is subtracted from this result to yield the mass of the completely dried residue in the crucible.
A crucible with a bottom perforated with small holes which is designed specifically for use in filtration, especially for gravimetric analysis as just described, is called a Gooch crucible after its inventor, Frank Austen Gooch.
For completely accurate results, the crucible is handled with clean tongs because fingerprints can add weighable mass to the crucible. Porcelain crucibles are hygroscopic, i. e. they absorb a bit of weighable moisture from the air. For this reason, the porcelain crucible and lid is also pre-fired (pre-heating to high temperature) to constant mass before the pre-weighing. This determines the mass of the completely dry crucible and lid. At least two firings, coolings, and weighings resulting in exactly the same mass are needed to confirm constant (completely dry) mass of the crucible and lid and similarly again for the crucible, lid, and sample residue inside. Since the mass of every crucible and lid is different, the pre-firing/pre-weighing must be done for every new crucible/lid used. The desiccator contains desiccant to absorb moisture from the air inside, so the air inside will be completely dry.
Ash is the completely unburnable inorganic salts in a sample. A crucible can be similarly used to determine the percentage of ash contained in an otherwise burnable sample of material such as coal, wood, or oil. A crucible and its lid are pre-weighed at constant mass as described above. The sample is added to the completely dry crucible and lid and together they are weighed to determine the mass of the sample by difference.
1 Crucible history

1.1 Typology and chronology
1.2 Chalcolithic
1.3 Iron Age
1.4 Roman period
1.5 Medieval period
1.6 Post Medieval


1.1 Typology and chronology
1.2 Chalcolithic
1.3 Iron Age
1.4 Roman period
1.5 Medieval period
1.6 Post Medieval
2 Modern day crucibles uses

2.1 Laboratory crucibles
2.2 Crucible materials and description
2.3 Use in chemical analysis
2.4 Use in ash content determination


2.1 Laboratory crucibles
2.2 Crucible materials and description
2.3 Use in chemical analysis
2.4 Use in ash content determination
3 See also
4 References
1.1 Typology and chronology
1.2 Chalcolithic
1.3 Iron Age
1.4 Roman period
1.5 Medieval period
1.6 Post Medieval
2.1 Laboratory crucibles
2.2 Crucible materials and description
2.3 Use in chemical analysis
2.4 Use in ash content determination
Hessian crucible
Micro-pulling-down
Craddock P., 1995, Early Metal Mining and Production, Edinburgh University Press Ltd, Edinburgh
Hauptmann A., T. Rehren & Schmitt-Strecker S., 2003, Early Bronze Age copper metallurgy at Shahr-i Sokhta (Iran), reconsidered, T. Stollner, G. Korlin, G. Steffens & J. Cierny, Eds., Man and mining, studies in honour of Gerd Weisgerber on occasion of his 65th birthday, Deutsches Bergbau Museum, Bochum
Martinon-Torres M. & Rehren Th., 2009, Post Medieval crucible Production and Distribution: A Study of Materials and Materialities, Archaeometry Vol.51 No.1 pp4974
O. Faolain S., 2004, Bronze Artefact Production in Late Bronze Age Ireland: A Survey, British Archaeological Report, British Series 382, Archaeopress, Oxford
Rehren, Th. and Papakhristu, O., 2000, Cutting Edge Technology  The Ferghana Process of Medieval crucible steel Smelting, Metalla, Bochum, 7(2) pp5569
Rehren T. & Thornton C. P, 2009, A truly refractory crucible from fourth millennium Tepe Hissar, Northeast Iran, Journal of Archaeological Science, Vol. 36, pp27002712
Rehren Th., 1999, Small Size, Large Scale Roman brass Production in Germania Inferior, Journal of Archaeological Science, Vol. 26, pp 10831087
Rehren Th., 2003, Crucibles as Reaction Vessels in Ancient Metallurgy, Ed in P. Craddock & J. Lang, Mining and Metal Production Through the Ages, British Museum Press, London pp207215
Roberts B. W., Thornton C. P. & Pigott V. C., 2009, Development of Metallurgy in Eurasia, Antiquity Vol. 83 pp 10121022
Scheel B., 1989, Egyptian Metalworking and Tools, Shire Egyptology, Bucks
Vavelidis M. & Andreou S., 2003, Gold and Gold working in Later Bronze Age Northern Greece, Naturwissenschaften, Vol. 95, pp 361366
Zwicker U., Greiner H., Hofmann K. & Reithinger M., 1985, Smelting, Refining and Alloying of copper and copper Alloys in Crucible Furnaces During Prehistoric up to Roman Times, P. Craddock & M. Hughes, Furnaces and smelting Technology in Antiquity, British Museum, London
.
#(`*Huckleberry Finn*`)#.
Huckleberry "Huck" Finn is a fictional character created by Mark Twain, who first appeared in the book The Adventures of Tom Sawyer and is the protagonist and narrator of its sequel, Adventures of Huckleberry Finn. He is 12 or 13 years old during the former and a year older ("thirteen or fourteen or along there," Chapter 17) at the time of the latter. Huck also narrates Tom Sawyer Abroad and Tom Sawyer, Detective, two shorter sequels to the first two books.
Huck is the son of the town's vagrant drunkard, "Pap" Finn. Sleeping on door-steps when the weather is fair, in empty hogsheads during storms, and living off of what he receives from others, Huck lives the life of a destitute vagabond. The author metaphorically names him "the juvenile pariah of the village." The author describes Huck as "idle, and lawless, and vulgar, and bad," qualities for which he was admired by all the children in the village, although their mothers "cordially hated and dreaded" him.
Huck is an archetypal innocent, able to discover the "right" thing to do despite the prevailing theology and prejudiced mentality of the South of that era. The best example of this is his decision to help Jim escape slavery, even though he believes he will go to hell for it (see Christian views on slavery).
His appearance is described in The Adventures of Tom Sawyer. He wears the clothes of full-grown men which he probably received as charity, and as Twain describes him, "he was fluttering with rags." He has a torn broken hat and his trousers are supported with only one suspender.
Tom's Aunt Polly calls Huck a "poor motherless thing." Huck confesses to Tom in The Adventures of Tom Sawyer that he remembers his mother and his parents' relentless fighting that only abated with her death.
Huck has a carefree life free from societal norms or rules, stealing watermelons and chickens and "borrowing" boats and cigars. Due to his unconventional childhood, Huck has received almost no education. At the end of The Adventures of Tom Sawyer, Huck is adopted by the Widow Douglas, who sends him to school in return for saving her life.
In Adventures of Huckleberry Finn, the sequel to The Adventures of Tom Sawyer, the Widow attempts to "sivilize" the newly wealthy Huck. Huck's father takes him from her, but Huck manages to fake his own death and escape to Jackson's Island, where he coincidentally meets up with Jim, a slave who was owned by the Widow Douglas's sister, Miss Watson.
Jim is running away because he overheard Miss Watson planning to "sell him South" for eight hundred dollars. Jim wants to escape to Ohio, where he can find work to eventually buy his family's freedom. The two take a raft down the Mississippi River in the hopes of finding freedom from slavery for Jim and freedom from Pap for Huck. Their adventures together, along with Huck's solo adventures, comprise the core of the book.
In the end, however, Jim gains his freedom through Miss Watson's death, as she freed him in her will. Pap, it is revealed, has died in Huck's absence, and although he could safely return to St. Petersburg, Huck plans to flee west to Indian Territory.
In Tom Sawyer Abroad and Tom Sawyer, Detective, the published sequels to Huck Finn, however, Huck is living in St. Petersburg again after the events of his eponymous novel. In Abroad, Huck joins Tom and Jim for a wild, fanciful balloon ride that takes them overseas. In Detective, which occurs about a year after the events of Huck Finn, Huck helps Tom solve a murder mystery.
Huck is Tom Sawyer's closest friend. Their friendship is partially rooted in Sawyer's emulation of Huck's freedom and ability to do what he wants, like swearing and smoking when he feels like it. In one moment in the novel, he openly brags to his teacher that he was late for school because he stopped to talk with Huck Finn, something for which he knew he would (and did) receive a whipping. Nonetheless, Tom remains a devoted friend to Huck in all of the novels they appear in.
Jim, a runaway slave that Huck befriends, is another dominant force in Huck's life. He is the symbol for the moral awakening Huck undergoes throughout Adventures of Huckleberry Finn.
Pap Finn is Huck's abusive, drunken father who shows up at the beginning of Adventures of Huckleberry Finn and forcibly takes his son to live with him. When present, Pap's only method of parenting is physical abuse. Although he seems derisive of education and civilized living, Pap seems to be jealous of Huck, and is infuriated that his son would try to amount to more than he did, and live in better conditions than he did.
The character of Huck Finn was based on Tom Blankenship, the real-life son of a sawmill laborer and sometime drunkard named Woodson Blankenship, who lived in a "ramshackle" house near the Mississippi River behind the house where the author grew up in Hannibal, Missouri.[1] The father of Huck, called "Pap" Finn is drawn possibly more from Jimmy Finn, a full-blown alcoholic who lived on the streets, and it is only through Twain's remembrances that Woodson is characterized as a drunkard. Twain left Hannibal and his boyhood at an early age and his memories of these people are colored by what he could have known and understood at the time, as a boy of less than 14 years old. Tom didn't attend school because there were no public schools at the time, and his family was too poor to send him to a private school. Left at loose ends in a busy household with six sisters and a mother who seems to have died when he was young, Tom was indeed "at liberty" most of the time.
The childhood friend Tom Blankenship as the inspiration for creating Huckleberry Finn was mentioned by Twain in his Autobiography: "In Huckleberry Finn I have drawn Tom Blankenship exactly as he was. He was ignorant, unwashed, insufficiently fed; but he had as good a heart as ever any boy had. His liberties were totally unrestricted. He was the only really independent personboy or manin the community, and by consequence he was tranquilly and continuously happy and envied by the rest of us. And as his society was forbidden us by our parents the prohibition trebled and quadrupled its value, and therefore we sought and got more of his society than any other boy's."  Mark Twain's Autobiography.
Huck's adventure is in part based on an incident that happened to Tom Blankenship's eldest sibling, his brother Benson, a teenage fisherman who had his own skiff. He aided a runaway slave in defiance of the law, spurning a probable reward to instead bring food and other items to the slave, who was hiding in the wilderness near the river over the course of a summer. Eventually, bounty hunters chased the slave onto a logjam where he drowned. Days later, Clemens and other local boys were exploring the logjam when the body was dislodged and sprang up from beneath the water violently, frightening the boys. Some critics have also connected certain traits of Benson Blankenship to the character Muff Potter, from Twain's novel Tom Sawyer, such as his owning a skiff and occasionally sharing his catch with the boys of the town, and mending their kites.
Tom Blankenship has passed from history with few solid clues as to his ultimate fate. His sister told Twain near the turn of the century that both of her brothers were dead, and local rumor says that Tom perished in a cholera epidemic, a number of which swept up the Mississippi river in the years after the Civil War. Twain himself told reporters that he heard that Tom moved to Montana and was a well-respected Justice of the Peace, but this is thought to be wishful thinking by some historians. Another Hannibal Mo., old timer related that Tom "left Hannibal for the penitentiary." Mention is made in the local newspapers that he was arrested for stealing food repeatedly in the early 1860s.
No death certificate has ever been located for Tom, but census records indicate that Benson moved to Texas and started a family alongside his uncles and cousins by the 1860s. This suggests that there may have been a period when Tom and his father Woodson were more or less alone in Hannibal, as the daughters all married or entered into service with local families. No record of Tom serving in any military during the civil war has emerged as of this date, either. A local was quoted as saying the family "played out" and disappeared from the area by the time the war was over. The ultimate truth seems to have passed into the unknowable realm, leaving us only with Twain's fiction.
Actors who have portrayed Huckleberry Finn in movies and TV:
1 Biography
2 Relationships
3 Inspiration
4 Appearances
5 Portrayals
6 See also
7 References
The Adventures of Tom Sawyer (1876)
Adventures of Huckleberry Finn (1884)
Tom Sawyer Abroad (1894)
Tom Sawyer, Detective (1896)
Huck Finn and Tom Sawyer Among the Indians  unfinished
Tom Sawyer's Conspiracy  unfinished
Lewis Sargent (1920)
Junior Durkin (1930 and 1931)
Jackie Moran (1938)
Donald O'Connor (1938)
Mickey Rooney (1939)
Eddie Hodges (1960)
Roman Madyanov (1973 in Hopelessly Lost)
Jeff East (1973 and 1974)
Steve Stark (1979)
Ron Howard
Ian Tracey
Patrick Day (1986)
Elijah Wood (1993)
Brad Renfro (1995)
Mark Wills (voice)
Mark Twain
Tom Sawyer
.
#(`*The Grapes of Wrath*`)#.
The Grapes of Wrath is an American realist novel written by John Steinbeck and published in 1939. For it he won the annual National Book Award[1] and Pulitzer Prize[2] for novels and it was cited prominently when he won the Nobel Prize in 1962.[3]
Set during the Great Depression, the novel focuses on the Joads, a poor family of tenant farmers driven from their Oklahoma home by drought, economic hardship, and changes in financial and agricultural industries. Due to their nearly hopeless situation, and in part because they were trapped in the Dust Bowl, the Joads set out for California. Along with thousands of other "Okies", they sought jobs, land, dignity, and a future.
The Grapes of Wrath is frequently read in American high school and college literature classes due to its historical context and enduring legacy.[4][5][6] A celebrated Hollywood film version, starring Henry Fonda and directed by John Ford, was made in 1940.
The narrative begins just after Tom Joad is paroled from McAlester prison for homicide. On his journey to his home near Sallisaw, Oklahoma, he meets former preacher Jim Casy whom he remembers from his childhood, and the two travel together. When they arrive at his childhood farm home, they find it deserted. Disconcerted and confused, he and Casy meet their old neighbor, Muley Graves, who tells them that the family has gone to stay at Uncle John Joad's home nearby. He goes on to tell them that the banks have evicted all the farmers off their land, but he refuses to leave the area.
Tom and Casy get up the next morning to go to Uncle John's. There, Tom finds his family loading a converted Hudson truck with what remains of their possessions; the crops were destroyed in the Dust Bowl and, as a result, the family had to default on their loans. With their farm repossessed, the Joads cling to hope, mostly in the form of handbills distributed everywhere in Sallisaw, Oklahoma, describing the fruitful state of California and the high pay to be had in that state. The Joads are seduced by this advertising and invest everything they have into the journey. Although leaving Oklahoma would be breaking parole, Tom decides that it is a risk worth taking. Casy is invited to join the family as well.
Going west on Route 66, the Joad family discovers that the road is saturated with other families making the same trek, ensnared by the same promise. In makeshift camps, they hear many stories from others, some coming back from California, and are forced to confront the possibility that their prospects may not be what they hoped. Along the road, Grampa dies and is buried in a field; Granma dies close to the California state line, both Noah (the eldest Joad son) and Connie (the husband of the pregnant Joad daughter, Rose of Sharon) split from the family; the remaining members, led by Ma, realize they have no choice but to go on, as there is nothing remaining for them in Oklahoma.
Upon arrival, they find little hope of making a decent wage, as there is an oversupply of labor and a lack of rights, and the big corporate farmers are in collusion, while smaller farmers are suffering from collapsing prices. A gleam of hope is presented at Weedpatch Camp, one of the clean, utility-supplied camps operated by the Resettlement Administration, a New Deal agency that has been established to help the migrants, but there is not enough money and space to care for all of the needy. As a Federal facility, the camp is also off-limits to California deputies who constantly harass and provoke the newcomers.
 Chapter 19
In response to the exploitation of laborers, there are people who attempt for the workers to join unions, including Casy, who had gone to jail after taking the blame for attacking a rogue deputy. The remaining Joads work as strikebreakers on a peach orchard where Casy is involved in a strike that eventually turns violent. Tom Joad witnesses the killing of Casy and kills the attacker, becoming a fugitive. They later leave the orchard for a cotton farm where Tom is at risk of being identified for the murder he committed.
He bids farewell to his mother, promising that no matter where he runs, he will be a tireless advocate for the oppressed. Rose of Sharon's baby is stillborn; however, Ma Joad remains steadfast and forces the family through the bereavement. When the rains arrive, the Joads' dwelling is flooded, and they move to higher ground.
The novel developed from "The Harvest Gypsies", a series of seven articles that ran in the San Francisco News, from October 5 to October 12, 1936. The newspaper commissioned that work on migrant workers from the Midwest in California's agriculture industry. (It was later compiled and published separately.[7])[8]
While writing the novel at his home, 16250 Greenwood Lane, in what is now Monte Sereno, California, Steinbeck had unusual difficulty devising a title. "The Grapes of Wrath", suggested by his wife, Carol Steinbeck,[9] was deemed more suitable than anything the author could come up with. The title is a reference to lyrics from "The Battle Hymn of the Republic", by Julia Ward Howe:
Mine eyes have seen the glory of the coming of the Lord:
He is trampling out the vintage where the grapes of wrath are stored;
He hath loosed the fateful lightning of His terrible swift sword:
His truth is marching on.
These lyrics refer, in turn, to the biblical passage Revelation 14:1920, an apocalyptic appeal to divine justice and deliverance from oppression in the final judgment.
And the angel thrust in his sickle into the earth, and gathered the vine of the earth, and cast it into the great winepress of the wrath of God. And the winepress was trodden without the city, and blood came out of the winepress, even unto the horse bridles, by the space of a thousand and six hundred furlongs.
The phrase also appears at the end of chapter 25 in The Grapes of Wrath which describes the purposeful destruction of food to keep the price high:
...and in the eyes of the hungry there is a growing wrath. In the souls of the people the grapes of wrath are filling and growing heavy, growing heavy for the vintage.
As might be expected, the image invoked by the title serves as a crucial symbol in the development of both the plot and the novel's greater thematic concerns: from the terrible winepress of Dust Bowl oppression will come terrible wrath but also the deliverance of workers through their cooperation, which is hinted at but does not materialize within the novel.
When preparing to write the novel, Steinbeck wrote: "I want to put a tag of shame on the greedy bastards who are responsible for this [the Great Depression and its effects]." He famously said, "I've done my damndest to rip a reader's nerves to rags," and this work won a large following among the working class due to Steinbeck's sympathy to the workers' movement and his accessible prose style.[10]
Steinbeck scholar John Timmerman sums up the book's impact: "The Grapes of Wrath may well be the most thoroughly discussed novel  in criticism, reviews, and college classrooms  of 20th century American literature."[8]
At the time of publication, Steinbeck's novel "was a phenomenon on the scale of a national event. It was publicly banned and burned by citizens, it was debated on national talk radio; but above all, it was read."[11] According to The New York Times it was the best-selling book of 1939 and 430,000 copies had been printed by February 1940.[1] In that month it won the National Book Award, favorite fiction book of 1939, voted by members of the American Booksellers Association.[1] Soon it won the Pulitzer Prize for Fiction.[2]
Part of its impact stemmed from its passionate depiction of the plight of the poor, and in fact, many of Steinbeck's contemporaries attacked his social and political views. Bryan Cordyack writes, "Steinbeck was attacked as a propagandist and a socialist from both the left and the right of the political spectrum. The most fervent of these attacks came from the Associated Farmers of California; they were displeased with the book's depiction of California farmers' attitudes and conduct toward the migrants. They denounced the book as a 'pack of lies' and labeled it 'communist propaganda'.[8] Some accused Steinbeck of exaggerating camp conditions to make a political point. Steinbeck had visited the camps well before publication of the novel [12] and argued their inhumane nature destroyed the settlers' spirit.
In 1962, the Nobel Prize committee cited Grapes of Wrath as a "great work" and as one of the committee's main reasons for granting Steinbeck the Nobel Prize for Literature.[3] Time magazine included the novel in its "TIME 100 Best English-language Novels from 1923 to 2005".[13] In 2009, The Daily Telegraph also included the novel in its "100 novels everyone should read".[14] In 1998, the Modern Library ranked The Grapes of Wrath tenth on its list of the 100 best English-language novels of the 20th century.
The book was quickly made into a famed Hollywood movie of the same name directed by John Ford and starring Henry Fonda as Tom Joad. The first part of the film version follows the book fairly accurately. However, the second half and the ending in particular are significantly different from the book.
The Steppenwolf Theatre Company produced a stage version of the book, adapted by Frank Galati. Gary Sinise played Tom Joad for its entire run of 188 performances on Broadway in 1990.[15] One of these performances was shown on PBS the following year.
It was revealed in the 2009 documentary American: The Bill Hicks Story that The Grapes of Wrath was the favorite novel of the comedian Bill Hicks, who was such a fan that he based his famous last words on Tom Joad's final speech: "I left in love, in laughter, and in truth, and wherever truth, love and laughter abide, I am there in spirit."
An opera based on the novel was co-produced by the Minnesota Opera and Utah Symphony and Opera, with music by Ricky Ian Gordon and libretto by Michael Korie. The world premiere performance of the opera was given in February 2007, to favorable local reviews.[16]
American rock singer-songwriter Bruce Springsteen named his eleventh studio album The Ghost of Tom Joad, after the character. The first track on the album is also called The Ghost of Tom Joad. The song  and to a lesser extent, the other songs on the album  draws comparisons between the Dust Bowl and modern times.[17] The song was later covered by American metal band Rage Against the Machine.
Woody Guthrie's song The Ballad Of Tom Joad focuses mainly on the main character's life since he was paroled from "the old McAlester Pen" and follows the book quite closely.
On April 16, 2008, the television series South Park aired an episode entitled "Over Logging"." Written and directed by Trey Parker, this episode parodies Steinbeck's "The Grapes of Wrath", as the Marsh family heads to California to find more internet.
1 Plot
2 Characters
3 Development

3.1 Title


3.1 Title
4 Author's note
5 Critical reception
6 Adaptations
7 See also
8 References

8.1 Notes
8.2 Bibliography


8.1 Notes
8.2 Bibliography
9 External links
3.1 Title
8.1 Notes
8.2 Bibliography
Tom Joad  Protagonist of the story; the Joad family's second son, named after his father. Later on, Tom takes leadership of the family even though he is young.
Ma Joad  matriarch. Practical and warm-spirited, she tries to hold the family together. Her given name is never learned; it is suggested that her maiden name was Hazlett.
Pa Joad  patriarch, also named Tom, age 50. Hardworking sharecropper and family man. Pa loses his place as leader of the family to his wife.
Uncle John Joad  Older brother of Pa Joad (Tom describes him as "a fella about 60", but the narrator later tells you he is 50), feels responsible for the death of his young wife years before when he ignored her pleas for a doctor because he thought she just had a stomachache, when she actually had a burst appendix. Filled with guilt, he is prone to binges involving alcohol and prostitutes, yet tries to repent for his sins and guilt by spoiling Ruthie and Winfield with candy when he can.
Jim Casy  A former preacher who lost his faith after fornicating with willing members of his church numerous times, and from his perception that religion has no solace or answer for the difficulties the people are experiencing. He is a Christ-like figure and is based on Ed Ricketts.
Al Joad  The second youngest son, a "smart-aleck sixteen-year-older" who cares mainly for cars and girls; looks up to Tom, but begins to find his own way.
Rose of Sharon Joad Rivers  Childish and dreamy teenage daughter (18) who develops as the novel progresses to become a mature woman. She symbolizes regrowth when she helps the starving stranger (see also Roman Charity, works of art based on the legend of a daughter as wet nurse to her dying father). Pregnant in the beginning of the novel, she delivers a stillborn baby, probably as a result of malnutrition. Her name is pronounced "Rosasharn" by the family.
Connie Rivers  Rose of Sharon's husband. Young and nave, he is overwhelmed by the responsibilities of marriage and impending fatherhood, and abandons her shortly after arriving in California. He is stated to be 19 years old upon his and Tom's first encounter before leaving for California.
Noah Joad  The oldest son who is the first to willingly leave the family, choosing to stay by the Colorado river and survive by fishing. Injured at birth, described as "strange", he may have slight learning difficulties.
Grampa Joad  Tom's grandfather, who expresses his strong desire to stay in Oklahoma. His full name is given as William James Joad. Grampa is drugged by his family with "soothin' syrup" to force him to leave, but dies in the evening of the first day on the road; Casy attributes his death to a stroke, but also says that Grampa is "jus' stayin' with the lan'. He couldn' leave it."
Granma Joad  The religious wife of Grampa Joad, she seems to lose the will to live (and consequently dies while crossing the desert, possibly as a result of exposure to the heat while crossing New Mexico and Arizona) after her husband's death.
Ruthie Joad  The youngest daughter, age twelve.
Winfield Joad  The youngest male in the family, age ten. He and Ruthie are close.
Jim Rawley  Manages the camp at Weedpatch, he shows the Joads surprising favor.
Muley Graves  A neighbor of the Joads, he is invited to come along to California with them but refuses. Two of the family dogs are left in his care, while the third goes along with the family and is killed by a car on the road when they stop for gas.
Ivy and Sairy Wilson  Kansas folks in a similar predicament, who help attend the death of Grampa and subsequently share the traveling with the Joads as far as the California state line. It is implied Sairy is too ill to carry on.
Mr. Wainwright  The father of Aggie Wainwright and husband of Mrs. Wainwright. Worries over his daughter who is sixteen and in his words "growed up".
Mrs. Wainwright  Mother to Aggie Wainwright and wife to Mr. Wainwright. She helps deliver Rose of Sharon's stillborn baby with Ma.
Aggie Wainwright  Sixteen years of age. Daughter to Mr. and Mrs. Wainwright. Intends to marry Al. Aggie takes care of Ruthie and Winfield when Rose of Sharon goes into labor. She has limited interactions with the other characters. Her real name is Agnes.
Floyd Knowles  the man at the Hooverville who urges Tom and Casy to join labor organizations. He agitates the police and this results in Casy going to jail.
Joe/Mike  A deputy who is hit by Floyd, tripped by Tom and knocked out by Casy at the Hooverville. He claims that Casy did not hit him (as he did not see it) but Casy convinces him he did. He is called Joe by the man who he arrives at the Hooverville with to ask for peach pickers, but strangely the other deputies refer to him as Mike.
George  A guard at the peach orchard. He kills Casy, and is then attacked by Tom.
Al  A cook at a restaurant who orders Mae to give bread to a poor migrant family.
Mae  A waitress at a restaurant who is ordered to give bread to a migrant family. She later gives them two pieces of candy for one cent, when it is later revealed that the candy was a nickel apiece. She is then given an extra-large tip by truckers who ate lunch there, perhaps to compensate for the loss of income for under-pricing the candies.
Big Bill  A trucker who eats lunch at the restaurant where Al and Mae work. He and his friend, another trucker, leave an extra-large tip and then leave.
Le Monde's 100 Books of the Century
Gregory, James N. "Dust Bowl Legacies: the Okie Impact on California, 19391989". California History 1989 68(3): 7485. Issn: 0162-2897
Saxton, Alexander. "In Dubious Battle: Looking Backward". Pacific Historical Review 2004 73(2): 249262. Issn: 0030-8684 Fulltext: online at Swetswise, Ingenta, Ebsco
Sobchack, Vivian C. "The Grapes of Wrath (1940): Thematic Emphasis Through Visual Style". American Quarterly 1979 31(5): 596615. Issn: 0003-0678 Fulltext: in Jstor. Discusses the visual style of John Ford's cinematic adaptation of the novel. Usually the movie is examined in terms of its literary roots or its social protest. But the imagery of the film reveals the important theme of the Joad family's coherence. The movie shows the family in closeups, cramped in small spaces on a cluttered screen, isolated from the land and their surroundings. Dim lighting helps abstract the Joad family from the reality of Dust Bowl migrants. The film's emotional and aesthetic power comes from its generalized quality attained through this visual style.
Windschuttle, Keith. "Steinbeck's Myth of the Okies". The New Criterion, Vol. 20, No. 10, June 2002.
Zirakzadeh, Cyrus Ernesto. "John Steinbeck on the Political Capacities of Everyday Folk: Moms, Reds, and Ma Joad's Revolt". Polity 2004 36(4): 595618. Issn: 0032-3497
John Steinbeck in the Santa Cruz Mountains  A history of Steinbeck's life living in the Santa Cruz Mountain's while writing the Grapes of Wrath
2 short radio episodes "Spring in California" and "Route 66" from The Grapes of Wrath, California Legacy Project.
"The Grapes of Wrath revisited," (videos) The Guardian [Chris McGreal journeys along Route 66  following in the footsteps of the Joads, the central characters in John Steinbeck's the Grapes of Wrath who fled the Oklahoma dustbowl for California  to see whether the tragedy and despair witnessed in the Great Depression is a long-forgotten nightmare or a present-day reality still haunting Barack Obama's America].
Encyclopedia of Oklahoma History and Culture  Grapes of Wrath
National Public Radio: Grapes of Wrath, Present at the Creation
Oklahoma Digital Maps: Digital Collections of Oklahoma and Indian Territory
The Grapes of Wrath on Open Library at the Internet Archive
.
#(`*A Separate Peace*`)#.
A Separate Peace (1959) is a novel by John Knowles. Based on his earlier short story "Phineas", it was Knowles' first published novel and became his best-known work.
Gene Forrester, the protagonist, returns to his old prep school, Devon (a thinly-veiled portrayal of Knowles' own alma mater, Phillips Exeter Academy), fifteen years after he graduated to visit two places he regards as "fearful sites": a flight of marble stairs and a tree by the river. First, he examines the stairs and notices that they are made of very hard marble. He then trudges through the mud to the tree. The tree brings back memories of Gene's time as a student at Devon. From this point, the plot follows Gene's description of the time span from the summer of 1942 to the summer of 1943. In 1942, he was 16 years old and living at Devon with his best friend and roommate, Phineas (nicknamed Finny). At the time, World War II is taking place, and has a prominent effect on the story.
Gene and Finny, despite being polar opposites in personality, become fast friends at Devon: Gene's quiet, introverted intellectual personality complements Finny's more extroverted, carefree, athletic demeanor. During the time at Devon, Gene goes through a period of intense friendship with Finny. One of Finny's ideas during Gene's "Sarcastic Summer of 1942" is to create a "Super Suicide Society of the Summer Session," with Gene and himself as charter members. Finny creates a rite of initiation by having members jump into the Devon River from a large, high tree. He also creates a game called "blitzball" (from the German blitzkrieg) in which there is no winner and Finny would make rules up as they played.
Following their period of intense friendship was a period of intense one-sided rivalry during which Gene strives to out-do Finny academically, since he believes Finny is trying to out-do him. This rivalry begins with Gene's jealousy towards Finny because Finny gets away with everything and can talk his way out of getting in trouble. This rivalry culminates (and is ended) when, as Finny and Gene are about to jump off the tree, Gene (possibly) purposely jounces the branch they were both standing on, causing Finny to fall and shatter his leg. Because of his "accident", Finny learns from the doctor that he will never again be able to compete in sports that are most dear to him. This leads to Gene also starting to think like Finny to try and be a better person and to try and solve some of his envy towards him. The remainder of the story revolves around Gene's attempts to come to grips with who he is, why he shook the branch, and how he will continue to go forward. Gene feels so guilty that he goes to Finny's house and tells Finny that he caused Finny's fall. At first Finny does not believe him and afterward feels extremely hurt.
During a meeting of the Golden Fleece Debating Society, a debate/trial organization that Brinker Hadley (another student) set up, Gene is confronted about the "accident" by Brinker, who accuses Gene of trying to kill Finny. Faced with the evidence, Finny leaves shamefully before Gene's deed is confirmed. On the way out, Finny falls down a flight of stairs (the ones Gene visits at the beginning of the novel), and again breaks the leg he had shattered before. Finny dismisses any of Gene's attempts to apologize at first, but he soon realizes that the "accident" was impulsive and not anger-based. The two forgive each other.
The next day, Finny dies during the operation to set the bone. The doctor summarizes that Finny died when bone marrow entered the blood stream, and stopped his heart during the surgery. Gene does not cry over Finny, but learns much from how he lived his life, stating that when Finny died, he took his (Gene's) anger with him. In Finny's death, Gene could finally come to terms with himself.
In 1972, the novel was adapted into a film of the same name, starring Parker Stevenson as Gene and John Heyl as Finny, with a screenplay by Fred Segal and John Knowles.[1] In 2004, it was adapted into a made-for-TV movie by Showtime.[2]
1 Plot summary
2 Characters
3 Adaptations
4 References
5 External links
Gene Forrester: A Separate Peace is told from Gene's point of view. Gene focuses on, and succeeds at, academics. He envies his roommate Finny's graceful, easy athleticism and social prowess. Gene is from "three states from Texas," and is therefore somewhat unaccustomed to Northeastern culture. Gene causes his best friend's fall in his suppressed envy, by making a small but deliberate quick move on a tree branch from which Phineas would not otherwise have fallen.
Phineas 'Finny': Incorrigible, good natured, athletic, daredevil type that in Gene's opinion can never leave anything well enough alone, and could always get away with anything. Gene's best friend and roommate. He always sees the best in others, seeks internal fulfillment free of accolades, and shapes the world around himself to fit his desires. He is a prodigious athlete, succeeding in every sport until his leg is shattered in his fall from the tree.
Brinker Hadley: Brinker is a classmate and friend of Gene and Finny's. He ceaselessly strives for order during the Winter Session at Devon. The main antagonist, Brinker wants to get to the bottom of Finny's accident, but it is unclear if he intended for the investigation to be a practical joke. He organizes the "midnight trial" to confront and accuse Gene of causing Finny's accident. During the questioning of Finny by Brinker, Finny changes the story to make Gene appear innocent of his actions in the tree. Finny cites Lepellier as an unreachable witness. Brinker ultimately reconciles with Gene, who appears to forgive him both for his part in Finny's death and for the trial.
Elwin 'Leper' Lepellier: Leper is Finny and Gene's friend, and was a key member of the Super Suicide Society of the Summer Session. He is the first student in his class to enlist in the military. Late in the novel, Leper goes insane from the stress of his enlistment in the army. He is a witness at Gene's "trial," testifying that Gene was responsible for Finny's fall.
.
#(`*Romeo and Juliet*`)#.
Romeo and Juliet is a tragedy written early in the career of William Shakespeare about two young star-crossed lovers whose deaths ultimately reconcile their feuding families. It was among Shakespeare's most popular plays during his lifetime and, along with Hamlet, is one of his most frequently performed plays. Today, the title characters are regarded as archetypal young lovers.
Romeo and Juliet belongs to a tradition of tragic romances stretching back to antiquity. Its plot is based on an Italian tale, translated into verse as The Tragical History of Romeus and Juliet by Arthur Brooke in 1562 and retold in prose in Palace of Pleasure by William Painter in 1567. Shakespeare borrowed heavily from both but, to expand the plot, developed supporting characters, particularly Mercutio and Paris. Believed to have been written between 1591 and 1595, the play was first published in a quarto version in 1597. This text was of poor quality, and later editions corrected it, bringing it more in line with Shakespeare's original.
Shakespeare's use of dramatic structure, especially effects such as switching between comedy and tragedy to heighten tension, his expansion of minor characters, and his use of sub-plots to embellish the story, has been praised as an early sign of his dramatic skill. The play ascribes different poetic forms to different characters, sometimes changing the form as the character develops. Romeo, for example, grows more adept at the sonnet over the course of the play.
Romeo and Juliet has been adapted numerous times for stage, film, musical and opera. During the English Restoration, it was revived and heavily revised by William Davenant. David Garrick's 18th-century version also modified several scenes, removing material then considered indecent, and Georg Benda's operatic adaptation omitted much of the action and added a happy ending. Performances in the 19th century, including Charlotte Cushman's, restored the original text, and focused on greater realism. John Gielgud's 1935 version kept very close to Shakespeare's text, and used Elizabethan costumes and staging to enhance the drama. In the 20th century the play has been adapted in versions as diverse as George Cukor's comparatively faithful 1936 production, Franco Zeffirelli's 1968 version, and Baz Luhrmann's 1996 MTV-inspired Romeo + Juliet.

The play, set in Verona, begins with a street brawl between Montague and Capulet supporters who are sworn enemies. The Prince of Verona intervenes and declares that further breach of the peace will be punishable by death. Later, Count Paris talks to Capulet about marrying his daughter, but Capulet asks Paris to wait another two years (then he later orders Juliet to marry Paris) and invites him to attend a planned Capulet ball. Lady Capulet and Juliet's nurse try to persuade Juliet to accept Paris's courtship.
Meanwhile, Benvolio talks with his cousin Romeo, Montague's son, about Romeo's recent depression. Benvolio discovers that it stems from unrequited infatuation for a girl named Rosaline, one of Capulet's nieces. Persuaded by Benvolio and Mercutio, Romeo attends the ball at the Capulet house in hopes of meeting Rosaline. However, Romeo instead meets and falls in love with Juliet. After the ball, in what is now called the "balcony scene", Romeo sneaks into the Capulet orchard and overhears Juliet at her window vowing her love to him in spite of her family's hatred of the Montagues. Romeo makes himself known to her and they agree to be married. With the help of Friar Laurence, who hopes to reconcile the two families through their children's union, they are secretly married the next day.
Juliet's cousin Tybalt, incensed that Romeo had sneaked into the Capulet ball, challenges him to a duel. Romeo, now considering Tybalt his kinsman, refuses to fight. Mercutio is offended by Tybalt's insolence, as well as Romeo's "vile submission,"[1] and accepts the duel on Romeo's behalf. Mercutio is fatally wounded when Romeo attempts to break up the fight. Grief-stricken and wracked with guilt, Romeo confronts and slays Tybalt.
Montague argues that Romeo has justly executed Tybalt for the murder of Mercutio. The Prince, now having lost a kinsman in the warring families' feud, exiles Romeo from Verona, with threat of execution upon return. Romeo secretly spends the night in Juliet's chamber, where they consummate their marriage. Capulet, misinterpreting Juliet's grief, agrees to marry her to Count Paris and threatens to disown her when she refuses to become Paris's "joyful bride."[2] When she then pleads for the marriage to be delayed, her mother rejects her.
Juliet visits Friar Laurence for help, and he offers her a drug that will put her into a deathlike coma for "two and forty hours."[3] The Friar promises to send a messenger to inform Romeo of the plan, so that he can rejoin her when she awakens. On the night before the wedding, she takes the drug and, when discovered apparently dead, she is laid in the family crypt.
The messenger, however, does not reach Romeo and, instead, Romeo learns of Juliet's apparent death from his servant Balthasar. Heartbroken, Romeo buys poison from an apothecary and goes to the Capulet crypt. He encounters Paris who has come to mourn Juliet privately. Believing Romeo to be a vandal, Paris confronts him and, in the ensuing battle, Romeo kills Paris. Still believing Juliet to be dead, he drinks the poison. Juliet then awakens and, finding Romeo dead, stabs herself with his dagger. The feuding families and the Prince meet at the tomb to find all three dead. Friar Laurence recounts the story of the two "star-cross'd lovers". The families are reconciled by their children's deaths and agree to end their violent feud. The play ends with the Prince's elegy for the lovers: "For never was a story of more woe / Than this of Juliet and her Romeo."[4]
Romeo and Juliet borrows from a tradition of tragic love stories dating back to antiquity. One of these is Pyramus and Thisbe, from Ovid's Metamorphoses, which contains parallels to Shakespeare's story: the lovers' parents despise each other, and Pyramus falsely believes his lover Thisbe is dead.[5] The Ephesiaca of Xenophon of Ephesus, written in the 3rd century, also contains several similarities to the play, including the separation of the lovers, and a potion that induces a deathlike sleep.[6]
One of the earliest references to the names Montague and Capulet is from Dante's Divine Comedy, who mentions the Montecchi (Montagues) and the Cappelletti (Capulets) in canto six of Purgatorio:[7]
Come and see, you who are negligent,
Montagues and Capulets, Monaldi and Filippeschi
One lot already grieving, the other in fear.
However, the reference is part of a polemic against the moral decay of Florence, Lombardy and the Italian Peninsula as a whole; Dante, through his characters, chastises Albert of Hapsburg for neglecting his responsibilities as temporal ruler of Christendom in the continent ("you who are negligent"), and successive Popes for their encroachment from purely spiritual affairs, thus leading to a climate of incessant bickering and warfare between rival political parties in Lombardy. Historicity records the name of the family Montague as being lent to such a political party in Verona, but that of the Capulets as from a Cremonese family, both of whom play out their conflict in Lombardy as a whole rather than within the confines of Verona.[9] Allied to rival political factions, the parties are grieving ("One lot already grieving") because their endless warfare has led to the destruction of both parties,[9] rather than a grief from the loss of their ill-fated offspring as the play sets forth, which appears to be a solely poetic creation within this context.
The earliest known version of the Romeo and Juliet tale akin to Shakespeare's play is the story of Mariotto and Gianozza by Masuccio Salernitano, in the 33rd novel of his Il Novellino published in 1476.[10] Salernitano sets the story in Siena and insists its events took place in his own lifetime. His version of the story includes the secret marriage, the colluding friar, the fray where a prominent citizen is killed, Mariotto's exile, Gianozza's forced marriage, the potion plot, and the crucial message that goes astray. In this version, Mariotto is caught and beheaded and Gianozza dies of grief.[11]
Luigi da Porto adapted the story as Giulietta e Romeo and included it in his Historia novellamente ritrovata di due Nobili Amanti published in 1530.[12] Da Porto drew on Pyramus and Thisbe and Boccacio's Decameron. He gave it much of its modern form, including the names of the lovers, the rival families of Montecchi and Capuleti, and the location in Verona.[10] He also introduces characters corresponding to Shakespeare's Mercutio, Tybalt, and Paris. Da Porto presents his tale as historically true and claims it took place in the days of Bartolomeo II della Scala (a century earlier than Salernitano). In da Porto's version Romeo takes poison and Giulietta stabs herself with his dagger.[13]
In 1554, Matteo Bandello published the second volume of his Novelle, which included his version of Giuletta e Romeo.[12] Bandello emphasises Romeo's initial depression and the feud between the families, and introduces the Nurse and Benvolio. Bandello's story was translated into French by Pierre Boaistuau in 1559 in the first volume of his Histories Tragiques. Boaistuau adds much moralising and sentiment, and the characters indulge in rhetorical outbursts.[14]
In his 1562 narrative poem The Tragical History of Romeus and Juliet, Arthur Brooke translated Boaistuau faithfully, but adjusted it to reflect parts of Chaucer's Troilus and Criseyde.[15] There was a trend among writers and playwrights to publish works based on Italian novellesItalian tales were very popular among theatre-goersand Shakespeare may well have been familiar with William Painter's 1567 collection of Italian tales titled Palace of Pleasure.[16] This collection included a version in prose of the Romeo and Juliet story named "The goodly History of the true and constant love of Rhomeo and Julietta". Shakespeare took advantage of this popularity: The Merchant of Venice, Much Ado About Nothing, All's Well That Ends Well, Measure for Measure, and Romeo and Juliet are all from Italian novelle. Romeo and Juliet is a dramatisation of Brooke's translation, and Shakespeare follows the poem closely, but adds extra detail to both major and minor characters (in particular the Nurse and Mercutio).[17]
Christopher Marlowe's Hero and Leander and Dido, Queen of Carthage, both similar stories written in Shakespeare's day, are thought to be less of a direct influence, although they may have helped create an atmosphere in which tragic love stories could thrive.[15]
It is unknown when exactly Shakespeare wrote Romeo and Juliet. Juliet's nurse refers to an earthquake she says occurred 11 years ago.[18] This may refer to the Dover Straits earthquake of 1580, which would date that particular line to 1591. Other earthquakesboth in England and in Veronahave been proposed in support of different dates.[19] But the play's stylistic similarities with A Midsummer Night's Dream and other plays conventionally dated around 159495, place its composition sometime between 1591 and 1595.[20] One conjecture is that Shakespeare may have begun a draft in 1591, which he completed in 1595.[21]
Shakespeare's Romeo and Juliet was published in two quarto editions prior to the publication of the First Folio of 1623. These are referred to as Q1 and Q2. The first printed edition, Q1, appeared in early 1597, printed by John Danter. Because its text contains numerous differences from the later editions, it is labelled a 'bad quarto'; the 20th-century editor T. J. B. Spencer described it as "a detestable text, probably a reconstruction of the play from the imperfect memories of one or two of the actors", suggesting that it had been pirated for publication.[22] An alternative explanation for Q1's shortcomings is that the play (like many others of the time) may have been heavily edited before performance by the playing company.[23] In any event, its appearance in early 1597 makes 1596 the latest possible date for the play's composition.[19]
The superior Q2 called the play The Most Excellent and Lamentable Tragedie of Romeo and Juliet. It was printed in 1599 by Thomas Creede and published by Cuthbert Burby. Q2 is about 800 lines longer than Q1.[23] Its title page describes it as "Newly corrected, augmented and amended". Scholars believe that Q2 was based on Shakespeare's pre-performance draft (called his foul papers), since there are textual oddities such as variable tags for characters and "false starts" for speeches that were presumably struck through by the author but erroneously preserved by the typesetter. It is a much more complete and reliable text, and was reprinted in 1609 (Q3), 1622 (Q4) and 1637 (Q5).[22] In effect, all later Quartos and Folios of Romeo and Juliet are based on Q2, as are all modern editions since editors believe that any deviations from Q2 in the later editions (whether good or bad) are likely to arise from editors or compositors, not from Shakespeare.[23]
The First Folio text of 1623 was based primarily on Q3, with clarifications and corrections possibly coming from a theatrical promptbook or Q1.[22][24] Other Folio editions of the play were printed in 1632 (F2), 1664 (F3), and 1685 (F4).[25] Modern versionsthat take into account several of the Folios and Quartosfirst appeared with Nicholas Rowe's 1709 edition, followed by Alexander Pope's 1723 version. Pope began a tradition of editing the play to add information such as stage directions missing in Q2 by locating them in Q1. This tradition continued late into the Romantic period. Fully annotated editions first appeared in the Victorian period and continue to be produced today, printing the text of the play with footnotes describing the sources and culture behind the play.[26]
Scholars have found it extremely difficult to assign one specific, overarching theme to the play. Proposals for a main theme include a discovery by the characters that human beings are neither wholly good nor wholly evil, but instead are more or less alike,[27] awaking out of a dream and into reality, the danger of hasty action, or the power of tragic fate. None of these have widespread support. However, even if an overall theme cannot be found it is clear that the play is full of several small, thematic elements that intertwine in complex ways. Several of those most often debated by scholars are discussed below.[28]
Romeo and Juliet is sometimes considered to have no unifying theme, save that of young love.[27] Romeo and Juliet have become emblematic of young lovers and doomed love. Since it is such an obvious subject of the play, several scholars have explored the language and historical context behind the romance of the play.[30]
On their first meeting, Romeo and Juliet use a form of communication recommended by many etiquette authors in Shakespeare's day: metaphor. By using metaphors of saints and sins, Romeo was able to test Juliet's feelings for him in a non-threatening way. This method was recommended by Baldassare Castiglione (whose works had been translated into English by this time). He pointed out that if a man used a metaphor as an invitation, the woman could pretend she did not understand him, and he could retreat without losing honour. Juliet, however, participates in the metaphor and expands on it. The religious metaphors of "shrine", "pilgrim" and "saint" were fashionable in the poetry of the time and more likely to be understood as romantic rather than blasphemous, as the concept of sainthood was associated with the Catholicism of an earlier age.[31] Later in the play, Shakespeare removes the more daring allusions to Christ's resurrection in the tomb he found in his source work: Brooke's Romeus and Juliet.[32]
In the later balcony scene, Shakespeare has Romeo overhear Juliet's soliloquy, but in Brooke's version of the story her declaration is done alone. By bringing Romeo into the scene to eavesdrop, Shakespeare breaks from the normal sequence of courtship. Usually a woman was required to be modest and shy to make sure that her suitor was sincere, but breaking this rule serves to speed along the plot. The lovers are able to skip a lengthy part of wooing, and move on to plain talk about their relationshipdeveloping into an agreement to be married after knowing each other for only one night.[30] In the final suicide scene, there is a contradiction in the messagein the Catholic religion, suicides were often thought to be condemned to hell, whereas people who die to be with their loves under the "Religion of Love" are joined with their loves in paradise. Romeo and Juliet's love seems to be expressing the "Religion of Love" view rather than the Catholic view. Another point is that although their love is passionate, it is only consummated in marriage, which prevents them from losing the audience's sympathy.[33]
The play arguably equates love and sex with death. Throughout the story, both Romeo and Juliet, along with the other characters, fantasise about it as a dark being, often equating it with a lover. Capulet, for example, when he first discovers Juliet's (faked) death, describes it as having deflowered his daughter.[34] Juliet later erotically compares Romeo and death. Right before her suicide she grabs Romeo's dagger, saying "O happy dagger! This is thy sheath. There rust, and let me die."[35][36]
Scholars are divided on the role of fate in the play. No consensus exists on whether the characters are truly fated to die together or whether the events take place by a series of unlucky chances. Arguments in favour of fate often refer to the description of the lovers as "star-cross'd". This phrase seems to hint that the stars have predetermined the lovers' future.[38] John W. Draper points out the parallels between the Elizabethan belief in the four humours and the main characters of the play (for example, Tybalt as a choleric). Interpreting the text in the light of humours reduces the amount of plot attributed to chance by modern audiences.[39] Still, other scholars see the play as a series of unlucky chancesmany to such a degree that they do not see it as a tragedy at all, but an emotional melodrama.[39] Ruth Nevo believes the high degree to which chance is stressed in the narrative makes Romeo and Juliet a "lesser tragedy" of happenstance, not of character. For example, Romeo's challenging Tybalt is not impulsive; it is, after Mercutio's death, the expected action to take. In this scene, Nevo reads Romeo as being aware of the dangers of flouting social norms, identity and commitments. He makes the choice to kill, not because of a tragic flaw, but because of circumstance.[40]
Scholars have long noted Shakespeare's widespread use of light and dark imagery throughout the play. Caroline Spurgeon considers the theme of light as "symbolic of the natural beauty of young love" and later critics have expanded on this interpretation.[40][42] For example, both Romeo and Juliet see the other as light in a surrounding darkness. Romeo describes Juliet as being like the sun,[43] brighter than a torch,[44] a jewel sparkling in the night,[45] and a bright angel among dark clouds.[46] Even when she lies apparently dead in the tomb, he says her "beauty makes This vault a feasting presence full of light."[47] Juliet describes Romeo as "day in night" and "Whiter than snow upon a raven's back."[48][49] This contrast of light and dark can be expanded as symbolscontrasting love and hate, youth and age in a metaphoric way.[40] Sometimes these intertwining metaphors create dramatic irony. For example, Romeo and Juliet's love is a light in the midst of the darkness of the hate around them, but all of their activity together is done in night and darkness, while all of the feuding is done in broad daylight. This paradox of imagery adds atmosphere to the moral dilemma facing the two lovers: loyalty to family or loyalty to love. At the end of the story, when the morning is gloomy and the sun hiding its face for sorrow, light and dark have returned to their proper places, the outward darkness reflecting the true, inner darkness of the family feud out of sorrow for the lovers. All characters now recognise their folly in light of recent events, and things return to the natural order, thanks to the love of Romeo and Juliet.[42] The "light" theme in the play is also heavily connected to the theme of time, since light was a convenient way for Shakespeare to express the passage of time through descriptions of the sun, moon, and stars.[50]
Time plays an important role in the language and plot of the play. Both Romeo and Juliet struggle to maintain an imaginary world void of time in the face of the harsh realities that surround them. For instance, when Romeo swears his love to Juliet by the moon, she protests "O swear not by the moon, th'inconstant moon, / That monthly changes in her circled orb, / Lest that thy love prove likewise variable."[52] From the very beginning, the lovers are designated as "star-cross'd"[53] referring to an astrologic belief associated with time. Stars were thought to control the fates of humanity, and as time passed, stars would move along their course in the sky, also charting the course of human lives below. Romeo speaks of a foreboding he feels in the stars' movements early in the play, and when he learns of Juliet's death, he defies the stars' course for him.[39][54]
Another central theme is haste: Shakespeare's Romeo and Juliet spans a period of four to six days, in contrast to Brooke's poem's spanning nine months.[50] Scholars such as G. Thomas Tanselle believe that time was "especially important to Shakespeare" in this play, as he used references to "short-time" for the young lovers as opposed to references to "long-time" for the "older generation" to highlight "a headlong rush towards doom".[50] Romeo and Juliet fight time to make their love last forever. In the end, the only way they seem to defeat time is through a death that makes them immortal through art.[55]
Time is also connected to the theme of light and dark. In Shakespeare's day, plays were often performed at noon in broad daylight. This forced the playwright to use words to create the illusion of day and night in his plays. Shakespeare uses references to the night and day, the stars, the moon, and the sun to create this illusion. He also has characters frequently refer to days of the week and specific hours to help the audience understand that time has passed in the story. All in all, no fewer than 103 references to time are found in the play, adding to the illusion of its passage.[56]
The earliest known critic of the play was diarist Samuel Pepys, who wrote in 1662: "it is a play of itself the worst that I ever heard in my life."[57] Poet John Dryden wrote 10 years later in praise of the play and its comic character Mercutio: "Shakespear show'd the best of his skill in his Mercutio, and he said himself, that he was forc'd to kill him in the third Act, to prevent being killed by him."[57] Criticism of the play in the 18th century was less sparse, but no less divided. Publisher Nicholas Rowe was the first critic to ponder the theme of the play, which he saw as the just punishment of the two feuding families. In mid-century, writer Charles Gildon and philosopher Lord Kames argued that the play was a failure in that it did not follow the classical rules of drama: the tragedy must occur because of some character flaw, not an accident of fate. Writer and critic Samuel Johnson, however, considered it one of Shakespeare's "most pleasing" plays.[58]
In the later part of the 18th and through the 19th century, criticism centred on debates over the moral message of the play. Actor and playwright David Garrick's 1748 adaptation excluded Rosaline: Romeo abandoning her for Juliet was seen as fickle and reckless. Critics such as Charles Dibdin argued that Rosaline had been purposely included in the play to show how reckless the hero was, and that this was the reason for his tragic end. Others argued that Friar Laurence might be Shakespeare's spokesman in his warnings against undue haste. With the advent of the 20th century, these moral arguments were disputed by critics such as Richard Green Moulton: he argued that accident, and not some character flaw, led to the lovers' deaths.[59]
In Romeo and Juliet, Shakespeare employs several dramatic techniques that have garnered praise from critics; most notably the abrupt shifts from comedy to tragedy (an example is the punning exchange between Benvolio and Mercutio just before Tybalt arrives). Before Mercutio's death in Act three, the play is largely a comedy.[60] After his accidental demise, the play suddenly becomes serious and takes on a tragic tone. When Romeo is banished, rather than executed, and Friar Laurence offers Juliet a plan to reunite her with Romeo, the audience can still hope that all will end well. They are in a "breathless state of suspense" by the opening of the last scene in the tomb: If Romeo is delayed long enough for the Friar to arrive, he and Juliet may yet be saved.[61] These shifts from hope to despair, reprieve, and new hope, serve to emphasise the tragedy when the final hope fails and both the lovers die at the end.[62]
Shakespeare also uses sub-plots to offer a clearer view of the actions of the main characters. For example, when the play begins, Romeo is in love with Rosaline, who has refused all of his advances. Romeo's infatuation with her stands in obvious contrast to his later love for Juliet. This provides a comparison through which the audience can see the seriousness of Romeo and Juliet's love and marriage. Paris' love for Juliet also sets up a contrast between Juliet's feelings for him and her feelings for Romeo. The formal language she uses around Paris, as well as the way she talks about him to her Nurse, show that her feelings clearly lie with Romeo. Beyond this, the sub-plot of the MontagueCapulet feud overarches the whole play, providing an atmosphere of hate that is the main contributor to the play's tragic end.[62]
Shakespeare uses a variety of poetic forms throughout the play. He begins with a 14-line prologue in the form of a Shakespearean sonnet, spoken by a Chorus. Most of Romeo and Juliet is, however, written in blank verse, and much of it in strict iambic pentameter, with less rhythmic variation than in most of Shakespeare's later plays.[63] In choosing forms, Shakespeare matches the poetry to the character who uses it. Friar Laurence, for example, uses sermon and sententiae forms, and the Nurse uses a unique blank verse form that closely matches colloquial speech.[63] Each of these forms is also moulded and matched to the emotion of the scene the character occupies. For example, when Romeo talks about Rosaline earlier in the play, he attempts to use the Petrarchan sonnet form. Petrarchan sonnets were often used by men to exaggerate the beauty of women who were impossible for them to attain, as in Romeo's situation with Rosaline. This sonnet form is used by Lady Capulet to describe Count Paris to Juliet as a handsome man.[64] When Romeo and Juliet meet, the poetic form changes from the Petrarchan (which was becoming archaic in Shakespeare's day) to a then more contemporary sonnet form, using "pilgrims" and "saints" as metaphors.[65] Finally, when the two meet on the balcony, Romeo attempts to use the sonnet form to pledge his love, but Juliet breaks it by saying "Dost thou love me?"[66] By doing this, she searches for true expression, rather than a poetic exaggeration of their love.[67] Juliet uses monosyllabic words with Romeo, but uses formal language with Paris.[68] Other forms in the play include an epithalamium by Juliet, a rhapsody in Mercutio's Queen Mab speech, and an elegy by Paris.[69] Shakespeare saves his prose style most often for the common people in the play, though at times he uses it for other characters, such as Mercutio.[70] Humour, also, is important: scholar Molly Mahood identifies at least 175 puns and wordplays in the text.[71] Many of these jokes are sexual in nature, especially those involving Mercutio and the Nurse.[72]
Early psychoanalytic critics saw the problem of Romeo and Juliet in terms of Romeo's impulsiveness, deriving from "ill-controlled, partially disguised aggression", which leads both to Mercutio's death and to the double suicide.[73] Romeo and Juliet is not considered to be exceedingly psychologically complex, and sympathetic psychoanalytic readings of the play make the tragic male experience equivalent with sicknesses.[74] Norman Holland, writing in 1966, considers Romeo's dream[75] as a realistic "wish fulfilling fantasy both in terms of Romeo's adult world and his hypothetical childhood at stages oral, phallic and oedipal"  while acknowledging that a dramatic character is not a human being with mental processes separate from those of the author.[76] Critics such as Julia Kristeva focus on the hatred between the families, arguing that this hatred is the cause of Romeo and Juliet's passion for each other. That hatred manifests itself directly in the lovers' language: Juliet, for example, speaks of "my only love sprung from my only hate"[77] and often expresses her passion through an anticipation of Romeo's death.[78] This leads on to speculation as to the playwright's psychology, in particular to a consideration of Shakespeare's grief for the death of his son, Hamnet.[79]
Feminist literary critics argue that the blame for the family feud lies in Verona's patriarchal society. For Copplia Kahn, for example, the strict, masculine code of violence imposed on Romeo is the main force driving the tragedy to its end. When Tybalt kills Mercutio, Romeo shifts into this violent mode, regretting that Juliet has made him so "effeminate".[80] In this view, the younger males "become men" by engaging in violence on behalf of their fathers, or in the case of the servants, their masters. The feud is also linked to male virility, as the numerous jokes about maidenheads aptly demonstrate.[81] Juliet also submits to a female code of docility by allowing others, such as the Friar, to solve her problems for her. Other critics, such as Dympna Callaghan, look at the play's feminism from a historicist angle, stressing that when the play was written the feudal order was being challenged by increasingly centralised government and the advent of capitalism. At the same time, emerging Puritan ideas about marriage were less concerned with the "evils of female sexuality" than those of earlier eras, and more sympathetic towards love-matches: when Juliet dodges her father's attempt to force her to marry a man she has no feeling for, she is challenging the patriarchal order in a way that would not have been possible at an earlier time.[82]
A number of critics have found the character of Mercutio to have unacknowledged homoerotic desire for Romeo.[83] Goldberg (1994) examined the sexuality of Mercutio and Romeo utilising "queer theory" in Queering the Renaissance, comparing their friendship with sexual love. Mercutio, in friendly conversation, mentions Romeo's phallus, suggesting traces of homoeroticism.[84] An example is his joking wish "To raise a spirit in his mistress' circle ... letting it there stand / Till she had laid it and conjured it down."[85][86] Romeo's homoeroticism can also be found in his attitude to Rosaline, a woman who is distant and unavailable and brings no hope of offspring. As Benvolio argues, she is best replaced by someone who will reciprocate. Shakespeare's procreation sonnets describe another young man who, like Romeo, is having trouble creating offspring and who may be seen as being a homosexual. Goldberg believes that Shakespeare may have used Rosaline as a way to express homosexual problems of procreation in an acceptable way. In this view, when Juliet says "...that which we call a rose, by any other name would smell as sweet",[87] she may be raising the question of whether there is any difference between the beauty of a man and the beauty of a woman.[88]
Romeo and Juliet ranks with Hamlet as one of Shakespeare's most-performed plays.[90] Its many adaptations have made it one of his most enduring and famous stories.[90] Even in Shakespeare's lifetime it was extremely popular. Scholar Gary Taylor measures it as the sixth most popular of Shakespeare's plays, in the period after the death of Christopher Marlowe and Thomas Kyd but before the ascendancy of Ben Jonson during which Shakespeare was London's dominant playwright.[91] The date of the first performance is unknown. The First Quarto, printed in 1597, says that "it hath been often (and with great applause) plaid publiquely", setting the first performance prior to that date. The Lord Chamberlain's Men were certainly the first to perform it. Besides their strong connections with Shakespeare, the Second Quarto actually names one of its actors, Will Kemp, instead of Peter in a line in Act five. Richard Burbage was probably the first Romeo, being the company's actor, and Master Robert Goffe (a boy) the first Juliet.[89] The premiere is likely to have been at "The Theatre", with other early productions at "The Curtain".[92] Romeo and Juliet is one of the first Shakespearean plays to have been performed outside England: a shortened and simplified version was performed in Nrdlingen in 1604.[93]
All theatres were closed down by the puritan government on 6 September 1642. Upon the restoration of the monarchy in 1660, two patent companies (the King's Company and the Duke's Company) were established, and the existing theatrical repertoire divided between them.[94]
Sir William Davenant of the Duke's Company staged a 1662 adaptation in which Henry Harris played Romeo, Thomas Betterton Mercutio, and Betterton's wife Mary Saunderson Juliet: she was probably the first woman to play the role professionally.[95][96] Another version closely followed Davenant's adaptation and was also regularly performed by the Duke's Company. This was a tragicomedy by James Howard, in which the two lovers survive.[97]
Thomas Otway's The History and Fall of Caius Marius, one of the more extreme of the Restoration adaptations of Shakespeare, debuted in 1680. The scene is shifted from Renaissance Verona to ancient Rome; Romeo is Marius, Juliet is Lavinia, the feud is between patricians and plebeians; Juliet/Lavinia wakes from her potion before Romeo/Marius dies. Otway's version was a hit, and was acted for the next seventy years.[96] His innovation in the closing scene was even more enduring, and was used in adaptations throughout the next 200 years: Theophilus Cibber's adaptation of 1744, and David Garrick's of 1748 both used variations on it.[98] These versions also eliminated elements deemed inappropriate at the time. For example, Garrick's version transferred all language describing Rosaline to Juliet, to heighten the idea of faithfulness and downplay the love-at-first-sight theme.[99] In 1750 a "Battle of the Romeos" began, with Spranger Barry and Susannah Maria Arne (Mrs. Theophilus Cibber) at Covent Garden versus David Garrick and George Anne Bellamy at Drury Lane.[100]
The earliest known production in North America was an amateur one: on 23 March 1730, a physician named Joachimus Bertrand placed an advertisement in the Gazette newspaper in New York, promoting a production in which he would play the apothecary.[101] The first professional performances of the play in North America were those of the Hallam Company.[102]
Garrick's altered version of the play was very popular, and ran for nearly a century.[96] Not until 1845 did Shakespeare's original return to the stage in the United States with the sisters Susan and Charlotte Cushman as Juliet and Romeo, respectively,[103] and then in 1847 in Britain with Samuel Phelps at Sadler's Wells Theatre.[104] Cushman adhered to Shakespeare's version, beginning a string of eighty-four performances. Her portrayal of Romeo was considered genius by many. The Times wrote: "For a long time Romeo has been a convention. Miss Cushman's Romeo is a creative, a living, breathing, animated, ardent human being."[105] Queen Victoria wrote in her journal that "no-one would ever have imagined she was a woman".[106] Cushman's success broke the Garrick tradition and paved the way for later performances to return to the original storyline.[96]
Professional performances of Shakespeare in the mid-19th century had two particular features: firstly, they were generally star vehicles, with supporting roles cut or marginalised to give greater prominence to the central characters. Secondly, they were "pictorial", placing the action on spectacular and elaborate sets (requiring lengthy pauses for scene changes) and with the frequent use of tableaux.[107] Henry Irving's 1882 production at the Lyceum Theatre (with himself as Romeo and Ellen Terry as Juliet) is considered an archetype of the pictorial style.[108] In 1895, Sir Johnston Forbes-Robertson took over from Irving, and laid the groundwork for a more natural portrayal of Shakespeare that remains popular today. Forbes-Robertson avoided the showiness of Irving and instead portrayed a down-to-earth Romeo, expressing the poetic dialogue as realistic prose and avoiding melodramatic flourish.[109]
American actors began to rival their British counterparts. Edwin Booth (brother to John Wilkes Booth) and Mary McVicker (soon to be Edwin's wife) opened as Romeo and Juliet at the sumptuous Booth's Theatre (with its European-style stage machinery, and an air conditioning system unique in New York) on 3 February 1869. Some reports said it was one of the most elaborate productions of Romeo and Juliet ever seen in America; it was certainly the most popular, running for over six weeks and earning over $60,000[110] (equal to about $1,048,000 today).[111] The programme noted that: "The tragedy will be produced in strict accordance with historical propriety, in every respect, following closely the text of Shakespeare."[112]
The first professional performance of the play in Japan may have been George Crichton Miln's company's production, which toured to Yokohama in 1890.[113] Throughout the 19th century, Romeo and Juliet had been Shakespeare's most popular play, measured by the number of professional performances. In the 20th century it would become the second most popular, behind Hamlet.[114]
In 1933, the play was revived by actress Katharine Cornell and her director husband Guthrie McClintic and was taken on a seven-month nationwide tour throughout the United States. It starred Orson Welles, Brian Aherne and Basil Rathbone. The production was a modest success, and so upon the return to New York, Cornell and McClintic revised it and for the first time, the play was presented with almost all the scenes intact, including the Prologue. The new production opened in December 1934 with Ralph Richardson as Mercutio and Maurice Evans as Romeo. Critics wrote that Cornell was "the finest Juliet of her time," "endlessly haunting," and "the most lovely and enchanting Juliet our present-day theatre has seen."[115]
John Gielgud's New Theatre production in 1935 featured Gielgud and Laurence Olivier as Romeo and Mercutio, exchanging roles six weeks into the run, with Peggy Ashcroft as Juliet.[116] Gielgud used a scholarly combination of Q1 and Q2 texts, and organised the set and costumes to match as closely as possible to the Elizabethan period. His efforts were a huge success at the box office, and set the stage for increased historical realism in later productions.[117] Olivier later compared his performance and Gielgud's: "John, all spiritual, all spirituality, all beauty, all abstract things; and myself as all earth, blood, humanity ... I've always felt that John missed the lower half and that made me go for the other ... But whatever it was, when I was playing Romeo I was carrying a torch, I was trying to sell realism in Shakespeare."[118]
Peter Brook's 1947 version was the beginning of a different style of Romeo and Juliet performances. Brook was less concerned with realism, and more concerned with translating the play into a form that could communicate with the modern world. He argued, "A production is only correct at the moment of its correctness, and only good at the moment of its success."[119] Brook excluded the final reconciliation of the families from his performance text.[120]
Throughout the century, audiences, influenced by the cinema, became less willing to accept actors distinctly older than the teenage characters they were playing.[121] A significant example of more youthful casting was in Franco Zeffirelli's Old Vic production in 1960, with John Stride and Judi Dench, which would serve as the basis for his 1968 film.[120] Zeffirelli borrowed from Brook's ideas, altogether removing around a third of the play's text to make it more accessible. In an interview with The Times, he stated that the play's "twin themes of love and the total breakdown of understanding between two generations" had contemporary relevance.[122]
Recent performances often set the play in the contemporary world. For example, in 1986 the Royal Shakespeare Company set the play in modern Verona. Switchblades replaced swords, feasts and balls became drug-laden rock parties, and Romeo committed suicide by hypodermic needle.[123] In 1997, the Folger Shakespeare Theatre produced a version set in a typical suburban world. Romeo sneaks into the Capulet barbecue to meet Juliet, and Juliet discovers Tybalt's death while in class at school.[124]
The play is sometimes given a historical setting, enabling audiences to reflect on the underlying conflicts. For example, adaptations have been set in the midst of the Israeli-Palestinian conflict,[125] in the apartheid era in South Africa,[126] and in the aftermath of the Pueblo Revolt.[127] Similarly, Peter Ustinov's 1956 comic adaptation, Romanoff and Juliet, is set in a fictional mid-European country in the depths of the Cold War.[128] A mock-Victorian revisionist version of Romeo and Juliet's final scene (with a happy ending, Romeo, Juliet, Mercutio and Paris restored to life, and Benvolio revealing that he is Paris's love, Benvolia, in disguise) forms part of the 1980 stage-play The Life and Adventures of Nicholas Nickleby.[129] Shakespeares R&J, by Joe Calarco, spins the classic in a modern tale of gay teenage awakening.[130] A recent comedic musical adaptation was The Second City's The Second City's Romeo and Juliet Musical: The People vs. Friar Laurence, the Man Who Killed Romeo and Juliet, set in modern times.[131]
In the 19th and 20th century, Romeo and Juliet has often been the choice of Shakespeare plays to open a classical theatre company, beginning with Edwin Booth's inaugural production of that play in his theatre in 1869, the newly reformed company of the Old Vic in 1929 with John Gielgud, Martita Hunt and Margaret Webster,[132] as well as the Riverside Shakespeare Company in its founding production in New York City in 1977, which used the 1968 film of Franco Zeffirelli's production as its inspiration.[133]
At least 24 operas have been based on Romeo and Juliet.[135] The earliest, Romeo und Julie in 1776, a Singspiel by Georg Benda, omits much of the action of the play and most of its characters, and has a happy ending. It is occasionally revived. The best-known is Gounod's 1867 Romo et Juliette (libretto by Jules Barbier and Michel Carr), a critical triumph when first performed and frequently revived today.[136] Bellini's I Capuleti e i Montecchi is also revived from time to time, but has sometimes been judged unfavourably because of its perceived liberties with Shakespeare; however, Bellini and his librettist, Felice Romani, worked from Italian sourcesprincipally Romani's libretto for an opera by Nicola Vaccairather than directly adapting Shakespeare's play.[137] Among later operas there is Heinrich Sutermeister's 1940 work Romeo und Julia.
Romo et Juliette by Berlioz is a "symphonie dramatique", a large scale work in three parts for mixed voices, chorus and orchestra, which premiered in 1839.[138] Tchaikovsky's Romeo and Juliet Fantasy-Overture (1869, revised 1870 and 1880) is a 15 minute symphonic poem, containing the famous melody known as the "love theme".[139] Tchaikovsky's device of repeating the same musical theme at the ball, in the balcony scene, in Juliet's bedroom and in the tomb[140] has been used by subsequent directors: for example Nino Rota's love theme is used in a similar way in the 1968 film of the play, as is Des'ree's Kissing You in the 1996 film.[141] Other classical composers influenced by the play include Henry Hugh Pearson (Romeo and Juliet, overture for orchestra, Op. 86), Svendsen (Romeo og Julie, 1876), Delius (A Village Romeo and Juliet, 18991901), Stenhammar (Romeo och Julia, 1922), and Kabalevsky (Incidental Music to Romeo and Juliet, Op. 56, 1956).[142]
The best-known ballet version is Prokofiev's Romeo and Juliet.[143] Originally commissioned by the Kirov Ballet, it was rejected by them when Prokofiev attempted a happy ending, and was rejected again for the experimental nature of its music. It has subsequently attained an "immense" reputation, and has been choreographed by John Cranko (1962) and Kenneth MacMillan (1965) among others.[144]
The play influenced several jazz works, including Peggy Lee's "Fever".[145] Duke Ellington's Such Sweet Thunder contains a piece entitled "The Star-Crossed Lovers"[146] in which the pair are represented by tenor and alto saxophones: critics noted that Juliet's sax dominates the piece, rather than offering an image of equality.[147] The play has frequently influenced popular music, including works by The Supremes, Bruce Springsteen, Tom Waits, Lou Reed[148], and Taylor Swift[citation needed]. The most famous such track is Dire Straits' "Romeo and Juliet".[149]
The most famous musical theatre adaptation is West Side Story with music by Leonard Bernstein and lyrics by Stephen Sondheim. It dbuted on Broadway in 1957 and in the West End in 1958, and became a popular film in 1961. This version updated the setting to mid-20th century New York City, and the warring families to ethnic gangs.[150] Other musical adaptations include Terrence Mann's 1999 rock musical William Shakespeare's Romeo and Juliet, co-written with Jerome Korman,[151] Grard Presgurvic's 2001 Romo et Juliette, de la Haine  l'Amour and Riccardo Cocciante's 2007 Giulietta & Romeo.[152]
Romeo and Juliet had a profound influence on subsequent literature. Before then, romance had not even been viewed as a worthy topic for tragedy.[153] In Harold Bloom's words, Shakespeare "invented the formula that the sexual becomes the erotic when crossed by the shadow of death."[154] Of Shakespeare's works, Romeo and Juliet has generated the mostand the most variedadaptations, including prose and verse narratives, drama, opera, orchestral and choral music, ballet, film, television and painting.[155] The word "Romeo" has even become synonymous with "male lover" in English.[156]
Romeo and Juliet was parodied in Shakespeare's own lifetime: Henry Porter's Two Angry Women of Abingdon (1598) and Thomas Dekker's Blurt, Master Constable (1607) both contain balcony scenes in which a virginal heroine engages in bawdy wordplay.[157] The play directly influenced later literary works. For example the preparations for a performance form a major plot arc in Charles Dickens' Nicholas Nickleby.[158]
Romeo and Juliet is one of Shakespeare's most-illustrated works.[159] The first known illustration was a woodcut of the tomb scene,[160] thought to be by Elisha Kirkall, which appeared in Nicholas Rowe's 1709 edition of Shakespeare's plays.[161] Five paintings of the play were commissioned for the Boydell Shakespeare Gallery in the late 18th century, one representing each of the five acts of the play.[162] The 19th century fashion for "pictorial" performances led to directors drawing on paintings for their inspiration, which in turn influenced painters to depict actors and scenes from the theatre.[163] In the 20th century, the play's most iconic visual images have derived from its popular film versions.[164]
Romeo and Juliet may be the most-filmed play of all time.[165] The most notable theatrical releases were George Cukor's multi-Oscar-nominated 1936 production, Franco Zeffirelli's 1968 version, and Baz Luhrmann's 1996 MTV-inspired Romeo + Juliet. The latter two were both, in their time, the highest-grossing Shakespeare film ever.[166] Romeo and Juliet was first filmed in the silent era, by Georges Mlis, although his film is now lost.[165] The play was first heard on film in The Hollywood Revue of 1929, in which John Gilbert recited the balcony scene opposite Norma Shearer.[167]
Shearer and Leslie Howard, with a combined age over 75, played the teenage lovers in George Cukor's MGM 1936 film version. Neither critics nor the public responded enthusiastically. Cinemagoers considered the film too "arty", staying away as they had from Warner's A Midsummer Night Dream a year before: leading to Hollywood abandoning the Bard for over a decade.[168] Renato Castellani won the Grand Prix at the Venice Film Festival for his 1954 film of Romeo and Juliet.[169] his Romeo, Laurence Harvey, was already an experienced screen actor.[170] By contrast, Susan Shentall, as Juliet, was a secretarial student who was discovered by the director in a London pub, and was cast for her "pale sweet skin and honey-blonde hair".[171]
Stephen Orgel describes Franco Zeffirelli's 1968 Romeo and Juliet as being "full of beautiful young people, and the camera, and the lush technicolour, make the most of their sexual energy and good looks."[172] Zeffirelli's teenage leads, Leonard Whiting and Olivia Hussey, had virtually no previous acting experience, but performed capably and with great maturity.[173] Zeffirelli has been particularly praised,[174] for his presentation of the duel scene as bravado getting out-of-control.[175] The film courted controversy by including a nude wedding-night scene[176] while Olivia Hussey was only fifteen.[177]
Baz Luhrmann's 1996 Romeo + Juliet and its accompanying soundtrack successfully targeted the "MTV Generation": a young audience of similar age to the story's characters.[178] Far darker than Zeffirelli's version, the film is set in the "crass, violent and superficial society" of Verona Beach and Sycamore Grove.[179] Leonardo DiCaprio was Romeo and Claire Danes was Juliet.
The play has been widely adapted for TV and film. In 1960, Peter Ustinov's cold-war stage parody, Romanoff and Juliet was filmed.[128] The 1961 film of West Side Storyset among New York gangsfeatured the Jets as white youths, equivalent to Shakespeare's Montagues, while the Sharks, equivalent to the Capulets, are Puerto Rican.[180] The 1994 film The Punk uses both the rough plot outline of Romeo and Juliet and names many of the characters in ways that reflect the characters in the play. In 2006, Disney's High School Musical made use of Romeo and Juliet's plot, placing the two young lovers in rival high school cliques instead of feuding families.[181] Film-makers have frequently featured characters performing scenes from Romeo and Juliet.[182] The conceit of dramatising Shakespeare writing Romeo and Juliet has been used several times,[183] including John Madden's 1998 Shakespeare in Love, in which Shakespeare writes the play against the backdrop of his own doomed love affair.[184] An anime series produced by Gonzo and SKY Perfect Well Think, called Romeo x Juliet, was made in 2007; its plot was an edited version of the original story's, and had many new supporting characters whose names were often derived from those of characters in other Shakespeare works.
In April and May 2010 the Royal Shakespeare Company and the Mudlark Production Company presented a version of the play, entitled Such Tweet Sorrow, as an improvised, real-time series of tweets on Twitter. The production used RSC actors who engaged with the audience as well each other, performing not from a traditional script but a "Grid" developed by the Mudlark production team and writers Tim Wright and Bethan Marlow. The performers also make use of other media sites such as YouTube for pictures and video.[185]
Act I prologue
Act I scene 1: Quarrel between Capulets and Montagues
Act I scene 2
Act I scene 3
Act I scene 4
Act I scene 5
Act I scene 5: Romeo's first interview with Juliet
Act II prologue
Act II scene 3
Act II scene 5: Juliet intreats her nurse
Act II scene 6
Act III scene 5: Romeo takes leave of Juliet
Act IV scene 5: Juliet's fake death
Act IV scene 5: Another depiction
Act V scene 3: Juliet awakes to find Romeo dead
All references to Romeo and Juliet, unless otherwise specified, are taken from the Arden Shakespeare second edition (Gibbons, 1980) based on the Q2 text of 1599, with elements from Q1 of 1597.[186] Under its referencing system, which uses Roman numerals, II.ii.33 means act 2, scene 2, line 33, and a 0 in place of a scene number refers to the prologue to the act.
   
1 Characters
2 Synopsis
3 Sources
4 Date and text
5 Themes and motifs

5.1 Love
5.2 Fate and chance
5.3 Duality (light and dark)
5.4 Time


5.1 Love
5.2 Fate and chance
5.3 Duality (light and dark)
5.4 Time
6 Criticism and interpretation

6.1 Critical history
6.2 Dramatic structure
6.3 Language
6.4 Psychoanalytic criticism
6.5 Feminist criticism
6.6 Queer theory


6.1 Critical history
6.2 Dramatic structure
6.3 Language
6.4 Psychoanalytic criticism
6.5 Feminist criticism
6.6 Queer theory
7 Legacy

7.1 Shakespeare's day
7.2 Restoration and 18th-century theatre
7.3 19th-century theatre
7.4 20th-century theatre
7.5 Music
7.6 Literature and art
7.7 Screen
7.8 Modern social media


7.1 Shakespeare's day
7.2 Restoration and 18th-century theatre
7.3 19th-century theatre
7.4 20th-century theatre
7.5 Music
7.6 Literature and art
7.7 Screen
7.8 Modern social media
8 Scene by scene
9 References

9.1 Notes
9.2 Secondary sources


9.1 Notes
9.2 Secondary sources
10 External links
5.1 Love
5.2 Fate and chance
5.3 Duality (light and dark)
5.4 Time
6.1 Critical history
6.2 Dramatic structure
6.3 Language
6.4 Psychoanalytic criticism
6.5 Feminist criticism
6.6 Queer theory
7.1 Shakespeare's day
7.2 Restoration and 18th-century theatre
7.3 19th-century theatre
7.4 20th-century theatre
7.5 Music
7.6 Literature and art
7.7 Screen
7.8 Modern social media
9.1 Notes
9.2 Secondary sources
Prince Escalus is the ruling Prince of Verona
Count Paris is a kinsman of Escalus who wishes to marry Juliet.
Mercutio is another kinsman of Escalus, and a friend of Romeo.
Capulet is the patriarch of the house of Capulet.
Capulet's wife is the matriarch of the house of Capulet.
Juliet is the 13-year-old daughter of Capulet, and the play's female protagonist.
Tybalt is a cousin of Juliet, and the nephew of Capulet's wife.
The Nurse is Juliet's personal attendant and confidante.
Rosaline is Lord Capulet's niece, and Romeo's love in the beginning of the story.
Peter, Sampson and Gregory are servants of the Capulet household.
Montague is the patriarch of the house of Montague.
Montague's wife is the matriarch of the house of Montague.
Romeo is the son of Montague, and the play's male protagonist.
Benvolio is Romeo's cousin and best friend.
Abram and Balthasar are servants of the Montague household.
Friar Laurence is a Franciscan friar, and is Romeo's confidant.
A Chorus reads a prologue to each of the first two acts.
Friar John is sent to deliver Friar Laurence's letter to Romeo.
An Apothecary who reluctantly sells Romeo poison.






Act I prologue









Act I scene 1: Quarrel between Capulets and Montagues









Act I scene 2









Act I scene 3









Act I scene 4









Act I scene 5









Act I scene 5: Romeo's first interview with Juliet









Act II prologue









Act II scene 3









Act II scene 5: Juliet intreats her nurse









Act II scene 6









Act III scene 5: Romeo takes leave of Juliet









Act IV scene 5: Juliet's fake death









Act IV scene 5: Another depiction









Act V scene 3: Juliet awakes to find Romeo dead



Appelbaum, Robert (1997). ""Standing to the Wall": The Pressures of Masculinity in Romeo and Juliet". Shakespeare Quarterly (Folger Shakespeare Library) 48 (3): 251272. doi:10.2307/2871016. ISSN00373222. JSTOR2871016.
Arafay, Mireia (2005). Books in Motion: Adaptation, Adaptability, Authorship. Editions Rodopi BV. ISBN978-90-420-1957-7.
Barranger, Milly S. (2004). Margaret Webster: A Life in the Theatre. University of Michigan Press. ISBN0-472-11390-9, 9780472113903.
Bauch, Marc A. (2007). Friar Lawrence's Plan in William Shakespeare's Romeo and Juliet And His Function as A Counsellor. Munich: Grin. ISBN978-3-638-77449-9.
Bloom, Harold (1998). Shakespeare: The Invention of the Human. New York: Riverhead Books. ISBN1-57322-120-1.
Bly, Mary (2001). "The Legacy of Juliet's Desire in Comedies of the Early 1600s". In Margaret M. S. Alexander; Wells, Stanley. Shakespeare and Sexuality. Cambridge: Cambridge University Press. pp.5271. ISBN0-521-80475-2.
Bonnard, Georges A. (1951). "Romeo and Juliet: A Possible Significance?". Review of English Studies II (5): 319327. doi:10.1093/res/II.5.319.
Bowling, Lawrence Edward (1949). "The Thematic Framework of Romeo and Juliet". PMLA (Modern Language Association of America) 64 (1): 208220. doi:10.2307/459678. JSTOR459678.
Branam, George C. (1984). "The Genesis of David Garrick's Romeo and Juliet". Shakespeare Quarterly (Folger Shakespeare Library) 35 (2): 170179. doi:10.2307/2869925. JSTOR2869925.
Brode, Douglas (2001). Shakespeare in the Movies: From the Silent Era to Today. New York: Berkley Boulevard Books. ISBN0-425-18176-6.
Buchanan, Judith (2009). Shakespeare on Silent Film: An Excellent Dumb Discourse. Cambridge: Cambridge University Press. ISBN978-0-521-87199-0.
Buhler, Stephen M. (2007). "Musical Shakespeares: attending to Ophelia, Juliet, and Desdemona". In Shaughnessy, Robert (ed.). The Cambridge Companion to Shakespeare and Popular Culture. Cambridge: Cambridge University Press. pp.150174. ISBN978-0-521-60580-9.
Collins, Michael (1982). "The Literary Background of Bellini's 'I Capuleti ed i Montecchi'". Journal of the American Musicological Society 35 (3): 532538. doi:10.1525/jams.1982.35.3.03a00050.
Dawson, Anthony B. (2002). "International Shakespeare". In Wells, Stanley; Stanton, Sarah. The Cambridge Companion to Shakespeare on Stage. Cambridge: Cambridge University Press. pp.174193. ISBN978-0-521-79711-5.
Draper, John W. (1939). "Shakespeare's 'Star-Crossed Lovers'". Review of English Studies os-XV (57): 1634. doi:10.1093/res/os-XV.57.16.
Driver, Tom F. (1964). "The Shakespearian Clock: Time and the Vision of Reality in Romeo and Juliet and The Tempest". Shakespeare Quarterly (Folger Shakespeare Library) 15 (4): 363370. doi:10.2307/2868094. JSTOR2868094.
Edgar, David (1982). The Life and Adventures of Nicholas Nickleby. New York: Dramatists' Play Service. ISBN0-8222-0817-2.
Ehren, Christine (3 September 1999). "Sweet Sorrow: Mann-Korman's Romeo and Juliet Closes Sept. 5 at MN's Ordway". Playbill. http://www.playbill.com/news/article/47546.html. Retrieved 13 August 2008.
Evans, Bertrand (1950). "The Brevity of Friar Laurence". PMLA (Modern Language Association) 65 (5): 841865. doi:10.2307/459577. JSTOR459577.
Fowler, James (1996). Stanley Wells. ed. "Picturing Romeo and Juliet". Shakespeare Survey. Shakespeare Survey (Cambridge University Press) 49: 111129. doi:10.1017/CCOL0521570476.009. ISBN0-521-57047-6.
Gay, Penny (2002). "Women and Shakespearean Performance". In Wells, Stanley; Stanton, Sarah. The Cambridge Companion to Shakespeare on Stage. Cambridge: Cambridge University Press. pp.155173. ISBN978-0-521-79711-5.
Gibbons, Brian (ed.) (1980). Romeo and Juliet. The Arden Shakespeare Second Series. London: Thomson Learning. ISBN978-1-903436-41-7.
Goldberg, Jonathan (1994). Queering the Renaissance. Durham: Duke University Press. ISBN0-8223-1385-5.
Groves, Beatrice (2007). Texts and Traditions: Religion in Shakespeare, 15921604. Oxford, England: Oxford University Press. ISBN0-19-920898-0.
Halio, Jay (1998). Romeo and Juliet: A Guide to the Play. Westport: Greenwood Press. ISBN0-313-30089-5.
Halliday, F.E. (1964). A Shakespeare Companion 15641964. Baltimore: Penguin.
Higgins, David H., ed. (1998). The Divine Comedy. Oxford World Classics. translated by C. H. Sisson. Oxford University Press. ISBN0-19-283502-5.
Holden, Amanda (Ed.) (1993). The Viking Opera Guide. London: Viking. ISBN0-670-81292-7.
Holland, Peter (2001). "Shakespeare in the Twentieth-Century Theatre". In Wells, Stanley; deGrazia Margreta. The Cambridge Companion to Shakespeare. Cambridge: Cambridge University Press. pp.199215. ISBN0-521-65881-0.
Holland, Peter (2002). "Touring Shakespeare". In Wells, Stanley; Stanton, Sarah. The Cambridge Companion to Shakespeare on Stage. Cambridge: Cambridge University Press. pp.194211. ISBN978-0-521-79711-5.
Honegger, Thomas (2006). "'Wouldst thou withdraw love's faithful vow?': The negotiation of love in the orchard scene (Romeo and Juliet Act II)". Journal of Historical Pragmatics 7 (1): 7388. doi:10.1075/jhp.7.1.04hon.
Hosley, Richard (1965). Romeo and Juliet. New Haven: Yale University Press.
Howard, Tony (2000). "Shakespeare's Cinematic Offshoots". In Jackson, Russell (ed.). The Cambridge Companion to Shakespeare on Film. Cambridge: Cambridge University Press. pp.295313. ISBN0-521-63975-1.
Kahn, Copplia (1977). "Coming of Age in Verona". Modern Language Studies (The Northeast Modern Language Association) 8 (1): 522. doi:10.2307/3194631. ISSN00477729. JSTOR3194631.
Keeble, N.H. (1980). Romeo and Juliet: Study Notes. York Notes. Longman. ISBN0-582-78101-9.
Lanier, Douglas (2007). "Shakespeare: myth and biographical fiction". In Shaughnessy, Robert (ed.). The Cambridge Companion to Shakespeare and Popular Culture. Cambridge University Press. pp.93113. ISBN978-0-521-60580-9.
Levenson (ed.), Jill L. (2000). Romeo and Juliet. The Oxford Shakespeare (Oxford World's Classics). Oxford: Oxford University Press. ISBN0-19-281496-6.
Levin, Harry (1960). "Form and Formality in Romeo and Juliet". Shakespeare Quarterly (Folger Shakespeare Library) 11 (1): 311. doi:10.2307/2867423. JSTOR2867423.
Lucking, David (2001). "Uncomfortable Time In Romeo And Juliet". English Studies 82 (2): 115126. doi:10.1076/enst.82.2.115.9595.
Lujan, James (2005). "A Museum of the Indian, Not for the Indian". The American Indian Quarterly 29 (34): 510516. doi:10.1353/aiq.2005.0098. ISSN0095182X.
MacKenzie, Clayton G. (2007). "Love, sex and death in Romeo and Juliet". English Studies 88 (1): 2242. doi:10.1080/00138380601042675.
McKernan, Luke; Terris, Olwen (1994). Walking Shadows: Shakespeare in the National Film and Television Archive. London: British Film Institute. ISBN0-85170-486-7.
Marks, Peter (29 September 1997). "Juliet of the Five O'Clock Shadow, and Other Wonders". New York Times. http://query.nytimes.com/gst/fullpage.html?res=9501E1DE123AF93AA1575AC0A961958260&pagewanted=all. Retrieved 10 November 2008.
Marsden, Jean I. (2002). "Shakespeare from the Restoration to Garrick". In Wells, Stanley; Stanton, Sarah. The Cambridge Companion to Shakespeare on Stage. Cambridge: Cambridge University Press. pp.2136. ISBN978-0-521-79711-5.
Meyer, Eve R. (1968). "Measure for Measure: Shakespeare and Music". Music Educators Journal (The National Association for Music Education) 54 (7): 3638, 139143. doi:10.2307/3391243. ISSN00274321. JSTOR3391243.
Moore, Olin H. (1930). "The Origins of the Legend of Romeo and Juliet in Italy". Speculum (Medieval Academy of America) 5 (3): 264277. doi:10.2307/2848744. ISSN00387134. JSTOR2848744.
Moore, Olin H. (1937). "Bandello and "Clizia"". Modern Language Notes (Johns Hopkins University Press) 52 (1): 3844. doi:10.2307/2912314. ISSN01496611. JSTOR2912314.
Morrison, Michael A. (2007). "Shakespeare in North America". In Shaughnessy, Robert (ed.). The Cambridge Companion to Shakespeare and Popular Culture. Cambridge: Cambridge University Press. pp.230258. ISBN978-0-521-60580-9.
Muir, Kenneth (2005). Shakespeare's Tragic Sequence. New York: Routledge. ISBN978-0-415-35325-0.
Nestyev, Israel (1960). Prokofiev. Stanford: Stanford University Press.
Nevo, Ruth (1972). Tragic Form in Shakespeare. Princeton, NJ: Princeton University Press. ISBN0-691-06217-X.
"Shakespeare on the Drive". The New York Times. 19 August 1977.
Orgel, Stephen (2007). "Shakespeare Illustrated". In Shaughnessy, Robert (Ed.). The Cambridge Companion to Shakespeare and Popular Culture. Cambridge: Cambridge University Press. ISBN978-0-521-60580-9.
Parker, D.H. (1968). "Light and Dark Imagery in Romeo and Juliet". Queen's Quarterly 75 (4).
Pedicord, Harry William (1954). The Theatrical Public in the Time of David Garrick. New York: King's Crown Press.
Potter, Lois (2001). "Shakespeare in the Theatre, 16601900". In Wells, Stanley; deGrazia Margreta. The Cambridge Companion to Shakespeare. Cambridge: Cambridge University Press. pp.183198. ISBN0-521-65881-0.
Quince, Rohan (2000). Shakespeare in South Africa: Stage Productions During the Apartheid Era. New York: Peter Lang. ISBN978-0-8204-4061-3.
Roberts, Arthur J. (1902). "The Sources of Romeo and Juliet". Modern Language Notes (Johns Hopkins University Press) 17 (2): 4144. doi:10.2307/2917639. ISSN01496611. JSTOR2917639.
"Romeo  Definition from the MerriamWebster Online Dictionary". MerriamWebster. http://www.merriam-webster.com/dictionary/Romeo. Retrieved 16 August 2008.
Rosenthal, Daniel (2007). BFI Screen Guides: 100 Shakespeare Films. London: British Film Institute. ISBN978-1-84457-170-3.
Rubinstein, Frankie (1989). A Dictionary of Shakespeare's Sexual Puns and their Significance (Second Edition). London: Macmillan. ISBN0-333-48866-0.
Sadie, Stanley (1992). The New Grove Dictionary of Opera. Oxford: Oxford University Press. ISBN978-1-56159-228-9.
Sanders, Julie (2007). Shakespeare and Music: Afterlives and Borrowings. Cambridge: Polity Press. ISBN978-0-7456-3297-1.
Schoch, Richard W. (2002). "Pictorial Shakespeare". In Wells, Stanley; Stanton, Sarah. The Cambridge Companion to Shakespeare on Stage. Cambridge: Cambridge University Press. pp.6263. ISBN978-0-521-79711-5.
Scott, Mark W. (Ed.); Schoenbaum, S. (Ed.) (1987). Shakespearean Criticism. 5. Detroit: Gale Research Inc.. ISBN0-8103-6129-9.
Shapiro, Stephen A. (1964). "Romeo and Juliet: Reversals, Contraries, Transformations, and Ambivalence". College English (National Council of Teachers of English) 25 (7): 498501. doi:10.2307/373235. JSTOR373235.
Siegel, Paul N. (1961). "Christianity and the Religion of Love in Romeo and Juliet". Shakespeare Quarterly (Folger Shakespeare Library) 12 (4): 371392. doi:10.2307/2867455. JSTOR2867455.
Smallwood, Robert (2002). "Twentieth-century Performance: the Stratford and London companies". In Wells, Stanley; Stanton, Sarah. The Cambridge Companion to Shakespeare on Stage. Cambridge: Cambridge University Press. pp.98117. ISBN978-0-521-79711-5.
Spencer (ed.), T.J.B. (1967). Romeo and Juliet. The New Penguin Shakespeare. London: Penguin. ISBN978-0-14-070701-4.
Stites, Richard (Ed.) (1995). Culture and Entertainment in Wartime Russia. Bloomington: Indiana University Press. ISBN978-0-253-20949-8.
Stone, George Winchester Jr (1964). "Romeo and Juliet: The Source of its Modern Stage Career". Shakespeare Quarterly (Folger Shakespeare Library) 15 (2): 191206. doi:10.2307/2867891. JSTOR2867891.
Tanselle, G. Thomas (1964). "Time in Romeo and Juliet". Shakespeare Quarterly (Folger Shakespeare Library) 15 (4): 349361. doi:10.2307/2868092. JSTOR2868092.
Tatspaugh, Patricia (2000). "The tragedies of love on film". In Jackson, Russell. The Cambridge Companion to Shakespeare on Film. Cambridge: Cambridge University Press. pp.135159. ISBN0-521-63975-1.
Taylor, Gary (2002). "Shakespeare plays on Renaissance Stages". In Wells, Stanley; Stanton, Sarah. The Cambridge Companion to Shakespeare on Stage. Cambridge: Cambridge University Press. pp.120. ISBN978-0-521-79711-5.
Van Lennep, William (Ed.); Avery, Emmett L.; Scouten, Arthur H. (1965). The London Stage, 16601800. Carbondale: Southern Illinois University Press. http://www.personal.psu.edu/users/h/b/hb1/London%20Stage%202001/. Retrieved August 2008.
Wells, Stanley (2004). Looking for Sex in Shakespeare. Cambridge: Cambridge University Press. ISBN0-521-54039-9.
Winter, William (1893). The Life and Art of Edwin Booth. London: MacMillan and Co. http://www.archive.org/details/lifeartofedwinbo00mattuoft. Retrieved August 2008.
 Romeo and Juliet at Wikipedia books
Romeo and Juliet Plain vanilla text from Project Gutenberg
Romeo and Juliet HTML version at MIT
Romeo and Juliet HTML Annotated Play
Romeo and Juliet Full text with audio.
.
#(`*Macbeth*`)#.
Macbeth is a play written by William Shakespeare. It is considered one of his darkest and most powerful tragedies. Set in Scotland, the play dramatizes the corroding psychological and political effects produced when its protagonist, the Scottish lord Macbeth, chooses evil as the way to fulfill his ambition for power. He commits regicide to become king and then furthers his moral descent with a reign of murderous terror to stay in power, eventually plunging the country into civil war. In the end, he loses everything that gives meaning and purpose to his life before losing his life itself.
The play is believed to have been written between 1603 and 1607, and is most commonly dated 1606. The earliest account of a performance of what was probably Shakespeare's play is April 1611, when Simon Forman recorded seeing such a play at the Globe Theatre. It was first published in the Folio of 1623, possibly from a prompt book. It was most likely written during the reign of James I, who had been James VI of Scotland before he succeeded to the English throne in 1603. James was a patron of Shakespeares acting company, and of all the plays Shakespeare wrote during Jamess reign, Macbeth most clearly reflects the playwrights relationship with the sovereign.
Macbeth is Shakespeares shortest tragedy, and tells the story of a brave Scottish general named Macbeth who receives a prophecy from a trio of witches that one day he will become King of Scotland. Consumed by ambition and spurred to action by his wife, Macbeth murders King Duncan and takes the throne for himself. His reign is racked with guilt and paranoia, and he soon becomes a tyrannical ruler as he is forced to commit more and more murders to protect himself from enmity and suspicion. The bloodbath swiftly takes Macbeth and Lady Macbeth into realms of arrogance, madness, and death.
Shakespeare's source for the tragedy are the accounts of King Macbeth of Scotland, Macduff, and Duncan in Holinshed's Chronicles (1587), a history of England, Scotland and Ireland familiar to Shakespeare and his contemporaries. However, the play bears little relation to real events in Scottish history, as the historical Macbeth was an admired and able monarch.
In the backstage world of theatre, some believe that the play is cursed, and will not mention its title aloud, referring to it instead as "the Scottish play". Over the course of many centuries, the play has attracted some of the most renowned actors to the roles of Macbeth and Lady Macbeth. It has been adapted to film, television, opera, novels, comic books, and other media.
Listed below are the dramatis person for Macbeth:
The play opens amidst thunder and lightning, and the Three Witches decide that their next meeting shall be with Macbeth. In the following scene, a wounded sergeant reports to King Duncan of Scotland that his generalsMacbeth, who is the Thane of Glamis, and Banquohave just defeated the allied forces of Norway and Ireland, who were led by the traitorous Macdonwald and the Thane of Cawdor. Macbeth, the King's kinsman, is praised for his bravery and fighting prowess.
In the following scene, Macbeth and Banquo discuss the weather and their victory. Macbeth's first line is "So foul and fair a day I have not seen" (1.3.38).[nb 1] As they wander onto a heath, the Three Witches enter and have been waiting to greet them with prophecies. Though Banquo challenges them first, they address Macbeth, hailing him as "Thane of Glamis," "Thane of Cawdor," and that he shall "be King hereafter." Macbeth appears to be stunned to silence. When Banquo asks of his own fortunes, the witches inform him that he will father a line of kings, though he himself will not be one. While the two men wonder at these pronouncements, the witches vanish, and another thane, Ross, arrives and informs Macbeth of his newly bestowed title: Thane of Cawdor, as the previous Thane of Cawdor shall be put to death for his traitorous activities. The first prophecy is thus fulfilled, and Macbeth immediately begins to harbour ambitions of becoming king.
King Duncan welcomes and praises Macbeth and Banquo, and declares that he will spend the night at Macbeth's castle at Inverness; he also names his son Malcolm as his heir. Macbeth sends a message ahead to his wife, Lady Macbeth, telling her about the witches' prophecies. Lady Macbeth suffers none of her husbands uncertainty, and wishes him to murder Duncan in order to obtain kingship. When Macbeth arrives at Inverness, she overrides all of her husbands objections by challenging his manhood, and successfully persuades him to kill the king that very night. He and Lady Macbeth plan to get Duncans two chamberlains drunk so that they will black out; the next morning they will frame the chamberlains for the murder. They will be defenseless, as they will remember nothing.
While Duncan is asleep, Macbeth stabs him, despite his doubts and a number of supernatural portents, including a hallucination of a bloody dagger. He is so shaken that Lady Macbeth has to take charge. In accordance with her plan, she frames Duncan's sleeping servants for the murder by placing bloody daggers on them. Early the next morning, Lennox, a Scottish nobleman, and Macduff, the loyal Thane of Fife, arrive. A porter opens the gate and Macbeth leads them to the king's chamber, where Macduff discovers Duncan's body. In a supposed fit of anger, Macbeth murders the guards (in truth, he kills them to prevent them from claiming their innocence). Macduff is immediately suspicious of Macbeth, but does not reveal his suspicions publicly. Duncans sons Malcolm and Donalbain flee to England and Ireland, respectively, fearing that whoever killed Duncan desires their demise as well. The rightful heirs' flight makes them suspects and Macbeth assumes the throne as the new King of Scotland as a kinsman of the dead king. Banquo reveals this to the audience, and while skeptical of the new King Macbeth, remembers the witches' prophecy about how his own descendants would inherit the throne.
Despite his success, Macbeth, also aware of this part of the prophecy, remains uneasy. Macbeth invites Banquo to a royal banquet, where he discovers that Banquo and his young son, Fleance, will be riding out that night. Macbeth hires two men to kill them; a third murderer appears in the park before the murder. The assassins succeed in killing Banquo, but Fleance escapes. Macbeth becomes furious: as long as Fleance is alive, he fears that his power remains insecure. At the banquet, Macbeth invites his lords and Lady Macbeth to a night of drinking and merriment. Banquo's ghost enters and sits in Macbeth's place. Macbeth raves fearfully, startling his guests, as the ghost is only visible to himself. The others panic at the sight of Macbeth raging at an empty chair, until a desperate Lady Macbeth tells them that her husband is merely afflicted with a familiar and harmless malady. The ghost departs and returns once more, causing the same riotous anger in Macbeth. This time, Lady Macbeth tells the lords to leave, and they do so.
Macbeth, disturbed, visits the three witches once more and asks them to reveal the truth of their prophecies to him. To answer his questions, they summon horrible apparitions, each of which offers predictions and further prophecies to allay Macbeths fears. First, they conjure an armed head, which tells him to beware of Macduff (4.1.72). Second, a bloody child tells him that no one born of a woman shall be able to harm him. Thirdly, a crowned child holding a tree states that Macbeth will be safe until Great Birnam Wood comes to Dunsinane Hill. Macbeth is relieved and feels secure, because he knows that all men are born of women and forests cannot move. Macbeth also asks if Banquo's sons will ever reign in Scotland: the witches conjure a procession of eight crowned kings, all similar in appearance to Banquo, and the last carrying a mirror that reflects even more kings. Macbeth realizes that these are all Banquo's descendants having acquired kingship in numerous countries. After the witches perform a mad dance and leave, Lennox enters and tells Macbeth that Macduff has fled to England. Macbeth orders Macduff's castle be seized, and, most cruelly, sends murderers to slaughter Macduffs wife and children. Everyone in Macduff's castle is put to death, including Lady Macduff and their young son.
Meanwhile, Lady Macbeth becomes racked with guilt from the crimes she and her husband have committed. At night, in the kings palace at Dunsinane, a doctor and a gentlewoman discuss Lady Macbeths strange habit of sleepwalking. Suddenly, Lady Macbeth enters in a trance with a candle in her hand. Bemoaning the murders of Duncan, Lady Macduff, and Banquo, she tries to wash off imaginary bloodstains from her hands, all the while speaking of the terrible things she knows she pressed her husband to do. She leaves, and the doctor and gentlewoman marvel at her descent into madness. Her belief that nothing can wash away the blood on her hands is an ironic reversal of her earlier claim to Macbeth that [a] little water clears us of this deed (2.2.66).
In England, Macduff is informed by Ross that his "castle is surprised; [his] wife and babes / Savagely slaughter'd" (4.3.204-5). When this news of his familys execution reaches him, Macduff is stricken with grief and vows revenge. Prince Malcolm, Duncans son, has succeeded in raising an army in England, and Macduff joins him as he rides to Scotland to challenge Macbeths forces. The invasion has the support of the Scottish nobles, who are appalled and frightened by Macbeths tyrannical and murderous behavior. Malcolm leads an army, along with Macduff and Englishmen Siward (the Elder), the Earl of Northumberland, against Dunsinane Castle. While encamped in Birnam Wood, the soldiers are ordered to cut down and carry tree limbs to camouflage their numbers.
Before Macbeths opponents arrive, he receives news that Lady Macbeth has killed herself, causing him to sink into a deep and pessimistic despair and deliver his "Tomorrow, and tomorrow, and tomorrow" soliloquy (5.5.1728). Though he reflects on the brevity and meaninglessness of life, he nevertheless awaits the English and fortifies Dunsinane. He is certain that the witches prophecies guarantee his invincibility, but is struck numb with fear when he learns that the English army is advancing on Dunsinane shielded with boughs cut from Birnam Wood. Birnam Wood is indeed coming to Dunsinane, fulfilling half of the witches prophecy.
A battle culminates in the slaying of the young Siward and Macduff's confrontation with Macbeth, and the English forces overwhelm his army and castle. Macbeth boasts that he has no reason to fear Macduff, for he cannot be killed by any man born of woman. Macduff declares that he was "from his mother's womb / Untimely ripp'd" (5.8.1516), (i.e., born by Caesarean section) and was not "of woman born" (an example of a literary quibble), fulfilling the second prophecy. Macbeth realizes too late that he has misinterpreted the witches' words. Though he realizes that he is doomed, he continues to fight. Macduff kills and beheads him, thus fulfilling the first part of the prophecy.
Macduff carries Macbeth's head onstage and Malcolm discusses how order has been restored. His last reference to Lady Macbeth, however, reveals "'tis thought, by self and violent hands / Took off her life" (5.9.7172), leading most to assume that she committed suicide, but the method is undisclosed. Malcolm, now the King of Scotland, declares his benevolent intentions for the country and invites all to see him crowned at Scone.
Although Malcolm, and not Fleance, is placed on the throne, the witches' prophecy concerning Banquo ("Thou shalt get kings") was known to the audience of Shakespeare's time to be true: James VI of Scotland (later also James I of England) was supposedly a descendant of Banquo.[1]
Macbeth has been compared to Shakespeare's Antony and Cleopatra. Both Antony and Macbeth as characters seek a new world, even at the cost of the old one. Both are fighting for a throne and have a 'nemesis' to face to achieve that throne. For Antony the nemesis is Octavius, for Macbeth it is Banquo. At one point Macbeth even compares himself to Antony, saying "under Banquo / My Genius is rebuk'd, as it is said / Mark Antony's was by Caesar." Lastly, both plays contain powerful and manipulative female figures: Cleopatra and Lady Macbeth.[2]
Shakespeare borrowed the story from several tales in Holinshed's Chronicles, a popular history of the British Isles known to Shakespeare and his contemporaries. In Chronicles, a man named Donwald finds several of his family put to death by his king, King Duff, for dealing with witches. After being pressured by his wife, he and four of his servants kill the King in his own house. In Chronicles, Macbeth is portrayed as struggling to support the kingdom in the face of King Duncan's ineptitude. He and Banquo meet the three witches, who make exactly the same prophecies as in Shakespeare's version. Macbeth and Banquo then together plot the murder of Duncan, at Lady Macbeth's urging. Macbeth has a long, ten-year reign before eventually being overthrown by Macduff and Malcolm. The parallels between the two versions are clear. However, some scholars think that George Buchanan's Rerum Scoticarum Historia matches Shakespeare's version more closely. Buchanan's work was available in Latin in Shakespeare's day.[3]
No other version of the story has Macbeth kill the king in Macbeth's own castle. Scholars have seen this change of Shakespeare's as adding to the darkness of Macbeth's crime as the worst violation of hospitality. Versions of the story that were common at the time had Duncan being killed in an ambush at Inverness, not in a castle. Shakespeare conflated the story of Donwald and King Duff in what was a significant change to the story.[4]
Shakespeare made another revealing change. In Chronicles, Banquo is an accomplice in Macbeth's murder of King Duncan. He also plays an important part in ensuring that Macbeth, not Malcolm, takes the throne in the coup that follows.[5] In Shakespeare's day, Banquo was thought to be a direct ancestor of the Stuart King James I[6] (Banquo's Stuart descent was disproven in the 19th century, when it was discovered that the Fitzalans actually descended from a Breton family). The Banquo portrayed in historical sources is significantly different from the Banquo created by Shakespeare. Critics have proposed several reasons for this change. First, to portray the king's ancestor as a murderer would have been risky. Other authors of the time who wrote about Banquo, such as Jean de Schelandre in his Stuartide, also changed history by portraying Banquo as a noble man, not a murderer, probably for the same reasons.[7] Second, Shakespeare may have altered Banquo's character simply because there was no dramatic need for another accomplice to the murder; there was, however, a need to give a dramatic contrast to Macbetha role which many scholars argue is filled by Banquo.[5]
Macbeth cannot be dated precisely, owing to significant evidence of later revisions. As the play appears to celebrate King James's ancestors and the Stuart accession to the throne in 1603 (James believed himself to be descended from Banquo),[8] scholars say that the play is unlikely to have been composed earlier than 1603 and suggest that the parade of eight kingswhich the witches show Macbeth in a vision in Act IVis a compliment to King James. The vast majority of critics think the play was written in 1606 in the aftermath of the Gunpowder Plot because of possible internal allusions to the 1605 plot and its ensuing trials.[9] In 1605, to commemorate King James escape, a medal was struck picturing a serpent hiding among lilies and roses. Shakespeare may allude to the image when Lady Macbeth says to her husband, "Look like the innocent flower, but be the serpent under't" (1.5.74-5).[10] And the Porter's speech (2.3.121), in particular, may allude to the trial of the Jesuit Henry Garnet in spring, 1606; "equivocator" (line 8) may refer to Garnet's defence of "equivocation", and "farmer" (4) to one of Garnet's aliases.[11] However, "farmer" is a common word, and "equivocation" was also the subject of a 1583 tract by Queen Elizabeth's chief councillor Lord Burghley, and of the 1584 Doctrine of Equivocation by the Spanish prelate Martin Azpilcueta, which was disseminated across Europe and into England in the 1590s.[12]
Scholars also cite an entertainment seen by King James at Oxford in the summer of 1605 that featured three "sibyls" like the weird sisters; Kermode surmises that Shakespeare could have heard about this and alluded to it with the weird sisters.[13] However, A. R. Braunmuller in the New Cambridge edition finds the 16056 arguments inconclusive, and argues only for an earliest date of 1603.[14] The play is not considered to have been written any later than 1607, since, as Kermode notes, there are "fairly clear allusions to the play in 1607."[13]
Macbeth was first printed in the First Folio of 1623 and the Folio is the only source for the text. The text that survives had been plainly altered by later hands. Most notable is the inclusion of two songs from Thomas Middleton's play The Witch (1615); Middleton is conjectured to have inserted an extra scene involving the witches and Hecate, for these scenes had proven highly popular with audiences. These revisions, which since the Clarendon edition of 1869 have been assumed to include all of Act III, scene v, and a portion of Act IV, scene I, are often indicated in modern texts.[15] On this basis, many scholars reject all three of the interludes with the goddess Hecate as inauthentic. Even with the Hecate material, the play is conspicuously short, and so the Folio text may derive from a prompt book that had been substantially cut for performance, or an adapter cut the text himself.
A main theme within Macbeth is the destruction that follows when ambition goes beyond moral constraints. Macbeth is a brave general who is not naturally inclined to commit evil, yet he is deeply ambitious and desires power. He murders King Duncan against his better judgement and then wallows in guilt and paranoia. Toward the play's end, he is in a kind of boastful madness. Lady Macbeth pursues her goals with greater determination, yet is less capable of dealing with the guilt from her immorality. One of Shakespeare's most forceful female characters, she spurs her husband mercilessly to kill Duncan and urges him to be strong afterward, yet is herself eventually driven to death by the effect of Macbeth's murders on her conscience. In each case, ambition, spurred by the prophecies of the witches, is what drives the couple to commit their atrocities. An issue that the play raises is that once one decides to use violence to further one's quest for power, it is difficult to stop. Macbeth finds that there are always potential threats to the throne  such as Banquo, Fleance, and Macduff  and he is tempted to use violent means to dispose of them.
Lady Macbeth manipulates her husband by questioning his manhood, wishing herself to be unsexed, and does not contradict Macbeth when he says that a woman like her should give birth only to boys. In the same manner that Lady Macbeth goads her husband on to murder, Macbeth provokes the assassins he hires to murder Banquo by questioning their manhood. Such acts show that both Macbeth and Lady Macbeth equate masculinity with naked aggression; whenever they discuss manhood, violence follows. Their understanding of manhood allows the political order depicted in the play to descend into chaos.
However, in "Macbeth", women are prone to contain violence and evil intentions. The witches prophecies spark Macbeths ambitions and then encourage his violent behavior, while Lady Macbeth provides the drive and the will behind her husbands plotting. After reading the letter her husband has sent telling of the witches' prophecies about him, Lady Macbeth believes:
Furthermore, the only divine being to appear is Hecate, the goddess of witchcraft. Because "Macbeth" traces the root of chaos and evil to women, some critics argue that it is Shakespeares most misogynistic play. The male characters are similarly brutal and prone to evil as the women, but the aggression of the female characters is more striking because it contradicts expectations of how women ought to behave. Lady Macbeths behavior certainly shows that women can be just as ambitious and ruthless as men. Whether it is the gender constraints of her society or because she is not fearless enough to kill, Lady Macbeth relies on manipulation of her husband rather than violence to achieve her ends.
The play does put forth less destructive definition of manhood towards the end. When Macduff learns of the murders of his wife and child, Malcolm consoles him unsympathetically with encouragement to take the news in manly fashion and use it to fuel his hatred of Macbeth. Macduff tells the young heir apparent that he has a mistaken understanding of masculinity. To Malcolms suggestion, Dispute it like a man, Macduff replies, I shall do so. But I must also feel it as a man (4.3.221223). After hearing the news of his son's death at the hands of Macbeth, Siward receives this fact somewhat complacently. Malcolm responds: Hes worth more sorrow [than you have expressed] / And that Ill spend for him (5.11.1617). Malcolms comment shows that he has learned the lesson Macduff gave him on the feeling nature of true masculinity. It also suggests that, with Malcolms coronation, order will be restored to the Kingdom of Scotland.
Macbeth is an anomaly among Shakespeare's tragedies in certain critical ways. It is short: more than a thousand lines shorter than Othello and King Lear, and only slightly more than half as long as Hamlet. This brevity has suggested to many critics that the received version is based on a heavily cut source, perhaps a prompt-book for a particular performance. That brevity has also been connected to other unusual features: the fast pace of the first act, which has seemed to be "stripped for action"; the comparative flatness of the characters other than Macbeth; the oddness of Macbeth himself compared with other Shakespearean tragic heroes.
At least since the days of Alexander Pope and Samuel Johnson, analysis of the play has centred on the question of Macbeth's ambition, commonly seen as so dominant a trait that it defines the character. Johnson asserted that Macbeth, though esteemed for his military bravery, is wholly reviled. This opinion recurs in critical literature, and, according to Caroline Spurgeon, is supported by Shakespeare himself, who apparently intended to degrade his hero by vesting him with clothes unsuited to him and to make Macbeth look ridiculous by several nimisms he applies: His garments seem either too big or too small for him as his ambition is too big and his character too small for his new and unrightful role as king. When he feels as if "dressed in borrowed clothes", after his new title as Thane of Cawdor, prophesied by the witches, has been confirmed by Ross (I, 3, ll. 108109), Banquo comments: "New honours come upon him, / Like our strange garments, cleave not to their mould, / But with the aid of use" (I, 3, ll. 145146). And, at the end, when the tyrant is at bay at Dunsinane, Caithness sees him as a man trying in vain to fasten a large garment on him with too small a belt: "He cannot buckle his distemper'd cause / Within the belt of rule" (V, 2, ll. 1415), while Angus, in a similar nimism, sums up what everybody thinks ever since Macbeth's accession to power: "now does he feel his title / Hang loose about him, like a giant's robe / upon a dwarfish thief" (V, 2, ll. 1820).[16]
Like Richard III, but without that character's perversely appealing exuberance, Macbeth wades through blood until his inevitable fall. As Kenneth Muir writes, "Macbeth has not a predisposition to murder; he has merely an inordinate ambition that makes murder itself seem to be a lesser evil than failure to achieve the crown." Some critics, such as E. E. Stoll, explain this characterisation as a holdover from Senecan or medieval tradition. Shakespeare's audience, in this view, expected villains to be wholly bad, and Senecan style, far from prohibiting a villainous protagonist, all but demanded it.
Yet for other critics, it has not been so easy to resolve the question of Macbeth's motivation. Robert Bridges, for instance, perceived a paradox: a character able to express such convincing horror before Duncan's murder would likely be incapable of committing the crime. For many critics, Macbeth's motivations in the first act appear vague and insufficient. John Dover Wilson hypothesised that Shakespeare's original text had an extra scene or scenes where husband and wife discussed their plans. This interpretation is not fully provable; however, the motivating role of ambition for Macbeth is universally recognised. The evil actions motivated by his ambition seem to trap him in a cycle of increasing evil, as Macbeth himself recognises: "I am in blood/Stepp'd in so far that, should I wade no more,/Returning were as tedious as go o'er."
The disastrous consequences of Macbeth's ambition are not limited to him. Almost from the moment of the murder, the play depicts Scotland as a land shaken by inversions of the natural order. Shakespeare may have intended a reference to the great chain of being, although the play's images of disorder are mostly not specific enough to support detailed intellectual readings. He may also have intended an elaborate compliment to James's belief in the divine right of kings, although this hypothesis, outlined at greatest length by Henry N. Paul, is not universally accepted. As in Julius Caesar, though, perturbations in the political sphere are echoed and even amplified by events in the material world. Among the most often depicted of the inversions of the natural order is sleep. Macbeth's announcement that he has "murdered sleep" is figuratively mirrored in Lady Macbeth's sleepwalking.
Macbeth's generally accepted indebtedness to medieval tragedy is often seen as significant in the play's treatment of moral order. Glynne Wickham connects the play, through the Porter, to a mystery play on the harrowing of hell. Howard Felperin argues that the play has a more complex attitude toward "orthodox Christian tragedy" than is often admitted; he sees a kinship between the play and the tyrant plays within the medieval liturgical drama.
The theme of androgyny is often seen as a special aspect of the theme of disorder. Inversion of normative gender roles is most famously associated with the witches and with Lady Macbeth as she appears in the first act. Whatever Shakespeare's degree of sympathy with such inversions, the play ends with a thorough return to normative gender values. Some feminist psychoanalytic critics, such as Janet Adelman, have connected the play's treatment of gender roles to its larger theme of inverted natural order. In this light, Macbeth is punished for his violation of the moral order by being removed from the cycles of nature (which are figured as female); nature itself (as embodied in the movement of Birnam Wood) is part of the restoration of moral order.
Critics in the early twentieth century reacted against what they saw as an excessive dependence on the study of character in criticism of the play. This dependence, though most closely associated with Andrew Cecil Bradley, is clear as early as the time of Mary Cowden Clarke, who offered precise, if fanciful, accounts of the predramatic lives of Shakespeare's female leads. She suggested, for instance, that the child Lady Macbeth refers to in the first act died during a foolish military action.
In the play, the Three Witches represent darkness, chaos, and conflict, while their role is as agents and witnesses.[17] Their presence communicates treason and impending doom. During Shakespeare's day, witches were seen as worse than rebels, "the most notorious traytor and rebell that can be."[18] They were not only political traitors, but spiritual traitors as well. Much of the confusion that springs from them comes from their ability to straddle the play's borders between reality and the supernatural. They are so deeply entrenched in both worlds that it is unclear whether they control fate, or whether they are merely its agents. They defy logic, not being subject to the rules of the real world.[19] The witches' lines in the first act: "Fair is foul, and foul is fair: Hover through the fog and filthy air" are often said to set the tone for the rest of the play by establishing a sense of confusion. Indeed, the play is filled with situations where evil is depicted as good, while good is rendered evil. The line "Double, double toil and trouble," communicates the witches' intent clearly: they seek only trouble for the mortals around them.[20]
While the witches do not tell Macbeth directly to kill King Duncan, they use a subtle form of temptation when they tell Macbeth that he is destined to be king. By placing this thought in his mind, they effectively guide him on the path to his own destruction. This follows the pattern of temptation used at the time of Shakespeare. First, they argued, a thought is put in a man's mind, then the person may either indulge in the thought or reject it. Macbeth indulges in it, while Banquo rejects.[20]
According to J. A. Bryant Jr., Macbeth also makes use of Biblical parallels, notably between King Duncan's murder and the murder of Christ:
While many today would say that any misfortune surrounding a production is mere coincidence, actors and other theatre people often consider it bad luck to mention Macbeth by name while inside a theatre, and sometimes refer to it indirectly, for example as "the Scottish play",[23] or "MacBee", or when referring to the character and not the play, "Mr. and Mrs. M", or "The Scottish King".
This is because Shakespeare is said to have used the spells of real witches in his text, purportedly angering the witches and causing them to curse the play.[24] Thus, to say the name of the play inside a theatre is believed to doom the production to failure, and perhaps cause physical injury or death to cast members. There are stories of accidents, misfortunes and even deaths taking place during runs of Macbeth.[23]
One particular incident that lent itself to the superstition was the Astor Place Riot. The cause of the riots was based on a conflict over two performances of Macbeth, and is usually ascribed to the curse.[25]
Several methods exist to dispel the curse, depending on the actor. One, attributed to Michael York, is to immediately leave the building the stage is in with the person who uttered the name, walk around it three times, spit over their left shoulders, say an obscenity then wait to be invited back into the building.[26] A related practice is to spin around three times as fast as possible on the spot, sometimes accompanied by spitting over their shoulder, and uttering an obscenity. Another popular "ritual" is to leave the room, knock three times, be invited in, and then quote a line from Hamlet. Yet another is to recite lines from The Merchant of Venice, thought to be a lucky play.[27]
The earliest account of a performance of the play is April 1611, when Simon Forman recorded seeing it at the Globe Theatre. Apart from that, there are no performances known with certainty in Shakespeare's era. Because of its Scottish theme, the play is sometimes said to have been written for, and perhaps debuted for, King James; however, no external evidence supports this hypothesis. The play's brevity and certain aspects of its staging (for instance, the large proportion of night-time scenes and the unusually large number of off-stage sounds) have been taken as suggesting that the text now extant was revised for production indoors, perhaps at the Blackfriars Theatre, which the King's Men acquired in 1608.[nb 2]
In the Restoration, Sir William Davenant produced a spectacular "operatic" adaptation of Macbeth, "with all the singing and dancing in it" and special effects like "flyings for the witches" (John Downes, Roscius Anglicanus, 1708). Davenant's revision also enhanced the role of Lady Macduff, making her a thematic foil to Lady Macbeth.[28] In an 19 April 1667, entry in his Diary, Samuel Pepys called Davenant's MacBeth "one of the best plays for a stage, and variety of dancing and music, that ever I saw."[28] The Davenant version held the stage until the middle of the next century. The famous Macbeths of the early 18th century, such as James Quin, employed this version.
David Garrick returned much closer to the Shakespearean original in a 1744 production.[29] He restored much of Shakespeare's language, which Davenant had simplified, and restored most of the characters to their original roles. However, he retained the witches' songs and added a moralizing speech for Macbeth to the conclusion. Garrick's Macbeth was celebrated; Thomas Davies claims that when the Duke of Parma asked Garrick to demonstrate his acting ability, he acted the scene of Banquo's ghost. Garrick's Lady Macbeth was Hannah Pritchard, and he did not act the role after her death in 1768.
Charles Macklin, not otherwise recalled as a great Macbeth, is remembered for performances at Covent Garden in 1773 at which riots broke out, related to Macklin's rivalries with Garrick and William Smith. Macklin performed in Scottish dress, reversing an earlier tendency to dress Macbeth as an English brigadier; he also removed Garrick's death speech and further trimmed Lady Macduff's role. The performance received generally respectful reviews, although George Steevens remarked on the inappropriateness of Macklin (then in his eighties) for the role.
After Garrick, the most celebrated Macbeth of the 18th century was John Philip Kemble; he performed the role most famously with his sister, Sarah Siddons, whose Lady Macbeth was widely regarded as unsurpassable. Kemble continued the trends toward realistic costume and to Shakespeare's language that had marked Macklin's production; Walter Scott reports that he experimented continually with the Scottish dress of the play. Response to Kemble's interpretation was divided; however, Siddons was unanimously praised. Her performance of the "sleepwalking" scene in the fifth act was especially noted; Leigh Hunt called it "sublime." The Kemble-Siddons performances were the first widely influential productions in which Lady Macbeth's villainy was presented as deeper and more powerful than Macbeth's. It was also the first in which Banquo's ghost did not appear on stage.
Kemble's Macbeth struck some critics as too mannered and polite for Shakespeare's text. His successor as the leading actor of London, Edmund Kean, was more often criticised for emotional excess, particularly in the fifth act. Kean's Macbeth was not universally admired; William Hazlitt, for instance, complained that Kean's Macbeth was too like his Richard III. As he did in other roles, Kean exploited his athleticism as a key component of Macbeth's mental collapse. He reversed Kemble's emphasis on Macbeth as noble, instead presenting him as a ruthless politician who collapses under the weight of guilt and fear. Kean, however, did nothing to halt the trend toward extravagance in scene and costume.
The Macbeth of the next predominant London actor, William Charles Macready, provoked responses at least as mixed as those given Kean. Macready debuted in the role in 1820 at Covent Garden. As Hazlitt noted, Macready's reading of the character was purely psychological; the witches lost all supernatural power, and Macbeth's downfall arose purely from the conflicts in Macbeth's character. Macready's most famous Lady Macbeth was Helena Faucit, who debuted dismally in the role while still in her mid-20s, but who later achieved acclaim in the role for an interpretation that, unlike Siddons', accorded with contemporary notions of female decorum. After Macready "retired" to America, he continued to perform in the role; in 1849, he was involved in a rivalry with American actor Edwin Forrest, whose partisans hissed Macready at Astor Place, leading to what is commonly called the Astor Place Riot.
The two most prominent Macbeths of mid-century, Samuel Phelps and Charles Kean, were both received with critical ambivalence and popular success. Both are famous less for their interpretation of character than for certain aspects of staging. At Sadler's Wells Theatre, Phelps brought back nearly all of Shakespeare's original text. He brought back the first half of the Porter scene, which had been ignored by directors since Davenant; the second remained cut because of its ribaldry. He abandoned the added music, and reduced the witches to their role in the folio. Just as significantly, he returned to the folio treatment of Macbeth's death.[30] Not all of these decisions succeeded in the Victorian context, and Phelps experimented with various combinations of Shakespeare and Davenant in his more than a dozen productions between 1844 and 1861. His most successful Lady Macbeth was Isabella Glyn, whose commanding presence reminded some critics of Siddons.
The outstanding feature of Kean's productions at the Princess's Theatre after 1850 was their accuracy of costume. Kean achieved his greatest success in modern melodrama, and he was widely viewed as not prepossessing enough for the greatest Elizabethan roles. Audiences did not mind, however; one 1853 production ran for twenty weeks. Presumably part of the draw was Kean's famous attention to historical accuracy; in his productions, as Allardyce Nicoll notes, "even the botany was historically correct."
Henry Irving's first attempt at the role, at the Lyceum Theatre, London in 1875, was a failure. Under the production of Sidney Frances Bateman, and starring alongside Kate Josephine Bateman, Irving may have been affected by the recent death of his manager Hezekiah Linthicum Bateman. Although the production lasted eighty performances, his Macbeth was judged inferior to his Hamlet. His next essay, opposite Ellen Terry at the Lyceum in 1888, fared better, playing for 150 performances.[31] At the urging of Herman Klein, Irving engaged Arthur Sullivan to write a suite of incidental music for the piece.[32] Friends such as Bram Stoker defended his "psychological" reading, based on the supposition that Macbeth had dreamed of killing Duncan before the start of the play. His detractors, among them Henry James, deplored his arbitrary word changes such as "would have" for "should have" in the speech at Lady Macbeth's death, and also his "neurasthenic" and "finicky" approach to the character.[33]
Barry Vincent Jackson staged an influential modern-dress production with the Birmingham Repertory in 1928; the production reached London, playing at the Royal Court Theatre. It received mixed reviews; Eric Maturin was judged an inadequate Macbeth, though Mary Merrall's vampish Lady was reviewed favourably. Though The Times judged it a "miserable failure," the production did much to overturn the tendency to scenic and antiquarian excess that had peaked with Charles Kean.
Among the most publicised productions of the 20th century was mounted by the Federal Theater Project at the Lafayette Theatre in Harlem from 14 April to 20 June 1936. Orson Welles, in his first stage production, directed Jack Carter and Edna Thomas, with Canada Lee playing Banquo, in an all African American production. It became known as the Voodoo Macbeth, because Welles set the play in post-colonial Haiti. His direction emphasised spectacle and suspense: his dozens of "African" drums recalled Davenant's chorus of witches. Welles later directed and played the starring role in a 1948 film adaptation of the play Macbeth.
Laurence Olivier played Malcolm in the 1929 production and Macbeth in 1937, at the Old Vic Theatre, in a production that saw the Vic's artistic director Lilian Baylis pass away the night before it opened. Olivier's makeup was so thick and stylised for that production that Vivien Leigh was quoted as saying "You hear Macbeth's first line, then Larry's makeup comes on, then Banquo comes on, then Larry comes on".[34] Olivier later starred in what is among the most famous 20th-century productions, by Glen Byam Shaw at Stratford-upon-Avon in 1955. Vivien Leigh played Lady Macbeth. The supporting cast, which Harold Hobson denigrated, included many actors who went on to successful Shakespearean careers: Ian Holm played Donalbain, Keith Michell was Macduff, and Patrick Wymark the Porter. Olivier was the key to success. The intensity of his performance, particularly in the conversation with the murderers and in confronting Banquo's ghost, seemed to many reviewers to recall Edmund Kean. Plans for a film version faltered after the box-office failure of Olivier's Richard III. Kenneth Tynan asserted flatly of this performance that "no one has ever succeeded as Macbeth"until Olivier.
Olivier's co-star in his 1937 Old Vic Theatre production, Judith Anderson, had an equally triumphant association with the play. She played Lady Macbeth on Broadway opposite Maurice Evans in a production directed by Margaret Webster that ran for 131 performances in 1941, the longest run of the play in Broadway history. Anderson and Evans performed the play on television twice, in 1954 and 1962, with Maurice Evans winning an Emmy Award the 1962 production and Anderson winning the award for both presentations. A film adaptation in 1971 titled The Tragedy of Macbeth was directed by Roman Polanski and executive-produced by Hugh Hefner.
A Japanese film adaptation, Throne of Blood (Kumonosu j, 1957), features Toshir Mifune in the lead role and is set in feudal Japan. It was well received, and, despite having almost none of the play's script, critic Harold Bloom called it "the most successful film version of Macbeth."[35]
One of the most notable 20th-century productions is that of Trevor Nunn for the Royal Shakespeare Company in 1976. Nunn had directed Nicol Williamson and Helen Mirren in the play two years earlier, but that production had largely failed to impress. In 1976, Nunn produced the play with a minimalist set at The Other Place; this small, nearly round stage focused attention on the psychological dynamics of the characters. Both Ian McKellen in the title role and Judi Dench as Lady Macbeth received exceptionally favourable reviews. Dench won the 1977 SWET Best Actress award for her performance and in 2004, members of the RSC voted her performance the greatest by an actress in the history of the company.
Nunn's production transferred to London in 1977 and was later filmed for television. It was to overshadow Peter Hall's 1978 production with Albert Finney as Macbeth and Dorothy Tutin as Lady Macbeth. An infamous version was staged at the Old Vic in 1980. Peter O'Toole and Frances Tomelty took the leads in a production (by Bryan Forbes) that was publicly disowned by Timothy West, artistic director of the theatre, before opening night, despite being a sellout because of its notoriety."[36] As critic Jack Tinker noted in the Daily Mail: "The performance is not so much downright bad as heroically ludicrous."[37]
On the stage, Lady Macbeth is considered one of the more "commanding and challenging" roles in Shakespeare's work.[38] Other actresses who have played the role include Gwen Ffrangcon-Davies, Janet Suzman, Glenda Jackson, and Jane Lapotaire.
In 2001 the film Scotland, PA was released. The action is moved to 1970s Pennsylvania and revolves around Joe Macbeth and his wife Pat taking control of a hamburger cafe from Norm Duncan. The film was directed by Billy Morrissette and stars James LeGros, Maura Tierney and Christopher Walken.
A performance was staged in Macbeth's home of Moray, produced by the National Theatre of Scotland, to take place at Elgin Cathedral. Professional actors, dancers, musicians, school children, and a community cast from the Moray area all took part in what was an important event in the Highland Year of Culture (2007).
In the same year, there was general consent among critics that Rupert Goold's production for the Chichester Festival 2007, starring Patrick Stewart and Kate Fleetwood, rivalled Trevor Nunn's acclaimed 1976 RSC production. And when it transferred to the Gielgud Theatre in London, Charles Spencer reviewing for the Daily Telegraph pronounced it the best Macbeth he had ever seen.[39] At the Evening Standard Theatre Awards 2007 the production won both the Best Actor award for Stewart, and the Best Director award for Goold.[40] The same production opened in the US at the Brooklyn Academy of Music in 2008, moving to Broadway (Lyceum Theatre) after a sold-out run. In 2010 a film version of their production Macbeth was produced for television. Goold again directed and Stewart and Fleetwood starred. It aired as part of PBS' Great Performances series on 6 October 2010 and on BBC Four on 12 December 2010.
In 2003, the British theatre company Punchdrunk used The Beaufoy Building in London, an old Victorian school to stage "Sleep No More", the story of Macbeth in the style of a Hitchcock thriller, using reworked music from the soundtrack of classic Hitchcock films.[41] Punchdrunk re-mounted the production, in a newly expanded version, at an abandoned school in Brookline, Massachusetts in October 2009 in association with the American Repertory Theatre,[42] and again in 2011 in New York City in the McKittrick Hotel, as Sleep No More (2011 theatrical production).[43]
In 2004, Indian director Vishal Bharadwaj directed his own adaptation to Macbeth, titled Maqbool. Set in the contemporary Mumbai underworld, the movie starred Irrfan Khan, Tabu, Pankaj Kapur, Om Puri, Naseeruddin Shah and Piyush Mishra in prominent roles.
In 2008, Pegasus Books published The Tragedy of Macbeth Part II: The Seed of Banquo, a play by American author and playwright Noah Lukeman which endeavoured to pick up where the original Macbeth left off, and to resolve its many loose ends.
David Greig's 2010 play Dunsinane took Macbeth's downfall at Dunsinane as its starting point, with Macbeth's just-ended reign portrayed as long and stable in contrast to Malcolm's.[citation needed]

1 Characters
2 Plot
3 Sources
4 Date and text
5 Themes and motifs

5.1 Ambition
5.2 Masculinity


5.1 Ambition
5.2 Masculinity
6 Analysis

6.1 As a tragedy of character
6.2 As a tragedy of moral order
6.3 As a poetic tragedy
6.4 Witchcraft and evil


6.1 As a tragedy of character
6.2 As a tragedy of moral order
6.3 As a poetic tragedy
6.4 Witchcraft and evil
7 Superstition and "the Scottish play"
8 Performance history

8.1 Shakespeare's day
8.2 Restoration and 18th century
8.3 Nineteenth century
8.4 Twentieth century to present


8.1 Shakespeare's day
8.2 Restoration and 18th century
8.3 Nineteenth century
8.4 Twentieth century to present
9 Sequels by other authors
10 Notes

10.1 Footnotes
10.2 Citations


10.1 Footnotes
10.2 Citations
11 References
12 External links

12.1 Performances
12.2 Audio recording
12.3 Text of play
12.4 Commentary
12.5 Maps


12.1 Performances
12.2 Audio recording
12.3 Text of play
12.4 Commentary
12.5 Maps
5.1 Ambition
5.2 Masculinity
6.1 As a tragedy of character
6.2 As a tragedy of moral order
6.3 As a poetic tragedy
6.4 Witchcraft and evil
8.1 Shakespeare's day
8.2 Restoration and 18th century
8.3 Nineteenth century
8.4 Twentieth century to present
10.1 Footnotes
10.2 Citations
12.1 Performances
12.2 Audio recording
12.3 Text of play
12.4 Commentary
12.5 Maps
Duncan  King of Scotland

Malcolm  Duncan's eldest son
Donalbain  Duncan's youngest son


Malcolm  Duncan's eldest son
Donalbain  Duncan's youngest son
Macbeth  a general in the army of King Duncan; originally Thane of Glamis, then Thane of Cawdor, and later King of Scotland
Lady Macbeth  Macbeth's wife and later Queen of Scotland
Banquo  Macbeth's friend and a general in the army of King Duncan

Fleance  Banquo's son


Fleance  Banquo's son
Macduff  Thane of Fife

Lady Macduff  Macduff's wife
Macduff's son


Lady Macduff  Macduff's wife
Macduff's son
Malcolm  Duncan's eldest son
Donalbain  Duncan's youngest son
Fleance  Banquo's son
Lady Macduff  Macduff's wife
Macduff's son
Ross, Lennox, Angus, Menteith, Caithness  Scottish Thanes
Siward  General of the English forces

Young Siward  Siward's son


Young Siward  Siward's son
Seyton  Macbeth's servant and attendant
Hecate  Queen of the witches
Three Witches  make the prediction of Macbeth becoming a King and Banquo's descendants being kings
Three Murderers
Porter  gatekeeper at Macbeth's home
Doctor  Lady Macbeth's doctor
Gentlewoman  Lady Macbeth's caretaker
Young Siward  Siward's son
Adams, J[oseph] Q[uincy] (1917). Shakespearean Playhouses. Houghton Mifflin.
Bald, R[obert] C[ecil] (1928). Review of English Studies (Oxford University Press) 4: 42931.
Barnet, Sylvan (1998). "Macbeth on Stage and Screen". In Barnet, Sylvan. Macbeth. Signet Classics. New American Library. pp.186200. ISBN978-0451524447.
Bentley, G[erald] E[ades] (1941). The Jacobean and Caroline Stage. 6. Clarendon Press.
Bloom, Harold (1999). Shakespeare: The Invention of the Human. ISBN1-57322-751-X.
Braunmuller, A[lbert] R. (1997). "Introduction". In Braunmuller, A[lbert] R.. Macbeth. Cambridge University Press. pp.193. ISBN0-521-22340-7. http://books.google.com/books?id=dvn3uent7VcC. Retrieved 16 August 2012.
Brooke, Nicholas (2008). "Introduction". In Brooke, Nicholas. Macbeth. Oxford University Press. pp.182. ISBN978-0-19-953583-5. http://books.google.com/books?id=HKRAAQAAIAAJ&q. Retrieved 16 August 2012.
Brown, Langdon (1986). Shakespeare around the Globe: A Guide to Notable Postwar Revivals. Greenwood Press.
Bryant, Jr., J. A. (1961). Hippolytas View: Some Christian Aspects of Shakespeares Plays. University of Kentucky Press. http://archive.org/details/hippolytasviewso012763mbp. Retrieved 1 November 2009.
Chambers, E. K. (1923). The Elizabethan Stage. 2. Clarendon Press. ISBN0-19-811511-3. OCLC336379.
Coddon, Karin S. (1989). "'Unreal Mockery': Unreason and the Problem of Spectacle in Macbeth". ELH (Johns Hopkins University) 56 (3): 485501. doi:10.2307/2873194.
Coursen, H[erbert] R. (1997). Macbeth: A Guide to the Play. Greenwood Press. ISBN0-313-30047-X. http://books.google.com/books?id=OVdlAAAAMAAJ. Retrieved 15 August 2012.
Dunning, Brian. "Toil and Trouble: The Curse of Macbeth". Skeptoid: Critical analysis of Pop Phenomena. Skeptoid.com. http://skeptoid.com/episodes/4222. Retrieved 28 November 2010.
Faires, Robert (13 October 2000). "The curse of the play". The Austin Chronicle. http://www.austinchronicle.com/arts/2000-10-13/78882/. Retrieved 19 August 2012.
Garber, Marjorie B. (2008). Profiling Shakespeare. Routledge. ISBN978-0-415-96446-3.
Halliday, F. E. (1964). A Shakespeare Companion 1564-1965. Penguin.
Kermode, Frank (1974). "Macbeth". In Evans, C. Blakemore. The Riverside Shakespeare. Houghton Mifflin. pp.130711. ISBN0-395-04402-2.
Kliman, Bernice; Santos, Rick (2005). Latin American Shakespeares. Fairleigh Dickinson University Press. ISBN0-8386-4064-8.
Maskell, D[avid] W. (1971). "The Transformation of History into Epic: The 'Stuartide' (1611) of Jean de Schelandre". Modern Language Review (Modern Humanities Research Association) 66: 5365. doi:10.2307/3722467.
Muir, Kenneth (1984) [1951]. "Introduction". In Muir, Kenneth. Macbeth (11 ed.). The Arden Shakespeare, Second Series. pp.xiiilxv. ISBN978-1-90-343648-6.
Nagarajan, S. (1956). "A Note on Banquo". Shakespeare Quarterly (Folger Shakespeare Library) 7 (4): 3716.
Palmer, J. Foster (1886). "The Celt in Power: Tudor and Cromwell". Transactions of the Royal Historical Society (Royal Historical Society) 3: 34370. doi:10.2307/3677851.
Paul, Henry Neill (1950). The Royal Play of Macbeth: When, Why, and How It Was Written by Shakespeare. Macmillan Publishers.
Perkins, William (1618). A Discourse of the Damned Art of Witchcraft, So Farre forth as it is revealed in the Scriptures, and manifest by true experience. Cambridge University Press. http://digital.library.cornell.edu/cgi/t/text/text-idx?c=witch;idno=wit075. Retrieved 24 June 2009.
Shirley, Francis (1963). Shakespeare's Use of Off-stage Sounds. University of Nebraska Press.
Spurgeon, Caroline (1969). "Shakespeare's Imagery and What It Tells Us". In Wain, John. Shakespeare: Macbeth: A Casebook. Casebook Series, AC16. Macmillan. pp.168177. ISBN0-876-95051-9.
Straczynski, J. Michael (2006). Babylon 5  The Scripts of J. Michael Straczynski, Vol. 6. Synthetic World.
Tanitch, Robert (2007). London Stage in the 20th Century. Haus Publishing. ISBN978-1-904950-74-5.
Tanitch, Robert (1985). Olivier. Abbeville Press Publishing.
Tritsch, Dina (April 1984). "The Curse of 'Macbeth'. Is there an evil spell on this ill-starred play?". Showbill (Playbill). http://pretallez.com/onstage/theatre/broadway/macbeth/macbeth_curse.html. Retrieved 28 November 2010.
Zagorin, Perez (1996). "The Historical Significance of Lying and DissimulationTruth-Telling, Lying, and Self-Deception". Social Research (The New School for Social Research) 63 (3): 863912.
Footage of Orson Welles' landmark "Voodoo Macbeth"  with informative annotations
Performances and Photographs from London and Stratford performances of Macbeth 19602000  From the Designing Shakespeare resource
The Shakespeare Video Society edition (Google Video  2 hours 12 mins)
Macbeth on Film
PBS Video directed by Rupert Goold starring Sir Patrick Stewart
Macbeth: Free Full-length Recording on ejunto.com
Annotated Text at The Shakespeare Project - annotated HTML version of Macbeth.
Macbeth Navigator  searchable, annotated HTML version of Macbeth.
The Complete Works of William Shakespeare  Entire play in basic HTML.
Classic Literature Library  HTML version of Macbeth.
Project Gutenberg: Macbeth  ASCII plain-text from Project Gutenberg
shakespeareNet  Act by Act summary of Macbeth
No Fear Shakespeare  By Sparknotes  Original Text and a Modern Translation side-by-side
Macbeth Analysis and Textual Notes
Annotated Bibliography of Macbeth Criticism
Shakespeare and the Uses of Power by Steven Greenblatt
Macbeth's Scotland on Google Maps This map shows some of the primary locations in Macbeth.
.
#(`*Lord of the Flies*`)#.
Lord of the Flies is a novel by Nobel Prize-winning English author William Golding about a group of British boys stuck on an uninhabited island who try to govern themselves, with disastrous results. Its stances on the already controversial subjects of human nature and individual welfare versus the common good earned it position 68 on the American Library Associations list of the 100 most frequently challenged books of 19901999.[2] In 2005 the novel was chosen by TIME magazine as one of the 100 best English-language novels from 1923 to 2005.[3] It was awarded a place on both lists of Modern Library 100 Best Novels, reaching number 41 on the editor's list, and 25 on the reader's list. In 2003, the novel was listed at number 70 on the BBC's survey The Big Read.[4]
Published in 1954, Lord of the Flies was Goldings first novel. Although it was not a great success at the time selling fewer than 3,000 copies in the United States during 1955 before going out of print it soon went on to become a best-seller, and by the early 1960s was required reading in many schools and colleges; the novel is currently renowned for being a popular choice of study for GCSE English Literature courses in the United Kingdom. It has been adapted to film twice in English, in 1963 by Peter Brook and 1990 by Harry Hook, and once in Filipino (1976).
The book indicates that it takes place in the midst of an unspecified nuclear war. Some of the marooned characters are ordinary students, while others arrive as a musical choir under an established leader. Most (with the exception of the choirboys) appear never to have encountered one another before. The book portrays their descent into savagery; left to themselves in a paradisiacal country, far from modern civilisation, the well-educated children regress to a primitive state.
At an allegorical level, the central theme is the conflicting impulses toward civilization live by rules, peacefully and in harmony and towards the will to power. Themes include the tension between groupthink and individuality, between rational and emotional reactions, and between morality and immorality. How these play out, and how different people feel the influences of these, form a major subtext of Lord of the Flies.[5]
In the midst of a wartime evacuation, a British plane crashes onto an isolated island in a remote region of the Pacific Ocean. The only survivors are male children below the age of thirteen.[6] Two boys, the fair-haired Ralph and an overweight, bespectacled boy reluctantly nicknamed "Piggy" find a conch, which Ralph uses as a horn to bring all the survivors to one area. Ralph emerges as one of the survivors' leaders during the meeting, as does Jack Merridew, a member of a boys' choir that survived the crash. The survivors elect Ralph as their "chief", losing only the votes of Jack's fellow choirboys, who support their leader. Ralph asserts two primary goals: to have fun and to maintain a smoke signal that could alert passing ships to their presence on the island. The boys decide that a conch shell they found embodies the society they shall create on the island, and declare that whoever holds the conch shall also receive the respect of the larger group.
Jack organises his choir group into a hunting party responsible for discovering a food source; Ralph, Jack, and a boy named Simon soon form a troika of leaders. Piggy, although Ralph's only confidante, is quickly made an outcast by his fellow "biguns" (older boys) and becomes an unwilling source of laughs for the other children. Simon, in addition to supervising the project of constructing shelters, feels an instinctive need to protect the younger boys.
The semblance of order imposed by Ralph and Simon quickly deteriorates as the majority of the boys turn idle and begin to develop paranoias about the island, referring to a supposed monster, the "beast", which dwells nearby. Jack, who has started a power struggle with Ralph, gains control of the discussion by boldly promising to kill the beast. At one point, Jack summons all of his hunters to hunt down a wild pig, drawing away those assigned to maintain the signal fire. After the fire burns out, a ship passes by the island, but does not stop as it has seen nothing amiss. Angered by this, Ralph considers relinquishing his position, but is convinced not to do so by Piggy.
While Jack schemes against Ralph, twins Sam and Eric, now assigned to the maintenance of the signal fire, see the corpse of a fighter pilot in the dark. Mistaking the corpse for the beast, they run to the cluster of shelters that Ralph and Simon have erected and warn the others. This unexpected meeting sees tensions between Jack and Ralph flare again. Shortly thereafter, Jack decides to lead a party to the other side of the island, where a heap of stones forms a place where he claims the beast resides. Only Ralph and Jack's supporter Roger agree to go; Ralph turns back shortly before the other two boys. When they arrive at the shelters, Jack calls an assembly and tries to turn the others against Ralph, asking for them to remove him from his position. Receiving little support, Jack, Roger, and another boy leave the shelters to form their own tribe. The tribe, which receives recruits from the main group of boys, grows in strength and begins to adopt customs common to primitive cultures, including face paint and bizarre rituals including sacrifices to the beast. When the tribe grows to a size that rivals Ralph's, they begin to harass those who remain at the shelters and make pronouncements encouraging them to abandon Ralph and the societal order he has imposed.
Simon, unable to bear the stress of his position, goes off to think. Alone, he finds a severed pig head, left by Jack as an offering to the beast. Simon envisions the pig head, now swarming with scavenging flies, as the "Lord of the Flies" and believes that it is talking to him. Simon hears the pig identifying itself as the real "Beast" and disclosing the truth about itself that the boys themselves "created" the beast, and that the real beast was inside them all. Simon also locates the dead parachutist who had been mistaken for the beast, and is the sole member of the group to recognise that the "monster" is a cadaver. Simon, hoping to tell others of the discovery, finds Jack's tribe in the island's interior during a ritual dance and, mistaken for the beast, is killed by the frenzied boys. Ralph, Piggy, Sam, and Eric feel guilty about what they did not stop.
Jack and his band of "savages" decide that they should possess Piggy's glasses, the only means of starting a fire on the island. Raiding Ralph's camp, the savages confiscate the glasses and return to their abode near the great rock heap, called Castle Rock. Ralph, deserted by most of his supporters, journeys to Castle Rock to confront Jack and secure the glasses. Taking the conch and accompanied only by Piggy, Sam, and Eric, Ralph finds the tribe and demands that they return the valuable object. Turning against Ralph, the tribe takes Sam and Eric captive while Roger drops a boulder from his vantage point above, killing Piggy and shattering the conch. Ralph manages to escape, but Sam and Eric are tortured until they agree to join Jack's tribe.
The following morning, Jack orders his tribe to begin a manhunt for Ralph. Fleeing through the forest, Ralph watches as the "savages" set fire to the forest, drawing the attention of a passing naval vessel. A landing party from the vessel encounters Ralph as he desperately tries to escape the onrushing "savages", and the British officer leading the party, at first mistaking the violence for a game, expected better from British boys.
When he and the others arrive on the island, Ralph quickly becomes the chief of the group, not by any harsh, overt or physical action, but by being elected.[7] Ralph is described as having "the directness of genuine leadership".[8] Ralph's first big decision is that they have "got to decide if this is an island".[9] After Ralph, Jack, and Simon discover that they are truly "on an uninhabited island",[10] Ralph suggests that a fire be lit because "if a ship comes near the island they may not notice us".[11] However, towards the end of the book he forgets the initial reason for maintaining the fire. This is representative of the debilitating effects corruption has even on the brightest mind. Ralph may seem to mean well, but often his obsession with being popular overcomes him and he resorts to bullying Piggy to regain his power. Still, in the midst of all the island's chaos, it should be noted that Ralph has a tendency to be polite and logical in the tensest of moments; for example, when the children are obliged to investigate Castle Rock, Ralph takes the lead despite being afraid of "the beast". Ralph is sometimes perceived as partially being a literary tool to aid the audience's realisation of inner evil throughout the duration of the novel; "Ralph wept for the end of innocence..."
Ralph embodies good intentions in the implementation of reason, but ultimately fails to execute these plans soundly. Ralph's refusal to resort to violence throughout the novel is counterpoised by Jack's inherent love of violence. Beginning with his self nomination as hunter, Jack eventually degenerates into the beast he is consumed with slaying. Towards the end of the story, Jack abandons the tribe and forms one of his own. His darkly irresistible nature, along with the lure of meat, immediately sways the majority of the island dwellers to his tribe, which is a much more violent group. Jack's insurrection begins a chain of events that drives the island further into chaos, initially resulting in the frenzied mob murdering Simon during a primal dance, and then culminating with the murder of Piggy by Roger before the group attempts to hunt down Ralph.[12]
Piggy is an intellectual, with poor eyesight, a weight problem, and asthma.[13] He is the most physically vulnerable of all the boys. By frequently quoting his aunt, he provides the only female voice.
Piggy's intellect benefits the group only through Ralph; he acts as Ralph's adviser. He cannot be the leader himself because he lacks leadership qualities and has no rapport with the other boys. Piggy relies on the power of social convention. He believes that holding the conch gives him the right to be heard. He believes that upholding social conventions produces results.
As the brainy representative of civilization, Piggy asserts that "Life... is scientific".[14] Ever the pragmatist, Piggy complains, "What good're you doing talking like that?"[15] when Ralph brings up the highly charged issue of Simon's death at their hands. Piggy tries to keep life scientific despite the incident, "searching for a formula"[16] to explain the death. He asserts that the assault on Simon was an accident, and justifiable because Simon asked for it by inexplicably crawling out of the forest into the ring.[16]
Piggy is so intent on preserving some remnant of civilization on the island that, after Jack's tribe attacks Ralph's group, he assumes they "wanted the conch",[17] when, in fact, they have come for Piggy's glasses[17] in order to make fire. Even up to the moment of his death, Piggy's perspective does not shift in response to the reality of their situation. Because his eminently intellectual approach to life is modelled on the attitudes and rules of the authoritative adult world, he thinks everyone should share his values and attitudes as a matter of course. When Ralph and Piggy confront Jack's tribe about the stolen spectacles, Piggy asks "Which is better to have rules and agree, or to hunt and kill?... law and rescue, or hunting and breaking things up?"[18] as if there is no doubt that the boys would choose his preference.
In the 2012 film Island, the character Max is a re-imagining of Piggy and is played by actor Robbie Curran.[19]
When first blown, it calls the children to an assembly, where Ralph is elected leader. They agree that only the boy holding the conch may speak at meetings to forestall arguments and chaos, and that it should be passed around to those who wish to voice their opinion. The conch symbolises democracy and, like Ralph, civility and order within the group. When Piggy is killed, the conch is smashed into pieces,[18] signalling the end of order and the onset of chaos.[5] Originally the conch is portrayed as being very vibrant and colourful, but as the novel progresses, its colours begin to fade, the same way society begins to fade on the island.
Jack epitomises the worst aspects of human nature when unrepressed or un-tempered by society. Like Ralph, Jack is a natural leader. Unlike Ralph, Jack appeals to more primal desires in the children and relies on his status as leader of the choirboys to justify his authority. Although his way of behaving is neither disruptive nor violent at the beginning of the book, he does, at that time, express an unquenchable desire to hunt and kill a pig and spends hours in solitude traversing the island.
This first time Jack has an opportunity to kill a pig, he cannot, "because of the enormity of the knife descending and cutting into living flesh; because of the unbearable blood".[20] After this hesitation, for which he is most ashamed, Jack's blood lust grows more and more irrational, to the point where he abandons the fire (and causes the boys to miss a potential rescue) in order to hunt. During Jack's metamorphosis, he begins to paint his face with clay and earth, masking his humanity from the pigs and inspiring terrible awe amongst the boys.
Jack's transition puts him on a collision course with Ralph's elected authority. As Jack leaves and takes the majority of the boys with him, lured by the promises of meat, play, and freedom, there has arisen a clear dividing line between the two. Jack represents the irrational nature of the boys, while Ralph represents rationality. Under Jack's rule, the baseness of human nature is unleashed, and he initiates a period of inter-tribal violence, punishing other children, inciting the frenzy that leads to the murder of Simon, and torturing the twins until they submit to his authority.
The tale ends with Jack leading many of the boys in a frenzied attempt to kill Ralph. At this time, the last remaining vestiges of civilization are gone, and Ralph's demise is only prevented by the abrupt and unexpected arrival of a naval officer, who is disappointed by the savage nature of the British boys.[12]
Roger, at first, is a simple "bigun" who is having fun during his stay on the island. Along with Maurice, he destroys the sand castles made by three small children. While Maurice feels guilt for kicking sand into a child's eye, Roger begins to throw stones at one of the boys. The book states that Roger threw the stones to miss, and felt the presence of civilization and society preventing him from harming the children.[21] Later, once he feels that all aspects of conventional society are gone, he is left alone to his animal urges. During a pig hunt, Roger shoves a sharpened stick up the animal's rectum while it is still alive.[22] He kills Piggy with a boulder that was no longer aimed to miss, and becomes the executioner and torturer of Jack's tribe. He also tortured Sam and Eric into joining Jack's tribe. In the final hunt for Ralph at the end of the novel, Roger is armed with "a stick sharpened at both ends,"[23] indicating his intentions of killing Ralph and offering his head as a sacrifice to the "beast". He represents the person who enjoys hurting others, and is only restrained by the rules of society.[24]
Simon is a character who represents peace and tranquillity and positivity. He is in tune with the island, and often experiences extraordinary sensations when listening to its sounds. He loves the nature of the island. He is positive about the future. He has an extreme aversion to the pig's head, the "Lord of the Flies", which derides and taunts Simon in a hallucination. After this experience, Simon emerges from the forest to tell the others that the "beast" that fell from the sky is actually a deceased parachutist caught on the mountain. He is brutally killed by the boys, who ironically mistake him for the beast and kill him in their "dance" in which they "ripped and tore at the beast". It is implied that Ralph, Piggy, Sam and Eric partake in the killing. The final words that the Lord of the Flies had said to Simon vaguely predicted that his death was about to occur in this manner. Earlier in the novel Simon himself also predicts his own death when he tells Ralph that he will "get back all right",[25] implying that, of the two of them, only Ralph will be saved. Simon's death represents the loss of truth, innocence, and common sense.[12] Simon is most commonly interpreted as a Christ figure because of his ability to see through misconception, unlike the rest of the boys, and the events he experiences in the book that parallel those of Jesus' life.
Arriving moments before Ralph's seemingly impending death, the Royal Navy officer is surprised and disappointed to learn that the boys' society has collapsed into chaos. He states that he would have expected "a better show"[26] from British children. The sudden looming appearance of an adult authority figure instantly reduces the savagery of the hunt to a brutal children's game. Upon the officer asking who is in charge, Ralph answers loudly, "I am",[26] and Jack, who was previously characterised as a powerful leader, is reduced to "A little boy who wore the remains of an extraordinary black cap on his red hair and who carried the remains of a pair of spectacles at his waist".[26] In the last sentence, the officer, embarrassed by the distress of the children, turns to look at the cruiser from which his party has landed a symbol of his own adult war.
The Beast is first mentioned by a "littlun" and the notion is immediately dismissed by Ralph. The Beast is thought to be within the water and described by the littluns as such. Soon after the rumours of the Beast begin to flourish, the corpse of a fighter pilot, ejected from his aircraft, falls to the island. His parachute becomes entangled in the jungle foliage in such a way that sporadic gusts of wind cause the chute to billow and the body to move as if still alive. Sam and Eric discover the parachutist in the dark and believe that it is the beast. Ralph, Jack, and Roger search for the Beast and encounter it on the mountain. The reality of the Beast is now firmly established in the boys' minds. Simon discovers the parachutist and realises that the beast is really only the corpse of a man. Jack's tribe feeds the Beast with the sow's head on a stick. This act symbolises Jack's willingness to succumb to the temptation of animalism.
Simon is the first child on the island to realise that the Beast is created by the boys' fear. He decides that "the news must reach the others as soon as possible".[27] Meanwhile, the boys have been feasting and begin to do their tribal pig-hunting dance. When "the beast stumble[s] in to the horseshoe",[28] the frenzied, terrified boys "leapt on to the beast, screamed, struck, bit, tore".[29] It becomes clear that the boys have mistaken Simon for the beast and murdered him both when Golding describes "Simon's dead body move[ing] out towards the open sea",[30] and on the morning after when Ralph tells Piggy, "That was Simon.... That was murder".[15]
The eponymous Lord of the Flies is a pig's head that has been cut off by Jack, put on a stick sharpened at both ends, stuck in the ground and left as an offering to the "beast". Created out of fear, the Lord of the Flies used to be a mother sow who, though at one time clean, loving, and innocent, has now become a manically smiling, bleeding image of horror. Near the end of the book, while Ralph is being hunted down, he strikes this twice in one moment of blind anger, causing it to crack and fall on the ground with a grin "now six feet across".[31] The name "Lord of the Flies" is a literal translation of Beelzebub.
There have been three film adaptations:
Many writers have borrowed plot elements from Lord of the Flies.
The final song on U2's 1980 debut album takes its title, "Shadows and Tall Trees", from Chapter 7 in the book. Some printings of the book's cover are similar to the album cover artwork.[32]
Stephen King got the name Castle Rock from the fictional mountain fort of the same name in Lord of the Flies, using the name to refer to a fictional town that has appeared in a number of King's novels.[33] The book itself appears prominently in his novels Hearts in Atlantis (1999), Misery (1987), and Cujo (1981).[34] King's fictional town of Castle Rock inspired the name of Rob Reiner's production company, Castle Rock Entertainment, which produced the 1990 film. King wrote an introduction for a new edition of the book to mark the centenary of William Golding's birth in 2011.[35]
1 Background
2 Plot summary
3 Allegorical relationships

3.1 Ralph
3.2 Piggy
3.3 The Conch
3.4 Jack Merridew
3.5 Roger
3.6 Simon
3.7 Naval Officer
3.8 The Beast
3.9 The Lord of the Flies


3.1 Ralph
3.2 Piggy
3.3 The Conch
3.4 Jack Merridew
3.5 Roger
3.6 Simon
3.7 Naval Officer
3.8 The Beast
3.9 The Lord of the Flies
4 Film adaptations
5 Influence

5.1 Music
5.2 Printed works


5.1 Music
5.2 Printed works
6 See also
7 References
8 External links
3.1 Ralph
3.2 Piggy
3.3 The Conch
3.4 Jack Merridew
3.5 Roger
3.6 Simon
3.7 Naval Officer
3.8 The Beast
3.9 The Lord of the Flies
5.1 Music
5.2 Printed works
Lord of the Flies (1963), directed by Peter Brook
Lord of the Flies (1990), directed by Harry Hook
Alkitrang dugo (1976), a Filipino film, with a female lead role
Heart of Darkness
Two Years' Vacation
Island mentality
The Coral Island, a novel with a similar plot but with an opposite perspective
"Das Bus", an episode of The Simpsons with a similar plot[36]
Golding, William (1958) [1954]. Lord of the Flies (Print ed.). Boston: Faber & Faber.
Chapter 1: "The Sound of the Shell" of the novel Lord of the Flies by William Golding on eNotes
Lord of the Flies student guide and teacher resources; themes, quotes, characters, study questions
Reading and teaching guide from Faber and Faber, the book's UK publisher
An interview with Judy Golding, the author's daughter, in which she discusses the inspiration for the book, and the reasons for its enduring legacy
William Golding official website run and administered by the William Golding Estate
.
#(`*Harry Potter and the Deathly Hallows*`)#.
Harry Potter and the Deathly Hallows is the seventh and final of the Harry Potter novels written by British author J. K. Rowling. The book was released on 21July2007 by Bloomsbury Publishing in the United Kingdom, in the United States by Scholastic, and in Canada by Raincoast Books, ending the series that began in 1997 with the publication of Harry Potter and the Philosopher's Stone. The novel chronicles the events directly following Harry Potter and the Half-Blood Prince (2005), and the final confrontation between the wizards Harry Potter and Lord Voldemort.
Rowling finished writing Harry Potter and the Deathly Hallows in January 2007. Before its release, Bloomsbury reportedly spent GB10million to keep the book's contents safe before its release date. American publisher Arthur Levine refused any copies of the novel to be released in advance for press review, although two reviews were submitted early. Shortly before release, photos of all 759 pages of the U.S. edition were leaked and transcribed, leading Scholastic to look for the source that had leaked it.[citation needed]
Released globally in 93 countries, Deathly Hallows broke sales records as the fastest-selling book ever, a record it still holds today.[2] It sold 15million copies in the first 24 hours following its release, including more than 11million in the U.S. and UK alone. The previous record, 9million in its first day, had been held by Harry Potter and the Half-Blood Prince. The novel has also been translated into over 120languages, including Ukrainian, Swedish, and Hindi.
Major themes in the novel are death and living in a corrupted society, and critics have compared them to Christian allegories. Generally well-received, the book won the 2008 Colorado Blue Spruce Book Award, and the American Library Association named it a "Best Book for Young Adults". A two-part film adaptation began showing in November 2010 when Harry Potter and the Deathly Hallows  Part 1 was released; Part 2 was released on 15 July 2011.
Throughout the six previous novels in the series, the titular character Harry Potter has struggled with the difficulties of adolescence along with being a famous wizard. When Harry was a baby, Lord Voldemort, a powerful evil wizard, murdered Harry's parents but vanished after attempting to kill Harry. Harry immediately became famous, and was placed in the care of his Muggle (non-magical) relatives Aunt Petunia and Uncle Vernon.
In Philosopher's Stone, Harry re-enters the wizarding world at age 11 and enrolls in Hogwarts School of Witchcraft and Wizardry. He makes friends with Ron Weasley and Hermione Granger. Harry also meets the school's headmaster, Albus Dumbledore, and Professor Severus Snape, who dislikes him. Harry fights Voldemort several times while at school, as the wizard tries to regain a physical form. In Goblet of Fire, Harry is entered in a dangerous magical competition called the Triwizard Tournament. At the conclusion of the Tournament, Harry witnesses the return of Lord Voldemort to full strength. During Order of the Phoenix, Harry and several of his friends face off against Voldemort's Death Eaters, a group of Dark witches and wizards, and narrowly defeat them. In Half-Blood Prince, Harry learns that Voldemort has created six "horcruxes" to become immortal. A Horcrux is a fragment of a person's soul placed within an object so that when the body dies, a part of the soul remains and the person can be regenerated or resurrected.[3] However, the destruction of the creator's body leaves the wizard or witch in a state of half-life, without corporeal form.[4] Two horcruxes have already been destroyed, one by Harry in the events of Chamber of Secrets and one by Dumbledore shortly before the events of Half-Blood Prince. When returning from a mission to discover a horcrux, Dumbledore is murdered by Snape, a former Death Eater whom Harry suspected of secretly remaining loyal to Voldemort. At the conclusion of the book, Harry decides to leave school, find and destroy the remaining four Horcruxes, and defeat the evil wizard Voldemort, once and for all.
Following Dumbledore's death, Voldemort continues to gain support and increase his power. When Harry turns seventeen, the protection he has at his aunt and uncle's house will be broken. Before that can happen, at Mad Eye Moody's suggestion, Harry flees to the Burrow with his friends, many of whom use Polyjuice Potion to impersonate him so as to confuse any Death Eaters that may attack. They are indeed attacked shortly after leaving Privet Drive; Mad Eye is killed, and George Weasley wounded, but the rest arrive safely at the Burrow. Ron and Hermione decide to accompany Harry, instead of returning to Hogwarts School for their seventh year, to finish the quest Dumbledore started: to hunt and destroy Voldemort's four remaining Horcruxes. They have little knowledge about the remaining Horcruxes except that one is a locket once owned by Hogwarts' co-founder Salazar Slytherin, one is possibly a cup once owned by co-founder Helga Hufflepuff, a third may be connected with co-founder Rowena Ravenclaw, and the fourth may be Nagini, Voldemort's snake familiar. The whereabouts of the founders' objects is unknown, and Nagini is presumed to be with Voldemort. Before leaving, they attend Ron's brother's Bill's wedding to Fleur Delacour, with Harry disguised by Polyjuice Potion, but the Ministry of Magic is taken over by Death Eaters during the wedding and they barely escape with their lives.
Harry, Ron, and Hermione flee to 12 Grimmauld Place in London, which is Sirius Black's family's house, where they learn from the house-elf Kreacher the whereabouts of Salazar Slytherin's locket, which Sirius' brother Regulus stole from Voldemort at the cost of his own life. They successfully recover this Horcrux by infiltrating the Ministry of Magic and stealing it from Dolores Umbridge. Under the object's evil influence and the stress of being on the run, Ron leaves the others. As Harry and Hermione search for the Horcruxes, they learn more about Dumbledore's past, including the insanity and death of Dumbledore's younger sister and his connection to the evil wizard Grindewald. Harry and Hermione ultimately travel to Godric's Hollow, Harry's birthplace and the place where his parents died. They meet the eldery magical historian Bathilda Bagshot, who turns out to be Nagini in disguise and attacks them. They escape into the Forest of Dean, where a mysterious silver doe that appears to be a Patronus leads Harry to the Sword of Hogwarts co-founder Godric Gryffindor, one of the few objects able to destroy Horcruxes, lying at the bottom of an icy lake. When Harry attempts to recover the sword from the pool, the Horcrux attempts to kill him. Ron reappears, saving Harry and then using the sword to destroy the locket. Resuming their search, the trio repeatedly encounter a strange symbol that an eccentric wizard named Xenophilius Lovegood tells them represents the mythical Deathly Hallows. The Hallows are three sacred objects: the Elder Wand, an unbeatable wand; the Resurrection Stone, with the power to summon the dead to the living world; and an infallible Invisibility Cloak. Harry learns that Voldemort is seeking the Elder Wand, recognizes the Resurrection Stone from the second Horcrux, which Dumbledore had destroyed, and realizes that his own Invisibility Cloak is the one mentioned in the story, but he is unaware of the Hallows' significance.
The trio are captured and taken to Malfoy Manor, where Bellatrix Lestrange tortures Hermione. Harry and Ron are thrown in the cellar, where they find Luna Lovegood, Ollivander, Dean Thomas, and Griphook. They escape to Shell Cottage (Bill and Fleur's house) with Dobby's help, but at the cost of the house-elf's life. Harry now realizes that the Hallows may have the power to defeat Death and knows that Voldemort robbed Dumbledore's tomb to procure the Elder Wand, but he decides to focus on finding the Horcruxes instead of the Hallows. With Griphook's help, they learn that Helga Hufflepuff's cup - a Horcrux - is hidden in Bellatrix's vault at Gringotts, break into her vault, retrieve the cup, and escape on a dragon, with Griphook swiping the sword and escaping on his own. From his connection to Voldemort's thoughts, Harry learns that another Horcrux is hidden in Hogwarts, which is under the control of Severus Snape. Harry, Ron, and Hermione enter the school through Hogsmeade (being saved by Aberforth Dumbledore, who explains more about Albus's backstory) and - with the help of the teachers - Snape is ousted from the school. Ron and Hermione go to the Chamber of Secrets and destroy the cup using a basilisk fang. The trio then finds Rowena Ravenclaw's diadem (another Horcrux) in the Room of Requirement. Vincent Crabbe casts a Fiendfyre curse in an attempt to kill Harry, Ron, and Hermione, but he instead destroys the diadem, the Room of Requirement, and himself. At this point, a total of five Horcruxes have been destroyed.
The Death Eaters and Voldemort besiege Hogwarts, while Harry, Ron, Hermione, their allies, and various magical creatures defend the school. Several major characters are killed in the first wave of the battle, including Remus Lupin, Nymphadora Tonks, and Fred Weasley. Voldemort kills Severus Snape because he believes doing so will make him the Elder Wand's true master, since Snape killed Dumbledore. Harry discovers while viewing Snape's memories that Voldemort inadvertently made Harry into a seventh Horcrux when he attacked him as a baby, which is the true significance of Harry's scar, and that Harry must die in order to destroy Voldemort. These memories also confirm Snape's unwavering loyalty to Dumbledore and that his role as a double-agent against Voldemort never wavered after Voldemort killed Lily Evans, Harry's mother and Snape's one true love. Harry also learns that Dumbledore had less than a year to live when he died, that his death by Snape's hand had been per Dumbledore's request, and that Dumbledore had known that Harry must die. After using the Resurrection Stone to bring back his deceased loved ones for a short while, Harry surrenders himself to death at Voldemort's hand. Voldemort casts the Killing Curse at him, but it only sends Harry into a limbo-like state between life and death.
While in this state, Dumbledore's spirit explains to Harry that when Voldemort used Harry's blood to regain his full strength, it protected Harry from Voldemort killing him; however, the Horcrux inside Harry has been destroyed, and Harry can return to his body despite being hit by the Killing Curse. Dumbledore also explains that Harry became the true master of the Deathly Hallows by facing Death, not by seeking to avoid it or conquer it. Harry returns to his body, feigning death, and Voldemort marches victoriously into the castle with his body. However, per Harry's prior instructions, Neville Longbottom kills Nagini, the last Horcrux, with the Sword of Gryffindor. Harry then reveals that he is still alive, and the battle resumes, with Bellatrix Lestrange being killed by Molly Weasley.
Harry and Voldemort engage in a final climactic duel. Harry reveals that because he willingly sacrificed himself to death by Voldemort's hand, his act of love would protect the Wizarding community from Voldemort in the same way the sacrifice Harry's mother made protected Harry. Harry also reveals that Snape was not loyal to Voldemort, did not murder Dumbledore, and was never the master of the Elder Wand. Instead, Draco was the master of the Elder Wand after disarming Dumbledore, but, because Harry had disarmed Draco at Malfoy Manor, Harry is the true master of the Elder Wand. Harry claims that the wand will refuse to kill the one to whom it owes allegiance, further protecting Harry. During the duel, Harry refuses to use the killing curse and even encourages Voldemort to feel remorse, one known way to restore Voldemort's shattered soul. Voldemort dies when his own killing curse backfires against Harry's disarming curse, killing himself; the Death Eaters are finally defeated. The wizarding world is able to live in peace once more.
The novel, the last in the series, closes with a brief epilogue set 19 years later, in which Harry and Ginny Weasley are a married couple with three children: James Sirius, Albus Severus, and Lily Luna. Ron and Hermione Weasley are also married and have two children, Rose and Hugo. The families meet at King's Cross station, where a nervous Albus is departing for his first year at Hogwarts. Harry's godson, Teddy Lupin, is found kissing Bill and Fleur Weasley's daughter Victoire in a train carriage. Harry sees Draco Malfoy and his wife with their son, Scorpius. Neville Longbottom is now the Hogwarts Herbology professor and remains friends with the two families. Harry comforts Albus, who is worried he will be sorted into Slytherin, and tells his son that one of his two namesakes, Severus Snape, was a Slytherin and the bravest man he had ever met. He adds that the Sorting Hat takes one's choice into account, like it did for Harry. The book ends with these final words: "The scar had not pained Harry for nineteen years. All was well."
Harry Potter and the Philosopher's Stone was published by Bloomsbury, the publisher of all Harry Potter books in the United Kingdom, on 30 June 1997.[5] It was released in the United States on 1 September 1998 by Scholasticthe American publisher of the booksas Harry Potter and the Sorcerer's Stone,[6] after Rowling had received US$105,000 for the American rightsan unprecedented amount for a children's book by a then-unknown author.[7]
The second book, Harry Potter and the Chamber of Secrets was originally published in the UK on 2 July 1998, and in the US on 2 June 1999. Harry Potter and the Prisoner of Azkaban was then published a year later in the UK on 8 July 1999, and in the US on 8 September 1999.[8] Harry Potter and the Goblet of Fire was published on 8 July 2000 at the same time by Bloomsbury and Scholastic.[9] Harry Potter and the Order of the Phoenix is the longest book in the series at 766 pages in the UK version and 870 pages in the US version.[10] It was published worldwide in English on 21 June 2003.[11] Harry Potter and the Half-Blood Prince was published on 16 July 2005, and it sold 9million copies in the first 24 hours of its worldwide release.[12][13]
Shortly before releasing the title, J. K. Rowling announced that she had considered three titles for the book.[14][15] The final title, Harry Potter and the Deathly Hallows, named after the mythical Deathly Hallows in the novel, was released to the public on 21December2006, via a special Christmas-themed hangman puzzle on Rowling's website, confirmed shortly afterwards by the book's publishers.[16] When asked during a live chat about the other titles she had been considering, Rowling mentioned Harry Potter and the Elder Wand and Harry Potter and the Peverell Quest.[14]
Rowling completed the book while staying at the Balmoral Hotel in Edinburgh in January2007, and left a signed statement on a marble bust of Hermes in her room which read: "J. K. Rowling finished writing Harry Potter and the Deathly Hallows in this room (652) on 11January2007".[17] In a statement on her website, she said, "I've never felt such a mixture of extreme emotions in my life, never dreamed I could feel simultaneously heartbroken and euphoric." She compared her mixed feelings to those expressed by Charles Dickens in the preface of the 1850 edition of David Copperfield, "a two-years' imaginative task". "To which," she added, "I can only sigh, try seventeen years, Charles". She ended her message by saying "Deathly Hallows is my favourite, and that is the most wonderful way to finish the series".[18]
When asked before publication about the forthcoming book, Rowling stated that she could not change the ending even if she wanted. "These books have been plotted for such a long time, and for six books now, that they're all leading a certain direction. So, I really can't".[19] She also commented that the final volume related closely to the previous book in the series, Harry Potter and the Half-Blood Prince, "almost as though they are two halves of the same novel".[20] She has said that the last chapter of the book was written "in something like 1990", as part of her earliest work on the series.[21] Rowling also revealed she originally wrote the last words to be "something like: 'Only those who he loved could see his lightning scar'". Rowling changed this because she did not want people to think Voldemort would rise again and to say that Harry's mission was over.[22]
In a 2006 interview, J. K. Rowling said that the main theme of the series is Harry dealing with death,[23] which was influenced by her mother's death in 1990, from multiple sclerosis.[22][23][24][25] Lev Grossman of Time stated that the main theme of the series was the overwhelming importance of continuing to love in the face of death.[26]
Academics and journalists have developed many other interpretations of themes in the books, some more complex than others, and some including political subtexts. Themes such as normality, oppression, survival, and overcoming imposing odds have all been considered as prevalent throughout the series.[27] Similarly, the theme of making one's way through adolescence and "going over one's most harrowing ordealsand thus coming to terms with them" has also been considered.[28] Rowling has stated that the books comprise "a prolonged argument for tolerance, a prolonged plea for an end to bigotry" and that also pass on a message to "question authority and... not assume that the establishment or the press tells you all of the truth".[29]
Some political commentators have seen J. K. Rowling's portrayal of the bureaucratised Ministry of Magic and the oppressive measures taken by the Ministry in the later books (like making attendance at Hogwarts School compulsory and the "registration of Mudbloods" with the Ministry) as an allegory of criticising the state.[30]
The Harry Potter series has been under criticism for supposedly supporting witchcraft and occult. Before publication of Deathly Hallows, Rowling refused to speak out about her religion, stating, "If I talk too freely, every reader, whether 10 or 60, will be able to guess what's coming in the books".[31] However, many have noted Christian allegories apparent in Deathly Hallows.[31] For example, Harry dies and then comes back to life to save mankind, like Christ. The location where this occurs is King's Cross.[32] Harry also urges Voldemort to show remorse, to restore his shattered soul. Rowling also stated that "my belief and my struggling with religious belief ... I think is quite apparent in this book", which is shown as Harry struggles with his faith in Dumbledore.[33]
Deathly Hallows begins with a pair of epigraphs, one by Quaker leader William Penn and one from Aeschylus' The Libation Bearers. Of this, Rowling said "I really enjoyed choosing those two quotations because one is pagan, of course, and one is from a Christian tradition. I'd known it was going to be those two passages since Chamber was published. I always knew [that] if I could use them at the beginning of book seven then I'd cued up the ending perfectly. If they were relevant, then I went where I needed to go. They just say it all to me, they really do".[34]
When Harry visits his parents' grave, the biblical reference "The last enemy that shall be destroyed is death" (1 Corinthians 15:26) is inscribed on the grave.[35] The Dumbledores' family tomb also holds a biblical quote: "Where your treasure is, there your heart will be also", which is from Matthew 6:21.[35] Rowling states, "They're very British books, so on a very practical note Harry was going to find biblical quotations on tombstones...[but] I think those two particular quotations he finds on the tombstones at Godric's Hollow, they sum up they almost epitomise the whole series".[35]
Harry Potter pundit John Granger additionally noted that one of the reasons the Harry Potter books were so popular is their use of literary alchemy (similar to Romeo and Juliet, C. S. Lewis's Perelandra and Charles Dickens's A Tale of Two Cities) and vision symbolism.[36] In this model, authors weave allegorical tales along the alchemical magnum opus. Since the medieval period, alchemical allegory has mirrored the passion, death and resurrection of Christ.[37] While the entire series utilizes symbols common in alchemy, the Deathly Hallows completes this cycle, tying themes of death, rebirth, and the Resurrection Stone to the principal motif of alchemical allegory, and topics presented in the first book of the series.
Christian author Nancy Carpentier Brown also noted many Christian themes, such as Harry marking Mad-Eye Moody's grave with a cross, showing remorse and giving Voldemort a chance to redeem himself, and the Resurrection Stone.[38] She also pointed out that Harry becomes a godfather to Tonks and Lupin's son, Teddy Lupin.[38]
The launch was celebrated by an all-night book signing and reading at the Natural History Museum in London, which Rowling attended along with 1,700 guests chosen by ballot.[39] Rowling toured the US in October 2007, where another event was held at Carnegie Hall in New York City with tickets allocated by sweepstake.[40]
Scholastic, the American publisher of the Harry Potter series, launched a multi-million dollar "There will soon be 7" marketing campaign with a "Knight Bus" travelling to 40 libraries across the United States, online fan discussions and competitions, collectible bookmarks, tattoos, and the staged release of seven Deathly Hallows questions most debated by fans.[41] In the build-up to the book's release, Scholastic released seven questions that fans would find answered in the final book:[42]
J. K. Rowling arranged with her publishers for a poster bearing the face of the missing British child Madeleine McCann to be made available to book sellers when Deathly Hallows was launched on 21July2007, and said that she hoped that the posters would be displayed prominently in shops all over the world.[43]
After it was told that the novel would be released on 21July2007, Warner Bros. shortly thereafter said that the film adaptation of Harry Potter and the Order of the Phoenix would be released shortly before the novel would be released, on 13July2007,[44] making many people proclaim that July2007, was the month of Harry Potter.[45]
Bloomsbury invested GB10million in an attempt to keep the book's contents secure until the 21July release date.[46] Arthur Levine, U.S. editor of the Harry Potter series, denied distributing any copies of Deathly Hallows in advance for press review, but two U.S. papers published early reviews anyway.[47] There was speculation that some shops would break the embargo and distribute copies of the book early, as the penalty imposed for previous instalmentsthat the distributor would not be supplied with any further copies of the serieswould no longer be a deterrent.[48]
In the week before its release, a number of texts purporting to be genuine leaks appeared in various forms. On 16July, a set of photographs representing all 759 pages of the U.S. edition was leaked and was fully transcribed prior to the official release date.[49][50][51][52] The photographs later appeared on websites and peer-to-peer networks, leading Scholastic to seek a subpoena in order to identify one source.[53] This represented the most serious security breach in the Harry Potter series' history.[54] Rowling and her lawyer confirmed that there were genuine online leaks.[55] Reviews published in both The Baltimore Sun and The New York Times on 18July2007, corroborated many of the plot elements from this leak, and about one day prior to release, The New York Times confirmed that the main circulating leak was real.[54]
Scholastic announced that approximately one-ten-thousandth (0.0001) of the U.S. supply had been shipped early interpreted to mean about 1,200 copies. One reader in Maryland received a copy of the book in the mail from DeepDiscount.com four days before it was launched, which evoked incredulous responses from both Scholastic and DeepDiscount. Scholastic initially reported that they were satisfied it had been a "human error" and would not discuss possible penalties;[56] however, the following day Scholastic announced that it would be launching legal action against DeepDiscount.com and its distributor, Levy Home Entertainment.[57] Scholastic filed for damages in Chicago's Circuit Court of Cook County, claiming that DeepDiscount engaged in a "complete and flagrant violation of the agreements that they knew were part of the carefully constructed release of this eagerly awaited book."[58] Some of the early release books soon appeared on eBay, in one case being sold to Publishers Weekly for US$250from an initial price of US$18.[59]
Asda,[60][61] along with several other UK supermarkets, having already taken pre-orders for the book at a heavily discounted price, sparked a price war two days before the book's launch by announcing they would sell it for just GB5 a copy (about US$8). Other retail chains then also offered the book at discounted prices. At these prices the book became a loss leader. This caused uproar from traditional UK booksellers who argued they had no hope of competing in those conditions. Independent shops protested loudest, but even Waterstone's, the UK's largest dedicated chain bookstore, could not compete with the supermarket price. Some small bookstores hit back by buying their stock from the supermarkets rather than their wholesalers. Asda attempted to counter this by imposing a limit of two copies per customer to prevent bulk purchases. Philip Wicks, a spokesman for the UK Booksellers Association, said, "It is a war we can't even participate in. We think it's a crying shame that the supermarkets have decided to treat it as a loss-leader, like a can of baked beans." Michael Norris, an analyst at Simba Information, said: "You are not only lowering the price of the book. At this point, you are lowering the value of reading."[62]
In Malaysia, a similar price war caused controversy regarding sales of the book.[63] Four of the biggest bookstore chains in Malaysia, MPH Bookstores, Popular Bookstores, Times and Harris, decided to pull Harry Potter and the Deathly Hallows off their shelves as a protest against Tesco and Carrefour hypermarkets. The retail price of the book in Malaysia is MYR 109.90 (about GB16), while the hypermarkets Tesco and Carrefour sold the book at MYR 69.90 (about GB10). The move by the bookstores was seen as an attempt to pressure the distributor Penguin Books to remove the books from the hypermarkets. However, as of 24July2007, the price war has ended, with the four bookstores involved resuming selling the books in their stores with discount. Penguin Books has also confirmed that Tesco and Carrefour are selling the book at a loss, urging them to practice good business sense and fair trade.[64]
The book's early Saturday morning release in Israel was criticised for violating Shabbat. Trade and Industry Minister Eli Yishai commented "It is forbidden, according to Jewish values and Jewish culture, that a thing like this should take place at 2a.m. on Saturday. Let them do it on another day."[65] Yishai indicated that he would issue indictments and fines based on the Hours of Work and Rest Law.[66]
The Baltimore Sun's critic, Mary Carole McCauley, noted that the book was more serious than the previous novels in the series and had more straightforward prose.[67] Furthermore, reviewer Alice Fordham from The Times wrote that "Rowling's genius is not just her total realisation of a fantasy world, but the quieter skill of creating characters that bounce off the page, real and flawed and brave and lovable". Fordham concluded, "We have been a long way together, and neither Rowling nor Harry let us down in the end".[68] The New York Times writer Michiko Kakutani agreed, praising Rowling's ability to make Harry both a hero and a character that can be related to.[69]
Time magazine's Lev Grossman named it one of the Top 10 Fiction Books of 2007, ranking it at #8, and praised Rowling for proving that books can still be a global mass medium.[26] Novelist Elizabeth Hand criticised that "...the spectacularly complex interplay of narrative and character often reads as though an entire trilogy's worth of summing-up has been crammed into one volume."[70] In a starred review from Kirkus Reviews, the reviewer said, "Rowling has shown uncommon skill in playing them with and against each other, and also woven them into a darn good bildungsroman, populated by memorable characters and infused with a saving, irrepressible sense of fun". They also praised the second half of the novel, but criticised the epilogue, calling it "provacatively sketchy".[71] In another review from The Times, reviewer Amanda Craig said that while Rowling was "not an original, high-concept author", she was "right up there with other greats of children's fiction". Craig went on to say that the novel was "beautifully judged, and a triumphant return to form", and that Rowling's imagination changed the perception of an entire generation, which "is more than all but a handful of living authors, in any genre, have achieved in the past half-century".[72]
In contrast, Jenny Sawyer of The Christian Science Monitor said that, "There is much to love about the Harry Potter series, from its brilliantly realised magical world to its multilayered narrative", however, "A story is about someone who changes. And, puberty aside, Harry doesn't change much. As envisioned by Rowling, he walks the path of good so unwaveringly that his final victory over Voldemort feels, not just inevitable, but hollow".[73] In The New York Times, Christopher Hitchens compared the series to World War Two-era English boarding school stories, and while he wrote that "Rowling has won imperishable renown" for the series as a whole, he also stated that he disliked Rowling's use of deus ex machina, that the mid-book camping chapters are "abysmally long", and Voldemort "becomes more tiresome than an Ian Fleming villain".[74] Catherine Bennett of The Guardian praised Rowling for putting small details from the previous books and making them large in Deathly Hallows, such as Grindelwald being mentioned on a Chocolate Frog Card in the first book. While she points out "as her critics say, Rowling is no Dickens", she says that Rowling "has willed into a fictional being, in every book, legions of new characters, places, spells, rules and scores of unimagined twists and subplots".[75]
Stephen King criticised the reactions of some reviewers to the books, including McCauley, for jumping too quickly to surface conclusions of the work.[76] He felt this was inevitable, because of the extreme secrecy before launch which did not allow reviewers time to read and consider the book, but meant that many early reviews lacked depth. Rather than finding the writing style disappointing, he felt it had matured and improved. He acknowledged that the subject matter of the books had become more adult, and that Rowling had clearly been writing with the adult audience firmly in mind since the middle of the series. He compared the works in this respect to Huckleberry Finn and Alice in Wonderland which achieved success and have become established classics, in part by appealing to the adult audience as well as children.[76]
Sales for Harry Potter and the Deathly Hallows were record setting. The initial U.S. print run for Deathly Hallows was 12million copies, and more than a million were pre-ordered through Amazon.com and Barnes & Noble,[77] 500percent higher than pre-sales had been for Half-Blood Prince.[78] On 12April2007, Barnes & Noble declared that Deathly Hallows had broken its pre-order record, with more than 500,000 copies pre-ordered through its site.[79] On opening day, a record 8.3million copies were sold in the United States (over 96 per second),[80][81] and 2.65million copies in the United Kingdom.[82] It holds the Guinness World record for fastest selling book of fiction in 24 hours for U.S. sales.[83] At WH Smith, sales reportedly reached a rate of 15 books sold per second.[84] By June2008, nearly a year after it was published, worldwide sales were reportedly around 44million.[1]
Harry Potter and the Deathly Hallows has won several awards.[85] In 2007, the book was named one of The New York Times 100 Notable Books,[86] and one of its Notable Children's Books.[87] The novel was named the best book of 2007, by Newsweek's critic Malcolm Jones.[88] Publishers Weekly also listed Harry Potter and the Deathly Hallows among their Best Books of 2007.[89] In 2008, the American Library Association named the novel one of its Best Books for Young Adults,[90] and also listed it as a Notable Children's Book.[91] Furthermore, Harry Potter and the Deathly Hallows received the 2008 Colorado Blue Spruce Book Award.[85]
Due to Harry Potter and the Deathly Hallows' worldwide fame, it has been translated into many languages. The first translation to be released was the Ukrainian translation, on 25September2007 (as     ).[92] The Swedish title of the book was revealed by Rowling as Harry Potter and the Relics of Death (Harry Potter och Ddsrelikerna), following a pre-release question from the Swedish publisher about the difficulty of translating the two words "Deathly Hallows" without having read the book.[93] This is also the title used for the French translation (Harry Potter et les reliques de la mort), the Spanish translation (Harry Potter y las Reliquias de la Muerte), the Dutch translation (Harry Potter en de Relieken van de Dood) and the Brazilian Portuguese translation (Harry Potter e as Relquias da Morte).[94] The first Polish translation was released with a new title: Harry Potter i Insygnia mierci Harry Potter and the Insignia of Death.[95] The Hindi translation Harry Potter aur Maut ke Tohfe (     ), which means "Harry Potter and the Gifts of Death", was released by Manjul Publication in India on 27June2008.[96]
Deathly Hallows was released in hardcover on 21July2007,[97] and in paperback in the United Kingdom on 10July2008,[98] and in the United States on 7July2009.[99] In SoHo, New York, there was a release party for the American paperback edition, with many games and activities.[100] An "Adult Edition" with a different cover illustration was released by Bloomsbury on 21July2007.[101] To be released simultaneously with the original U.S. hardcover on 21July with only 100,000 copies was a Scholastic deluxe edition, highlighting a new cover illustration by Mary GrandPr.[102] In October2010, Bloomsbury released a "Celebratory" paperback edition, which featured a foiled and starred cover.[103] Lastly, on 1November2010, a "Signature" edition of the novel was released in paperback by Bloomsbury.[104]
A two-part film adaptation of Harry Potter and the Deathly Hallows is directed by David Yates, written by Steve Kloves and produced by David Heyman, David Barron and J. K. Rowling. Part 1 was released on 19November2010, and Part 2 on 15July2011.[105][106] Filming began in February2009, and ended on 12June2010.[107] However, the cast confirmed they would reshoot the epilogue scene as they only had two days to shoot the original.[108] Reshoots officially ended around December 2010.[note 1][109] Part 1 ended at Chapter 24 of the book, when Voldemort regained the Elder Wand.[110] However, there were a few omissions, such as the appearances of Dean Thomas and Viktor Krum, and Peter Pettigrew's death.[111] James Bernadelli of Reelviews said that the script stuck closest to the text since Harry Potter and the Chamber of Secrets,[112] yet this was met with negativity from some audiences as the film inherited "the book's own problems".[113]
Harry Potter and the Deathly Hallows was released simultaneously on 21 July 2007, in both the UK and the United States.[114][115] The UK edition features the voice of Stephen Fry and runs about 24 hours[116] while the U.S. edition features the voice of Jim Dale and runs about 21 hours.[117] Both Fry and Dale recorded 146 different and distinguishable character voices, and was the most recorded by an individual on an audiobook at the time.[118]
For his work on Deathly Hallows, Dale won the 2008 Grammy Award for the Best Spoken Word Album for Children.[119] He also was awarded an Earphone Award by AudioFile, who claimed, "Dale has raised the bar on audiobook interpretation so high it's hard to imagine any narrator vaulting over it."[120]
On 4 December 2008, Rowling released The Tales of Beedle the Bard both in the UK and US.[121] The Tales of Beedle the Bard is a spin-off of Deathly Hallows and contains fairy tales that are told to children in the "Wizarding World". The book includes five short stories, including "The Tale of the Three Brothers" which is the story of the Deathly Hallows.
Amazon.com released an exclusive collector's edition of the book which is a replica of the book that Amazon.com purchased at auction in December 2007.[122] Seven copies were auctioned off in London by Sothebys. Each was illustrated and handwritten by Rowling and is 157 pages. It was bound in brown Moroccan leather and embellished with five hand-chased hallmarked sterling silver ornaments and mounted moonstones.[123]
 
1 Contents

1.1 Plot introduction
1.2 Plot summary
1.3 Epilogue


1.1 Plot introduction
1.2 Plot summary
1.3 Epilogue
2 Background

2.1 Franchise
2.2 Choice of title
2.3 Rowling on finishing the book


2.1 Franchise
2.2 Choice of title
2.3 Rowling on finishing the book
3 Major themes

3.1 Death
3.2 Living in a corrupted society
3.3 Christian allegories


3.1 Death
3.2 Living in a corrupted society
3.3 Christian allegories
4 Release

4.1 Marketing and promotion
4.2 Spoiler embargo
4.3 Online leaks and early delivery
4.4 Price wars and other controversies


4.1 Marketing and promotion
4.2 Spoiler embargo
4.3 Online leaks and early delivery
4.4 Price wars and other controversies
5 Publication and reception

5.1 Critical response
5.2 Sales, awards and honours


5.1 Critical response
5.2 Sales, awards and honours
6 Translations
7 Editions
8 Adaptations

8.1 Film
8.2 Audiobooks


8.1 Film
8.2 Audiobooks
9 The Tales of Beedle the Bard
10 Notes
11 References
12 Bibliography
13 External links
1.1 Plot introduction
1.2 Plot summary
1.3 Epilogue
2.1 Franchise
2.2 Choice of title
2.3 Rowling on finishing the book
3.1 Death
3.2 Living in a corrupted society
3.3 Christian allegories
4.1 Marketing and promotion
4.2 Spoiler embargo
4.3 Online leaks and early delivery
4.4 Price wars and other controversies
5.1 Critical response
5.2 Sales, awards and honours
8.1 Film
8.2 Audiobooks
"The Tales of Beedle the Bard, Standard Edition (Harry Potter) (9780545128285): J.K. Rowling: Books". Amazon.com. http://www.amazon.com/Tales-Beedle-Standard-Edition-Potter/dp/0545128285/ref=sr_1_1?s=books&ie=UTF8&qid=1335223887&sr=1-1. Retrieved 23 April 2012.
"The Tales of Beedle the Bard, Standard Edition: Amazon.co.uk: J.K. Rowling: Books". Amazon.co.uk. http://www.amazon.co.uk/Tales-Beedle-Bard-Standard-Edition/dp/0747599874/ref=sr_1_1?s=books&ie=UTF8&qid=1335223949&sr=1-1. Retrieved 23 April 2012.
Granger, John. The Deathly Hallows Lectures: The Hogwarts Professor Explains the Final Harry Potter Adventure. Zossima Press: 2008. ISBN 0-9723221-7-5.
Hall, Susan. Reading Harry Potter: critical essays. Greenwood Publishing: 2003. ISBN 0-313-32067-5.
Rowling, JK. Harry Potter and the Half-Blood Prince. London: Bloomsbury/New York City: Scholastic: 2005. UK ISBN 0-747-58108-8/U.S. ISBN 0-439-78454-9.
Rowling, JK. Harry Potter and the Goblet of Fire. London: Bloomsbury/New York City: Scholastic: 2000. UK ISBN 0-747-54624-X/U.S. ISBN 0-439-13959-7.
Shapiro, Marc. J. K. Rowling: The Wizard Behind Harry Potter. St. Martin's Press: 2007. ISBN 0-312-37697-9.
Heckl, Raik. "The Tale of the Three Brothers" and the Idea of the Speaking Dead in the Harry Potter Novels. Leipzig: 2008.
Book: Harry Potter
Harry Potter at Bloomsbury.com web site UK publisher book information
Harry Potter at Scholastic.com web site U.S. publisher book information
Harry Potter at Allen & Unwin web site Australia-New Zealand publisher book information
.
#(`*Harry Potter and the Philosopher's Stone*`)#.
Harry Potter and the Philosopher's Stone is the first novel in the Harry Potter series written by J. K. Rowling and featuring Harry Potter, a young wizard. It describes how Harry discovers he is a wizard, makes close friends and a few enemies at the Hogwarts School of Witchcraft and Wizardry, and with the help of his friends thwarts an attempted comeback by the evil wizard Lord Voldemort, who killed Harry's parents when Harry was one year old.
The book, which is J.K. Rowling's debut novel, was published on 26June 1997 by Bloomsbury in London. In 1998 Scholastic Corporation published an edition for the United States market under the title Harry Potter and the Sorcerer's Stone. The novel won most of the UK book awards that were judged by children, and other awards in the US. The book reached the top of the New York Times list of best-selling fiction in August 1999, and stayed near the top of that list for much of 1999 and 2000. It has been translated into several other languages and has been made into a feature-length film of the same name.
Most reviews were very favourable, commenting on Rowling's imagination, humour, simple, direct style and clever plot construction, although a few complained that the final chapters seemed rushed. The writing has been compared to that of Jane Austen, one of Rowling's favourite authors, of Roald Dahl, whose works dominated children's stories before the appearance of Harry Potter, and of the Ancient Greek story-teller Homer. While some commentators thought the book looked backwards to Victorian and Edwardian boarding school stories, others thought it placed the genre firmly in the modern world by featuring contemporary ethical and social issues.
Harry Potter and the Philosopher's Stone, along with the rest of the Harry Potter series, has been attacked by several religious groups and banned in some countries because of accusations that the novels promote witchcraft; however, some Christian commentators have written that the book exemplifies important Christian viewpoints, including the power of self-sacrifice and the ways in which people's decisions shape their personalities. Educators regard Harry Potter and the Philosopher's Stone and its sequels as an important aid in improving literacy because of the books' popularity. The series has also been used as a source of object lessons in educational techniques, sociological analysis and marketing.
Before the start of the novel, Voldemort, considered the most evil and powerful dark wizard in history, kills Harry's parents but mysteriously vanishes after trying to kill the infant Harry. While the wizarding world celebrates Voldemort's downfall, Professor Dumbledore, Professor McGonagall and Rubeus Hagrid place the oneyear-old orphan in the care of his Muggle (non-wizard) uncle and aunt: Vernon and Petunia Dursley.
For ten years, they and their son Dudley neglect, torment and abuse Harry. Shortly before Harry's eleventh birthday, a series of letters addressed to Harry arrive, but Vernon destroys them before Harry can read them. To get away from the letters, Vernon takes the family to a small island. As they are settling in, Hagrid bursts through the door to tell Harry what the Dursleys have kept him from finding out: Harry is a wizard and has been accepted at Hogwarts.
Hagrid takes Harry to Diagon Alley, a magically-concealed shopping precinct in London, where Harry is bewildered to discover how famous he is among wizards as "the boy who lived". He also finds that he is quite wealthy, since a bequest from his parents has remained on deposit at Gringotts Wizarding Bank. Guided by Hagrid, he buys the books and equipment he needs for Hogwarts, as well as Hedwig the owl. At the wand shop, he finds that the wand that suits him best is the twin of Voldemort's; both wands contain feathers from the same phoenix.[1]
A month later Harry leaves the Dursleys' home to catch the Hogwarts Express from King's Cross railway station. There he meets the Weasley family, who show him how to pass through the magical wall to Platform 9, where the train is waiting. While on the train Harry makes friends with Ron Weasley, who tells him that someone tried to rob a vault at Gringotts. During the ride they meet Hermione Granger. Another new pupil, Draco Malfoy, accompanied by his sidekicks Vincent Crabbe and Gregory Goyle, offers to advise Harry, but Harry dislikes Draco's arrogance and prejudice.
Before the term's first dinner in the school's Great Hall, the new pupils are allocated to houses by the magical Sorting Hat. Before it is Harry's turn, he catches Professor Snape's eye and feels a pain in the scar Voldemort left on his forehead. When it is Harry's turn to be sorted, the Hat wonders whether he should be in Slytherin, but when Harry objects, the Hat sends him to join the Weasleys in Gryffindor. While Harry is eating, he questions Percy Weasley about Snape.
After a terrible first Potions lesson with Snape, Harry and Ron visit Hagrid, who lives in a rustic house on the edge of the Forbidden Forest. There they learn that the attempted robbery at Gringotts happened the day Harry withdrew money. Harry remembers that Hagrid had removed a small package from the vault that was broken into and searched.
During the new pupils' first broom-flying lesson, Neville Longbottom breaks his wrist, and Draco takes advantage to throw the forgetful Neville's fragile Remembrall high in the air. Harry gives chase on his broomstick, catching the Remembrall inches from the ground. Professor McGonagall dashes out and appoints him as the new Seeker for the Gryffindor Quidditch team.[2]
When Draco tricks Ron and Harry, accompanied by Neville and Hermione Granger, into a midnight excursion, they accidentally enter a forbidden corridor and find a huge three-headed dog. The group hastily retreats, and Hermione notices that the dog is standing over a trap-door. Harry concludes that the monster is guarding the package Hagrid retrieved from Gringotts.
After Ron criticises Hermione's ostentatious proficiency in Charms, she hides in tears in the girls' toilet. At the Halloween Night dinner, Professor Quirrell hastily reports that a troll has entered the dungeons. While everyone else returns to their dormitories, Harry and Ron rush to warn Hermione. The troll corners Hermione in the toilet but when Harry sticks his wand up one of its nostrils, Ron uses the levitation spell to knock out the troll with its own club. Afterwards, several professors arrive and Hermione takes the blame for the battle and becomes a firm friend of the two boys.
The evening before Harry's first Quidditch match, he sees Snape receiving medical attention from Filch for a bite on his leg by the three-headed dog. During the game, Harry's broomstick goes out of control, endangering his life, and Hermione notices that Snape is staring at Harry and muttering. She dashes over to the Professors' stand, knocking over Professor Quirrell in her haste, and sets fire to Snape's robe. Harry regains control of his broomstick and catches the Golden Snitch, winning the game for Gryffindor. Hagrid refuses to believe that Snape was responsible for Harry's danger, but lets slip that he bought the three-headed dog, and that the monster is guarding a secret that belongs to Professor Dumbledore and someone called Nicolas Flamel.
Harry and the Weasleys stay at Hogwarts for Christmas, and one of Harry's presents, from an anonymous donor, is an Invisibility Cloak owned by his father. Harry uses the Cloak to search the library's Restricted Section for information about the mysterious Flamel, has to evade Snape and Filch after an enchanted book shrieks an alarm, and slips into a room containing the Mirror of Erised, which shows his parents and several of their ancestors. Harry becomes addicted to the Mirror's visions and is rescued by Professor Dumbledore, who explains that it shows what the viewer most desperately longs for.
When the rest of the pupils return for the next term, Draco plays a prank on Neville, and Harry consoles Neville with a sweet. The collectible card wrapped with the sweet identifies Flamel as an alchemist. Hermione soon finds that he is a 665-year-old man who possesses the only known Philosopher's Stone, from which can be extracted an elixir of life. A few days later Harry notices Snape sneaking towards the outskirts of the Forbidden Forest. There he half-hears a furtive conversation about the Philosopher's Stone, in which Snape asks Professor Quirrell if he has found a way past the three-headed dog and menacingly tells Quirrell to decide whose side he is on. Harry concludes that Snape is trying to steal the Stone and Quirrell has helped prepare a series of defences for it, which was an almost fatal mistake.
The three friends discover that Hagrid is raising a baby dragon, which is against wizard law, and arrange to smuggle it out of the country around midnight. Draco arrives, hoping to raise the alarm and get them into trouble, and goes to tell Professor McGonagall. Although Ron is bitten by the dragon and is sent to the infirmary, Harry and Hermione spirit the dragon safely away. However, they are caught, and Harry loses the Invisibility Cloak. As part of their punishment, Harry, Hermione, Draco, and Neville (who, trying to stop Harry and Hermione after hearing what Draco had been saying, had been caught by McGonagall as well) are compelled to help Hagrid to rescue a badly-injured unicorn in the Forbidden Forest. They split into two parties, and Harry and Draco find the unicorn dead, surrounded by its blood. A hooded figure crawls to the corpse and drinks the blood, while Draco screams and flees. The hooded figure moves towards Harry, who is knocked out by an agonising pain spreading from his scar. When Harry regains consciousness, the hooded figure has gone and a centaur, Firenze, offers to give him a ride back to the school. The centaur tells Harry that drinking a unicorn's blood will save the life of a mortally injured person, but at the price of having a cursed life from that moment on. Firenze suggests Voldemort drank the unicorn's blood to gain enough strength to make the elixir of life from the Philosopher's Stone, and regain full health by drinking that. On his return, Harry finds that someone has slipped the Invisibility Cloak under his sheets.
A few weeks later, while relaxing after the end-of-session examinations, Harry suddenly wonders how something as illegal as a dragon's egg came into Hagrid's possession. The gamekeeper says he was given it by a hooded stranger who bought him several drinks and asked him how to get past the three-headed dog, which Hagrid admits is easy  music sends it to sleep. Realising that one of the Philosopher's Stone's defences is no longer secure, Harry goes to inform Professor Dumbledore, only to find that the headmaster has just left for an important meeting. Harry concludes that Snape faked the message that called Dumbledore away and will try to steal the Stone that night.
Covered by the Invisibility Cloak, Harry and his two friends go to the three-headed dog's chamber, where Harry sends the beast to sleep by playing a flute given to him by Hagrid for Christmas. After lifting the trap-door, they encounter a series of obstacles, each of which requires special skills possessed by one of the three, and one of which requires Ron to sacrifice himself in a game of wizard's chess. In the final room Harry, now alone, finds Quirrell rather than Snape. Quirrell admits that he let in the troll that tried to kill Hermione on Halloween, and that he tried to kill Harry during the first Quidditch match but was knocked over by Hermione. Snape had been trying to protect Harry and suspected Quirrell. Quirrell serves Voldemort and, after failing to steal the Philosopher's Stone from Gringotts, allowed his master to possess him in order to improve their chances of success. However the only other object in the room is the Mirror of Erised, and Quirrell can see no sign of the Stone. At Voldemort's bidding, Quirrell forces Harry to stand in front of the Mirror. Harry feels the Stone drop into his pocket and tries to stall. Quirrell removes his turban, revealing the face of Voldemort on the back of his head. Voldemort/Quirrell tries to grab the Stone from Harry, but simply touching Harry causes Quirrell's flesh to burn. After further struggles Harry passes out.
He awakes in the school hospital, where Professor Dumbledore tells him that he survived because his mother sacrificed her life to protect him, and Voldemort could not understand the power of such love. Voldemort left Quirrell to die, and is likely to return by some other means. Dumbledore had foreseen that the Mirror would show Voldemort/Quirrell only themselves making the elixir of life, as they wanted to use the Philosopher's Stone; Harry was able to see the Stone in the Mirror because he wanted to find it but not to use it. The Stone has now been destroyed.
Harry returns to the Dursleys for the summer holiday, but does not tell them that under-age wizards are forbidden to use magic outside Hogwarts.
After ten years, Harry became an eleven year-old boy. The Dursleys have kept the truth about Harry's parents from him, but it is revealed in the form of Rubeus Hagrid, who tells Harry that he is a wizard and has been accepted at Hogwarts for the autumn term. Harry takes the train to Hogwarts from King's Cross Station. On the train, Harry sits with and quickly befriends Ron Weasley; the two are also briefly visited by Neville Longbottom and Hermione Granger. Later on in the journey, Malfoy comes into Harry and Ron's compartment with his friends Crabbe and Goyle and introduces himself. After Ron laughs at Draco's name, Draco offers to help Harry distinguish the wrong sort of wizards, but Harry declines.
Upon arrival, the Sorting Hat places Harry, Hermione, Neville and Ron into Gryffindor House, one of the school's four houses, while Draco and his cronies are placed in Slytherin. After a broom-mounted game to save Neville's Remembrall, Harry joins Gryffindor's Quidditch team as their youngest Seeker in over a century.
Shortly after school begins, Harry and his friends hear that someone broke into a previously emptied vault at the wizarding bank, Gringotts. The mystery deepens when they discover a monstrous three-headed dog, Fluffy, who guards a trapdoor in the forbidden third floor passageway. On Halloween, a troll enters the castle and traps Hermione in one of the girls' lavatories. Harry and Ron rescue her, but are caught by Professor McGonagall. Hermione defends the boys and takes the blame, which results in the three becoming close friends.
Harry's broom becomes jinxed during his first Quidditch match, nearly resulting in Harry falling from a great height. Hermione believes that Professor Snape has cursed the broom and distracts him by setting his robes on fire, allowing Harry to catch the Golden Snitch and win the game for Gryffindor.
At Christmas, Harry receives his father's Invisibility Cloak from an unknown source. Later, he discovers the Mirror of Erised, a strange mirror that shows Harry surrounded by his parents and the extended family he never knew. Later, Harry learns that Nicolas Flamel is the maker of the Sorcerer's Stone, a stone that gives the owner eternal life.
Harry sees Professor Snape interrogating Professor Quirrell about getting past Fluffy, seemingly confirming the suspicion that Snape is trying to steal the Philosopher's Stone in order to restore Lord Voldemort to power. The trio discover that Hagrid is hiding a dragon egg, which hatches; since dragon breeding is illegal, they convince Hagrid to send the dragon to live with others of its kind. Harry and Hermione are caught returning to their dormitories after sending Norbert off and are forced to serve detention with Hagrid in the Forbidden Forest. In the forest, Harry sees a hooded figure drink the blood of an injured unicorn. Firenze, a centaur, tells Harry that the hooded figure is Voldemort.
Hagrid accidentally tells Harry, Ron, and Hermione how to get past Fluffy; and they rush to tell the headmaster, Albus Dumbledore, what they know, only to find that he has been called away from the school. Convinced that Dumbledore's summons was a red herring to take him away while the Philosopher's Stone is stolen, the trio set out to reach the Stone first. They navigate a series of complex magical challenges set up by the school's faculty, and at the end of these challenges, Harry enters the inner chamber alone, only to find that it is the timid Professor Quirrell, not Snape, who is after the Stone. The final challenge protecting the Stone is the Mirror of Erised. Quirrell forces Harry to look into the mirror to discover where the Stone is hidden; and Harry successfully resists, and the Stone drops into his own pocket. Lord Voldemort reveals himself: he has possessed Quirrell and appears as a ghastly face on the back of Quirrell's head. Quirrell tries to attack Harry, but merely touching Harry proves to be agony for him. Voldemort flees and Quirrell dies as Dumbledore arrives back in time to save Harry.
As Harry recovers, Dumbledore confirms that Lily had died while trying to protect Harry as an infant. Her pure, loving sacrifice provides her son with an ancient magical protection against Voldemort's lethal spells. Dumbledore also explains that the Philosopher's Stone has been destroyed to prevent Voldemort from ever using it. He then tells Harry that only those who wanted to find the Stone, but not use it, would be able to retrieve it from the mirror, which is why Harry could acquire it. When Harry asks Dumbledore why Voldemort attempted to kill him when he was an infant, Dumbledore promises to tell Harry when he is older.
At the end-of-year feast, where Harry is welcomed as a hero. Dumbledore gives a few last-minute additions, granting enough points to Harry, Ron, Hermione, and Neville Longbottom for Gryffindor to win the House Cup, ending Slytherin's six-year reign as house champions.
Harry Potter is an orphan whom Rowling imagined as a "scrawny, black-haired, bespectacled boy who didn't know he was a wizard."[3] She developed the series' story and characters to explain how Harry came to be in this situation and how his life unfolded from there.[4] Apart from the first chapter, the events of this book take place just before and in the year following Harry's eleventh birthday. Voldemort's attack left a lightning bolt-shaped scar on Harry's forehead,[4] which produces stabbing pains when Voldemort or a close associate of the dark wizard feels any strong emotion. Harry has prodigious natural talent for Quidditch and the ability to persuade friends by passionate speeches.
Ron Weasley is Harry's age and Rowling describes him as the ultimate best friend, "always there when you need him."[5]
Hermione Granger, the daughter of an all-Muggle family, is a bossy girl who has apparently memorised most of the textbooks before the start of term. Rowling described Hermione as a "very logical, upright and good" character[6] with "a lot of insecurity and a great fear of failure beneath her swottiness".[6] Despite her nagging efforts to keep Harry and Ron out of trouble, she becomes a close friend of the two boys after they save her from a troll, and her magical and analytical skills play a vital part in finding the Philosopher's Stone.
Neville Longbottom is a plump, diffident boy, so forgetful that his grandmother gives him a Remembrall. Neville's magical abilities are weak and appeared just in time to save his life when he was eight. Despite his timidity, Neville will fight anyone after some encouragement or if he thinks it is right and important.
Hagrid, a half-giant nearly 12 feet (3.7m) tall, with tangled black hair and beard, was expelled from Hogwarts and his wand was broken, but Professor Dumbledore let him stay on as the school's gamekeeper, a job which enables him to lavish affection and pet names on even the most dangerous of magical creatures. Hagrid is fiercely loyal to Dumbledore and quickly becomes a close friend of Harry, Ron and later Hermione, but his carelessness makes him unreliable.
Professor Dumbledore, a tall, thin man who wears half-moon spectacles and has silver hair and a beard that tucks into his belt, is the headmaster of Hogwarts, and thought to be the only wizard Voldemort fears. Dumbledore, while renowned for his achievements in magic, finds it difficult to resist sweets and has a whimsical sense of humour. Although he shrugs off praise, he is aware of his own brilliance. Rowling described him as the "epitome of goodness".[7]
Professor McGonagall, a tall, severe-looking woman with black hair tied in a tight bun, teaches Transfiguration, and sometimes transforms herself into a cat. She is in charge of Gryffindor House, and according to the author, "under that gruff exterior" is "a bit of an old softy".[8]
Petunia Dursley, the sister of Harry's mother Lily, is a thin woman with a long neck that she uses for spying on the neighbours. She regards her magical sister as a freak and tries to pretend that she never existed. Her husband Vernon is a heavily-built man whose irascible bluster covers a narrow mind and a fear of anything unusual. Their son Dudley is an overweight, spoiled bully.
Draco Malfoy is a slim, pale boy who speaks in a bored drawl. He is arrogant about his skill in Quidditch, and despises anyone who is not a pure blood wizard  and wizards who do not share his views. His parents had supported Voldemort, but changed sides after the dark wizard's disappearance. Draco avoids direct confrontations, and tries to get Harry and his friends into trouble.
Twitching, stammering Professor Quirrell teaches Defence Against the Dark Arts. Reputedly he was a brilliant scholar, but his nerve was shattered by an encounter with vampires. Quirrell wears a turban to conceal the fact that he is voluntarily possessed by Voldemort, whose face appears on the back of Quirrell's head.
Professor Snape, who has a hooked nose, sallow complexion and greasy black hair, teaches Potions, but is eager to teach Defence Against the Dark Arts. Snape praises pupils in Slytherin, his own House, but seizes every opportunity to humiliate others, especially Harry. Several incidents, beginning with the shooting pain in Harry's scar near the end of the first dinner, lead Harry and his friends to think Snape is a follower of Voldemort.

The school's caretaker, Filch, knows the school's secret passages better than anyone else except possibly the Weasley twins. His cat, Mrs. Norris, aids his hunts for misbehaving pupils. Other members of Hogwarts staff include: the dumpy Herbology teacher Professor Sprout; Professor Flitwick, the tiny and excitable Charms teacher, who is discreetly friendly towards Harry; the soporific History of Magic teacher, Professor Binns, a ghost who does not seem to have noticed his own death; and Madam Hooch, the Quidditch coach, who is strict but a considerate, methodical teacher. The poltergeist Peeves wanders around the buildings causing trouble for whomever he can.
In the book, Rowling introduced an eclectic cast of characters. The first character to be introduced is Vernon Dursley, Harry's uncle. Most of the actions centre on the eponymous hero Harry Potter, an orphan who escapes his miserable childhood with the Dursley family. Rowling imagined him as a "scrawny, black-haired, bespectacled boy who didn't know he was a wizard",[3] and says she transferred part of her pain about losing her mother to him.[9] During the book, Harry makes two close friends, Ronald Weasley and Hermione Granger. Ron is described by Rowling as the ultimate best friend, "always there when you need him".[5] Rowling has described Hermione as a "very logical, upright and good" character[6] with "a lot of insecurity and a great fear of failure beneath her swottiness".[6]
Rowling also imagined a supporting cast of adults. Headmaster of Hogwarts is powerful but kind wizard Albus Dumbledore, who becomes Harry's confidant; Rowling described him as "epitome of goodness".[7] His right hand is severe Minerva McGonagall, who according to the author "under that gruff exterior" is "a bit of an old softy",[8] the friendly half-giant Rubeus Hagrid, who saved Harry from the Dursley family and the sinister Severus Snape.[10] Professor Quirrell is also featured in the novel.
The main antagonists are Draco Malfoy, an elitist, bullying classmate[11] and Lord Voldemort, the most powerful evil wizard who becomes disembodied when he tries to kill baby Harry. According to a 1999 interview with Rowling, the character of Voldemort was created as a literary foil for Harry, and his backstory was intentionally not fleshed-out at first:
In 1990 Jo Rowling, as she preferred to be known,[13] wanted to move with her boyfriend to a flat in Manchester and in her words, "One weekend after flat hunting, I took the train back to London on my own and the idea for Harry Potter fell into my head... A scrawny, little, black-haired, bespectacled boy became more and more of a wizard to me... I began to write Philosopher's Stone that very evening. Although, the first couple of pages look nothing like the finished product."[9] Then Rowling's mother died and, to cope with her pain, Rowling transferred her own anguish to the orphan Harry.[9] Rowling spent six years working on Harry Potter and the Philosopher's Stone, and in 1996 obtained a grant of 8,000 from the Scottish Arts Council, which enabled her to finish the book and plan the sequels.[14] She sent the book to an agent and a publisher, and then the second agent she approached spent a year trying to sell the book to publishers, most of whom thought it was too long at about 90,000words. Barry Cunningham, who was building a portfolio of distinctive fantasies by new authors for Bloomsbury Children's Books, recommended accepting the book,[14] and the eight-year-old daughter of Bloomsbury's chief executive said it was "so much better than anything else".[15]
Bloomsbury accepted the book, paying Rowling a 2,500 advance,[16] and Cunningham sent proof copies to carefully chosen authors, critics and booksellers in order to obtain comments that could be quoted when the book was launched.[14] He was less concerned about the book's length than about its author's name, as the title sounded like a boys' book and boys prefer books by male authors. Rowling therefore adopted the nom de plume J.K. Rowling just before publication.[14] In June 1997, Bloomsbury published Philosopher's Stone with an initial print-run of 500 copies in hardback, three hundred of which were distributed to libraries.[17] The short initial print run was standard for first novels, and Cunningham hoped booksellers would read the book and recommend it to customers.[14] Examples from this initial print run have become quite valuable, selling for as much as US$33,460 in a 2007 Heritage Auction.[18]
Lindsey Fraser, who had supplied one of the blurb comments,[14] wrote what is thought to be the first published review, in The Scotsman on 28 June 1997. She described Harry Potter and the Philosopher's Stone as "a hugely entertaining thriller" and Rowling as "a first-rate writer for children".[14][19] Another early review, in The Herald, said, "I have yet to find a child who can put it down." Newspapers outside Scotland started to notice the book, with glowing reviews in The Guardian, The Sunday Times and The Mail on Sunday, and in September 1997 Books for Keeps, a magazine that specialised in children's books, gave the novel four stars out of five.[14]
The Mail on Sunday rated it as "the most imaginative debut since Roald Dahl"; a view echoed by the Sunday Times ("comparisons to Dahl are, this time, justified"), while The Guardian called it "a richly textured novel given lift-off by an inventive wit" and The Scotsman said it had "all the makings of a classic".[20]
In 1997 the UK edition won a National Book Award and a gold medal in the 9to11year-olds category of the Nestl Smarties Book Prize.[21] The Smarties award, which is voted for by children, made the book well-known within six months of publication, while most children's books have to wait for years.[14]
The following year, Philosopher's Stone won almost all the other major UK awards that were decided by children.[14][22] It was also shortlisted for children's books awards adjudicated by adults,[23] but did not win. Sandra Beckett comments that books which were popular with children were regarded as undemanding and as not of the highest literary standards  for example the literary establishment disdained the works of Roald Dahl, an overwhelming favourite of children before the appearance of Rowling's books.[24]
Harry Potter and the Philosopher's Stone won two publishing industry awards given for sales rather than literary merit, the British Book Awards Children's Book of the Year and the Booksellers' Association / Bookseller Author of the Year.[14] By March 1999 UK editions had sold just over 300,000copies,[25] and the story was still the UK's best-selling title in December 2001.[26] A Braille edition was published in May 1998 by the Scottish Braille Press.[27]
Platform 9, from which the Hogwarts Express left London, was commemorated in the real-life King's Cross railway station with a sign and a trolley apparently passing through the wall.[28]
Scholastic Corporation bought the U.S. rights at the Bologna Book Fair in April 1997 for US$105,000, an unusually high sum for a children's book.[14] They thought that a child would not want to read a book with the word "philosopher" in the title and,[32] after some discussion, the American edition was published in October 1998 under the title Rowling suggested, Harry Potter and the Sorcerer's Stone.[14] Rowling claimed that she regretted this change and would have fought it if she had been in a stronger position at the time.[33] Philip Nel has pointed out that the change lost the connection with alchemy, and the meaning of some other terms changed in translation, for example from UK English "crumpets" to US English "muffin". While Rowling accepted the change from both UK English "mum" and Seamus Finnegan's Irish variant "mam" to "mom" in Harry Potter and the Sorcerer's Stone, she vetoed this change in the later books. However Nel considered that Scholastic's translations were considerably more sensitive than most of those imposed on UK English books of the time, and that some other changes could be regarded as useful copyedits.[29] Since the UK editions of early titles in the series were published a few months earlier than the American versions, some American readers became familiar with the British English versions after buying them via the Internet.[34]
At first the most prestigious reviewers ignored the book, leaving it to book trade and library publications such as Kirkus Reviews and Booklist, which examined it only by the entertainment-oriented criteria of children's fiction. However, more penetrating specialist reviews (such as one by Cooperative Children's Book Center Choices, which pointed out the complexity, depth and consistency of the world Rowling had built) attracted the attention of reviewers in major newspapers.[35] Although The Boston Globe and Michael Winerip in The New York Times complained that the final chapters were the weakest part of the book[19][36] they and most other American reviewers gave glowing praise.[14][19] A year later the US edition was selected as an American Library Association Notable Book, a Publishers Weekly Best Book of 1998, and a New York Public Library 1998 Best Book of the Year, and won Parenting Magazine's Book of the Year Award for 1998,[21] the School Library Journal Best Book of the Year, and the American Library Association Best Book for Young Adults.[14]
In August 1999 Harry Potter and the Sorcerer's Stone topped the New York Times list of best-selling fiction,[37] and stayed near the top of the list for much of 1999 and 2000, until the New York Times split its list into children's and adult sections under pressure from other publishers who were eager to see their books given higher placings.[24][35] Publishers Weekly's report in December 2001 on cumulative sales of children's fiction placed Harry Potter and the Sorcerer's Stone 19th among hardbacks (over 5 million copies) and 7th among paperbacks (over 6.6million copies).[38]
In May 2008, Scholastic announced the creation of a 10th Anniversary Edition of the book that was released on 23 September 2008 to mark the tenth anniversary of the original American release.[39]
By mid-2008 official translations of the book were published in 67 languages.[40][41] Bloomsbury have published translations in Latin and in Ancient Greek,[42][43] and the latter was described as "one of the most important pieces of Ancient Greek prose written in many centuries".[44]
Religious controversy surrounding the Harry Potter and the Philosopher's Stone along with the rest of the Harry Potter series have stemmed mainly from assertions that the novel contains occult or Satanic subtexts. In the United States, calls for the book to be banned from schools have led occasionally to widely publicised legal challenges, usually on the grounds that witchcraft is a government-recognised religion and that to allow the novels to be held in public schools violates the separation of church and state.[45][46][47] The series was at the top of the American Library Association's "most challenged books" list for 19992001.[21]
Religious opposition has also surfaced in other nations. The Orthodox churches of Greece and Bulgaria have campaigned against the series.[48][49] The books have been banned from private schools in the United Arab Emirates and criticised in the Iranian state-run press.[50][51]
Roman Catholic opinion over the series was divided. In 2003 Catholic World Report criticised Harry's disrespect for rules and authority, and regarded the series' mixing of the magical and mundane worlds as "a fundamental rejection of the divine order in creation."[52] In 2005 Cardinal Joseph Ratzinger, who became Pope later that year but was then Prefect of the Congregation for the Doctrine of the Faith, described the series as "subtle seductions, which act unnoticed and by this deeply distort Christianity in the soul before it can grow properly,"[53] and gave permission for publication of the letter that expressed this opinion.[54] A spokesman for the Archbishop of Westminster said that Cardinal Ratzinger's words were not binding as they were not an official pronouncement of the Congregation for the Doctrine of the Faith.[53] In 2003 Monsignor Peter Fleetwood, a member of a Church working party on New Age phenomena, said that the Harry Potter stories "are not bad or a banner for anti-Christian theology. They help children understand the difference between good and evil," that Rowling's approach was Christian, and that the stories illustrated the need to make sacrifices in order to defeat evil.[53][55]
Some religious responses have been positive. "At least as much as they've been attacked from a theological point of view," notes Rowling, "[the books] have been lauded and taken into pulpit, and most interesting and satisfying for me, it's been by several different faiths."[56] Emily Griesinger wrote that fantasy literature helps children to survive reality for long enough to learn how to deal with it, described Harry's first passage through to Platform 9 as an application of faith and hope, and his encounter with the Sorting Hat as the first of many in which Harry is shaped by the choices he makes. She noted that the self-sacrifice of Harry's mother, which protects the boy in the first book and throughout the series, was the most powerful of the "deeper magics" that transcend the magical "technology" of the wizards, and one which the power-hungry Voldemort fails to understand.[57]
Philip Nel highlighted the influence of Jane Austen, whom Rowling has greatly admired since the age of twelve. Both novelists encourage re-reading, because details that look insignificant foreshadow important events or characters much later in the story-line  for example Sirius Black is briefly mentioned near the beginning of Harry Potter and the Philosopher's Stone, and then becomes a major character in the third to fifth books. Like Austen's heroines, Harry often has to re-examine his ideas near the ends of books. Some social behaviour in the Harry Potter books is remininiscent of Austen, for example the excited communal reading of letters. Both authors satirise social behaviour and give characters names that express their personalities. However in Nel's opinion Rowling's humour is more based on caricature and the names she invents are more like those found in Charles Dickens's stories,[58] and Amanda Cockrell noted that many of these express their owners' traits through allusions that run from ancient Roman mythology to eighteenth century German literature.[59] Rowling, like the Narnia series' author C.S. Lewis, thinks there is no rigid distinction between stories for children and for adults. Nel also noted that, like many good writers for children, Rowling combines literary genres  fantasy, young-adult fiction, boarding school stories, Bildungsroman and many others.[60]
Some reviewers compared Philosopher's Stone to the stories of Roald Dahl, who died in 1990. Many writers since the 1970s had been hailed as his successor, but none had attained anything near his popularity with children and, in a poll conducted shortly after the launch of Philosopher's Stone, seven of the ten most popular children's books were by Dahl, including the one in top place. The only other really popular children's author of the late 1990s was an American, R. L. Stine. Some of the story elements in Philosopher's Stone resembled parts of Dahl's stories; for example, the hero of James and the Giant Peach lost his parents and had to live with a pair of unpleasant aunts  one fat and one thin rather like Mr. and Mrs. Dursley, who treated Harry as a servant. However Harry Potter was a distinctive creation, able to take on the responsibilities of an adult while remaining a child inside.[14]
Librarian Nancy Knapp and marketing professor Stephen Brown noted the liveliness and detail of descriptions, especially of shop scenes such as Diagon Alley.[21][61] Tad Brennan commented that Rowling's writing resembles that of Homer: "rapid, plain, and direct in expression."[44] Stephen King admired "the sort of playful details of which only British fantasists seem capable" and concluded that they worked because Rowling enjoys a quick giggle and then moves briskly forward.[62]
Nicholas Tucker described the early Harry Potter books as looking back to Victorian and Edwardian children's stories: Hogwarts was an old-style boarding school in which the teachers addressed pupils formally by their surnames and were most concerned with the reputations of the houses with which they were associated; characters' personalities were plainly shown by their appearances, starting with the Dursleys; evil or malicious characters were to be crushed rather than reformed, including Filch's cat Mrs Norris; and the hero, a mistreated orphan who found his true place in life, was charismatic and good at sports, but considerate and protective towards the weak.[63] Several other commentators have stated that the books present a highly stratified society including many social stereotypes.[55] However Karin Westerman drew parallels with 1990s Britain: a class system that was breaking down but defended by those whose power and status it upheld; the multi-ethnic composition of Hogwarts' students; the racial tensions between the various intelligent species; and school bullying.[64]
Susan Hall wrote that there is no rule of law in the books, as the actions of Ministry of Magic officials are unconstrained by laws, accountability or any kind of legal challenge. This provides an opportunity for Voldemort to offer his own horrific version of order. As a side-effect Harry and Hermione, who were brought up in the highly regulated Muggle world, find solutions by thinking in ways unfamiliar to wizards. For example Hermione notes that one obstacle to finding the Philosopher's Stone is a test of logic rather than magical power, and that most wizards have no chance of solving it.[65]
Nel suggested that the unflattering characterisation of the extremely conventional, status-conscious, materialistic Dursleys was Rowling's reaction to the family policies of the British government in the early 1990s, which treated the married heterosexual couple as the "preferred norm", while the author was a single mother. Harry's relationships with adult and juvenile wizards are based on affection and loyalty. This is reflected in his happiness whenever he is a temporary member of the Weasley family throughout the series, and in his treatment of first Rubeus Hagrid and later Remus Lupin and Sirius Black as father-figures.[59][66]
The second book, Harry Potter and the Chamber of Secrets, was originally published in the UK on 2July1998 and in the US on 2June1999.[67][68] Harry Potter and the Prisoner of Azkaban was then published a year later in the UK on 8July1999 and in the US on 8September1999.[67][68] Harry Potter and the Goblet of Fire was published on 8July2000 at the same time by Bloomsbury and Scholastic.[69] Harry Potter and the Order of the Phoenix is the longest book in the series at 766 pages in the UK version and 870 pages in the US version.[70] It was published worldwide in English on 21June2003.[71] Harry Potter and the Half-Blood Prince was published on 16July2005, and sold 11million copies in the first 24hours of its worldwide release.[72][73] The seventh and final novel, Harry Potter and the Deathly Hallows, was published 21July2007.[74] The book sold 11 million copies within 24hours of its release: 2.7million copies in the UK and 8.3million in the US.[75]
In 1999, Rowling sold the film rights of the first four Harry Potter books to Warner Bros. for a reported 1million ($1,982,900).[76] Rowling demanded that the principal cast be kept strictly British, but allowed for the casting of Irish actors such as the late Richard Harris as Dumbledore, and of foreign actors as characters of the same nationalities in later books.[77] After extensive casting,[78] filming began in October 2000 at Leavesden Film Studios and in London, with production ending in July 2001.[79] Harry Potter and the Philosopher's Stone was released in London on 14 November 2001.[80][81] Reviewers' comments were positive, as reflected by a 78% Fresh rating on Rotten Tomatoes,[82] and by a score of 64% at Metacritic representing "generally favourable reviews".[83]
Video games loosely based on the book were released between 2001 and 2003, generally under the American title Harry Potter and the Sorcerer's Stone. Most were published by Electronic Arts but produced by different developers:
Educationalists have found that children's literacy is directly related to how many words they read per year, and they read much more if they find material they like. A 2001 survey by The New York Times estimated that almost 60% of US children aged between 6 and 17 had read at least one Harry Potter book. Surveys in other countries, including India and South Africa, found that children were enthusiastic about the series. Since even the first two books are quite long, a child who has read the first four will have read over four times the number of pages in a year's worth of school reading texts. This greatly improves children's skills and their motivation to read.[21]
Writers on education and business subjects have used the book as an object lesson. Writing about clinical teaching in medical schools, Jennifer Conn contrasted Snape's technical expertise with his intimidating behaviour towards students; on the other hand Quidditch coach Madam Hooch illustrated useful techniques in the teaching of physical skills, including breaking down complex actions into sequences of simple ones and helping students to avoid common errors.[100] Joyce Fields wrote that the books illustrate four of the five main topics in a typical first-year sociology class: "sociological concepts including culture, society, and socialisation; stratification and social inequality; social institutions; and social theory".[55]
Stephen Brown noted that the early Harry Potter books, especially Harry Potter and the Philosopher's Stone, were a runaway success despite inadequate and poorly-organised marketing. Brown advised marketing executives to be less preoccupied with rigorous statistical analyses and the "analysis, planning, implementation, and control" model of management. Instead he recommended that they should treat the stories as "a marketing masterclass", full of enticing products and brand names.[61] For example, a real-world analogue of Bertie Bott's Every Flavour Beans was introduced under licence in 2000 by toymaker Hasbro.[61][101]

1 Synopsis

1.1 Plot
1.2 Main characters


1.1 Plot
1.2 Main characters
2 Development, publication and reception

2.1 Development
2.2 UK publication and reception
2.3 U.S. publication and reception
2.4 Translations
2.5 Religious controversy


2.1 Development
2.2 UK publication and reception
2.3 U.S. publication and reception
2.4 Translations
2.5 Religious controversy
3 Style and themes
4 Legacy

4.1 Sequels
4.2 Film version
4.3 Video games
4.4 Uses in education and business


4.1 Sequels
4.2 Film version
4.3 Video games
4.4 Uses in education and business
5 References
6 External links
1.1 Plot
1.2 Main characters
2.1 Development
2.2 UK publication and reception
2.3 U.S. publication and reception
2.4 Translations
2.5 Religious controversy
4.1 Sequels
4.2 Film version
4.3 Video games
4.4 Uses in education and business
 Harry Potter at Wikipedia books
 Quotations related to Harry Potter and the Philosopher's Stone at Wikiquote
Background information and storylines from the Harry Potter Lexicon
Harry Potter and the Philosopher's Stone on Open Library at the Internet Archive
.
#(`*Harry Potter and the Chamber of Secrets*`)#.
Harry Potter and the Chamber of Secrets is the second novel in the Harry Potter series written by J. K. Rowling. The plot follows Harry's second year at Hogwarts School of Witchcraft and Wizardry, during which a series of messages on the walls on the school's corridors warn that the "Chamber of Secrets" has been opened and that the "heir of Slytherin" would kill all pupils who do not come from all-magical families. These threats are followed by attacks which leave residents of the school "petrified" (that is, frozen). Throughout the year, Harry and his friends Ron Weasley and Hermione Granger investigate the attacks, and Harry is confronted by Tom Riddle, later known as Lord Voldemort, who is attempting to regain full power.
The book was published in the United Kingdom on 2July1998 by Bloomsbury and in the United States on 2June1999 by Scholastic Inc. Although Rowling found it difficult to finish the book, it won high praise and awards from critics, young readers and the book industry, although some critics thought the story was perhaps too frightening for younger children. Although much like the rest of the series, some religious authorities have condemned its use of magical themes, while others have praised its emphasis on self-sacrifice and on the way in which a person's character is the result of the person's choices.
Several commentators have noted that personal identity is a strong theme in the book, and that it addresses issues of racism through the treatment of non-magical, non-human and non-living characters. Some commentators regard the diary as a warning against uncritical acceptance of information from sources whose motives and reliability cannot be checked. Institutional authority is portrayed as self-serving and incompetent. The book is also known to have some connections to the sixth novel of the series, Harry Potter and the Half-Blood Prince.
The film version of Harry Potter and the Chamber of Secrets, released in 2002, became the third film to exceed 600million in international box office sales and received generally favourable reviews. Video games loosely based on Harry Potter and the Chamber of Secrets were also released for several platforms, and most obtained favourable reviews.
Harry Potter and the Chamber of Secrets begins as Harry spends a miserable summer with his only remaining family, the Dursleys. During a dinner party hosted by his uncle and aunt, Harry is visited by Dobby, a house-elf. Dobby warns Harry not to return to Hogwarts, the magical school for wizards that Harry attended the previous year, explaining that terrible things will happen there. Harry politely disregards the warning, and Dobby wreaks havoc in the kitchen, infuriating the Dursleys. The Dursleys angrily imprison Harry in his room for a while after they find from a letter that Harry is not allowed to use magic away from Hogwarts. Harry is rescued by his friend Ron Weasley and his brothers Fred and George in a flying car, and spends the rest of the summer at the Weasley home.
When Harry uses Floo Powder to get to Diagon Alley he accidentally ends up in a dark-arts dealing end of town, Knockturn Alley. Fortunately, he meets Hagrid who gets him back to Diagon Alley. While shopping for school supplies there with the Weasleys, Harry encounters Gilderoy Lockhart, a wizard famous for all manner of deeds, who announces he is the new Defense Against the Dark Arts teacher, and demands to be in a photo shoot with Harry. Harry then encounters Lucius Malfoy, a Hogwarts governor and the father of the school bully, Draco, who gets into an argument with Ron's father when he insults the Weasley family. As Harry prepares to return to Hogwarts, he finds that he and Ron are unable to go through the secret entrance to Platform 9 , so they fly the Weasley's car to Hogwarts. They land messily, and both boys are detained for obvious reasons. Next day Molly Weasley sends a Howler to Ron, a letter that berates him with her much louder voice, and threatens to send him home if he gets into trouble again.
Lockhart quickly proves to be an incompetent teacher, more concerned with students learning about his personal accomplishments. On Halloween, something petrifies the school caretaker's cat and writes a message declaring that "The Chamber of Secrets" has been opened. Before the cat is attacked, Harry twice hears an eerie voice. He hears it first during his detention and second during a party, moments before the cat is attacked, and third before a Quidditch match. Everybody in the school is alarmed. Harry, Ron and their other friend, Hermione Granger, learn that during the founding of Hogwarts one of the founders, Salazar Slytherin, left the school, disagreeing with the decision to teach magic to Muggle-born students. According to legend, Slytherin secretly built the Chamber of Secrets, which supposedly houses a monster only Slytherin's heir can control.
Suspecting that Draco is the heir of Slytherin, the trio start making Polyjuice Potion, a brew which allows them to take on another's form. During the school's first game of Quidditch, Harry is pursued continually by a Bludger, an enchanted ball that knocks players off their brooms, despite their purpose being to unseat as many players as possible. As a result, Harry's arm is broken, and Lockhart then proceeds to unintentionally remove the broken bones. That night, as he recovers from the injury, Harry is visited by Dobby, who admits to having orchestrated the platform incident and the rogue Bludger, both of which were attempts to keep Harry away from Hogwarts. Soon after, a first year student, Colin Creevy, is attacked and petrified.
Lockhart begins a dueling club; and during the first meeting Harry unknowingly speaks Parseltongue to persuade a snake from attacking a student. Harry's ability frightens the others because Salazar Slytherin was also able to speak Parseltongue, and his heir would also have this ability. Harry comes under further suspicion when he stumbles upon the petrified bodies of Justin Finch-Fletchley and Nearly Headless Nick.
At Christmas, Harry and Ron use the finished Polyjuice Potion to disguise themselves as Draco's friends Crabbe for Ron and Goyle for Harry. Hermione was going to be Millicent Bulstrode, another Slytherin student, but was instead given some features of a cat, so does not joins them. Harry and Ron find out that Draco is not the heir of Slytherin, but he does reveal that the Chamber was opened before. No more attacks occur for a while, and right before Valentine's Day, Harry finds a diary in a flooded bathroom and takes it. He writes in the diary, which responds by writing back. Through this dialogue, Harry meets Tom Riddle, a boy who many years before had accused Hagrid, the Hogwarts gamekeeper, of first opening the Chamber of Secrets. Some time later, Harry's room is ransacked and the diary is taken.
Later on, Hermione and a Ravenclaw girl, Penelope Clearwater, are petrified. Harry and Ron venture out of the castle to question Hagrid. Before they can question him, however, the Minister of Magic, Cornelius Fudge, takes Hagrid to Azkaban as the supposed previous culprit; while at the same time Lucius Malfoy orchestrates the removal of Hogwarts Headmaster Albus Dumbledore for his failure to stop the attacks. As Hagrid is led away, he instructs the boys to "follow the spiders", as they will be able to provide more information. Harry and Ron then sneak into the Forbidden Forest to follow the spiders. They encounter Aragog who reveals the monster who killed the girl fifty years before was not a spider, that the girl's body was found in a bathroom, and that Hagrid is innocent. The boys are almost eaten by the colony of giant spiders. After they escape, Harry and Ron realize that Moaning Myrtle, the ghost who haunts the bathroom where they made the Polyjuice Potion, must have been the girl killed by the monster.
A few days later, Ron and Harry discover a piece of paper with a description of a Basilisk, a giant serpent that kills all who look it directly in the eye, in Hermione's petrified hand. They deduce that the Chamber's monster is indeed a Basilisk, since as a snake Harry can understand what it was saying when it travelled through the schools pipes. As for the petrifications, these were due to the victims looking at the Basilisk's eyes indirectly. Before the boys can act on their knowledge, the teachers announce that Ron's sister Ginny Weasley has been taken into the Chamber. Lockhart arrives, and is pressured by the other teachers into venturing into the Chamber and dealing with the monster unknown to them and to him. Harry and Ron go to give him their information, only to discover that he is a fraud. Regardless, they force him to accompany them to the Chamber.
The trio discovers that the entrance to the Chamber is in Myrtle's bathroom, and Harry's Parseltongue is able to open it. Inside the Chamber, Lockhart steals Ron's wand, and attempts to wipe the memories of the other two, in order to keep his secrets safe. However, Ron's wand, which has been broken since the car crash at the start of the year, deflects the spell back at Lockhart, wiping his memory. A cave-in then separates him and Ron from Harry, who is forced to proceed alone.
Harry finds Ginny's unconscious body, as well as the almost-physical form of Riddle. Riddle explains that Ginny has been talking with him via his diary. Through this, Riddle was able to possess Ginny, and use her to control the Basilisk. Ginny eventually became suspicious of the diary and tried to dispose of it in a toilet, where it was picked up by Harry, but stole it back for fear Harry would find out her role in the attacks. Riddle forced her to enter the Chamber, and possessing her soul was able to obtain a physical form. Riddle reveals that Tom Marvolo Riddle is an anagram although it is his real name for I am Lord Voldemort, who is the wizard who murdered Harry's parents eleven years ago, and sets the Basilisk on Harry.
Just when it seems Harry will be killed by the Basilisk, Fawkes, Dumbledore's pet phoenix, appears and blinds the Basilisk, depriving it of its deadly stare. Fawkes also drops the school Sorting Hat, from which Harry draws a sword and uses it to kill the Basilisk. As he does so, one of the Basilisk's fangs pierce Harry's arm and Harry is saved by Fawkes, as phoenix tears have immense healing powers. Harry then stabs the diary with a Basilisk fang, defeating Riddle and saving Ginny. The five of them later leave the Chamber. Back at Hogwarts, they discover that Dumbledore has been reinstated as Headmaster.
After Harry finishes explaining things to Dumbledore, Lucius Malfoy suddenly bursts in to Dumbledore's office. It is implied that he had planted Riddle's diary on Ginny in the first place, in the hopes of discrediting Dumbledore and the Weasleys. Discovering that Mr. Malfoy is Dobby's master, Harry then tricks him into freeing Dobby by concealing a sock in the diary (clothing being the only object able to free a house elf). All the petrified people are revived by a Mandrake Draught potion, Lockhart is sent to the wizarding hospital where he tries to regain his memories, and Hagrid returns to the school.
Rowling found it difficult to finish Harry Potter and the Chamber of Secrets because she was afraid it would not live up to the expectations raised by Harry Potter and the Philosopher's Stone (Harry Potter and the Sorcerer's Stone in the U.S.). After delivering the manuscript to Bloomsbury on schedule, she took it back for six weeks of revision.[1]
In early drafts of the book, the ghost Nearly Headless Nick sang a self-composed song explaining his condition and the circumstances of his death. This was cut as the book's editor did not care for the poem, which has been subsequently published as an extra on J. K. Rowling's official website.[2] The family background of Dean Thomas was removed because Rowling and her publishers considered it an "unnecessary digression", and she considered Neville Longbottom's own journey of discovery "more important to the central plot".[3]
Harry Potter and the Chamber of Secrets was published in the UK on 2July1998 and in the US on 2June1999.[4][5] It immediately took first place in UK best-seller lists, displacing popular authors such as John Grisham, Tom Clancy,[1] and Terry Pratchett,[6] and making Rowling the first author to win the British Book Awards Children's Book of the Year for two years in succession.[7] In June 1999 it went straight to the top of three US best-seller lists,[8] including The New York Times'.[9]
First edition printings had several errors, which were fixed in subsequent reprints.[10] Initially Dumbledore said that Voldemort was the last remaining ancestor of Salazar Slytherin, instead of his descendant.[10] Gilderoy Lockhart's book on werewolves is entitled Weekends with Werewolves at one point and Wanderings with Werewolves later in the book.[11]
"Harry Potter and the Chamber of Secrets" was met with near universal acclaim. In The Times, Deborah Loudon described Harry Potter and the Chamber of Secrets as a children's book that would be "re-read into adulthood" and highlighted its "strong plots, engaging characters, excellent jokes and a moral message which flows naturally from the story".[12] Fantasy author Charles de Lint agreed, and considered the second Harry Potter book as good as Harry Potter and the Philosopher's Stone, a rare achievement among series of books.[13] Thomas Wagner regarded the plot as very similar to that of the first book, based on searching for a secret hidden under the school. However, he enjoyed the parody of celebrities and their fans that centres round Gilderoy Lockhart, and approved of the book's handling of racism.[14] Tammy Nezol found the book more disturbing than its predecessor, particularly in the rash behaviour of Harry and his friends after Harry withholds information from Dumbledore, and in the human-like behaviour of the mandrakes used to make a potion that cures petrification. Nevertheless she considered the second story as enjoyable as the first.[15]
Mary Stuart thought the final conflict with Tom Riddle in the Chamber was almost as scary as in some of Stephen King's works, and perhaps too strong for young or timid children. She commented that "there are enough surprises and imaginative details thrown in as would normally fill five lesser books." Like other reviewers, she thought the book would give pleasure to both children and adult readers.[16] According to Philip Nel, the early reviews gave unalloyed praise while the later ones included some criticisms, although they still agreed that the book was outstanding.[17]
Writing after all seven books had been published, Graeme Davis regarded Harry Potter and the Chamber of Secrets as the weakest of the series, and agreed that the plot structure is much the same as in Harry Potter and the Philosopher's Stone. He described Fawkes's appearance to arm Harry and then to heal him as a deus ex machina: he said that the book does not explain how Fawkes knew where to find Harry; and Fawkes's timing had to be very precise, as arriving earlier would probably have prevented the battle with the basilisk, while arriving later would have been fatal to Harry and Ginny.[18]
Rowling's Harry Potter and the Chamber of Secrets was the recipient of several awards.[19] The American Library Association listed the novel among its 2000 Notable Children's Books,[20] as well as its Best Books for Young Adults.[21] In 1999, Booklist named Harry Potter and the Chamber of Secrets as one of its Editors' Choices,[22] and as one of its Top Ten Fantasy Novels for Youth.[19] The Cooperative Children's Book center made the novel a CCBC Choice of 2000 in the "Fiction for Children" category.[23] The novel also won Children's Book of the Year British Book Award,[24] and was shortlisted for the 1998 Guardian Children's Award and the 1998 Carnegie Award.[19]
Harry Potter and the Chamber of Secrets won the Nestl Smarties Book Prize 1998 Gold Medal in the 911 years division.[24] Rowling also won two other Nestl Smarties Book Prizes for Harry Potter and the Philosopher's Stone and Harry Potter and the Prisoner of Azkaban. The Scottish Arts Council awarded their first ever Childrens Book Award to the novel in 1999,[25] and it was also awarded Whitaker's Platinum Book Award in 2001.[19][26]
Religious controversy surrounding Harry Potter and the Chamber of the Secrets and the other books in the Harry Potter series mainly deal with claims that the novel contains occult or Satanic subtexts. Religious response to the series has not been exclusively negative, however, and several religious groups have spoken in defence of the moralistic themes found in the book. The American Library Association even placed the series atop the "most challenged books" list for 19992001.[27]
The Orthodox churches of Greece and Bulgaria have campaigned against the series,[28][29] and in the United States, calls for the book to be banned from schools have led to legal challenges. Most of these are held on the grounds that witchcraft is a government-recognised religion and that to allow the novels to be held in public schools violates the separation of church and state.[30][31][32]
Some religious responses have been positive. Emily Griesinger wrote that fantasy literature helps children to survive reality for long enough to learn how to deal with it, described Harry's first passage through to Platform 9 as an application of faith and hope, and his encounter with the Sorting Hat as the first of many in which Harry is shaped by the choices he makes. She noted that the self-sacrifice of Harry's mother, which protected the boy in the first book and throughout the series, was the most powerful of the "deeper magics" that transcend the magical "technology" of the wizards, and one which the power-hungry Voldemort fails to understand.[33] Christianity Today published an editorial in favour of the books in January 2000, calling the series a "Book of Virtues" and averring that although "modern witchcraft is indeed an ensnaring, seductive false religion that we must protect our children from", the Harry Potter books represent "wonderful examples of compassion, loyalty, courage, friendship, and even self-sacrifice".[34] "At least as much as they've been attacked from a theological point of view", commented Rowling, "[the books] have been lauded and taken into pulpit, and most interesting and satisfying for me, it's been by several different faiths".[35]
Harry Potter and the Chamber of Secrets continues the examination of what makes a person who he or she is, which began in the first book. As well as maintaining that Harry's identity is shaped by his decisions rather than any aspect of his birth,[15][36]Harry Potter and the Chamber of Secrets provides contrasting characters who try to conceal their true personalities: as Tammy Nezol puts it, Gilderoy Lockhart "lacks any real identity" because he is nothing more than a charming liar.[15] Riddle also complicates Harry's struggle to understand himself by pointing out the similarities between the two: "both half-bloods, orphans raised by Muggles, probably the only two Parselmouths to come to Hogwarts since the great Slytherin."[37]
Opposition to class, prejudice, and racism is a constant theme of the series. In Harry Potter and the Chamber of Secrets Harry's consideration and respect for others extends to the lowly, non-human Dobby and the ghost Nearly Headless Nick.[27] According to Marguerite Krause, achievements in the novel depend more on ingenuity and hard work than on natural talents.[38]
Edward Duffy, an associate professor at Marquette University, says that one of the central characters of Chamber of Secrets is a book, Tom Riddle's enchanted diary, which takes control of Ginny Weasley  just as Riddle planned. Duffy suggests that Rowling intended this as a warning against passively consuming information from sources that have their own agendas.[39] Although Bronwyn Williams and Amy Zenger regard the diary as more like an instant messaging or chat room system, they agree about the dangers of relying too much on the written word, which can camouflage the author, and they highlight a comical example, Lockhart's self-promoting books.[40]
Immorality and the portrayal of authority as negative are significant themes in the novel. Marguerite Krause states that there are few absolute moral rules in Harry Potter's world, for example Harry prefers to tell the truth, but lies whenever he considers it necessary very like his enemy Draco Malfoy.[38] At the end of Harry Potter and the Chamber of Secrets, Dumbledore retracts his promise to punish Harry, Ron, and Hermione if they break any more school rules  after Professor McGonagall estimates that they have broken over 100 and lavishly rewards them for ending the threat from the Chamber of Secrets.[41] Krause further states that authority figures and political institutions receive little respect from Rowling.[38] William MacNeil of Griffith University, Queensland, Australia states that the Minister for Magic is presented as a mediocrity.[42] In his article "Harry Potter And The Secular City", Ken Jacobson suggests that the Ministry as a whole is portrayed as a tangle of bureaucratic empires, saying that "Ministry officials busy themselves with minutiae (e.g. standardising cauldron thicknesses) and coin politically correct euphemisms like 'non-magical community' (for Muggles) and 'memory modification' (for magical brainwashing)."[36]
This novel implies that it begins in 1992: the cake for Nearly-Headless Nick's 500th deathday party bears the words "Sir Nicholas De Mimsy Porpington died 31 October 1492".[43][44]
Chamber of Secrets has many links with the sixth book of the series, Harry Potter and the Half-Blood Prince. In fact, Half-Blood Prince was the working title of Chamber of Secrets and Rowling says she originally intended to present some "crucial pieces of information" in the second book, but ultimately felt that "this information's proper home was book six".[45] Some objects that play significant roles in Half-Blood Prince first appear in Chamber of Secrets: the Hand of Glory and the opal necklace that are on sale in Borgin and Burkes; a Vanishing Cabinet in Hogwarts that is damaged by Peeves the Poltergeist; and Tom Riddle's diary, which is later shown to be a Horcrux.[46] Additionally, these two novels are the ones with the most focus on Harry's relationship with Ginny Weasley.
The film version of Harry Potter and the Chamber of Secrets was released in 2002.[47] Chris Columbus directed the film,[48] and the screenplay was written by Steve Kloves. It became the third film to exceed $600million in international box office sales, preceded by Titanic, released in 1997, and Harry Potter and the Philosopher's Stone, released in 2001.[49] The film was nominated for a Saturn Award for the Best Fantasy Film,[49][50] According to Metacritic, the film version of Harry Potter and the Chamber of Secrets received "generally favourable reviews" with an average score of 63%,[51] and another aggregator, Rotten Tomatoes, gave it a score of 82%.[48]
Video games loosely based on the book were released in 2002, mostly published by Electronic Arts but produced by different developers:

1 Plot
2 Publication and reception

2.1 Development
2.2 Publication
2.3 Critical response
2.4 Awards and honours
2.5 Religious response


2.1 Development
2.2 Publication
2.3 Critical response
2.4 Awards and honours
2.5 Religious response
3 Themes

3.1 Connection to Harry Potter and the Half-Blood Prince


3.1 Connection to Harry Potter and the Half-Blood Prince
4 Adaptations

4.1 Film
4.2 Video game


4.1 Film
4.2 Video game
5 References
6 External links
2.1 Development
2.2 Publication
2.3 Critical response
2.4 Awards and honours
2.5 Religious response
3.1 Connection to Harry Potter and the Half-Blood Prince
4.1 Film
4.2 Video game
Book: Harry Potter
.
#(`*Harry Potter and the Prisoner of Azkaban*`)#.
Harry Potter and the Prisoner of Azkaban is the third novel in the Harry Potter series written by J. K. Rowling. The book was published on 8 July 1999. The novel won the 1999 Whitbread Children's Book Award, the Bram Stoker Award, the 2000 Locus Award for Best Fantasy Novel, and was short-listed for other awards, including the Hugo. A film based on the novel was released on 31 May 2004, in the United Kingdom and 4 June 2004 in the U.S. and many other countries.
The book opens on the night before Harry's thirteenth birthday, when he receives gifts by owl post from his friends at school. The next morning at breakfast, Harry sees on television that a man named Black is on the loose from prison. At this time, Aunt Marge comes to stay with the Dursleys, and she insults Harry's parents numerous times. Harry accidentally causes her to inflate, and leaves the Dursley's house and is picked up by the Knight Bus, but only after an alarming sighting of a large, black dog. The Knight Bus drops Harry off at Diagon Alley, where he is greeted by Cornelius Fudge, the Minister of Magic. Harry rents a room and awaits the start of school. In Diagon Alley, Harry finishes his schoolwork, admires a Firebolt broomstick in the window of a shop, and after some time, finds his friends Ron and Hermione. At a pet shop, Hermione buys a cat named Crookshanks, who chases Scabbers, Ron's aging pet rat. Ron is most displeased. The night before they all head off to Hogwarts, Harry overhears Ron's parents discussing the fact that Sirius Black is after Harry.
Harry, Ron, Hermione, and the other students board the Hogwarts Express train and are stopped once by an entity called a Dementor. Harry faints and is revived by Professor Lupin, the new Defense Against the Dark Arts teacher. Soon afterward, the students arrive at Hogwarts and classes begin. In Divination class, Professor Trelawney foresees Harry's death by reading tealeaves and finding the representation of a Grim, a large black dog symbolising death. In the Care of Magical Creatures class, Hagrid introduces the students to Hippogriffs, large, deeply dignified crosses between a horse and a griffin. Malfoy insults one of these beasts, Buckbeak, and is attacked. Malfoy drags out the injury in an attempt to have Hagrid fired and Buckbeak put to death. In Defense Against the Dark Arts, Professor Lupin leads the class in a defeat of a Boggart, which changes shape to appear as the viewer's greatest fear. For Ron, a spider, for Neville, professor Snape. For Harry it turns into a dementor During a Hogwarts visit to Hogsmeade, a wizard village which Harry is unable to visit because he has no permission slip, Harry has tea with Professor Lupin. Harry discovers that Professor Lupin had worried about whether the boggart would take the shape of Voldemort. Snape brings Lupin a steaming potion, which Lupin drinks, much to Harry's alarm. Later that night, Sirius Black breaks into Hogwarts and destroys the Fat Lady portrait that guards Gryffindor Tower. The students spend the night sleeping in the Great Hall while the teachers search the castle. Soon afterwards, Quidditch moves into full swing, and Gryffindor House plays against Hufflepuff. During the game, Harry spies the large black dog, and seconds later he sees a hoard of Dementors. He loses consciousness and falls off his broomstick. Harry wakes to find that his trusty broomstick had flown into the Whomping Willow and been smashed in his fall, and the game itself had lost. Later, Harry learns from Lupin that the Dementors affect Harry so much because Harry's past is so horrible.
During the next Hogsmeade visit, from which Harry is forbidden because he didn't get his permission slip signed, Fred and George Weasley give Harry the Marauder's Map, written by the mysterious quartet of Moony, Wormtail, Padfoot, and Prongs. This map leads Harry through a secret passageway into Hogsmeade, where he rejoins Ron and Hermione. Inside the Hogsmeade tavern, Harry overhears professor McGonagall, and some other Hogwarts teacher's discussing Sirius Black's responsibility for Harry's parents' deaths, as well as for the death of another Hogwarts student, Peter Pettigrew, who was blown to bits, leaving only a finger. Back at Hogwarts, Harry learns that Hagrid received a notice saying that Buckbeak, the hippogriff who attacked Malfoy, is going to be put on trial, and Hagrid is inconsolable. The winter holidays roll around. For Christmas, Harry receives a Firebolt, the most impressive racing broomstick in the world. Much to his and Ron's dismay, Hermione reports the broomstick to Professor McGonagall, who takes it away, fearing that it may have been sent (and cursed) by Sirius Black.
After the holidays, Harry begins working with Professor Lupin to fight Dementors with the Patronus Charm; he is moderately successful, but still not entirely confident in his ability to ward them off. Soon before the game against Ravenclaw, Harry's broomstick is returned to him, and as Ron takes it up to the dormitory, he discovers evidence that Scabbers has been eaten by Crookshanks. Ron is furious at Hermione. Soon afterwards, Gryffindor plays Ravenclaw at Quidditch. Harry, on his Firebolt, triumphs, winning the game. Once all the students have gone to bed, Sirius Black breaks into Harry's dormitory and slashes the curtain around Ron's bed. Several days later, Hagrid invites Harry and Ron over for tea and scolds them for shunning Hermione on account of Scabbers and the Firebolt. They feel slightly guilty, but not terrible. Soon Harry, under his invisibility cloak, meets Ron during a Hogsmeade trip; when he returns, Snape catches him and confiscates his Marauder's Map. Lupin saves Harry from Snape's rage, but afterwards he reprimands him severely for risking his safety for "a bag of magic tricks." As Harry leaves Lupin's office, he runs into Hermione, who informs him that Buckbeak's execution date has been set. Ron, Hermione, and Harry are reconciled in their efforts to help Hagrid. Around this time, Hermione is exceptionally stressed by all of her work, and in a day she slaps Malfoy for picking on Hagrid and she quits Divination, concluding that Professor Trelawney is a great fraud. Days later, Gryffindor beats Slytherin in a dirty game of Quidditch, winning the Quidditch Cup.
Exams roll around, and during Harry's pointless Divination exam, Professor Trelawney predicts the return of Voldemort's servant before midnight. Ron, Hermione, and Harry shield themselves in Harry's invisibility cloak and head off to comfort Hagrid before the execution. While at his cabin, Hermione discovers Scabbers in Hagrid's milk jug. They leave as Buckbeak seems to be executed. As Ron, Harry, and Hermione are leaving Hagrid's house and reeling from the sound of the axe, the large black dog approaches them, pounces on Ron, and drags him under the Whomping Willow. Harry and Hermione and Crookshanks dash down after them; oddly, Crookshanks knows the secret knob to press to still the flailing tree. They move through an underground tunnel and arrive at the Shrieking Shack. They find that the black dog has turned into Sirius Black and is in a room with Ron. Harry, Ron, and Hermione manage to disarm Black, and before Harry can kill Black, avenging his parents' deaths, Professor Lupin enters the room and disarms him. Harry, Ron, and Hermione are aghast as Lupin and Black exchange a series of nods and embrace.
Once the three students calm down enough to listen, Lupin and Black explain everything. Lupin is a werewolf who remains tame through a special steaming potion made for him by Snape. While Lupin was a student at Hogwarts, his best friends, James Potter, Sirius Black, and Peter Pettigrew, became Animagi (humans able to take on animal forms) so that they could romp in the grounds with Lupin at the full moon. They explain how Snape once followed Lupin toward his transformation site in a practical joke set up by Sirius, and was rescued narrowly by James Potter. At this moment, Snape reveals himself from underneath Harry's dropped invisibility cloak, but Harry, Ron, and Hermione disarm him, rendering him unconscious. Lupin and Black then explain that the real murderer of Harry's parents is not Black, but Peter Pettigrew, who has been presumed dead but really hidden all these years disguised as Scabbers. Lupin transforms Scabbers into Pettigrew, who squeals and hedges but ultimately confesses, revealing himself to be Voldemort's servant, and Black to be innocent. They all travel back to Hogwarts, but at the sight of the full moon, Lupin, who has forgotten to take his controlling potion (the steaming liquid), turns into a werewolf. Sirius Black responds by turning into the large black dog in order to protect Harry, Ron, and Hermione from Lupin. As Black returns from driving the werewolf into the woods, a swarm of Dementors approaches, and Black is paralyzed with fear. One of the Dementors prepares to suck the soul out of Harry, whose patronus charm is simply not strong enough. Out of somewhere comes a patronus that drives the Dementors away. Harry faints.
Harry awakens in the hospital wing to hear Snape and Cornelius Fudge discussing the fact that Sirius Black is about to be given the fatal Dementor's Kiss. Harry and Hermione protest, claiming Black's innocence, but to no avail; then Dumbledore enters the room, shoos out the others, and mysteriously suggests that Harry and Hermione travel back through Hermione's time-turning device, which she has been using from the starting of the school for her studies, and save both Black and Buckbeak. Hermione turns her hour-glass necklace back three turns, and Harry and Hermione are thrust into the past, where they rescue Buckbeak shortly before his execution.[1] From a hiding place in the forest, Harry watches the Dementor sequence and discovers that he had been the one who conjured the patronus, and he is touched and confused to note that his patronus had taken the shape of a stag that he recognises instantly as Prongs, his father's animagi form. After saving his past self from the Dementors, Harry and Hermione fly to the tower where Black is imprisoned, and they rescue Black, sending him away to freedom on Buckbeak's back. The next day, Harry is saddened to learn that Professor Lupin is leaving Hogwarts because of the previous night's scare. Dumbledore meets with Harry and gives him wise fatherly advice on the events that have happened. On the train ride home, Harry receives an owl-post letter from Sirius that contains a Hogsmeade permission letter, words of confirmation that he is safe in hiding with Buckbeak and that he was, in fact, the sender of the Firebolt, and a small pet owl for Ron. Harry feels slightly uplifted as he returns to spend his summer with the Dursleys.
Harry Potter and the Prisoner of Azkaban is the third book in the Harry Potter series. The first, Harry Potter and the Philosopher's Stone, was published by Bloomsbury on 26 June 1997 and the second, Harry Potter and the Chamber of Secrets, was published on 2 July 1998.[2]
Of the first three books in the series, Prisoner of Azkaban took the shortest amount of time to write  Philosopher's Stone took five years to complete and Chamber of Secrets required two years, while Harry Potter and the Prisoner of Azkaban was written in one year.[3] Rowling's favourite aspect of this book was introducing the character Remus Lupin.[3] Rowling said that Prisoner of Azkaban was "the best writing experience I ever had...I was in a very comfortable place writing (number) three. Immediate financial worries were over, and press attention wasn't yet by any means excessive."[4]
Gregory Maguire wrote a review in The New York Times for Prisoner of Azkaban. In it he said, "So far, in terms of plot, the books do nothing new, but they do it brilliantly...so far, so good."[5] A reviewer for Kidsreads.com said, "This crisply-paced fantasy will leave you hungry for the four additional Harry books that J.K. Rowling is working on. Harry's third year is a charm. Don't miss it."[6] Kirkus Reviews did not give a starred review but said, "a properly pulse-pounding climax...The main characters and the continuing story both come along so smartly...that the book seems shorter than its page count: have readers clear their calendars if they are fans, or get out of the way if they are not."[7]
However, Anthony Holden, who was one of the judges against Prisoner of Azkaban for the Whitbread Award, was very negative about the book, calling it tedious and clunkily written, the characters black-and-white and the storylines predictable.[8]
Harry Potter and the Prisoner of Azkaban won several awards, including the 1999 Bram Stoker Award for best work for young readers,[9] the 2000 Locus Award for Best Fantasy Novel,[10] and the 1999 Whitbread Book of the Year for children's books.[11] It was also nominated for the 2000 Hugo Award for Best Novel, but lost to A Deepness in the Sky.[12] Prisoner of Azkaban additionally won the 2004 Indian Paintbrush Book Award[13] and was named an American Library Association Notable Children's Book in 2000[14] as well as one of their Best Books for Young Adults.[15] As with the previous two books in the series, Prisoner of Azkaban won the Nestl Smarties Book Prize Gold Medal for children aged 911. It was the last in the series to do so.[16]
Prisoner of Azkaban sold more than 68,000 copies in the UK within two days of publication.[2]
Harry Potter and the Prisoner of Azkaban was released in hardcover in the UK on 8 July 1999[17] and in the U.S. on 8 September.[18] The British paperback edition was released on 1 April 2000,[19] while the U.S. paperback was released 1 October 2001.[20]
Bloomsbury additionally released an adult edition with a new cover design in paperback on 10 July 2004[21] and in hardcover on October 2004.[22] A hardcover special edition, featuring a green border and signature, was released on 8 July 1999.[23] In May 2004, Bloomsbury released a Celebratory Edition,[24] and on 1 November 2010, they released a Signature edition.[25]
The film version of Harry Potter and the Prisoner of Azkaban was released in 2004 and was directed by Alfonso Cuarn from a screenplay by Steve Kloves.[26] The film dbuted at number one at the box office and held that position for two weeks.[27] It made a total of $795.6million worldwide,[28] which made it the second highest-grossing film of 2004 behind Shrek 2. However, among all eight entries in the Harry Potter franchise, Prisoner of Azkaban grossed the lowest.[29] The film ranks at number 471 in Empire magazine's 2008 list of the 500 greatest movies of all time.[30]
1 Plot
2 Pre-release history
3 Publication and reception

3.1 Critical reception
3.2 Awards
3.3 Sales


3.1 Critical reception
3.2 Awards
3.3 Sales
4 Editions
5 Film adaptation
6 References
7 External links
3.1 Critical reception
3.2 Awards
3.3 Sales
Book: Harry Potter

 The template below (HarryPotterWiki) is being considered for deletion. See templates for discussion to help reach a consensus.

Harry Potter and the Prisoner of Azkaban on Harry Potter Wiki,  an external wiki
.
#(`*Harry Potter and the Goblet of Fire*`)#.
Harry Potter and the Goblet of Fire is the fourth novel in the Harry Potter series written by British author J. K. Rowling. Set during the protagonist Harry Potter's fourth year at Hogwarts School of Witchcraft and Wizardry, it follows the mystery surrounding the entry of Harry's name into the Triwizard Tournament, in which he is forced to compete.
The book was published in the United Kingdom by Bloomsbury and in the United States by Scholastic on 8July 2000, the first time a book in the series was published in both countries at the same time. The novel won a Hugo Award in 2001, the only Harry Potter novel to do so. The book was made into a film, which was released worldwide on 18November 2005.
Throughout the three previous novels in the Harry Potter series, the main character, Harry Potter, has struggled with the difficulties that come with growing up and the added challenge of being a famous wizard. When Harry was a baby, Voldemort, the most powerful Dark wizard in history, killed Harry's parents but mysteriously vanished after unsuccessfully trying to kill Harry, which left a lightning-shaped scar on his forehead. This results in Harry's immediate fame and his being placed in the care of his muggle, or non-magical, Aunt Petunia and Uncle Vernon, who have a son named Dudley Dursley.
Harry enters the wizarding world at the age of 11, enrolling in Hogwarts School of Witchcraft and Wizardry. He befriends Ron Weasley and Hermione Granger, and is confronted by Lord Voldemort trying to regain power. In Harry's first year he has to protect the Philosopher's Stone from Voldemort and one of his faithful followers at Hogwarts. After returning to the school after summer break, students at Hogwarts are attacked after the legendary "Chamber of Secrets" is opened. Harry ends the attacks by killing a Basilisk and defeating another attempt by Lord Voldemort to return to full strength. The following year, Harry hears that he has been targeted by escaped murderer Sirius Black. Despite stringent security measures at Hogwarts, Harry is confronted by Black at the end of his third year of schooling, and Harry learns that Black was framed and is actually Harry's godfather. He also learned that it was Sirius's, Lupin's and James Potter's friend Peter Pettigrew who actually betrayed his parents.
The story begins in the 1940s in a small town called Little Hangleton, describing how the Riddle family was mysteriously killed at supper, and how their groundsman, Frank Bryce, was suspected of the crime, then declared innocent due to lack of evidence. In 1994, Bryce investigates a disturbance at the house and overhears Lord Voldemort and Peter Pettigrew (also known as Wormtail) plotting to kill a boy named Harry Potter. Voldemort's snake, Nagini, notices Bryce and informs Voldemort; Voldemort invites Bryce inside and kills him on the spot.
The scene then shifts to Harry Potter as he wakes in the night with a throbbing pain in his scar. The next morning, Harry's Uncle Vernon receives a letter from the Weasleys asking Harry to join them at the Quidditch World Cup. Harry is brought to The Burrow the next day. Early the next morning, the Weasleys, Harry and Hermione head off to the Quidditch World Cup. They travel by Portkey, an object which wizards use to travel quickly to another linked destination. While traveling, they meet Cedric Diggory, another Hogwarts student. At their seat, Harry, Ron, and Hermione meet Winky, a house-elf who says she is saving a seat for her master, Bartemius 'Barty' Crouch. That night, after the game, a crowd of Voldemort's followers destroy the campground and torture its Muggle owners. Harry, Hermione and Ron escape by fleeing into the woods, where Harry discovers that his wand is missing. Moments later, someone fires Voldemort's symbol, using Harry's wand. Winky is found holding Harry's wand at the scene of the crime, and Mr Crouch fires her. Later at the Burrow, Cedric's father brings news that a man named Mad-Eye Moody attacked an intruder at his house.
Upon arriving at Hogwarts, Professor Dumbledore announces that the Triwizard Tournament will take place at Hogwarts throughout the school year. The Tournament is a competition between three delegates, or "champions", one from each of the three great European schools of magic - Hogwarts, Beauxbatons, and Durmstrang. These champions compete in three tasks and they are given scores by the judges based on their performance; at the conclusion, one champion is chosen as the victor and given a thousand Galleons prize money. However, owing to the dangerous nature of the tournament, no one under seventeen years of age is allowed to enter. He also introduces Mad-Eye Moody as the new Defense Against the Dark Arts professor. Moody's unorthodox teaching methods cause controversy within the school, notably his use of Transfiguration as punishment and his lessons on the Unforgivable Curses.
In late October, the delegations from Beauxbatons and Durmstrang arrive; the Triwizard Tournament is officially opened, and students who wish to compete submit their names to the Goblet of Fire. On Halloween, the Goblet of Fire chooses the champions; and to everyone's great surprise, Harry is selected to compete alongside Cedric Diggory, Fleur Delacour, and Viktor Krum. Though he did not enter himself, Harry is magically bound to compete as the fourth champion. Ron feels let down, refusing to speak to Harry. Harry's situation only worsens with the publication of a sappy, exaggerated article about his past, written by ruthless reporter Rita Skeeter. A few nights before the first task, Hagrid invites Harry for a late night walk, ultimately informing him that the task will somehow contain dragons. Back in the Gryffindor common room, Harry converses with Sirius; who informs Harry that Igor Karkaroff, the Headmaster of Durmstrang, was once a Death Eater and is not to be trusted. The next day, realizing that Fleur and Krum know about the dragons as well, Harry warns Cedric about the first task; Moody overhears, and drops hints that Harry should use his flying skills to best the dragon. Harry and Hermione then spend hours practising Summoning charms, which would allow him to retrieve his broom. During the task, Harry successfully Summons his broomstick and flies past the dragon, capturing the golden egg - a necessary clue to the nature of the second task - and receiving high marks. Ron and Harry reconcile shortly afterward.
Professor McGonagall announces that the Yule Ball is approaching and that the champions must find partners as they will open the ball. Harry gathers his courage to ask his crush Cho Chang, but finds out that she is already going with Cedric. Harry and Ron eventually ask Parvati and Padma Patil. At the ball, Ron becomes jealous of Viktor Krum, who has brought Hermione as his date. Harry and Ron leave the ball and overhear Karkaroff confiding fearfully to Potions master Snape that something on his arm has become more prominent. At the end of the ball, Cedric tells Harry to take a bath with the golden egg. During a trip to Hogsmeade, Ludo Bagman mentions to Harry that Mr Crouch has stopped coming to work.
Harry takes the egg into the bathtub. The egg sings that he will have an hour to reclaim something valuable that has been taken into the lake. As he returns to his dormitory, he notices Mr Crouch searching Snape's office, but is unable to investigate. Harry falls asleep in the library, searching for answers from the clue, and is awakened in the morning by the house-elf Dobby, who now works at Hogwarts, who gives him a ball of gillyweed. The gillyweed gives Harry gills and he swims easily through the lake, finding Hermione, Ron, Cho, and Fleur's sister Gabrielle asleep and tied together in a merpeople village. Harry waits to make sure all of the champions rescue their hostages before returning to the surface. When Fleur does not come, he returns with Gabrielle and Ron and comes up last, but gains high marks for his moral fibre in his completion of the task.
The following day in Hogsmeade, Harry, Ron, and Hermione meet Sirius Black, disguised as his animagus, a dog. He informs them that Crouch's son was convicted as a Death Eater (Voldemort's followers). Later, the champions are taken to see the grounds to see a maze, the third task. On the way back, when Krum pulls Harry away to talk, they find a dishevelled Mr Crouch, who is speaking to trees and demanding to see Dumbledore. Harry runs to get Dumbledore while Krum waits with Crouch; when Harry returns, Krum has been stunned and Mr Crouch gone. In Divination class, Harry falls asleep and dreams about Voldemort, waking up screaming. Harry leaves class to discuss this with Dumbledore; as he waits for Dumbledore to return to his office, he peers into a Pensieve and enters Dumbledore's memories of various Death Eater trials, including that of Ludo Bagman, Karkaroff, and Mr Crouch's son. Dumbledore returns, pulls Harry from the memories and listens to his story. On the evening of the task, the four champions enter the maze, and Harry finds his path relatively manageable. Soon both Fleur and Krum are out of the running, and Harry and Cedric arrive at the trophy at the same time, agreeing to touch it together.
The trophy turns out to be a Portkey, taking both to the graveyard in Little Hangleton, where a man in a hood quickly kills Cedric. Harry realises the man is Wormtail, who ties Harry to a gravestone. Wormtail drops the bundle he is carrying (Voldemort's current form) into a cauldron, as well as a bone from Voldemort's father, Wormtail's own right hand, and blood from Harry's arm. Voldemort resumes his body and rises from the cauldron. Voldemort presses a tattoo of the Dark Mark on Wormtail's arm, and suddenly Death Eaters begin appearing in a circle around them. Voldemort creates a silver hand for Wormtail and then challenges Harry to a duel. Harry tries to use the disarming spell on Voldemort just as Voldemort uses the Killing Curse. The lights from the two wands meet in midair and remain connected. Voldemort's past victims emerge from his wand and protect Harry once the wand connection is broken, giving him time to grab Cedric's body and touch the trophy, thus returning to Hogwarts.
Once Harry returns, Moody carries him into the castle, where he reveals that he is a Death Eater, and that he was responsible for placing Harry's name in the Goblet and for turning the trophy into a portkey. Moody also informs Harry that Karkaroff has fled the castle. Soon after, Dumbledore and other teachers burst into the room, stunning Moody and saving Harry. Under the influence of a truth potion, Moody confessed that he was young Barty Crouch Jr. He has made the switch by using Moody's hair and drinking Polyjuice potion every hour. His father smuggled him out of prison and allowed him to live under an invisibility cloak, guarded by Winky, and how Ministry of Magic worker Bertha Jorkins discovered him and ultimately was relieved of her information by Voldemort, who then returned to find Crouch Jr in his father's house. He also says that he killed his father in the Forest the day he stumbled upon Harry and Krum, and that he was hoping to bring Voldemort back to power by bringing Harry to him.
Cornelius Fudge, the Minister for Magic, refuses to believe that Voldemort is back. He gives Harry the tournament prize money and leaves quickly. After the term ends, Harry, Ron and Hermione return home on the Hogwarts Express. Hermione shows Harry and Ron a beetle in a jar Rita Skeeter's animagus form, which she has been using to spy on people and acquire news about them that she caught and warned not to write untrue things. Harry gives the gold he won in the Triwizard Tournament to the Weasley twins to help start their practical joke company. Harry then returns to the Dursleys for the summer.
Harry Potter and the Goblet of Fire is the fourth book in the Harry Potter series. The first, Harry Potter and the Philosopher's Stone, was published by Bloomsbury on 26 June 1997; the second, Harry Potter and the Chamber of Secrets, was published on 2 July 1998; and the third, Harry Potter and the Prisoner of Azkaban, followed on 8 July 1999.[2] Goblet of Fire is considerably longer than the first three; almost twice the size. Rowling stated that she "knew from the beginning it would be the biggest of the first four". She said there needed to be a "proper run-up" for the conclusion and rushing the "complex plot" could confuse readers. She also stated that "everything is on a bigger scale" which was symbolic, as Harry's horizons widened both literally and metaphorically as he grew up. She also wanted to explore more of the magical world.[3]
Until the official title's announcement on 27 June 2000, the book was called by its working title, Harry Potter and the Doomspell Tournament.[4] J.K. Rowling expressed her indecision about the title in an Entertainment Weekly interview. "I changed my mind twice on what [the title] was. The working title had got out Harry Potter and the Doomspell Tournament. Then I changed Doomspell to Triwizard Tournament. Then I was teetering between Goblet of Fire and Triwizard Tournament. In the end, I preferred Goblet of Fire because it's got that kind of cup of destiny feel about it, which is the theme of the book."[3]
Rowling mentioned that she originally had a Weasley relative named Malfalda, who, according to Rowling, "was the daughter of the 'second cousin who's a stockbroker' mentioned in Philosopher's Stone. This stockbroker had been very rude to Mr. and Mrs.Weasley in the past, but now he and his (Muggle) wife had inconveniently produced a witch, they came back to the Weasleys asking for their help in introducing her to wizarding society before she starts at Hogwarts".[5] Malfalda was supposed to be a Slytherin and who was to fill in the Rita Skeeter subplot, but eventually was removed as "there were obvious limitations to what an eleven year old closeted at school could discover". Rowling considered Rita Skeeter to be "much more flexible".[5] Rowling also admitted that the fourth book was the most difficult to write at the time, because she noticed a giant plot hole halfway through writing.[3] In particular, Rowling had trouble with the ninth chapter, "The Dark Mark", which she rewrote 13times.[6]
Jeff Jensen, who interviewed Rowling for Entertainment Weekly in 2000, pointed out that bigotry is a big theme in the Harry Potter novels and Goblet of Fire in particular. He mentioned how Voldemort and his followers are prejudiced towards Muggles and how in Goblet of Fire Hermione forms a group to liberate Hogwarts' house-elves who have "been indentured servants so long they lack desire for anything else".[3] When asked why she explored this theme, Rowling replied,
She also commented that she did not feel this was too "heavy" for children, as it was one of those things that "huge number of children at that age start to think about".[3]
Goblet of Fire was the first book in the Harry Potter series to be released in the United States on the same date as the United Kingdom, on 8July 2000, strategically on a Saturday so children did not have to worry about school conflicting with buying the book. The three previous books had been released in the United Kingdom several months before the U.S. edition. It had a combined first-printing of over five million copies.[2] It was given a record-breaking print run of 3.9 million. Three million copies of the book were sold over the first weekend in the US alone.[7] The pressure in editing caused a mistake which shows Harry's father emerging first from Voldemort's wand; however, as confirmed in Prisoner of Azkaban, James died first, so then Harry's mother ought to have come out first.[8] This was corrected in later editions.[9]
To publicise the book, a special train named Hogwarts Express was organised by Bloomsbury, and run from King's Cross to Perth, carrying J.K. Rowling, a consignment of books for her to sign and sell, also representatives of Bloomsbury and the press. The book was launched on 8July 2000, on platform1 at King's Cross which had been given "Platform 9 34" signs for the occasion following which the train departed. En route it called at Didcot Railway Centre, Kidderminster, the Severn Valley Railway, Crewe (overnight stop), Manchester, Bradford, York, the National Railway Museum (overnight stop), Newcastle, Edinburgh, arriving at Perth on 11July. The locomotive was West Country class steam locomotive no.34027 Taw Valley, which was specially repainted red for the tour; it later returned to its normal green livery (the repaints were requested and paid for by Bloomsbury). The coaches of the train included a sleeping car. A Diesel locomotive was coupled at the other end, for use when reversals were necessary, such as the first stage of the journey as far as Ferme Park, just south of Hornsey. The tour generated considerably more press interest than the launch of the film Thomas and the Magic Railroad which was premired in London the same weekend.[10][11][12]
Harry Potter and the Goblet of Fire has received mostly positive reviews. In the New York Times Book Review, author Stephen King stated the Goblet of Fire was "every bit as good as Potters1 through 3" and praised the humour and subplots, although he commented that "there's also a moderately tiresome amount of adolescent squabbling...it's a teenage thing".[13] Kirkus Reviews called it "another grand tale of magic and mystery...and clicking along so smoothly that it seems shorter than it is". However, they commented that it did tend to lag, especially at the end where two "bad guys" stopped the action to give extended explanations, and that the issues to be resolved in sequels would leave "many readers, particularly American ones, uncomfortable".[14]
Kristin Lemmerman of CNN pointed out that it is not great literature: 'Her prose has more in common with your typical beach-blanket fare and the beginning contained too much recap to introduce characters to new readers, athought Rowling quickly gets back on track, introducing readers to a host of well-drawn new characters.'[15] Writing for Salon.com, Charles Taylor was generally positive about the change of mood and development of characters.[16] Entertainment Weekly's reviewer Kristen Baldwin gave Goblet of Fire the grade of A-, praising the development of the characters as well as the many themes presented. However, she did worry that a shocking climax may be a nightmare factory for young reader.[17]
Harry Potter and the Goblet of Fire won several awards, including the 2001 Hugo Award for Best Novel.[18] It won the 2002 Indian Paintbrush Book Award, the third after Philosopher's Stone and Prisoner of Azkaban.[19] The novel also won an Oppenheim Toy Portfolio Platinum Award for one of the best books, who claimed it was "more intense than the first three books".[20]
Harry Potter and the Goblet of Fire was adapted into a film, released worldwide on 18November 2005, which was directed by Mike Newell and written by Steve Kloves. The film grossed $102.7 million for the opening weekend,[21] and eventually grossed $896 million worldwide.[22] The film was also nominated for Best Art Direction at the 78th Academy Awards.[23]
1 Synopsis

1.1 Plot introduction
1.2 Plot summary


1.1 Plot introduction
1.2 Plot summary
2 Development
3 Themes
4 Publication and reception

4.1 UK/U.S. release

4.1.1 Launch publicity


4.2 Critical reception
4.3 Awards and honours


4.1 UK/U.S. release

4.1.1 Launch publicity


4.1.1 Launch publicity
4.2 Critical reception
4.3 Awards and honours
5 Film
6 References
7 External links
1.1 Plot introduction
1.2 Plot summary
4.1 UK/U.S. release

4.1.1 Launch publicity


4.1.1 Launch publicity
4.2 Critical reception
4.3 Awards and honours
4.1.1 Launch publicity
Book: Harry Potter
Harry Potter and the Goblet of Fire on Harry Potter Wiki,  an external wiki
.
#(`*Harry Potter and the Order of the Phoenix*`)#.
Harry Potter and the Order of the Phoenix is the fifth in the Harry Potter series written by J. K. Rowling, and was published on 21 June 2003 by Bloomsbury in the United Kingdom, Scholastic in the United States, and Raincoast in Canada. Five million copies were sold in the first 24 hours of publication.[1]
The novel features Harry Potter's struggles through his fifth year at Hogwarts School of Witchcraft and Wizardry, including the surreptitious return of the antagonist Lord Voldemort, O.W.L. exams, and an obstructive Ministry of Magic.
Harry Potter and the Order of the Phoenix has won several awards, including being named an American Library Association Best Book for Young Adults in 2003. The book has also been made into a film, which was released in 2007, and into several video games by Electronic Arts.
Harry Potter is spending another summer with his dreadful Aunt Petunia and Uncle Vernon. when a pair of Dementors stage an unexpected attack on Harry and his cousin Dudley. After he uses magic to defend himself and Dudley, he is temporarily expelled from Hogwarts for using magic outside of the school, despite being legally allowed to do in self-defence, before it is rescinded. A few days afterwards, Harry is visited by a group of wizards and Mad-Eye Moody and is whisked off to Number 12, Grimmauld Place, London, the home of Harry's godfather, Sirius Black, and the headquarters of the Order of the Phoenix. As Harry learns from his best friends, Ron Weasley and Hermione Granger The Order is a group of witches and wizards, led by Hogwarts headmaster Albus Dumbledore, dedicated to fighting the evil Lord Voldemort and his followers. The Order is forced to operate in secrecy, outside of the jurisdiction of the Ministry of Magic, which is headed by the dense and corrupt Cornelius Fudge, who refuses to believe that Lord Voldemort has returned. In addition, Harry learns that he and Dumbledore have been made victims of a ministry smear campaign aimed at discrediting them and their beliefs about Voldemort. Because of his use of magic, Harry's fate is to be determined at a discipliniary hearing at the Ministry of Magic, which turns out to be an apparent show trial. With Dumbledore's help, Harry is cleared by the Wizengamot and permitted to return to Hogwarts School of Witchcraft and Wizardry.
Reunited with his best friends, Ron Weasley and Hermione Granger, Harry returns to Hogwarts and learns that Dolores Umbridge, an employee of Fudge, will be his new Defense Against the Dark Arts teacher. The Sorting Hat, which traditionally sorts all new students into one of four houses, cautions the students against becoming too internally divided. Meanwhile, due to the smear campaign against him, Harry is the subject of unwanted gossip from the student body at large, and a number of people turn against him. Professor Umbridge and Harry soon clash, as she, like Fudge, refuses to believe that Voldemort has returned and punishes Harry when he points out Voldemort's return by forcing him to write lines with a special quill that carves "I must not tell lies" into the back of his hand.
Umbridge refuses to teach her students how to perform defensive spells, and before long, Fudge appoints her High Inquisitor of Hogwarts, giving her the authority to inspect all faculty members and evaluate their skills. In desperation, Harry, Hermione, and Ron form their own Defense Against the Dark Arts group, also known as the D.A., or Dumbledore's Army. Twenty-five other students sign up, including several of Harry's friends as well as the eccentric Luna Lovegood, and they meet as often as possible to learn and practice Defense spells, and learn well from Harry. One night, Harry has a vision where he inhabits the body of a large snake, and attacks Ron's father. Harry wakes up horrified, and Professor McGonagall takes him to Dumbledore immediately. Dumbledore uses the portraits on the walls of his office to raise an alert, and Mr. Weasley is promptly rescued by two members of the Order. The Weasley family, accompanied by Harry and the Order, visit Arthur Weasley in St. Mungo's Hospital for Magical Maladies and Injuries. Afterwards, Dumbledore demands that Harry take Occlumency lessons with Professor Snape, for the purpose of protecting his mind against further invasions by Lord Voldemort. During the lessons, Harry learns that a corridor he has been repeatedly visiting in his dreams is part of the Department of Mysteries.
Harry is unsuccessful at Occlumency because he has such difficulty clearing his mind of all thoughts, making it difficult for him to focus on closing his mind off to all outside influence, in addition to wanting to find out what they mean. Meanwhile, his scar (from the attack in which Voldemort killed Harry's parents) burns horribly every time Voldemort experiences a powerful emotion. The D.A. continues to meet regularly, and Harry's peers show great improvement until they are caught by Umbridge. Dumbledore takes full responsibility for the group and resigns as Headmaster, and Umbridge takes over his position. Shortly afterwards, Harry ends up viewing a memory of Snape's, showing him being bullied by Harry's father James and Sirius, back in their schooldays. Harry wishes desperately to contact his godfather to talk about his father, but Umbridge has been inspecting all owl posts and patrolling the fires of Hogwarts, preventing communication via the Floo Network. Ron's brothers, Fred and George Weasley agree to distract Umbridge so that Harry can use her fireplace to talk to Sirius, who clears up Harry's doubts about his father. Immediately afterwards, they leave Hogwarts, moving to London where they plan to open a joke shop in the wizarding town of Diagon Alley using the money Harry won the previous year in the Triwizard Tournament.
The students begin taking their O.W.L. exams, and Harry has another vision, this time about Sirius being held captive and tortured by Voldemort. Horrified, Harry becomes determined to save him. Hermione warns Harry that Voldemort may be deliberately trying to lure Harry to the Department of Mysteries, but Harry is too concerned about Sirius to pay heed. Harry sneaks into Umbridge's office, and, using her fireplace, transports himself to 12, Grimmauld Place to look for Sirius. Kreacher, the House of Black's house elf, tells Harry that Sirius is at the Ministry of Magic. Harry returns to Hogwarts when he is pulled back through the fire by Umbridge to find that he and his friends have been caught in Umbridge's office. Ron, Luna, Ginny, and Neville, who tried to distract Umbridge so that Harry could use her fireplace, have all been seized by Slytherins and gagged. Hermione and Harry convince Umbridge to follow them into the forest, where they claim to be hiding a weapon for Dumbledore which they had just finished and wanted to tell him about.
Once in the forest, Umbridge provokes the resident herd of centaurs, and is taken into the forest by them. Harry and his friends use the school's thestrals, winged skeletal horses to fly to the Ministry. Once they arrive, Harry cannot find Sirius and realises that Hermione was right. Harry also sees that one of the glass spheres has his name on it, as well as Voldemort's. Harry grabs the sphere, and Death Eaters led by Lucius Malfoy surround to attack, demanding that Harry hand over the prophecy. Employing all of their Defence skills, Harry, Ron, Hermione, Ginny, Luna, and Neville have moderate success fighting the Death Eaters, but they are ultimately helped enormously by the arrival of several members of the Order, including Dumbledore. In the midst of the fight, Harry drops the glass sphere and it shatters. Sirius is killed by his own cousin, Bellatrix Lestrange, when she blasts him through the veil.
Harry tries to avenge his godfather and follows Bellatrix, but is met by Voldemort at the fountain. Dumbledore appears shortly after Voldemort and the two engage in an intense duel. Voldemort fights Dumbledore to stalemate, then possesses Harry in an attempt to get Dumbledore to sacrifice Harry in the hope of killing him. Voldemort and Lestrange escape, just as Fudge appears at the Ministry, finally faced with incontrovertible evidence that the Dark Lord has returned. Dumbledore sends Harry back to school, where, after Harry has a breakdown, screaming that "he's had enough" of all the pain and anguish and death and destruction, he explains that the sphere was a prophecy which stated that Harry has a power that Voldemort will never know: the power of love, given to him by his mother's sacrifice fifteen years earlier. The prophecy goes on to claim that neither Harry nor Voldemort can live while the other survives. Dumbledore takes this opportunity to tell Harry why he must spend his summers with the Dursleys in Little Whinging: because Harry's mother died to save him, he is blessed with her love, a blessing that can be sealed only by blood. Harry's Aunt Petunia, his mother's sister, makes that bond complete by taking Harry into her home. As long as he still calls Little Whinging home, Harry is safe. At the end of the year, the Order warn the Dursleys they will have to answer to them should they mistreat Harry, who returns to them for the summer.
Potter fans waited three years between the releases of the fourth and fifth books.[2][3] Before the release of the fifth book, 200million copies of the first four books had already been sold and translated into 55languages in 200countries.[4] As the series was already a global phenomenon, the book forged new pre-order records, with thousands of people queuing outside book stores on 20 June 2003to secure their copy at midnight.[4] Despite the security, thousands of copies were stolen from an Earlestown, Merseyside warehouse on 15 June 2003.[5]
Harry Potter and the Order of the Phoenix was met with generally positive reviews, and received several awards. The book was cited as an American Library Association Best Book for Young Adults and as an American Library Association Notable Book, both in 2004.[6][7] It also received the Oppenheim Toy Portfolio 2004 Gold Medal along with several other awards.[8]
The novel was also received generally well by critics. Rowling was praised for her imagination by USA Today writer Deirdre Donahue.[9] Most of the negative reviewers were concerned with the violence contained in the novel and with morality issues occurring throughout the book.[10]
The New York Times writer John Leonard praised the novel, saying "The Order of the Phoenix starts slow, gathers speed and then skateboards, with somersaults, to its furious conclusion....As Harry gets older, Rowling gets better."[11] However, he also criticises "the one-note Draco Malfoy" and the predictable Lord Voldemort.[11] Another review by Julie Smithouser, of the Christian-right group Focus on the Family, said the book was, "Likely to be considered the weakest book in the series, Phoenix does feel less oppressive than the two most previous novels."[10] Smithouser's main criticism was that the book was not moral. Harry lies to authority to escape punishment, and that, at times, the violence is too "gruesome and graphic."[10]
Several Christian groups have expressed concerns that the book, and the rest of the Harry Potter series, contain references to witchcraft or occultism. Several religious groups also expressed their support for the series. Christianity Today published an editorial in favour of the books in January 2000, calling the series a "Book of Virtues" and averring that although "modern witchcraft is indeed an ensnaring, seductive false religion that we must protect our children from", this does not represent the Potter books, which have "wonderful examples of compassion, loyalty, courage, friendship, and even self-sacrifice".[12]
Harry Potter and the Order of the Phoenix is the fifth book in the Harry Potter series.[2] The first book in the series, Harry Potter and the Philosopher's Stone was first published by Bloomsbury in 1997 with an initial print-run of 500copies in hardback, three hundred of which were distributed to libraries. Harry Potter and the Order of the Phoenix is also the longest book from the series, yet the second shortest film at 2 hours and 18 minutes.[13] By the end of 1997, the UK edition won a National Book Award and a gold medal in the 9to11year-olds category of the Nestl Smarties Book Prize.[14][15][16] Harry Potter and the Prisoner of Azkaban was published a year later in the UK on 8 July 1999 and in the US on 8 September 1999.[15][16] Harry Potter and the Goblet of Fire was published on 8 July 2000 simultaneously by Bloomsbury and Scholastic.[17]
After the publishing of Order of the Phoenix, the sixth book of the series, Harry Potter and the Half-Blood Prince, was published on 16 July 2005, and sold 9million copies in the first 24hours of its worldwide release.[1][18] The seventh and final novel, Harry Potter and the Deathly Hallows, was published 21 July 2007.[19] The book sold 11 million copies within 24hours of its release: 2.7million copies in the UK and 8.3million in the US.[18]
In 2007, Harry Potter and the Order of the Phoenix was released in a film version directed by David Yates and written by Michael Goldenberg. The film was produced by David Heyman's company, Heyday Films, alongside David Barron. The budget was reportedly between 75and 100million (US$150200million),[20][21] and it became the unadjusted eleventh-highest grossing film of all time, and a critical and commercial success.[22] The film opened to a worldwide 5-day opening of $333million, third all-time, and grossed $938.377.000million total, the second to Pirates of the Caribbean: At World's End for the greatest total of 2007.[23][24]
A video game adaptation of the book and film versions of Harry Potter and the Order of the Phoenix was made for Microsoft Windows, PlayStation 2, PlayStation 3, Xbox 360, PSP, Nintendo DS, Wii, Game Boy Advance and Mac OS X.[25] It was released on 25 June 2007in the U.S., 28 June 2007in Australia and 29 June 2007in the UK and Europe for PlayStation 3, PSP, PlayStation 2, Windows and the 3 July 2007for most other platforms.[26] The games were published by Electronic Arts.[27]
The book is also depicted in the 2011 video game Lego Harry Potter: Years 57.
Religious controversy surrounding Harry Potter and the Order of the Phoenix and the other books in the Harry Potter series mainly deal with the claims that novel contains occult or Satanic subtexts. Religious response to the series has not been exclusively negative. "At least as much as they've been attacked from a theological point of view", notes Rowling, "[the books] have been lauded and taken into pulpit, and most interesting and satisfying for me, it's been by several different faiths".[28]
In the United States, calls for the book to be banned from schools have led occasionally to widely publicised legal challenges, usually on the grounds that witchcraft is a government-recognised religion and that to allow the novels to be held in public schools violates the separation of church and state.[2][29][30] The series was at the top of the American Library Association's "most challenged books" list for 19992001.[14]
Religious opposition to the series has also occurred in other nations. The Orthodox churches of Greece and Bulgaria have campaigned against the series.[31][32] The books have been banned from private schools in the United Arab Emirates and criticised in the Iranian state-run press.[33][34]
Roman Catholic opinion over the series is divided. In 2003 Catholic World Report criticised Harry's disrespect for rules and authority, and regarded the series' mixing of the magical and mundane worlds as "a fundamental rejection of the divine order in creation."[35] In 2005, Cardinal Joseph Ratzinger, who became Pope later that year but was at the time Prefect of the Congregation for the Doctrine of the Faith, described the series as "subtle seductions, which act unnoticed and by this deeply distort Christianity in the soul before it can grow properly,"[36] and gave permission for publication of the letter that expressed this opinion.[37] However, a spokesman for the Archbishop of Westminster said that Cardinal Ratzinger's words were not binding as they were not an official pronouncement of the Congregation for the Doctrine of the Faith.[36]
Some religious responses have been positive. Emily Griesinger wrote that fantasy literature helps children to survive reality for long enough to learn how to deal with it, described Harry's first passage through to Platform 9 as an application of faith and hope, and his encounter with the Sorting Hat as the first of many in which Harry is shaped by the choices he makes. She noted that the self-sacrifice of Harry's mother, which protected the boy in the first book and throughout the series, was the most powerful of the "deeper magics" that transcend the magical "technology" of the wizards, and one which the power-hungry Voldemort fails to understand.[38]
There is some positive Roman Catholic opinion on the books. In 2003, Monsignor Peter Fleetwood, a member of a Church working party on New Age phenomena, said that the Harry Potter stories "are not bad or a banner for anti-Christian theology. They help children understand the difference between good and evil," that Rowling's approach was Christian, and that the stories illustrated the need to make sacrifices to defeat evil.[36][39]
The first official foreign translation of the book appeared in Vietnamese on 21 July 2003, when the first of twenty-two instalments was released. The first official European translation appeared in Serbia and Montenegro in Serbian, by the official publisher Narodna Knjiga, in early September 2003. Other translations appeared later (e.g. in November 2003 in Dutch and German). The English language version has topped the best-seller list in France, while in Germany and the Netherlands an unofficial distributed translation process has been started on the Internet.[40]

1 Synopsis

1.1 Plot
1.2 Publication and release
1.3 Critical response
1.4 Prequels and sequels


1.1 Plot
1.2 Publication and release
1.3 Critical response
1.4 Prequels and sequels
2 Adaptations

2.1 Film
2.2 Video games


2.1 Film
2.2 Video games
3 Religious response

3.1 Opposition to the series
3.2 Positive response


3.1 Opposition to the series
3.2 Positive response
4 Translations
5 References
6 External links
1.1 Plot
1.2 Publication and release
1.3 Critical response
1.4 Prequels and sequels
2.1 Film
2.2 Video games
3.1 Opposition to the series
3.2 Positive response
Book: Harry Potter
Harry Potter and the Order of the Phoenix on Harry Potter Wiki,  an external wiki
.
#(`*Harry Potter and the Half-Blood Prince*`)#.
Harry Potter and the Half-Blood Prince is the sixth and penultimate novel in the Harry Potter series by British author J. K. Rowling. Set during protagonist Harry Potter's sixth year at Hogwarts, the novel explores the past of Harry's nemesis, Lord Voldemort, and Harry's preparations for the final battle alongside his headmaster and mentor Albus Dumbledore.
The book was published in the United Kingdom by Bloomsbury and in the United States by Scholastic on 16 July 2005, as well as in several other countries. It sold nine million copies in the first 24hours after its release, a record at the time which was eventually broken by its sequel, Harry Potter and the Deathly Hallows. There were many controversies before and after it was published, including the right to read the copies delivered prior to the release date in Canada. Reception to the novel was generally positive and it won several awards and honours, including the 2006 British Book of the Year award.
Reviewers noted that the book took on a darker tone than its predecessors, though it did contain humour. Some considered the main themes to be love and death, and trust and redemption. The character development of Harry and several other teenage characters was also remarked upon.
The film adaptation of Harry Potter and the Half-Blood Prince was released 15 July 2009 by Warner Bros.
Lord Voldemort has returned and his wrath has been felt in both the Muggle (non-magical) and Wizarding worlds. Severus Snape, a member of Dumbledore's anti-Voldemort Order of the Phoenix but formerly one of Voldemort's Death Eaters, meets with Narcissa Malfoy, mother of Harry Potter's school rival Draco. Snape makes an Unbreakable Vow to Narcissa, promising to assist and protect Draco.
Meanwhile, Dumbledore collects Harry from his aunt and uncle's house and takes him to the home of Horace Slughorn, former Potions teacher at Hogwarts. Dumbledore tries to persuade a reluctant Slughorn to return to teaching and finally succeeds. Later, when shopping for schoolbooks, Harry and his friends Ron Weasley and Hermione Granger follow Draco Malfoy to Dark Arts supplier Borgin and Burkes, where they overhear Draco insisting that the store-owner fix an unknown object. Harry is instantly suspicious of Draco, whom he believes to be a Death Eater like his father.
The students return to school, where Dumbledore announces that Snape, the previous Potions teacher, will be teaching Defence Against the Dark Arts, while Slughorn will resume his post as Potions teacher. This allows Harry to continue with a Potions course, in which he now excels, thanks mainly due to having received a used Potions textbook that once belonged to someone named "The Half-Blood Prince", which is heavily annotated.
Harry falls in love with Ron's sister Ginny, and Ron and his girlfriend Lavender Brown break up, to Hermione's delight. Harry spends much of his time following Draco Malfoy for any proof of suspicious actions, though he often cannot find him on his Marauder's Map, a magical map of Hogwarts. Harry realises that when Draco is not on the map, he is using the Room of Requirement on the seventh floor of Hogwarts, which transforms into whatever its user needs. Harry is unable to gain access to the room unless he knows for what exact purpose Draco is using the room.
Believing that Harry needs to learn Voldemort's past to gain advantage in a foretold fight, Dumbledore schedules regular meetings with Harry in which they use Dumbledore's Pensieve to look at memories of those who have had direct contact with Voldemort. Harry learns about Voldemort's family and his evolution into a psychopath obsessed with power and blood purity. Harry eventually succeeds in retrieving one of Slughorn's memories about how he revealed the secrets about splitting one's soul and hiding it in several objects called Horcruxes. Dumbledore explains that two of these have already been destroyed but that others remain. He suspects three of those to be objects belonging to three of the Hogwarts founders (one for each except Gryffindor), and the last one to reside in Voldemort's snake.
Harry and Dumbledore leave Hogwarts to fetch and destroy one of the Horcruxes. They journey into a cave important to Voldemort's youth that Dumbledore senses is protected with magic. They reach the basin where the purported Horcrux is hidden underneath a potion. Dumbledore drinks the potion and Harry fights off Voldemort's Inferi, an army of re-animated corpses. They take the Horcrux, Slytherin's locket, and return to Hogwarts as quickly as possible. Dumbledore is very weak, and when they reach Hogsmeade they can see the Dark Mark, Voldemort's symbol, visible above the astronomy tower.
When they arrive at the tower, Dumbledore uses his magic to freeze Harry in place while Harry remains hidden by his cloak of invisibility. When Draco Malfoy arrives, he disarms and threatens to kill Dumbledore, acting on his mission from Voldemort. Dumbledore tries to stall Draco by telling him he is not a killer, but Snape bursts into the tower and kills Dumbledore. Because of Dumbledore's death, his spell on Harry is broken and Harry rushes after Snape to avenge Dumbledore's death. Snape reveals that he is the Half-Blood Prince and manages to escape. Later, Harry finds out that the locket that he and Dumbledore retrieved is not the real Horcrux; containing only a note from someone named "R. A. B".
After Dumbledore's funeral, Hermione explains to Harry that Snape was called the Half-Blood Prince because he had a Muggle father and a magical mother (whose maiden name was Prince). Harry is devastated to think that he trusted and took help from the man who would turn out to be Dumbledore's murderer. He tells his friends that he will not be returning to Hogwarts next year and will instead search out and kill Voldemort by destroying all of the Horcruxes. Ron and Hermione vow to join him.
Harry Potter and the Half-Blood Prince is the sixth book in the Harry Potter series.[2] The first book in the series, Harry Potter and the Philosopher's Stone, was first published by Bloomsbury in 1997, with an initial print-run of 500 copies in hardback, 300 of which were distributed to libraries.[3] By the end of 1997, the UK edition won a National Book Award and a gold medal in the 9-to11-year-olds category of the Nestl Smarties Book Prize.[4] The second book, Harry Potter and the Chamber of Secrets, was originally published in the UK on 2July1998 and in the US on 2June1999.[5][6] Harry Potter and the Prisoner of Azkaban was then published a year later in the UK on 8July1999 and in the US on 8September1999.[5][6] Harry Potter and the Goblet of Fire was published on 8July2000 at the same time by Bloomsbury and Scholastic.[7] Harry Potter and the Order of the Phoenix, the longest novel in the Harry Potter series, was released 21 June 2003.[8] After the publishing of Harry Potter and the Half-Blood Prince, the seventh and final novel, Harry Potter and the Deathly Hallows, was released 21July2007.[9] The book sold 11million copies within 24hours of its release: 2.7million copies in the UK and 8.3million in the US.[10]
Rowling stated that she had Harry Potter and the Half-Blood Prince "planned for years", but she spent two months going over her plan before she began writing seriously. This was a lesson learned after she did not check the plan for Goblet of Fire and had to rewrite a third of the book.[11] She started writing the book before her second child, David, was born, but she took a break to care for him.[12] The first chapter, "The Other Minister", which features the meeting between the Muggle Prime Minister, the Minister for Magic Cornelius Fudge, and his successor, Rufus Scrimgeour, was a concept Rowling tried to start in Philosopher's Stone, Prisoner of Azkaban, and Order of the Phoenix, but she found "it finally works" in Half-Blood Prince.[13] She stated that she was "seriously upset" writing the end of the book, although Goblet of Fire was the hardest to write.[14] When asked if she liked the book, she responded, "I like it better than I liked 'Goblet', 'Phoenix or 'Chamber' when I finished them. Book six does what I wanted it to do and even if nobody else likes it (and some won't), I know it will remain one of my favourites of the series. Ultimately you have to please yourself before you please anyone else!"[15]
Rowling revealed the title of Half-Blood Prince on her website on 29 June 2004.[16][17] This was the title she had once considered for the second book, Chamber of Secrets, though she decided that the information disclosed belonged better in book six.[17] On 21 December 2004, she announced she had finished writing it, along with the release date of 16 July.[18][19] Bloomsbury unveiled the cover on 8 March 2005.[20]
The record-breaking publication of Half-Blood Prince was accompanied by controversy. In May 2005, bookmakers in the UK suspended bets on which main character would die in the book amid fears of insider knowledge. A number of high value bets were made on the death of Albus Dumbledore, many coming from the town of Bungay where, it was believed, the books were being printed at the time. Betting was later reopened.[21] Additionally, in response to Greenpeace's campaign on using forest friendly paper for big-name authors, Bloomsbury published the book on 30% recycled paper.[22]
In early July 2005, a Real Canadian Superstore in Coquitlam, British Columbia, Canada, accidentally sold fourteen copies of The Half-Blood Prince before the authorised release date. The Canadian publisher, Raincoast Books, obtained an injunction from the Supreme Court of British Columbia prohibiting the purchasers from reading the books before the official release date or from discussing the contents.[23] Purchasers were offered a Harry Potter T-shirt and an autographed copy of the book if they returned their copies before 16 July.[23]
On 15 July, less than twelve hours before the book went on sale in the Eastern time zone, Raincoast warned The Globe and Mail newspaper that publishing a review from a Canada-based writer at midnight, as the paper had promised, would be seen as a violation of the trade secret injunction. The injunction sparked a number of news articles alleging that the injunction had restricted fundamental rights. Canadian law professor Michael Geist posted commentary on his blog;[24] Richard Stallman called for a boycott, requesting the publisher issue an apology.[25] The Globe and Mail published a review from two UK-based writers in its 16 July edition and posted the Canadian writer's review on its website at 9:00 that morning.[26] Commentary was also provided on the Raincoast website.[27]
Some reviewers noted that Half-Blood Prince contained a darker tone than the previous Potter novels. The Christian Science Monitor's reviewer Yvonne Zipp considered the first half to contain a lighter tone to soften the unhappy ending.[28] The Boston Globe reviewer Liz Rosenberg wrote, "lightness [is] slimmer than ever in this darkening series...[there is] a new charge of gloom and darkness. I felt depressed by the time I was two-thirds of the way through". She also compared the setting to Charles Dickens's depictions of London, as it was "brooding, broken, gold-lit, as living character as any other".[29] Christopher Paolini called the darker tone "disquieting" because it was so different from the earlier books.[30] Liesl Schillinger, a contributor to The New York Times book review, also noted that Half-Blood Prince was "far darker" but "leavened with humor, romance and snappy dialogue". She suggested a connection to the 11 September attacks, as the later, darker novels were written after that event.[31] David Kipen, a critic of the San Francisco Chronicle considered the "darkness as a sign of our paranoid times" and singled out curfews and searches that were part of the tightened security at Hogwarts, as a resemblance to our world.[32]
Julia Keller, a critic for the Chicago Tribune, highlighted the humour found in the novel and claimed it to be the success of the Harry Potter saga. She acknowledged that "the books are dark and scary in places" but "no darkness in Half-Blood Prince...is so immense that it cannot be rescued by a snicker or a smirk." She considered that Rowling was suggesting that difficult times could be worked through with imagination, hope, and humour, and compared this concept to works such as Madeline L'Engle's A Wrinkle in Time and Kenneth Grahame's The Wind in the Willows.[33]
Rosenberg wrote that the two main themes of Half-Blood Prince were love and death and praised Rowling's "affirmation of their central position in human lives". She considered love to be represented in several forms: the love of parent to child, teacher to student, and the romances that developed between the characters.[29] Zipp noted trust and redemption to be themes promising to continue in the final book, which she thought "would add a greater layer of nuance and complexity to some characters who could sorely use it."[28] Deepti Hajela also pointed out Harry's character development, that he was "no longer a boy wizard; he's a young man, determined to seek out and face a young man's challenges".[34] Paolini had similar views, claiming, "the children have changed...they act like real teenagers."[30]
Harry Potter and the Half-Blood Prince received generally positive reviews. Liesl Schillinger of The New York Times praised the novel's various themes and suspenseful ending. However, she considered Rowling's gift "not so much for language as for characterisation and plotting".[31] Kirkus Reviews said it "will leave readers pleased, amused, excited, scared, infuriated, delighted, sad, surprised, thoughtful and likely wondering where Voldemort has got to, since he appears only in flashbacks". They considered Rowling's "wry wit" to turn into "outright merriment", but called the climax "tragic, but not uncomfortably shocking".[35] Yvonne Zipp of The Christian Science Monitor praised the way Rowling evolved Harry into a teenager and how the plot threads found as far back as Chamber of Secrets came into play. On the other hand, she noted that it "gets a little exposition-heavy in spots" and older readers may have seen the ending coming.[28]
Christopher Paolini, writing for Entertainment Weekly, pointed out that the change of tone was "disquieting" as the world evolved, and praised the character development, though he considered Harry Potter and the Goblet of Fire the best.[30] The Boston Globe correspondent Liz Rosenberg wrote, "The book bears the mark of genius on every page" and praised the imagery and darker tone of the book, considering that the series could be crossing over from fantasy to horror.[29] The Associated Press writer Deepti Hajela praised the newfound emotional tones and ageing Harry to the point where "younger fans may find [the series] has grown up too much".[34] Emily Green, a staff writer of the Los Angeles Times, was generally positive about the book but was concerned whether young children could handle the material.[36] Cultural critic Julia Keller of the Chicago Tribune called it the "most eloquent and substantial addition to the series thus far" and considered the key to the success of the Potter novels to be humour.[33]
Harry Potter and the Half-Blood Prince has won several awards, including the 2006 British Book of the Year Award[37] and the 2006 Royal Mail Award for Scottish Children's Books for ages 812 in its native United Kingdom.[38] In the United States, the American Library Association listed it among its 2006 Best Books for Young Adults.[39] It won both the 2005 reader-voted Quill Awards for Best Book of the Year and Best Children's Book.[40][41] It also won the Oppenheim Toy Portfolio Platinum Seal for notable book.[42]
Before publication, 1.4million advanced orders were placed for Half-Blood Prince on Amazon.com, breaking the record held by the previous novel, Order of the Phoenix, with 1.3million.[43] The initial print run for Half-Blood Prince was a record-breaking 10.8million.[44] Within the first 24hours of release, the book sold 9million copies worldwide, 2million in the UK and about 6.9million in the U.S.,[45] which prompted Scholastic to rush an additional 2.7million copies into print.[46] Within the first nine weeks of publication, 11 million copies of the U.S. edition were reported to have been sold.[47] The U.S. audiobook, read by Jim Dale, set sales records with 165,000 sold over two days, besting the adaption of Order of the Phoenix by twenty percent.[48]
Harry Potter and the Half-Blood Prince was published simultaneously in the UK, the US, Canada, Australia, New Zealand and South Africa.[49] Along with the rest of the books in the Harry Potter series, it was eventually translated into 67 languages.[50] However, because of high security surrounding the manuscript, translators did not get to start on translating Half-Blood Prince until its English release date, and the earliest were not expected to be released until the fall of 2005.[51] In Germany, a group of "hobby translators" translated the book via Internet in less than two days after release, far before German translator Klaus Fritz could translate and publish the book.[52]
Since its wide hardcover release on 16 July 2005, Half-Blood Prince was released as a paperback on 23 June 2006 in the UK.[53] Two days later on 25 July, the paperback edition was released in Canada[54] and the U.S., where it had an initial print run of 2 million copies.[55] To celebrate the release of the American paperback edition, Scholastic held a six-week sweepstakes event in which participants in an online poll were entered to win prizes.[56] Simultaneous to the original hardcover release was the UK adult edition, featuring a new cover,[57] and which was also released as a paperback on 23 June.[58] Also released on 16 July was the Scholastic "Deluxe Edition", which featured reproductions of Marie Grandpr's artwork and had a print run of about 100,000 copies.[59] Bloomsbury later released a paperback "Special Edition" on 6 July 2009[60] and a "Signature Edition" paperback on 1 November 2010.[61]
The film adaptation of the sixth book was originally scheduled to be released on 21 November 2008, but was changed to 15 July 2009.[62][63] Directed by David Yates, the screenplay was adapted by Steve Kloves and produced by David Heyman and David Barron.[64] The film grossed over $934million worldwide,[65] which made it the second-highest grossing film of 2009 worldwide[66] and the fifteenth highest of all time.[67] Additionally, Half-Blood Prince gained an Academy Award nomination for Best Cinematography.[68][69]
1 Synopsis
2 Development

2.1 Franchise
2.2 Background


2.1 Franchise
2.2 Background
3 Controversies

3.1 Right-to-read controversy


3.1 Right-to-read controversy
4 Style and themes
5 Publication and reception

5.1 Critical reception
5.2 Awards and honours
5.3 Sales


5.1 Critical reception
5.2 Awards and honours
5.3 Sales
6 Translations
7 Editions
8 Film adaptation
9 References
10 External links
2.1 Franchise
2.2 Background
3.1 Right-to-read controversy
5.1 Critical reception
5.2 Awards and honours
5.3 Sales
Book: Harry Potter
Harry Potter and the Half-Blood Prince on Harry Potter Wiki,  an external wiki
.
#(`*Harry Potter and the Deathly Hallows*`)#.
Harry Potter and the Deathly Hallows is the seventh and final of the Harry Potter novels written by British author J. K. Rowling. The book was released on 21July2007 by Bloomsbury Publishing in the United Kingdom, in the United States by Scholastic, and in Canada by Raincoast Books, ending the series that began in 1997 with the publication of Harry Potter and the Philosopher's Stone. The novel chronicles the events directly following Harry Potter and the Half-Blood Prince (2005), and the final confrontation between the wizards Harry Potter and Lord Voldemort.
Rowling finished writing Harry Potter and the Deathly Hallows in January 2007. Before its release, Bloomsbury reportedly spent GB10million to keep the book's contents safe before its release date. American publisher Arthur Levine refused any copies of the novel to be released in advance for press review, although two reviews were submitted early. Shortly before release, photos of all 759 pages of the U.S. edition were leaked and transcribed, leading Scholastic to look for the source that had leaked it.[citation needed]
Released globally in 93 countries, Deathly Hallows broke sales records as the fastest-selling book ever, a record it still holds today.[2] It sold 15million copies in the first 24 hours following its release, including more than 11million in the U.S. and UK alone. The previous record, 9million in its first day, had been held by Harry Potter and the Half-Blood Prince. The novel has also been translated into over 120languages, including Ukrainian, Swedish, and Hindi.
Major themes in the novel are death and living in a corrupted society, and critics have compared them to Christian allegories. Generally well-received, the book won the 2008 Colorado Blue Spruce Book Award, and the American Library Association named it a "Best Book for Young Adults". A two-part film adaptation began showing in November 2010 when Harry Potter and the Deathly Hallows  Part 1 was released; Part 2 was released on 15 July 2011.
Throughout the six previous novels in the series, the titular character Harry Potter has struggled with the difficulties of adolescence along with being a famous wizard. When Harry was a baby, Lord Voldemort, a powerful evil wizard, murdered Harry's parents but vanished after attempting to kill Harry. Harry immediately became famous, and was placed in the care of his Muggle (non-magical) relatives Aunt Petunia and Uncle Vernon.
In Philosopher's Stone, Harry re-enters the wizarding world at age 11 and enrolls in Hogwarts School of Witchcraft and Wizardry. He makes friends with Ron Weasley and Hermione Granger. Harry also meets the school's headmaster, Albus Dumbledore, and Professor Severus Snape, who dislikes him. Harry fights Voldemort several times while at school, as the wizard tries to regain a physical form. In Goblet of Fire, Harry is entered in a dangerous magical competition called the Triwizard Tournament. At the conclusion of the Tournament, Harry witnesses the return of Lord Voldemort to full strength. During Order of the Phoenix, Harry and several of his friends face off against Voldemort's Death Eaters, a group of Dark witches and wizards, and narrowly defeat them. In Half-Blood Prince, Harry learns that Voldemort has created six "horcruxes" to become immortal. A Horcrux is a fragment of a person's soul placed within an object so that when the body dies, a part of the soul remains and the person can be regenerated or resurrected.[3] However, the destruction of the creator's body leaves the wizard or witch in a state of half-life, without corporeal form.[4] Two horcruxes have already been destroyed, one by Harry in the events of Chamber of Secrets and one by Dumbledore shortly before the events of Half-Blood Prince. When returning from a mission to discover a horcrux, Dumbledore is murdered by Snape, a former Death Eater whom Harry suspected of secretly remaining loyal to Voldemort. At the conclusion of the book, Harry decides to leave school, find and destroy the remaining four Horcruxes, and defeat the evil wizard Voldemort, once and for all.
Following Dumbledore's death, Voldemort continues to gain support and increase his power. When Harry turns seventeen, the protection he has at his aunt and uncle's house will be broken. Before that can happen, at Mad Eye Moody's suggestion, Harry flees to the Burrow with his friends, many of whom use Polyjuice Potion to impersonate him so as to confuse any Death Eaters that may attack. They are indeed attacked shortly after leaving Privet Drive; Mad Eye is killed, and George Weasley wounded, but the rest arrive safely at the Burrow. Ron and Hermione decide to accompany Harry, instead of returning to Hogwarts School for their seventh year, to finish the quest Dumbledore started: to hunt and destroy Voldemort's four remaining Horcruxes. They have little knowledge about the remaining Horcruxes except that one is a locket once owned by Hogwarts' co-founder Salazar Slytherin, one is possibly a cup once owned by co-founder Helga Hufflepuff, a third may be connected with co-founder Rowena Ravenclaw, and the fourth may be Nagini, Voldemort's snake familiar. The whereabouts of the founders' objects is unknown, and Nagini is presumed to be with Voldemort. Before leaving, they attend Ron's brother's Bill's wedding to Fleur Delacour, with Harry disguised by Polyjuice Potion, but the Ministry of Magic is taken over by Death Eaters during the wedding and they barely escape with their lives.
Harry, Ron, and Hermione flee to 12 Grimmauld Place in London, which is Sirius Black's family's house, where they learn from the house-elf Kreacher the whereabouts of Salazar Slytherin's locket, which Sirius' brother Regulus stole from Voldemort at the cost of his own life. They successfully recover this Horcrux by infiltrating the Ministry of Magic and stealing it from Dolores Umbridge. Under the object's evil influence and the stress of being on the run, Ron leaves the others. As Harry and Hermione search for the Horcruxes, they learn more about Dumbledore's past, including the insanity and death of Dumbledore's younger sister and his connection to the evil wizard Grindewald. Harry and Hermione ultimately travel to Godric's Hollow, Harry's birthplace and the place where his parents died. They meet the eldery magical historian Bathilda Bagshot, who turns out to be Nagini in disguise and attacks them. They escape into the Forest of Dean, where a mysterious silver doe that appears to be a Patronus leads Harry to the Sword of Hogwarts co-founder Godric Gryffindor, one of the few objects able to destroy Horcruxes, lying at the bottom of an icy lake. When Harry attempts to recover the sword from the pool, the Horcrux attempts to kill him. Ron reappears, saving Harry and then using the sword to destroy the locket. Resuming their search, the trio repeatedly encounter a strange symbol that an eccentric wizard named Xenophilius Lovegood tells them represents the mythical Deathly Hallows. The Hallows are three sacred objects: the Elder Wand, an unbeatable wand; the Resurrection Stone, with the power to summon the dead to the living world; and an infallible Invisibility Cloak. Harry learns that Voldemort is seeking the Elder Wand, recognizes the Resurrection Stone from the second Horcrux, which Dumbledore had destroyed, and realizes that his own Invisibility Cloak is the one mentioned in the story, but he is unaware of the Hallows' significance.
The trio are captured and taken to Malfoy Manor, where Bellatrix Lestrange tortures Hermione. Harry and Ron are thrown in the cellar, where they find Luna Lovegood, Ollivander, Dean Thomas, and Griphook. They escape to Shell Cottage (Bill and Fleur's house) with Dobby's help, but at the cost of the house-elf's life. Harry now realizes that the Hallows may have the power to defeat Death and knows that Voldemort robbed Dumbledore's tomb to procure the Elder Wand, but he decides to focus on finding the Horcruxes instead of the Hallows. With Griphook's help, they learn that Helga Hufflepuff's cup - a Horcrux - is hidden in Bellatrix's vault at Gringotts, break into her vault, retrieve the cup, and escape on a dragon, with Griphook swiping the sword and escaping on his own. From his connection to Voldemort's thoughts, Harry learns that another Horcrux is hidden in Hogwarts, which is under the control of Severus Snape. Harry, Ron, and Hermione enter the school through Hogsmeade (being saved by Aberforth Dumbledore, who explains more about Albus's backstory) and - with the help of the teachers - Snape is ousted from the school. Ron and Hermione go to the Chamber of Secrets and destroy the cup using a basilisk fang. The trio then finds Rowena Ravenclaw's diadem (another Horcrux) in the Room of Requirement. Vincent Crabbe casts a Fiendfyre curse in an attempt to kill Harry, Ron, and Hermione, but he instead destroys the diadem, the Room of Requirement, and himself. At this point, a total of five Horcruxes have been destroyed.
The Death Eaters and Voldemort besiege Hogwarts, while Harry, Ron, Hermione, their allies, and various magical creatures defend the school. Several major characters are killed in the first wave of the battle, including Remus Lupin, Nymphadora Tonks, and Fred Weasley. Voldemort kills Severus Snape because he believes doing so will make him the Elder Wand's true master, since Snape killed Dumbledore. Harry discovers while viewing Snape's memories that Voldemort inadvertently made Harry into a seventh Horcrux when he attacked him as a baby, which is the true significance of Harry's scar, and that Harry must die in order to destroy Voldemort. These memories also confirm Snape's unwavering loyalty to Dumbledore and that his role as a double-agent against Voldemort never wavered after Voldemort killed Lily Evans, Harry's mother and Snape's one true love. Harry also learns that Dumbledore had less than a year to live when he died, that his death by Snape's hand had been per Dumbledore's request, and that Dumbledore had known that Harry must die. After using the Resurrection Stone to bring back his deceased loved ones for a short while, Harry surrenders himself to death at Voldemort's hand. Voldemort casts the Killing Curse at him, but it only sends Harry into a limbo-like state between life and death.
While in this state, Dumbledore's spirit explains to Harry that when Voldemort used Harry's blood to regain his full strength, it protected Harry from Voldemort killing him; however, the Horcrux inside Harry has been destroyed, and Harry can return to his body despite being hit by the Killing Curse. Dumbledore also explains that Harry became the true master of the Deathly Hallows by facing Death, not by seeking to avoid it or conquer it. Harry returns to his body, feigning death, and Voldemort marches victoriously into the castle with his body. However, per Harry's prior instructions, Neville Longbottom kills Nagini, the last Horcrux, with the Sword of Gryffindor. Harry then reveals that he is still alive, and the battle resumes, with Bellatrix Lestrange being killed by Molly Weasley.
Harry and Voldemort engage in a final climactic duel. Harry reveals that because he willingly sacrificed himself to death by Voldemort's hand, his act of love would protect the Wizarding community from Voldemort in the same way the sacrifice Harry's mother made protected Harry. Harry also reveals that Snape was not loyal to Voldemort, did not murder Dumbledore, and was never the master of the Elder Wand. Instead, Draco was the master of the Elder Wand after disarming Dumbledore, but, because Harry had disarmed Draco at Malfoy Manor, Harry is the true master of the Elder Wand. Harry claims that the wand will refuse to kill the one to whom it owes allegiance, further protecting Harry. During the duel, Harry refuses to use the killing curse and even encourages Voldemort to feel remorse, one known way to restore Voldemort's shattered soul. Voldemort dies when his own killing curse backfires against Harry's disarming curse, killing himself; the Death Eaters are finally defeated. The wizarding world is able to live in peace once more.
The novel, the last in the series, closes with a brief epilogue set 19 years later, in which Harry and Ginny Weasley are a married couple with three children: James Sirius, Albus Severus, and Lily Luna. Ron and Hermione Weasley are also married and have two children, Rose and Hugo. The families meet at King's Cross station, where a nervous Albus is departing for his first year at Hogwarts. Harry's godson, Teddy Lupin, is found kissing Bill and Fleur Weasley's daughter Victoire in a train carriage. Harry sees Draco Malfoy and his wife with their son, Scorpius. Neville Longbottom is now the Hogwarts Herbology professor and remains friends with the two families. Harry comforts Albus, who is worried he will be sorted into Slytherin, and tells his son that one of his two namesakes, Severus Snape, was a Slytherin and the bravest man he had ever met. He adds that the Sorting Hat takes one's choice into account, like it did for Harry. The book ends with these final words: "The scar had not pained Harry for nineteen years. All was well."
Harry Potter and the Philosopher's Stone was published by Bloomsbury, the publisher of all Harry Potter books in the United Kingdom, on 30 June 1997.[5] It was released in the United States on 1 September 1998 by Scholasticthe American publisher of the booksas Harry Potter and the Sorcerer's Stone,[6] after Rowling had received US$105,000 for the American rightsan unprecedented amount for a children's book by a then-unknown author.[7]
The second book, Harry Potter and the Chamber of Secrets was originally published in the UK on 2 July 1998, and in the US on 2 June 1999. Harry Potter and the Prisoner of Azkaban was then published a year later in the UK on 8 July 1999, and in the US on 8 September 1999.[8] Harry Potter and the Goblet of Fire was published on 8 July 2000 at the same time by Bloomsbury and Scholastic.[9] Harry Potter and the Order of the Phoenix is the longest book in the series at 766 pages in the UK version and 870 pages in the US version.[10] It was published worldwide in English on 21 June 2003.[11] Harry Potter and the Half-Blood Prince was published on 16 July 2005, and it sold 9million copies in the first 24 hours of its worldwide release.[12][13]
Shortly before releasing the title, J. K. Rowling announced that she had considered three titles for the book.[14][15] The final title, Harry Potter and the Deathly Hallows, named after the mythical Deathly Hallows in the novel, was released to the public on 21December2006, via a special Christmas-themed hangman puzzle on Rowling's website, confirmed shortly afterwards by the book's publishers.[16] When asked during a live chat about the other titles she had been considering, Rowling mentioned Harry Potter and the Elder Wand and Harry Potter and the Peverell Quest.[14]
Rowling completed the book while staying at the Balmoral Hotel in Edinburgh in January2007, and left a signed statement on a marble bust of Hermes in her room which read: "J. K. Rowling finished writing Harry Potter and the Deathly Hallows in this room (652) on 11January2007".[17] In a statement on her website, she said, "I've never felt such a mixture of extreme emotions in my life, never dreamed I could feel simultaneously heartbroken and euphoric." She compared her mixed feelings to those expressed by Charles Dickens in the preface of the 1850 edition of David Copperfield, "a two-years' imaginative task". "To which," she added, "I can only sigh, try seventeen years, Charles". She ended her message by saying "Deathly Hallows is my favourite, and that is the most wonderful way to finish the series".[18]
When asked before publication about the forthcoming book, Rowling stated that she could not change the ending even if she wanted. "These books have been plotted for such a long time, and for six books now, that they're all leading a certain direction. So, I really can't".[19] She also commented that the final volume related closely to the previous book in the series, Harry Potter and the Half-Blood Prince, "almost as though they are two halves of the same novel".[20] She has said that the last chapter of the book was written "in something like 1990", as part of her earliest work on the series.[21] Rowling also revealed she originally wrote the last words to be "something like: 'Only those who he loved could see his lightning scar'". Rowling changed this because she did not want people to think Voldemort would rise again and to say that Harry's mission was over.[22]
In a 2006 interview, J. K. Rowling said that the main theme of the series is Harry dealing with death,[23] which was influenced by her mother's death in 1990, from multiple sclerosis.[22][23][24][25] Lev Grossman of Time stated that the main theme of the series was the overwhelming importance of continuing to love in the face of death.[26]
Academics and journalists have developed many other interpretations of themes in the books, some more complex than others, and some including political subtexts. Themes such as normality, oppression, survival, and overcoming imposing odds have all been considered as prevalent throughout the series.[27] Similarly, the theme of making one's way through adolescence and "going over one's most harrowing ordealsand thus coming to terms with them" has also been considered.[28] Rowling has stated that the books comprise "a prolonged argument for tolerance, a prolonged plea for an end to bigotry" and that also pass on a message to "question authority and... not assume that the establishment or the press tells you all of the truth".[29]
Some political commentators have seen J. K. Rowling's portrayal of the bureaucratised Ministry of Magic and the oppressive measures taken by the Ministry in the later books (like making attendance at Hogwarts School compulsory and the "registration of Mudbloods" with the Ministry) as an allegory of criticising the state.[30]
The Harry Potter series has been under criticism for supposedly supporting witchcraft and occult. Before publication of Deathly Hallows, Rowling refused to speak out about her religion, stating, "If I talk too freely, every reader, whether 10 or 60, will be able to guess what's coming in the books".[31] However, many have noted Christian allegories apparent in Deathly Hallows.[31] For example, Harry dies and then comes back to life to save mankind, like Christ. The location where this occurs is King's Cross.[32] Harry also urges Voldemort to show remorse, to restore his shattered soul. Rowling also stated that "my belief and my struggling with religious belief ... I think is quite apparent in this book", which is shown as Harry struggles with his faith in Dumbledore.[33]
Deathly Hallows begins with a pair of epigraphs, one by Quaker leader William Penn and one from Aeschylus' The Libation Bearers. Of this, Rowling said "I really enjoyed choosing those two quotations because one is pagan, of course, and one is from a Christian tradition. I'd known it was going to be those two passages since Chamber was published. I always knew [that] if I could use them at the beginning of book seven then I'd cued up the ending perfectly. If they were relevant, then I went where I needed to go. They just say it all to me, they really do".[34]
When Harry visits his parents' grave, the biblical reference "The last enemy that shall be destroyed is death" (1 Corinthians 15:26) is inscribed on the grave.[35] The Dumbledores' family tomb also holds a biblical quote: "Where your treasure is, there your heart will be also", which is from Matthew 6:21.[35] Rowling states, "They're very British books, so on a very practical note Harry was going to find biblical quotations on tombstones...[but] I think those two particular quotations he finds on the tombstones at Godric's Hollow, they sum up they almost epitomise the whole series".[35]
Harry Potter pundit John Granger additionally noted that one of the reasons the Harry Potter books were so popular is their use of literary alchemy (similar to Romeo and Juliet, C. S. Lewis's Perelandra and Charles Dickens's A Tale of Two Cities) and vision symbolism.[36] In this model, authors weave allegorical tales along the alchemical magnum opus. Since the medieval period, alchemical allegory has mirrored the passion, death and resurrection of Christ.[37] While the entire series utilizes symbols common in alchemy, the Deathly Hallows completes this cycle, tying themes of death, rebirth, and the Resurrection Stone to the principal motif of alchemical allegory, and topics presented in the first book of the series.
Christian author Nancy Carpentier Brown also noted many Christian themes, such as Harry marking Mad-Eye Moody's grave with a cross, showing remorse and giving Voldemort a chance to redeem himself, and the Resurrection Stone.[38] She also pointed out that Harry becomes a godfather to Tonks and Lupin's son, Teddy Lupin.[38]
The launch was celebrated by an all-night book signing and reading at the Natural History Museum in London, which Rowling attended along with 1,700 guests chosen by ballot.[39] Rowling toured the US in October 2007, where another event was held at Carnegie Hall in New York City with tickets allocated by sweepstake.[40]
Scholastic, the American publisher of the Harry Potter series, launched a multi-million dollar "There will soon be 7" marketing campaign with a "Knight Bus" travelling to 40 libraries across the United States, online fan discussions and competitions, collectible bookmarks, tattoos, and the staged release of seven Deathly Hallows questions most debated by fans.[41] In the build-up to the book's release, Scholastic released seven questions that fans would find answered in the final book:[42]
J. K. Rowling arranged with her publishers for a poster bearing the face of the missing British child Madeleine McCann to be made available to book sellers when Deathly Hallows was launched on 21July2007, and said that she hoped that the posters would be displayed prominently in shops all over the world.[43]
After it was told that the novel would be released on 21July2007, Warner Bros. shortly thereafter said that the film adaptation of Harry Potter and the Order of the Phoenix would be released shortly before the novel would be released, on 13July2007,[44] making many people proclaim that July2007, was the month of Harry Potter.[45]
Bloomsbury invested GB10million in an attempt to keep the book's contents secure until the 21July release date.[46] Arthur Levine, U.S. editor of the Harry Potter series, denied distributing any copies of Deathly Hallows in advance for press review, but two U.S. papers published early reviews anyway.[47] There was speculation that some shops would break the embargo and distribute copies of the book early, as the penalty imposed for previous instalmentsthat the distributor would not be supplied with any further copies of the serieswould no longer be a deterrent.[48]
In the week before its release, a number of texts purporting to be genuine leaks appeared in various forms. On 16July, a set of photographs representing all 759 pages of the U.S. edition was leaked and was fully transcribed prior to the official release date.[49][50][51][52] The photographs later appeared on websites and peer-to-peer networks, leading Scholastic to seek a subpoena in order to identify one source.[53] This represented the most serious security breach in the Harry Potter series' history.[54] Rowling and her lawyer confirmed that there were genuine online leaks.[55] Reviews published in both The Baltimore Sun and The New York Times on 18July2007, corroborated many of the plot elements from this leak, and about one day prior to release, The New York Times confirmed that the main circulating leak was real.[54]
Scholastic announced that approximately one-ten-thousandth (0.0001) of the U.S. supply had been shipped early interpreted to mean about 1,200 copies. One reader in Maryland received a copy of the book in the mail from DeepDiscount.com four days before it was launched, which evoked incredulous responses from both Scholastic and DeepDiscount. Scholastic initially reported that they were satisfied it had been a "human error" and would not discuss possible penalties;[56] however, the following day Scholastic announced that it would be launching legal action against DeepDiscount.com and its distributor, Levy Home Entertainment.[57] Scholastic filed for damages in Chicago's Circuit Court of Cook County, claiming that DeepDiscount engaged in a "complete and flagrant violation of the agreements that they knew were part of the carefully constructed release of this eagerly awaited book."[58] Some of the early release books soon appeared on eBay, in one case being sold to Publishers Weekly for US$250from an initial price of US$18.[59]
Asda,[60][61] along with several other UK supermarkets, having already taken pre-orders for the book at a heavily discounted price, sparked a price war two days before the book's launch by announcing they would sell it for just GB5 a copy (about US$8). Other retail chains then also offered the book at discounted prices. At these prices the book became a loss leader. This caused uproar from traditional UK booksellers who argued they had no hope of competing in those conditions. Independent shops protested loudest, but even Waterstone's, the UK's largest dedicated chain bookstore, could not compete with the supermarket price. Some small bookstores hit back by buying their stock from the supermarkets rather than their wholesalers. Asda attempted to counter this by imposing a limit of two copies per customer to prevent bulk purchases. Philip Wicks, a spokesman for the UK Booksellers Association, said, "It is a war we can't even participate in. We think it's a crying shame that the supermarkets have decided to treat it as a loss-leader, like a can of baked beans." Michael Norris, an analyst at Simba Information, said: "You are not only lowering the price of the book. At this point, you are lowering the value of reading."[62]
In Malaysia, a similar price war caused controversy regarding sales of the book.[63] Four of the biggest bookstore chains in Malaysia, MPH Bookstores, Popular Bookstores, Times and Harris, decided to pull Harry Potter and the Deathly Hallows off their shelves as a protest against Tesco and Carrefour hypermarkets. The retail price of the book in Malaysia is MYR 109.90 (about GB16), while the hypermarkets Tesco and Carrefour sold the book at MYR 69.90 (about GB10). The move by the bookstores was seen as an attempt to pressure the distributor Penguin Books to remove the books from the hypermarkets. However, as of 24July2007, the price war has ended, with the four bookstores involved resuming selling the books in their stores with discount. Penguin Books has also confirmed that Tesco and Carrefour are selling the book at a loss, urging them to practice good business sense and fair trade.[64]
The book's early Saturday morning release in Israel was criticised for violating Shabbat. Trade and Industry Minister Eli Yishai commented "It is forbidden, according to Jewish values and Jewish culture, that a thing like this should take place at 2a.m. on Saturday. Let them do it on another day."[65] Yishai indicated that he would issue indictments and fines based on the Hours of Work and Rest Law.[66]
The Baltimore Sun's critic, Mary Carole McCauley, noted that the book was more serious than the previous novels in the series and had more straightforward prose.[67] Furthermore, reviewer Alice Fordham from The Times wrote that "Rowling's genius is not just her total realisation of a fantasy world, but the quieter skill of creating characters that bounce off the page, real and flawed and brave and lovable". Fordham concluded, "We have been a long way together, and neither Rowling nor Harry let us down in the end".[68] The New York Times writer Michiko Kakutani agreed, praising Rowling's ability to make Harry both a hero and a character that can be related to.[69]
Time magazine's Lev Grossman named it one of the Top 10 Fiction Books of 2007, ranking it at #8, and praised Rowling for proving that books can still be a global mass medium.[26] Novelist Elizabeth Hand criticised that "...the spectacularly complex interplay of narrative and character often reads as though an entire trilogy's worth of summing-up has been crammed into one volume."[70] In a starred review from Kirkus Reviews, the reviewer said, "Rowling has shown uncommon skill in playing them with and against each other, and also woven them into a darn good bildungsroman, populated by memorable characters and infused with a saving, irrepressible sense of fun". They also praised the second half of the novel, but criticised the epilogue, calling it "provacatively sketchy".[71] In another review from The Times, reviewer Amanda Craig said that while Rowling was "not an original, high-concept author", she was "right up there with other greats of children's fiction". Craig went on to say that the novel was "beautifully judged, and a triumphant return to form", and that Rowling's imagination changed the perception of an entire generation, which "is more than all but a handful of living authors, in any genre, have achieved in the past half-century".[72]
In contrast, Jenny Sawyer of The Christian Science Monitor said that, "There is much to love about the Harry Potter series, from its brilliantly realised magical world to its multilayered narrative", however, "A story is about someone who changes. And, puberty aside, Harry doesn't change much. As envisioned by Rowling, he walks the path of good so unwaveringly that his final victory over Voldemort feels, not just inevitable, but hollow".[73] In The New York Times, Christopher Hitchens compared the series to World War Two-era English boarding school stories, and while he wrote that "Rowling has won imperishable renown" for the series as a whole, he also stated that he disliked Rowling's use of deus ex machina, that the mid-book camping chapters are "abysmally long", and Voldemort "becomes more tiresome than an Ian Fleming villain".[74] Catherine Bennett of The Guardian praised Rowling for putting small details from the previous books and making them large in Deathly Hallows, such as Grindelwald being mentioned on a Chocolate Frog Card in the first book. While she points out "as her critics say, Rowling is no Dickens", she says that Rowling "has willed into a fictional being, in every book, legions of new characters, places, spells, rules and scores of unimagined twists and subplots".[75]
Stephen King criticised the reactions of some reviewers to the books, including McCauley, for jumping too quickly to surface conclusions of the work.[76] He felt this was inevitable, because of the extreme secrecy before launch which did not allow reviewers time to read and consider the book, but meant that many early reviews lacked depth. Rather than finding the writing style disappointing, he felt it had matured and improved. He acknowledged that the subject matter of the books had become more adult, and that Rowling had clearly been writing with the adult audience firmly in mind since the middle of the series. He compared the works in this respect to Huckleberry Finn and Alice in Wonderland which achieved success and have become established classics, in part by appealing to the adult audience as well as children.[76]
Sales for Harry Potter and the Deathly Hallows were record setting. The initial U.S. print run for Deathly Hallows was 12million copies, and more than a million were pre-ordered through Amazon.com and Barnes & Noble,[77] 500percent higher than pre-sales had been for Half-Blood Prince.[78] On 12April2007, Barnes & Noble declared that Deathly Hallows had broken its pre-order record, with more than 500,000 copies pre-ordered through its site.[79] On opening day, a record 8.3million copies were sold in the United States (over 96 per second),[80][81] and 2.65million copies in the United Kingdom.[82] It holds the Guinness World record for fastest selling book of fiction in 24 hours for U.S. sales.[83] At WH Smith, sales reportedly reached a rate of 15 books sold per second.[84] By June2008, nearly a year after it was published, worldwide sales were reportedly around 44million.[1]
Harry Potter and the Deathly Hallows has won several awards.[85] In 2007, the book was named one of The New York Times 100 Notable Books,[86] and one of its Notable Children's Books.[87] The novel was named the best book of 2007, by Newsweek's critic Malcolm Jones.[88] Publishers Weekly also listed Harry Potter and the Deathly Hallows among their Best Books of 2007.[89] In 2008, the American Library Association named the novel one of its Best Books for Young Adults,[90] and also listed it as a Notable Children's Book.[91] Furthermore, Harry Potter and the Deathly Hallows received the 2008 Colorado Blue Spruce Book Award.[85]
Due to Harry Potter and the Deathly Hallows' worldwide fame, it has been translated into many languages. The first translation to be released was the Ukrainian translation, on 25September2007 (as     ).[92] The Swedish title of the book was revealed by Rowling as Harry Potter and the Relics of Death (Harry Potter och Ddsrelikerna), following a pre-release question from the Swedish publisher about the difficulty of translating the two words "Deathly Hallows" without having read the book.[93] This is also the title used for the French translation (Harry Potter et les reliques de la mort), the Spanish translation (Harry Potter y las Reliquias de la Muerte), the Dutch translation (Harry Potter en de Relieken van de Dood) and the Brazilian Portuguese translation (Harry Potter e as Relquias da Morte).[94] The first Polish translation was released with a new title: Harry Potter i Insygnia mierci Harry Potter and the Insignia of Death.[95] The Hindi translation Harry Potter aur Maut ke Tohfe (     ), which means "Harry Potter and the Gifts of Death", was released by Manjul Publication in India on 27June2008.[96]
Deathly Hallows was released in hardcover on 21July2007,[97] and in paperback in the United Kingdom on 10July2008,[98] and in the United States on 7July2009.[99] In SoHo, New York, there was a release party for the American paperback edition, with many games and activities.[100] An "Adult Edition" with a different cover illustration was released by Bloomsbury on 21July2007.[101] To be released simultaneously with the original U.S. hardcover on 21July with only 100,000 copies was a Scholastic deluxe edition, highlighting a new cover illustration by Mary GrandPr.[102] In October2010, Bloomsbury released a "Celebratory" paperback edition, which featured a foiled and starred cover.[103] Lastly, on 1November2010, a "Signature" edition of the novel was released in paperback by Bloomsbury.[104]
A two-part film adaptation of Harry Potter and the Deathly Hallows is directed by David Yates, written by Steve Kloves and produced by David Heyman, David Barron and J. K. Rowling. Part 1 was released on 19November2010, and Part 2 on 15July2011.[105][106] Filming began in February2009, and ended on 12June2010.[107] However, the cast confirmed they would reshoot the epilogue scene as they only had two days to shoot the original.[108] Reshoots officially ended around December 2010.[note 1][109] Part 1 ended at Chapter 24 of the book, when Voldemort regained the Elder Wand.[110] However, there were a few omissions, such as the appearances of Dean Thomas and Viktor Krum, and Peter Pettigrew's death.[111] James Bernadelli of Reelviews said that the script stuck closest to the text since Harry Potter and the Chamber of Secrets,[112] yet this was met with negativity from some audiences as the film inherited "the book's own problems".[113]
Harry Potter and the Deathly Hallows was released simultaneously on 21 July 2007, in both the UK and the United States.[114][115] The UK edition features the voice of Stephen Fry and runs about 24 hours[116] while the U.S. edition features the voice of Jim Dale and runs about 21 hours.[117] Both Fry and Dale recorded 146 different and distinguishable character voices, and was the most recorded by an individual on an audiobook at the time.[118]
For his work on Deathly Hallows, Dale won the 2008 Grammy Award for the Best Spoken Word Album for Children.[119] He also was awarded an Earphone Award by AudioFile, who claimed, "Dale has raised the bar on audiobook interpretation so high it's hard to imagine any narrator vaulting over it."[120]
On 4 December 2008, Rowling released The Tales of Beedle the Bard both in the UK and US.[121] The Tales of Beedle the Bard is a spin-off of Deathly Hallows and contains fairy tales that are told to children in the "Wizarding World". The book includes five short stories, including "The Tale of the Three Brothers" which is the story of the Deathly Hallows.
Amazon.com released an exclusive collector's edition of the book which is a replica of the book that Amazon.com purchased at auction in December 2007.[122] Seven copies were auctioned off in London by Sothebys. Each was illustrated and handwritten by Rowling and is 157 pages. It was bound in brown Moroccan leather and embellished with five hand-chased hallmarked sterling silver ornaments and mounted moonstones.[123]
 
1 Contents

1.1 Plot introduction
1.2 Plot summary
1.3 Epilogue


1.1 Plot introduction
1.2 Plot summary
1.3 Epilogue
2 Background

2.1 Franchise
2.2 Choice of title
2.3 Rowling on finishing the book


2.1 Franchise
2.2 Choice of title
2.3 Rowling on finishing the book
3 Major themes

3.1 Death
3.2 Living in a corrupted society
3.3 Christian allegories


3.1 Death
3.2 Living in a corrupted society
3.3 Christian allegories
4 Release

4.1 Marketing and promotion
4.2 Spoiler embargo
4.3 Online leaks and early delivery
4.4 Price wars and other controversies


4.1 Marketing and promotion
4.2 Spoiler embargo
4.3 Online leaks and early delivery
4.4 Price wars and other controversies
5 Publication and reception

5.1 Critical response
5.2 Sales, awards and honours


5.1 Critical response
5.2 Sales, awards and honours
6 Translations
7 Editions
8 Adaptations

8.1 Film
8.2 Audiobooks


8.1 Film
8.2 Audiobooks
9 The Tales of Beedle the Bard
10 Notes
11 References
12 Bibliography
13 External links
1.1 Plot introduction
1.2 Plot summary
1.3 Epilogue
2.1 Franchise
2.2 Choice of title
2.3 Rowling on finishing the book
3.1 Death
3.2 Living in a corrupted society
3.3 Christian allegories
4.1 Marketing and promotion
4.2 Spoiler embargo
4.3 Online leaks and early delivery
4.4 Price wars and other controversies
5.1 Critical response
5.2 Sales, awards and honours
8.1 Film
8.2 Audiobooks
"The Tales of Beedle the Bard, Standard Edition (Harry Potter) (9780545128285): J.K. Rowling: Books". Amazon.com. http://www.amazon.com/Tales-Beedle-Standard-Edition-Potter/dp/0545128285/ref=sr_1_1?s=books&ie=UTF8&qid=1335223887&sr=1-1. Retrieved 23 April 2012.
"The Tales of Beedle the Bard, Standard Edition: Amazon.co.uk: J.K. Rowling: Books". Amazon.co.uk. http://www.amazon.co.uk/Tales-Beedle-Bard-Standard-Edition/dp/0747599874/ref=sr_1_1?s=books&ie=UTF8&qid=1335223949&sr=1-1. Retrieved 23 April 2012.
Granger, John. The Deathly Hallows Lectures: The Hogwarts Professor Explains the Final Harry Potter Adventure. Zossima Press: 2008. ISBN 0-9723221-7-5.
Hall, Susan. Reading Harry Potter: critical essays. Greenwood Publishing: 2003. ISBN 0-313-32067-5.
Rowling, JK. Harry Potter and the Half-Blood Prince. London: Bloomsbury/New York City: Scholastic: 2005. UK ISBN 0-747-58108-8/U.S. ISBN 0-439-78454-9.
Rowling, JK. Harry Potter and the Goblet of Fire. London: Bloomsbury/New York City: Scholastic: 2000. UK ISBN 0-747-54624-X/U.S. ISBN 0-439-13959-7.
Shapiro, Marc. J. K. Rowling: The Wizard Behind Harry Potter. St. Martin's Press: 2007. ISBN 0-312-37697-9.
Heckl, Raik. "The Tale of the Three Brothers" and the Idea of the Speaking Dead in the Harry Potter Novels. Leipzig: 2008.
Book: Harry Potter
Harry Potter at Bloomsbury.com web site UK publisher book information
Harry Potter at Scholastic.com web site U.S. publisher book information
Harry Potter at Allen & Unwin web site Australia-New Zealand publisher book information
.
#(`*Harry Potter (film series)*`)#.
The Harry Potter film series is a British-American film series based on the Harry Potter novels by the British author J. K. Rowling. The series is distributed by Warner Bros. and consists of eight fantasy films[1] beginning with Harry Potter and the Philosopher's Stone (2001) and culminating with Harry Potter and the Deathly Hallows  Part 2 (2011).[2] It is the highest-grossing film series of all-time in inflation unadjusted dollars, with $7.7billion in worldwide receipts. Each film is in the list of highest-grossing films of all-time in inflation unadjusted dollars[3] and is a critical success.
The series was produced by David Heyman and stars Daniel Radcliffe, Rupert Grint and Emma Watson as the three leading characters, Harry Potter, Ron Weasley and Hermione Granger. Four directors worked on the series: Chris Columbus, Alfonso Cuarn, Mike Newell, and David Yates.[4] Production took over ten years to complete, with the main story arc following Harry Potter's quest to overcome his conflict with Lord Voldemort.[5]
Harry Potter and the Deathly Hallows, the seventh and final novel in the series, was adapted into two feature-length parts.[6] Part 1 was released in November 2010 and Part 2 was released in July 2011.[7][8]
It has been revealed that J. K. Rowling was paid around three quarters of a billion dollars by Warner Bros. during the making of the films.[9]
Late in 1997, film producer David Heyman's London offices received a copy of the first book in what would become Rowling's series of seven Harry Potter novels. The book, Harry Potter and the Philosopher's Stone, was relegated to a low-priority bookshelf, where it was discovered by a secretary who read it and gave it to Heyman with a positive review. Consequently Heyman, who had originally disliked "the rubbish title", read the book himself. Highly impressed by Rowling's work, he began the process that led to one of the most successful cinematic franchises of all time.[10]
Heyman's enthusiasm led to Rowling's 1999 sale of the film rights for the first four Harry Potter books to Warner Bros. for a reported 1million (US$2,000,000).[11] A demand Rowling made was that the principal cast be kept strictly British, allowing nevertheless for the inclusion of many Irish actors such as Richard Harris as Dumbledore, and for casting of French and Eastern European actors in Harry Potter and the Goblet of Fire where characters from the book are specified as such.[12] Rowling was hesitant to sell the rights because she "didn't want to give them control over the rest of the story" by selling the rights to the characters, which would have enabled Warner Bros. to make non-author-written sequels.[13]
Although Steven Spielberg initially negotiated to direct the first film, he declined the offer.[14] Spielberg wanted the adaptation to be an animated film, with American actor Haley Joel Osment to provide Harry Potter's voice.[15] Spielberg contended that, in his opinion, there was every expectation of profit in making the film, and that making money would have been like "shooting ducks in a barrel. It's just a slam dunk. It's just like withdrawing a billion dollars and putting it into your personal bank accounts. There's no challenge".[16] In the Rubbish Bin section of her website, Rowling maintains that she has no role in choosing directors for the films, writing "Anyone who thinks I could (or would) have 'veto-ed' him [Spielberg] needs their Quick-Quotes Quill serviced."[17] After Spielberg left, talks began with other directors, including: Chris Columbus, Terry Gilliam, Jonathan Demme, Mike Newell, Alan Parker, Wolfgang Petersen, Rob Reiner, Tim Robbins, Brad Silberling, and Peter Weir.[18] Petersen and Reiner both pulled out of the running in March 2000.[19] It was then narrowed down to Silberling, Columbus, Parker and Gilliam.[20] Rowling's first choice was Terry Gilliam.[21] However on 28 March 2000 Columbus was appointed as director of the film, with Warner Bros. citing his work on other family films such as Home Alone and Mrs Doubtfire as influences for their decision.[22]
Steve Kloves was selected to write the screenplay for the first film. He described adapting the book as "tough", as it did not "lend itself to adaptation as well as the next two books." Kloves was sent a "raft" of synopses of books proposed as film adaptations, with Harry Potter being the only one that jumped out at him. He went out and bought the book, and became an instant fan. When speaking to Warner Bros. he stated that the film had to be British, and had to be true to the characters.[23] David Heyman was confirmed to produce the film.[22] Rowling received a large amount of creative control for the film, an arrangement that Columbus did not mind.[24]
Warner Bros. had initially planned to release the first film over the 4 July 2001 weekend, making for such a short production window that several of the originally proposed directors had withdrawn themselves from contention. Eventually, due to time constraints, the date was put back to 16 November 2001.[25]
In 2000, after a seven month search, lead actor Daniel Radcliffe was discovered by producer David Heyman and writer Steve Kloves seated just behind them in a theatre. In Heyman's own words, "There sitting behind me was this boy with these big blue eyes. It was Dan Radcliffe. I remember my first impressions: He was curious and funny and so energetic. There was real generosity too, and sweetness. But at the same time he was really voracious and with hunger for knowledge of whatever kind."
Radcliffe had already established himself as an actor in the 1999 BBC television production of David Copperfield in which he played the title role's childhood years. Heyman convinced Radcliffe's parents to allow him to audition for the part of Harry Potter, which involved Radcliffe being filmed.[10] (This screen test footage was released via the first set of Ultimate Editions in 2009.)[26] Rowling was enthusiastic after viewing Radcliffe's filmed test, saying she didn't think there was a better choice for the part of Harry Potter.[10][27]
Also in 2000, the then unknown British actors Emma Watson and Rupert Grint were selected from thousands of auditioning children to play the roles of Hermione Granger and Ron Weasley, respectively. Prior to them being chosen, their only previous acting experience was in school plays. Grint was eleven years old and Watson ten at the time they were cast.[28]
Los Angeles Times writer Geoff Boucher, who conducted the above-mentioned interview with Heyman, added that the casting of the three major roles "is especially impressive in hindsight. The trio's selection was arguably one of the best show-business decisions over the past decade... they have shown admirable grace and steadiness in the face of teen superstardom."[10][27]
Filming of the series began at Leavesden Studios in September 2000 and ended in December 2010, with post-production on the final film lasting until Summer 2011.[5] Leavesden Studios was the main base for filming Harry Potter and it opened to the public as a studio tour in 2012 (renamed as Warner Bros. Studios, Leavesden).[29]
David Heyman has produced all the films in the series with his company Heyday Films, while David Barron joined the series as an executive producer on Chamber of Secrets and Goblet of Fire. He was later appointed producer on the last three adaptations. Chris Columbus was an executive producer on the first two films alongside Mark Radcliffe and Michael Barnathan, but became a producer on the third film alongside Heyman and Radcliffe. Other executive producers include Tanya Seghatchian and Lionel Wigram. J. K. Rowling, author of the series, was asked to become a producer on Goblet of Fire, but declined. However, she accepted the role on the two-part Deathly Hallows.[30] Heyday Films and Columbus' company 1492 Pictures collaborated with Duncan Henderson Productions in 2001, Miracle Productions in 2002 and P of A Productions in 2004. Even though Prisoner of Azkaban was the final film produced by 1492 Pictures, Heyday Films continued with the franchise and collaborated with Patalex IV Productions in 2005. The sixth film in the series, Half-Blood Prince, was the most expensive film to produce as of 2009. Warner Bros. split the seventh and final novel in the series, Deathly Hallows, into two cinematic parts. The two parts were filmed back-to-back from early 2009 to summer 2010, with the completion of reshoots taking place on 21 December 2010; this marked the end of filming Harry Potter. Heyman stated that Deathly Hallows was "shot as one film", but was released in two feature-length parts.[31]
Tim Burke, the visual effects supervisor of the series, said of the production on Harry Potter, "It was this huge family; I think there were over 700 people working at Leavesden, an industry in itself." David Heyman said, "When the first film opened, no way did I think we'd make eight films. That didn't seem feasible until after we'd done the fourth." Nisha Parti, the production consultant on the first film, said that Heyman "made the first film very much the way he felt the studio [Warner Bros.] wanted to make it." After the film's success, Heyman was given "more freedom".[32]
One of the aims of the filmmakers from the beginning of production was to develop the maturity of the films. Chris Columbus stated, "We realised that these movies would get progressively darker. Again, we didn't know how dark but we realised that as the kids get older, the movies get a little edgier and darker."[33] This transpired with the succeeding three directors who would work on the series in the following years, with the films beginning to deal with issues such as death, betrayal, prejudice, and political corruption as the series developed narratively and thematically.[4][34]
After Chris Columbus had finished working on Harry Potter and the Philosopher's Stone, he was hired to direct the second film, Harry Potter and the Chamber of Secrets. The production started within a week after the release of the first film. Columbus was set to direct all entries in the series,[35] however he did not want to return for the third film, Harry Potter and the Prisoner of Azkaban, as he claimed he was "burned out".[36] He moved to the position of producer, while Alfonso Cuarn was approached for the role of director. He was initially nervous about directing the instalment as he had not read any of the books or seen the films. After reading the series, he changed his mind and signed on to direct as he had immediately connected to the story.[37]
Because Cuarn decided not to direct the fourth instalment, Harry Potter and the Goblet of Fire, a new director had to be selected.[38] Mike Newell was chosen to direct the film, but declined to direct the next film, Harry Potter and the Order of the Phoenix, which was given to David Yates, who also directed Harry Potter and the Half-Blood Prince and Harry Potter and the Deathly Hallows, becoming the only director to helm more than one film since Chris Columbus.
Chris Columbus said his vision of the first two films was of a "golden storybook, an old-fashioned look", while Alfonso Cuarn changed the tone of the series, desaturated the colour palette, and expanded the landscape around Hogwarts.[33][38] Mike Newell decided to direct the fourth film as a "paranoid thriller", while David Yates wanted to "bring a sense of jeopardy and character to the world".[39][40] Cuarn, Newell and Yates have said that the challenge was striking a balance between making the films according to their individual vision, while working within a cinematic world already established by Columbus.[38][39][40]
David Heyman commented on the "generosity of the directors" by revealing that "Chris spent time with Alfonso, Alfonso spent time with Mike and Mike spent time with David, showing him an early cut of the film, talking through what it means to be a director and how they went about [making the films]."[41]
David Heyman also said, "I suppose Chris Columbus was the most conservative choice from the studio's point of view. But he expressed real passion."[32] Producer Tanya Seghatchian said they were "more adventurous" in choosing a director for the third film and went straight to Alfonso Cuarn.[32] Mike Newell became the first British director of the series when he was chosen for the fourth film; Newell was considered to direct the first film before he dropped out.[32] David Yates directed the final films after David Heyman thought him capable of handling the edgy, emotional, and political material of the later novels.[42]
All the directors have been supportive of each other. Chris Columbus praised the character development in the films, while Alfonso Cuarn admired the "quiet poetry" of David Yates' films.[33][38] Mike Newell noted that each director had a different heroism and David Yates views the first four films "respectfully and enjoy[s] them."[39][40] Daniel Radcliffe said Yates "took the charm of the films that Chris made and the visual flair of everything that Alfonso did and the thoroughly British, bombastic nature of the film directed by Mike Newell" and added "his own sense" of realism.[43]
Steve Kloves wrote the screenplays for all but the fifth film, which was penned by Michael Goldenberg. Kloves had direct assistance of J. K. Rowling, though she allowed him what he described as "tremendous elbow room". Rowling once asked Kloves to keep being faithful to the spirit of the books, thus the plot and tone of each film and its corresponding book are virtually the same, albeit with some changes and omissions for purposes of cinematic style, time and budget constraints. Michael Goldenberg also received input from Rowling during his adaptation of the fifth novel; Goldenberg was originally considered to adapt the first novel before the studio chose Kloves.[44]
David Heyman briefly explained the book-to-film transition. He commented on Rowling's involvement in the series, stating that she understands that "books and films are different" and is "the best support" a producer could have. Rowling had overall approval on the scripts, which were viewed and discussed by the director and the producers. Heyman also said that Kloves was the "key voice" in the process of adapting the novels and that certain aspects from the books needed to have been excluded from the scripts due to the filmmakers' decision to keep the main focus on Harry's journey as a character, which would ultimately give the films a defined structure. Heyman mentioned that some fans "don't necessarily understand the adaptation process" and that the filmmakers would love to "have everything" from the books in the films, but noted that it is not possible as they had "neither time nor cinematic structure" to do so. He finished by saying that adapting a novel to the screen is "a really considered process."[45]
Due to the fact that the films were being made as the novels were being published, the filmmakers had no idea of the story's outcome until the release of the final novel in 2007. Kloves spoke of his relationship with Rowling when adapting the novels by saying, "The thing is about Jo, which is remarkable for someone who had no experience with the filmmaking process, was her intuition. We had a conversation the very first day I met her where she said, 'I know the movies can't be the books... because I know what's coming and it's impossible to fully dramatise on screen what I'm going to write. But I just ask you to be true to the characters, that's all I care about.'"[46] Kloves also said, "I don't know what compelled me to say this [to Rowling] but I said, 'I've just got to warn you my favourite character is not Harry. My favourite character is Hermione.' And I think for some weird reason, from that moment on, she sort of trusted me."[46]
Aside from the three lead actors, other notable cast members include Robbie Coltrane as Rubeus Hagrid, Alan Rickman as Severus Snape, Maggie Smith as Minerva McGonagall, and Tom Felton as Draco Malfoy. Richard Harris, who played the role of Professor Albus Dumbledore, passed away on 25 October 2002 causing the role to be re-cast for the third installment, Harry Potter and the Prisoner of Azkaban. David Heyman and director Alfonso Cuarn chose Michael Gambon to portray the character of Dumbledore, which he did for all succeeding films. Notable recurring cast members include Helena Bonham Carter as Bellatrix Lestrange, Warwick Davis as Filius Flitwick, Ralph Fiennes as Lord Voldemort, Brendan Gleeson as Alastor Moody, Richard Griffiths as Vernon Dursley, Jason Isaacs as Lucius Malfoy, Gary Oldman as Sirius Black, Fiona Shaw as Petunia Dursley, Timothy Spall as Peter Pettigrew, David Thewlis as Remus Lupin, Emma Thompson as Sybill Trelawney, and Julie Walters as Molly Weasley.
The series has seen many returning crew members from various departments, including: Nick Dudman, make-up and creature effects designer; Amanda Knight, make-up designer; Jany Temime, costume designer; Fiona Weir, casting director; Tim Burke, visual effects supervisor; Peter Doyle, digital film colourist; Greg Powell, stunt coordinator; and David Holmes, stunt double.
The production designer for all eight films is Stuart Craig. Assisted by Stephanie McMillan, Craig has created iconic sets pieces including the Ministry of Magic, the Chamber of Secrets, Malfoy Manor and the layout for the CGI Horcrux Cave. Due to the fact that the novels were being published as the films were being made, Craig was required to rebuild some sets for future films and alter the design of Hogwarts. He said, "In the early days, every time you saw the exterior of Hogwarts, it was a physical miniature" which was made by craftsmen and occupied a large sound stage.[49][50] "We ended up with a profile of how Hogwarts looked, a skyline that actually I didnt design, and it wasnt always satisfactory, and as all the novels got written and movies got made there were new requirements [for buildings]. The [Astronomy Tower] definitely wasnt there originally and so we were able to add that substantial piece. And in the last film, we needed an arena for the battle for Hogwarts  the big courtyard outside doubled in size and if you look at the first movie it wasnt there at all. There were quite some liberties taken with the continuity of Hogwarts."[51] In the last film, Craig used a digital model instead of a miniature to "embrace the latest technology".[50]
On the method of creating the sets, Craig said he often started by sketching ideas onto a blank sheet of paper.[52] Stephanie McMillan also said that "each film always had plenty of new challenges", citing the changes in visual style between directors and cinematographers as an example, along with the developing story in the novels. Due to J. K. Rowling's descriptions of various settings in the novels, Craig noted his "responsibility was to place it together".[53]
Craig commented on his experience working in the studio environment, "Im the production designer, but on a big movie like Harry Potter I may be responsible for 30 to 35 people; from the supervising art director, and a team of art directors and assistants, to draughtsmen and junior draughtsmen, and then on to model makers, sculptors and scenic artists." He said, "Ten years go, all the Harry Potter drawings were done in pencil. I would take my roughs and plans and sections and give them to a professional architectural illustrator, who would create concept art using pencil and colour wash on watercolour paper." He said the process changed slightly throughout the years due to, what he called, the "digital revolution" of making films.[50]
When filming of the series was completed, some of Craig's sets had to be rebuilt or transported for them to be displayed at the Warner Bros. studio tour.[49]
There have been six different directors of photography in the franchise: John Seale worked on the first film; Roger Pratt on the second and fourth; Michael Seresin on the third; Sawomir Idziak on the fifth; Bruno Delbonnel on the sixth; and Eduardo Serra on the seventh and eighth. Delbonnel was considered to return for both parts of Deathly Hallows, however he declined and stated that he was "scared of repeating" himself.[54] Delbonnel's cinematography in Half-Blood Prince gained the series its only Academy Award nomination for Best Cinematography. As the series progressed, each cinematographer faced the challenge of shooting and lighting older sets (which had been around since the first few films) in unique and different ways.[55] Chris Columbus said the series' vivid colouring decreased as each film was made, reflecting a more real world.[33][56]
Michael Seresin commented on the change of visual style from the first two films in Prisoner of Azkaban: "The lighting is moodier, with more shadowing and cross-lighting." Seresin and Alfonso Cuarn moved away from the strongly coloured and brightly lit cinematography of the first two films and set the stage for the succeeding five films, with the lighting becoming dimmer and the colour palette more muted with Roger Pratt's work on the fourth film.[57] Sawomir Idziak provided the fifth film with a cold, blue tint which mirrored the bleakness of the story and, after comparing a range of digital cameras with 35mm film, Bruno Delbonnel decided to shoot the sixth movie, Half-Blood Prince, on film rather than the increasingly popular digital format; this decision was kept for the two-part Deathly Hallows with Eduardo Serra, who said that he preferred to work with film because it was "more technically accurate and dependable".[58]
Because the majority of Deathly Hallows takes place in various settings away from Hogwarts, David Yates wanted to "shake things up" by using different photographic techniques such as using hand-held cameras and very wide camera lenses.[59] Eduardo Serra said, "Sometimes we are combining elements shot by the main unit, a second unit, and the visual effects unit. You have to know what is being captured  colours, contrast, et cetera  with mathematical precision." He noted that with Stuart Craig's "amazing sets and the story", the filmmakers could not "stray too far from the look of the previous Harry Potter films."[58][60]
Along with continuous changes in cinematographers, there have been five film editors to work in post-production on the series: Richard Francis-Bruce edited the first instalment; Peter Honess did the second; Steven Weisberg edited the third, Mick Audsley did the fourth; and Mark Day edited films five to eight.
The Harry Potter series has had four composers. John Williams was the first composer to enter the series and is known for creating Hedwig's Theme, which is heard at the start of each film. Williams scored the first three films: Philosopher's Stone, Chamber of Secrets, and Prisoner of Azkaban. However, the second entry was adapted and conducted by William Ross due to Williams' conflicting commitments.
After Williams left the series to pursue other projects, Patrick Doyle scored the fourth entry, Goblet of Fire, which was directed by Mike Newell with whom Doyle had worked with previously. In 2006, Nicholas Hooper started work on the soundtrack to Order of the Phoenix by reuniting with old friend director David Yates. Hooper also composed the soundtrack to Half-Blood Prince but decided not to return for the final films.
In January 2010, Alexandre Desplat was confirmed to compose the score for Harry Potter and the Deathly Hallows  Part 1.[61] The film's orchestration started in the summer with Conrad Pope, the orchestrator on the first three Harry Potter films, collaborating with Desplat. Pope commented that the music "reminds one of the old days."[62] Desplat returned to score Harry Potter and the Deathly Hallows  Part 2 in 2011.[63]
Director David Yates stated that he wanted John Williams to return to the series for the final instalment, but their schedules did not align due to the urgent demand of a rough cut of the film sooner than was possible.[64] The final recording sessions of Harry Potter took place on 27 May 2011 at Abbey Road Studios with the London Symphony Orchestra, orchestrator Conrad Pope and composer Alexandre Desplat.[65]
Doyle, Hooper and Desplat introduced their own personal themes to their respective soundtracks, while keeping a few of John Williams' themes.
There have been many visual effects companies to work on the Harry Potter series. Some of these include Rising Sun Pictures, Double Negative, Cinesite, Framestore and Industrial Light & Magic. The latter three have worked on all the films in the series, while Double Negative and Rising Sun Pictures began their commitments with Prisoner of Azkaban and Goblet of Fire respectively. Framestore contributed by developing many memorable creatures and sequences to the series.[66] Cinesite were involved in producing both miniature and digital effects for the films.[67] Producer David Barron said that "Harry Potter created the UK effects industry as we know it. On the first film, all the complicated visual effects were done on the [US] west coast. But on the second, we took a leap of faith and gave much of what would normally be given to Californian vendors to UK ones. They came up trumps." Tim Burke, the visual effects supervisor, said many studios "are bringing their work to UK effects companies. Every facility is fully booked, and that wasn't the case before Harry Potter. That's really significant."[32]
On 12 June 2010, filming of the Deathly Hallows  Part 1 and Deathly Hallows  Part 2 was completed with actor Warwick Davis stating on his Twitter account, "The end of an Era  today is officially the last day of principal photography on 'Harry Potter'  ever. I feel honoured to be here as the director shouts cut for the very last time. Farewell Harry & Hogwarts, it's been magic!".[68] However, reshoots of the epilogue scene were confirmed to begin in the winter of 2010. The filming was completed on 21 December 2010, marking the official closure of filming the Harry Potter franchise.[69] Exactly four years ago on that day, author J. K. Rowling's official website revealed the title of the final novel in the series  Harry Potter and the Deathly Hallows.[70]
Harry Potter is an orphaned boy brought up by his unfriendly aunt and uncle. At the age of eleven, half-giant Rubeus Hagrid informs him that he is actually a wizard and that his parents were murdered by an evil wizard named Lord Voldemort. Voldemort also attempted to kill one-year old Harry on the same night, but his killing curse mysteriously rebounded and reduced him to a weak and helpless form. Harry became extremely famous in the Wizarding World as a result. Harry begins his first year at Hogwarts School of Witchcraft and Wizardry and learns about magic. During the year, Harry and his friends Ron Weasley and Hermione Granger become entangled in the mystery of the Philosopher's Stone which is being kept within the school.
Harry, Ron, and Hermione return to Hogwarts for their second year, which proves to be more challenging than the last. The Chamber of Secrets has been opened, leaving students and ghosts petrified by an unleashed monster. Harry must face up to claims that he is the heir of Salazar Slytherin (founder of the Chamber), learn that he can speak Parseltongue, and also discover the properties of a mysterious diary only to find himself trapped within the Chamber of Secrets itself.
Harry Potter's third year sees the boy wizard, along with his friends, attending Hogwarts School once again. Professor R. J. Lupin joins the staff as Defence Against the Dark Arts teacher, while convicted murderer Sirius Black escapes from Azkaban Prison. The Ministry of Magic entrusts the Dementors of Azkaban to guard Hogwarts from Black. Harry learns more about his past and his connection with the escaped prisoner.
During Harry's fourth year, the Dark Mark appears in the sky after a Death Eater attack at the Quidditch World Cup, Hogwarts plays host to a legendary event: the Triwizard Tournament, there is a new Defence Against the Dark Arts professor Alastor Moody and frequent nightmares bother Harry all year. Three European schools participate in the tournament, with three 'champions' representing each school in the deadly tasks. The Goblet of Fire chooses Fleur Delacour, Viktor Krum and Cedric Diggory to compete against each other. However, curiously, Harry's name is also produced from the Goblet making him a fourth champion, which results in a terrifying encounter with a reborn Lord Voldemort.
Harry's fifth year begins with him being attacked by Dementors in Little Whinging. Later, he finds out that the Ministry of Magic is in denial of Lord Voldemort's return. Harry is also beset by disturbing and realistic nightmares, while Professor Umbridge, a representative of Minister for Magic Cornelius Fudge, is the new Defence Against the Dark Arts teacher. Harry becomes aware that Voldemort is after a prophecy which reveals: "neither can live while the other survives". Therefore, the rebellion involving the students of Hogwarts, secret organisation Order of the Phoenix, the Ministry of Magic, and the Death Eaters begins.
In Harry's sixth year at Hogwarts, Lord Voldemort and his Death Eaters are increasing their terror upon the Wizarding and Muggle worlds. Headmaster Albus Dumbledore persuades his old friend Horace Slughorn to return to Hogwarts as a professor as there is a vacancy to fill. There is a more important reason, however, for Slughorn's return. While in a Potions lesson, Harry takes possession of a strangely annotated school textbook, inscribed 'This is the property of the Half-Blood Prince'. As romance and hormones lurk within the castle's walls all year, Draco Malfoy struggles to carry out a deed presented to him by Voldemort. Meanwhile, Dumbledore and Harry secretly work together to discover the method on how to destroy the Dark Lord once and for all.
After unexpected events at the end of the previous year, Harry, Ron, and Hermione are entrusted with a quest to find and destroy Lord Voldemort's secret to immortality  the Horcruxes. It is supposed to be their final year at Hogwarts, but the collapse of the Ministry of Magic and Voldemort's rise to power prevents them from attending. The trio undergo a long adventure with many obstacles in their path including Death Eaters, Snatchers, the mysterious Deathly Hallows and Harry's connection with the Dark Lord's mind becoming ever stronger.
After destroying one Horcrux and discovering the significance of the three Deathly Hallows, Harry, Ron and Hermione continue to seek the other Horcruxes in an attempt to destroy Voldemort, who has now obtained the Elder Wand. The Dark Lord discovers Harry's hunt for Horcruxes and launches an attack on Hogwarts School, where the trio return for one last stand against the dark forces that threaten to rid the Wizarding World of non-magical heritage to achieve pure-blood dominance.
The rights for the first four novels in the series were sold to Warner Bros. for 1,000,000 by J. K. Rowling. After the release of the fourth book in July 2000, the first film, Harry Potter and the Philosopher's Stone, was released on 16 November 2001. The film grossed $90million in the United States alone which set a record opening worldwide. The succeeding three motion picture adaptations followed suit in financial success, while garnering positive reviews from fans and critics. The fifth film, Harry Potter and the Order of the Phoenix was released by Warner Bros. on 11 July 2007 in English-speaking countries, except for the UK and Ireland which released the movie on 12 July.[71] The sixth, Harry Potter and the Half-Blood Prince, was released on 15 July 2009 to critical acclaim and finished its theatrical run ranked as the number two grossing film of 2009 on the worldwide charts. The final novel, Harry Potter and the Deathly Hallows, was split into two cinematic parts: Part 1 was released on 19 November 2010 and Part 2, the conclusion to both the final film and the series, was released on 15 July 2011.[72] Part 1 was originally scheduled to be released in 3D and 2D,[73] but due to a delay in the 3D conversion process, Warner Bros. released the film only in 2D and IMAX cinemas. However, Part 2 was released in 2D and 3D cinemas as originally planned.[74]
All the films have been a success financially and critically, making the franchise one of the major Hollywood tent-poles akin to James Bond, Star Wars, Indiana Jones and Pirates of the Caribbean. The series is noted by audiences for growing visually darker and more mature as each film was released.[33][75][76][77][78] However, opinions of the films generally divide book fans, with some preferring the more faithful approach of the first two films, and others preferring the more stylised character-driven approach of the later films. Some also feel the series has a "disjointed" feel due to the changes in directors, as well as Michael Gambon's interpretation of Albus Dumbledore differing from that of Richard Harris'. Author J. K. Rowling has been constantly supportive of the films,[79][80][81] and evaluated Deathly Hallows as her favourite one in the series. She wrote on her website of the changes in the book-to-film transition, "It is simply impossible to incorporate every one of my storylines into a film that has to be kept under four hours long. Obviously films have restrictions  novels do not have constraints of time and budget; I can create dazzling effects relying on nothing but the interaction of my own and my readers' imaginations".[82]
At the 64th British Academy Film Awards in February 2011, J. K. Rowling, David Heyman, David Barron, David Yates, Alfonso Cuarn, Mike Newell, Rupert Grint and Emma Watson collected the Michael Balcon Award for Outstanding British Contribution to Cinema for the series.[107][108] Harry Potter was also recognised by the BAFTA Los Angeles Britannia Awards, as David Yates won the Britannia Award for Artistic Excellence in Directing for his four Harry Potter films.[109][110] Though the series did not win any Academy Awards, six of the eight films were nominated for a total of 12 Oscars.
Some critics, fans and general audiences have expressed disappointment that the series has not gained any Academy Awards for its achievements.[111][112] Despite this, the series has gained success in many other award ceremonies including the annual Saturn Awards, Art Directors Guild Awards and Grammy Awards. The series has also gained a total of 30 nominations at the British Academy Film Awards presented at the annual BAFTAs.
Philosopher's Stone achieved seven BAFTA Award nominations including Best British Film and Best Supporting Actor for Robbie Coltrane.[113] The film was also nominated for eight Saturn Awards and won for its costumes design.[114] It was also nominated at the Art Directors Guild Awards for its production design[115] and received the Broadcast Film Critics Award for Best Live Action Family Film along with gaining two other nominations.[116] Chamber of Secrets won the award for Best Live Action Family Film in the Phoenix Film Critics Society. It was nominated for seven Saturn Awards including Best Director and Best Fantasy Film. The film was nominated for four BAFTA Awards and a Grammy Award for John Williams' score. Prisoner of Azkaban won an Audience Award at the BAFTA Awards, as well as Best Feature Film. The film also won a BMI Film Music award along with being nominated at the Grammy Awards, Visual Effect Society Awards and the Amanda Awards. Goblet of Fire won a BAFTA award for Best Production Design as well as being nominated at the Saturn Awards, Critic's Choice Awards and the Visual Effects Society Awards.
Order of the Phoenix picked up three awards at the inaugural ITV National Movie Awards.[117] At the Empire Awards, David Yates won Best Director.[118] Composer Nicholas Hooper received a nomination for a World Soundtrack Discovery Award.[119] The film was nominated at the BAFTA Awards, but did not win for Best Production Design or Best Special Visual Effects.[120] Half-Blood Prince was nominated for BAFTA Awards in Production Design and Visual Effects,[121] and was in the longlists for several other categories, including Best Supporting Actor for Alan Rickman.[122] Amongst other nominations and wins, the film also achieved Best Family Movie at the National Movie Awards as well as Best Live Action Family Film at the Phoenix Film Critics Society Awards, along with being nominated for Best Motion Picture at the Satellite Awards. Deathly Hallows  Part 1 gained two nominations at the BAFTA Awards for Best Make-Up and Hair and Best Visual Effects, along with receiving nominations for the same categories at the Broadcast Film Critics Association Awards. Eduardo Serra's cinematography and Stuart Craig's production design were also nominated in various award ceremonies and David Yates attained his second win at the Empire Awards, this time for Best Fantasy Film. He also obtained another Best Director nomination at the annual Saturn Awards, which also saw the film gain a Best Fantasy Film nomination.[123][124] Deathly Hallows  Part 2 was released to critical acclaim, gaining a mix of audience awards. Part 2 of Deathly Hallows was also recognised at the Saturn Awards as well as the BAFTA Awards where the film achieved a win for Best Special Visual Effects.[125]
As of 2012[update], the Harry Potter film franchise is the highest grossing film franchise of all time, with the eight films released grossing over $7.7billion worldwide. Without adjusting for inflation, this is higher than the first 22 James Bond films and the six films in the Star Wars franchise.[126] Chris Columbus' Philosopher's Stone became the highest-grossing Harry Potter film worldwide upon completing its theatrical run in 2002, but it was eventually topped by David Yates' Deathly Hallows  Part 2, while Alfonso Cuarn's Prisoner of Azkaban grossed the least.[127][128][129][130]
(55,913,000)
(45,093,000)
(40,184,000)
(45,244,000)
(42,443,000)
(40,261,000)
(37,500,000)
(56,000,000)
1 Origins

1.1 Casting the roles of Harry, Ron and Hermione


1.1 Casting the roles of Harry, Ron and Hermione
2 Production

2.1 Directors
2.2 Scripts
2.3 Cast and crew
2.4 Set design
2.5 Cinematography
2.6 Editing
2.7 Music
2.8 Visual effects
2.9 Final filming


2.1 Directors
2.2 Scripts
2.3 Cast and crew
2.4 Set design
2.5 Cinematography
2.6 Editing
2.7 Music
2.8 Visual effects
2.9 Final filming
3 Plot

3.1 Harry Potter and the Philosopher's Stone (2001)
3.2 Harry Potter and the Chamber of Secrets (2002)
3.3 Harry Potter and the Prisoner of Azkaban (2004)
3.4 Harry Potter and the Goblet of Fire (2005)
3.5 Harry Potter and the Order of the Phoenix (2007)
3.6 Harry Potter and the Half-Blood Prince (2009)
3.7 Harry Potter and the Deathly Hallows  Part 1 (2010)
3.8 Harry Potter and the Deathly Hallows  Part 2 (2011)


3.1 Harry Potter and the Philosopher's Stone (2001)
3.2 Harry Potter and the Chamber of Secrets (2002)
3.3 Harry Potter and the Prisoner of Azkaban (2004)
3.4 Harry Potter and the Goblet of Fire (2005)
3.5 Harry Potter and the Order of the Phoenix (2007)
3.6 Harry Potter and the Half-Blood Prince (2009)
3.7 Harry Potter and the Deathly Hallows  Part 1 (2010)
3.8 Harry Potter and the Deathly Hallows  Part 2 (2011)
4 Release
5 Reaction

5.1 Critical response

5.1.1 Review aggregate results


5.2 Accolades
5.3 Box office

5.3.1 All-time ranks




5.1 Critical response

5.1.1 Review aggregate results


5.1.1 Review aggregate results
5.2 Accolades
5.3 Box office

5.3.1 All-time ranks


5.3.1 All-time ranks
6 Notes
7 References
8 External links
1.1 Casting the roles of Harry, Ron and Hermione
2.1 Directors
2.2 Scripts
2.3 Cast and crew
2.4 Set design
2.5 Cinematography
2.6 Editing
2.7 Music
2.8 Visual effects
2.9 Final filming
3.1 Harry Potter and the Philosopher's Stone (2001)
3.2 Harry Potter and the Chamber of Secrets (2002)
3.3 Harry Potter and the Prisoner of Azkaban (2004)
3.4 Harry Potter and the Goblet of Fire (2005)
3.5 Harry Potter and the Order of the Phoenix (2007)
3.6 Harry Potter and the Half-Blood Prince (2009)
3.7 Harry Potter and the Deathly Hallows  Part 1 (2010)
3.8 Harry Potter and the Deathly Hallows  Part 2 (2011)
5.1 Critical response

5.1.1 Review aggregate results


5.1.1 Review aggregate results
5.2 Accolades
5.3 Box office

5.3.1 All-time ranks


5.3.1 All-time ranks
5.1.1 Review aggregate results
5.3.1 All-time ranks
Book: Harry Potter
Official website
Growing Up with Harry Potter  photo essay by Time
.
#(`*Harry Potter (film series)*`)#.
The Harry Potter film series is a British-American film series based on the Harry Potter novels by the British author J. K. Rowling. The series is distributed by Warner Bros. and consists of eight fantasy films[1] beginning with Harry Potter and the Philosopher's Stone (2001) and culminating with Harry Potter and the Deathly Hallows  Part 2 (2011).[2] It is the highest-grossing film series of all-time in inflation unadjusted dollars, with $7.7billion in worldwide receipts. Each film is in the list of highest-grossing films of all-time in inflation unadjusted dollars[3] and is a critical success.
The series was produced by David Heyman and stars Daniel Radcliffe, Rupert Grint and Emma Watson as the three leading characters, Harry Potter, Ron Weasley and Hermione Granger. Four directors worked on the series: Chris Columbus, Alfonso Cuarn, Mike Newell, and David Yates.[4] Production took over ten years to complete, with the main story arc following Harry Potter's quest to overcome his conflict with Lord Voldemort.[5]
Harry Potter and the Deathly Hallows, the seventh and final novel in the series, was adapted into two feature-length parts.[6] Part 1 was released in November 2010 and Part 2 was released in July 2011.[7][8]
It has been revealed that J. K. Rowling was paid around three quarters of a billion dollars by Warner Bros. during the making of the films.[9]
Late in 1997, film producer David Heyman's London offices received a copy of the first book in what would become Rowling's series of seven Harry Potter novels. The book, Harry Potter and the Philosopher's Stone, was relegated to a low-priority bookshelf, where it was discovered by a secretary who read it and gave it to Heyman with a positive review. Consequently Heyman, who had originally disliked "the rubbish title", read the book himself. Highly impressed by Rowling's work, he began the process that led to one of the most successful cinematic franchises of all time.[10]
Heyman's enthusiasm led to Rowling's 1999 sale of the film rights for the first four Harry Potter books to Warner Bros. for a reported 1million (US$2,000,000).[11] A demand Rowling made was that the principal cast be kept strictly British, allowing nevertheless for the inclusion of many Irish actors such as Richard Harris as Dumbledore, and for casting of French and Eastern European actors in Harry Potter and the Goblet of Fire where characters from the book are specified as such.[12] Rowling was hesitant to sell the rights because she "didn't want to give them control over the rest of the story" by selling the rights to the characters, which would have enabled Warner Bros. to make non-author-written sequels.[13]
Although Steven Spielberg initially negotiated to direct the first film, he declined the offer.[14] Spielberg wanted the adaptation to be an animated film, with American actor Haley Joel Osment to provide Harry Potter's voice.[15] Spielberg contended that, in his opinion, there was every expectation of profit in making the film, and that making money would have been like "shooting ducks in a barrel. It's just a slam dunk. It's just like withdrawing a billion dollars and putting it into your personal bank accounts. There's no challenge".[16] In the Rubbish Bin section of her website, Rowling maintains that she has no role in choosing directors for the films, writing "Anyone who thinks I could (or would) have 'veto-ed' him [Spielberg] needs their Quick-Quotes Quill serviced."[17] After Spielberg left, talks began with other directors, including: Chris Columbus, Terry Gilliam, Jonathan Demme, Mike Newell, Alan Parker, Wolfgang Petersen, Rob Reiner, Tim Robbins, Brad Silberling, and Peter Weir.[18] Petersen and Reiner both pulled out of the running in March 2000.[19] It was then narrowed down to Silberling, Columbus, Parker and Gilliam.[20] Rowling's first choice was Terry Gilliam.[21] However on 28 March 2000 Columbus was appointed as director of the film, with Warner Bros. citing his work on other family films such as Home Alone and Mrs Doubtfire as influences for their decision.[22]
Steve Kloves was selected to write the screenplay for the first film. He described adapting the book as "tough", as it did not "lend itself to adaptation as well as the next two books." Kloves was sent a "raft" of synopses of books proposed as film adaptations, with Harry Potter being the only one that jumped out at him. He went out and bought the book, and became an instant fan. When speaking to Warner Bros. he stated that the film had to be British, and had to be true to the characters.[23] David Heyman was confirmed to produce the film.[22] Rowling received a large amount of creative control for the film, an arrangement that Columbus did not mind.[24]
Warner Bros. had initially planned to release the first film over the 4 July 2001 weekend, making for such a short production window that several of the originally proposed directors had withdrawn themselves from contention. Eventually, due to time constraints, the date was put back to 16 November 2001.[25]
In 2000, after a seven month search, lead actor Daniel Radcliffe was discovered by producer David Heyman and writer Steve Kloves seated just behind them in a theatre. In Heyman's own words, "There sitting behind me was this boy with these big blue eyes. It was Dan Radcliffe. I remember my first impressions: He was curious and funny and so energetic. There was real generosity too, and sweetness. But at the same time he was really voracious and with hunger for knowledge of whatever kind."
Radcliffe had already established himself as an actor in the 1999 BBC television production of David Copperfield in which he played the title role's childhood years. Heyman convinced Radcliffe's parents to allow him to audition for the part of Harry Potter, which involved Radcliffe being filmed.[10] (This screen test footage was released via the first set of Ultimate Editions in 2009.)[26] Rowling was enthusiastic after viewing Radcliffe's filmed test, saying she didn't think there was a better choice for the part of Harry Potter.[10][27]
Also in 2000, the then unknown British actors Emma Watson and Rupert Grint were selected from thousands of auditioning children to play the roles of Hermione Granger and Ron Weasley, respectively. Prior to them being chosen, their only previous acting experience was in school plays. Grint was eleven years old and Watson ten at the time they were cast.[28]
Los Angeles Times writer Geoff Boucher, who conducted the above-mentioned interview with Heyman, added that the casting of the three major roles "is especially impressive in hindsight. The trio's selection was arguably one of the best show-business decisions over the past decade... they have shown admirable grace and steadiness in the face of teen superstardom."[10][27]
Filming of the series began at Leavesden Studios in September 2000 and ended in December 2010, with post-production on the final film lasting until Summer 2011.[5] Leavesden Studios was the main base for filming Harry Potter and it opened to the public as a studio tour in 2012 (renamed as Warner Bros. Studios, Leavesden).[29]
David Heyman has produced all the films in the series with his company Heyday Films, while David Barron joined the series as an executive producer on Chamber of Secrets and Goblet of Fire. He was later appointed producer on the last three adaptations. Chris Columbus was an executive producer on the first two films alongside Mark Radcliffe and Michael Barnathan, but became a producer on the third film alongside Heyman and Radcliffe. Other executive producers include Tanya Seghatchian and Lionel Wigram. J. K. Rowling, author of the series, was asked to become a producer on Goblet of Fire, but declined. However, she accepted the role on the two-part Deathly Hallows.[30] Heyday Films and Columbus' company 1492 Pictures collaborated with Duncan Henderson Productions in 2001, Miracle Productions in 2002 and P of A Productions in 2004. Even though Prisoner of Azkaban was the final film produced by 1492 Pictures, Heyday Films continued with the franchise and collaborated with Patalex IV Productions in 2005. The sixth film in the series, Half-Blood Prince, was the most expensive film to produce as of 2009. Warner Bros. split the seventh and final novel in the series, Deathly Hallows, into two cinematic parts. The two parts were filmed back-to-back from early 2009 to summer 2010, with the completion of reshoots taking place on 21 December 2010; this marked the end of filming Harry Potter. Heyman stated that Deathly Hallows was "shot as one film", but was released in two feature-length parts.[31]
Tim Burke, the visual effects supervisor of the series, said of the production on Harry Potter, "It was this huge family; I think there were over 700 people working at Leavesden, an industry in itself." David Heyman said, "When the first film opened, no way did I think we'd make eight films. That didn't seem feasible until after we'd done the fourth." Nisha Parti, the production consultant on the first film, said that Heyman "made the first film very much the way he felt the studio [Warner Bros.] wanted to make it." After the film's success, Heyman was given "more freedom".[32]
One of the aims of the filmmakers from the beginning of production was to develop the maturity of the films. Chris Columbus stated, "We realised that these movies would get progressively darker. Again, we didn't know how dark but we realised that as the kids get older, the movies get a little edgier and darker."[33] This transpired with the succeeding three directors who would work on the series in the following years, with the films beginning to deal with issues such as death, betrayal, prejudice, and political corruption as the series developed narratively and thematically.[4][34]
After Chris Columbus had finished working on Harry Potter and the Philosopher's Stone, he was hired to direct the second film, Harry Potter and the Chamber of Secrets. The production started within a week after the release of the first film. Columbus was set to direct all entries in the series,[35] however he did not want to return for the third film, Harry Potter and the Prisoner of Azkaban, as he claimed he was "burned out".[36] He moved to the position of producer, while Alfonso Cuarn was approached for the role of director. He was initially nervous about directing the instalment as he had not read any of the books or seen the films. After reading the series, he changed his mind and signed on to direct as he had immediately connected to the story.[37]
Because Cuarn decided not to direct the fourth instalment, Harry Potter and the Goblet of Fire, a new director had to be selected.[38] Mike Newell was chosen to direct the film, but declined to direct the next film, Harry Potter and the Order of the Phoenix, which was given to David Yates, who also directed Harry Potter and the Half-Blood Prince and Harry Potter and the Deathly Hallows, becoming the only director to helm more than one film since Chris Columbus.
Chris Columbus said his vision of the first two films was of a "golden storybook, an old-fashioned look", while Alfonso Cuarn changed the tone of the series, desaturated the colour palette, and expanded the landscape around Hogwarts.[33][38] Mike Newell decided to direct the fourth film as a "paranoid thriller", while David Yates wanted to "bring a sense of jeopardy and character to the world".[39][40] Cuarn, Newell and Yates have said that the challenge was striking a balance between making the films according to their individual vision, while working within a cinematic world already established by Columbus.[38][39][40]
David Heyman commented on the "generosity of the directors" by revealing that "Chris spent time with Alfonso, Alfonso spent time with Mike and Mike spent time with David, showing him an early cut of the film, talking through what it means to be a director and how they went about [making the films]."[41]
David Heyman also said, "I suppose Chris Columbus was the most conservative choice from the studio's point of view. But he expressed real passion."[32] Producer Tanya Seghatchian said they were "more adventurous" in choosing a director for the third film and went straight to Alfonso Cuarn.[32] Mike Newell became the first British director of the series when he was chosen for the fourth film; Newell was considered to direct the first film before he dropped out.[32] David Yates directed the final films after David Heyman thought him capable of handling the edgy, emotional, and political material of the later novels.[42]
All the directors have been supportive of each other. Chris Columbus praised the character development in the films, while Alfonso Cuarn admired the "quiet poetry" of David Yates' films.[33][38] Mike Newell noted that each director had a different heroism and David Yates views the first four films "respectfully and enjoy[s] them."[39][40] Daniel Radcliffe said Yates "took the charm of the films that Chris made and the visual flair of everything that Alfonso did and the thoroughly British, bombastic nature of the film directed by Mike Newell" and added "his own sense" of realism.[43]
Steve Kloves wrote the screenplays for all but the fifth film, which was penned by Michael Goldenberg. Kloves had direct assistance of J. K. Rowling, though she allowed him what he described as "tremendous elbow room". Rowling once asked Kloves to keep being faithful to the spirit of the books, thus the plot and tone of each film and its corresponding book are virtually the same, albeit with some changes and omissions for purposes of cinematic style, time and budget constraints. Michael Goldenberg also received input from Rowling during his adaptation of the fifth novel; Goldenberg was originally considered to adapt the first novel before the studio chose Kloves.[44]
David Heyman briefly explained the book-to-film transition. He commented on Rowling's involvement in the series, stating that she understands that "books and films are different" and is "the best support" a producer could have. Rowling had overall approval on the scripts, which were viewed and discussed by the director and the producers. Heyman also said that Kloves was the "key voice" in the process of adapting the novels and that certain aspects from the books needed to have been excluded from the scripts due to the filmmakers' decision to keep the main focus on Harry's journey as a character, which would ultimately give the films a defined structure. Heyman mentioned that some fans "don't necessarily understand the adaptation process" and that the filmmakers would love to "have everything" from the books in the films, but noted that it is not possible as they had "neither time nor cinematic structure" to do so. He finished by saying that adapting a novel to the screen is "a really considered process."[45]
Due to the fact that the films were being made as the novels were being published, the filmmakers had no idea of the story's outcome until the release of the final novel in 2007. Kloves spoke of his relationship with Rowling when adapting the novels by saying, "The thing is about Jo, which is remarkable for someone who had no experience with the filmmaking process, was her intuition. We had a conversation the very first day I met her where she said, 'I know the movies can't be the books... because I know what's coming and it's impossible to fully dramatise on screen what I'm going to write. But I just ask you to be true to the characters, that's all I care about.'"[46] Kloves also said, "I don't know what compelled me to say this [to Rowling] but I said, 'I've just got to warn you my favourite character is not Harry. My favourite character is Hermione.' And I think for some weird reason, from that moment on, she sort of trusted me."[46]
Aside from the three lead actors, other notable cast members include Robbie Coltrane as Rubeus Hagrid, Alan Rickman as Severus Snape, Maggie Smith as Minerva McGonagall, and Tom Felton as Draco Malfoy. Richard Harris, who played the role of Professor Albus Dumbledore, passed away on 25 October 2002 causing the role to be re-cast for the third installment, Harry Potter and the Prisoner of Azkaban. David Heyman and director Alfonso Cuarn chose Michael Gambon to portray the character of Dumbledore, which he did for all succeeding films. Notable recurring cast members include Helena Bonham Carter as Bellatrix Lestrange, Warwick Davis as Filius Flitwick, Ralph Fiennes as Lord Voldemort, Brendan Gleeson as Alastor Moody, Richard Griffiths as Vernon Dursley, Jason Isaacs as Lucius Malfoy, Gary Oldman as Sirius Black, Fiona Shaw as Petunia Dursley, Timothy Spall as Peter Pettigrew, David Thewlis as Remus Lupin, Emma Thompson as Sybill Trelawney, and Julie Walters as Molly Weasley.
The series has seen many returning crew members from various departments, including: Nick Dudman, make-up and creature effects designer; Amanda Knight, make-up designer; Jany Temime, costume designer; Fiona Weir, casting director; Tim Burke, visual effects supervisor; Peter Doyle, digital film colourist; Greg Powell, stunt coordinator; and David Holmes, stunt double.
The production designer for all eight films is Stuart Craig. Assisted by Stephanie McMillan, Craig has created iconic sets pieces including the Ministry of Magic, the Chamber of Secrets, Malfoy Manor and the layout for the CGI Horcrux Cave. Due to the fact that the novels were being published as the films were being made, Craig was required to rebuild some sets for future films and alter the design of Hogwarts. He said, "In the early days, every time you saw the exterior of Hogwarts, it was a physical miniature" which was made by craftsmen and occupied a large sound stage.[49][50] "We ended up with a profile of how Hogwarts looked, a skyline that actually I didnt design, and it wasnt always satisfactory, and as all the novels got written and movies got made there were new requirements [for buildings]. The [Astronomy Tower] definitely wasnt there originally and so we were able to add that substantial piece. And in the last film, we needed an arena for the battle for Hogwarts  the big courtyard outside doubled in size and if you look at the first movie it wasnt there at all. There were quite some liberties taken with the continuity of Hogwarts."[51] In the last film, Craig used a digital model instead of a miniature to "embrace the latest technology".[50]
On the method of creating the sets, Craig said he often started by sketching ideas onto a blank sheet of paper.[52] Stephanie McMillan also said that "each film always had plenty of new challenges", citing the changes in visual style between directors and cinematographers as an example, along with the developing story in the novels. Due to J. K. Rowling's descriptions of various settings in the novels, Craig noted his "responsibility was to place it together".[53]
Craig commented on his experience working in the studio environment, "Im the production designer, but on a big movie like Harry Potter I may be responsible for 30 to 35 people; from the supervising art director, and a team of art directors and assistants, to draughtsmen and junior draughtsmen, and then on to model makers, sculptors and scenic artists." He said, "Ten years go, all the Harry Potter drawings were done in pencil. I would take my roughs and plans and sections and give them to a professional architectural illustrator, who would create concept art using pencil and colour wash on watercolour paper." He said the process changed slightly throughout the years due to, what he called, the "digital revolution" of making films.[50]
When filming of the series was completed, some of Craig's sets had to be rebuilt or transported for them to be displayed at the Warner Bros. studio tour.[49]
There have been six different directors of photography in the franchise: John Seale worked on the first film; Roger Pratt on the second and fourth; Michael Seresin on the third; Sawomir Idziak on the fifth; Bruno Delbonnel on the sixth; and Eduardo Serra on the seventh and eighth. Delbonnel was considered to return for both parts of Deathly Hallows, however he declined and stated that he was "scared of repeating" himself.[54] Delbonnel's cinematography in Half-Blood Prince gained the series its only Academy Award nomination for Best Cinematography. As the series progressed, each cinematographer faced the challenge of shooting and lighting older sets (which had been around since the first few films) in unique and different ways.[55] Chris Columbus said the series' vivid colouring decreased as each film was made, reflecting a more real world.[33][56]
Michael Seresin commented on the change of visual style from the first two films in Prisoner of Azkaban: "The lighting is moodier, with more shadowing and cross-lighting." Seresin and Alfonso Cuarn moved away from the strongly coloured and brightly lit cinematography of the first two films and set the stage for the succeeding five films, with the lighting becoming dimmer and the colour palette more muted with Roger Pratt's work on the fourth film.[57] Sawomir Idziak provided the fifth film with a cold, blue tint which mirrored the bleakness of the story and, after comparing a range of digital cameras with 35mm film, Bruno Delbonnel decided to shoot the sixth movie, Half-Blood Prince, on film rather than the increasingly popular digital format; this decision was kept for the two-part Deathly Hallows with Eduardo Serra, who said that he preferred to work with film because it was "more technically accurate and dependable".[58]
Because the majority of Deathly Hallows takes place in various settings away from Hogwarts, David Yates wanted to "shake things up" by using different photographic techniques such as using hand-held cameras and very wide camera lenses.[59] Eduardo Serra said, "Sometimes we are combining elements shot by the main unit, a second unit, and the visual effects unit. You have to know what is being captured  colours, contrast, et cetera  with mathematical precision." He noted that with Stuart Craig's "amazing sets and the story", the filmmakers could not "stray too far from the look of the previous Harry Potter films."[58][60]
Along with continuous changes in cinematographers, there have been five film editors to work in post-production on the series: Richard Francis-Bruce edited the first instalment; Peter Honess did the second; Steven Weisberg edited the third, Mick Audsley did the fourth; and Mark Day edited films five to eight.
The Harry Potter series has had four composers. John Williams was the first composer to enter the series and is known for creating Hedwig's Theme, which is heard at the start of each film. Williams scored the first three films: Philosopher's Stone, Chamber of Secrets, and Prisoner of Azkaban. However, the second entry was adapted and conducted by William Ross due to Williams' conflicting commitments.
After Williams left the series to pursue other projects, Patrick Doyle scored the fourth entry, Goblet of Fire, which was directed by Mike Newell with whom Doyle had worked with previously. In 2006, Nicholas Hooper started work on the soundtrack to Order of the Phoenix by reuniting with old friend director David Yates. Hooper also composed the soundtrack to Half-Blood Prince but decided not to return for the final films.
In January 2010, Alexandre Desplat was confirmed to compose the score for Harry Potter and the Deathly Hallows  Part 1.[61] The film's orchestration started in the summer with Conrad Pope, the orchestrator on the first three Harry Potter films, collaborating with Desplat. Pope commented that the music "reminds one of the old days."[62] Desplat returned to score Harry Potter and the Deathly Hallows  Part 2 in 2011.[63]
Director David Yates stated that he wanted John Williams to return to the series for the final instalment, but their schedules did not align due to the urgent demand of a rough cut of the film sooner than was possible.[64] The final recording sessions of Harry Potter took place on 27 May 2011 at Abbey Road Studios with the London Symphony Orchestra, orchestrator Conrad Pope and composer Alexandre Desplat.[65]
Doyle, Hooper and Desplat introduced their own personal themes to their respective soundtracks, while keeping a few of John Williams' themes.
There have been many visual effects companies to work on the Harry Potter series. Some of these include Rising Sun Pictures, Double Negative, Cinesite, Framestore and Industrial Light & Magic. The latter three have worked on all the films in the series, while Double Negative and Rising Sun Pictures began their commitments with Prisoner of Azkaban and Goblet of Fire respectively. Framestore contributed by developing many memorable creatures and sequences to the series.[66] Cinesite were involved in producing both miniature and digital effects for the films.[67] Producer David Barron said that "Harry Potter created the UK effects industry as we know it. On the first film, all the complicated visual effects were done on the [US] west coast. But on the second, we took a leap of faith and gave much of what would normally be given to Californian vendors to UK ones. They came up trumps." Tim Burke, the visual effects supervisor, said many studios "are bringing their work to UK effects companies. Every facility is fully booked, and that wasn't the case before Harry Potter. That's really significant."[32]
On 12 June 2010, filming of the Deathly Hallows  Part 1 and Deathly Hallows  Part 2 was completed with actor Warwick Davis stating on his Twitter account, "The end of an Era  today is officially the last day of principal photography on 'Harry Potter'  ever. I feel honoured to be here as the director shouts cut for the very last time. Farewell Harry & Hogwarts, it's been magic!".[68] However, reshoots of the epilogue scene were confirmed to begin in the winter of 2010. The filming was completed on 21 December 2010, marking the official closure of filming the Harry Potter franchise.[69] Exactly four years ago on that day, author J. K. Rowling's official website revealed the title of the final novel in the series  Harry Potter and the Deathly Hallows.[70]
Harry Potter is an orphaned boy brought up by his unfriendly aunt and uncle. At the age of eleven, half-giant Rubeus Hagrid informs him that he is actually a wizard and that his parents were murdered by an evil wizard named Lord Voldemort. Voldemort also attempted to kill one-year old Harry on the same night, but his killing curse mysteriously rebounded and reduced him to a weak and helpless form. Harry became extremely famous in the Wizarding World as a result. Harry begins his first year at Hogwarts School of Witchcraft and Wizardry and learns about magic. During the year, Harry and his friends Ron Weasley and Hermione Granger become entangled in the mystery of the Philosopher's Stone which is being kept within the school.
Harry, Ron, and Hermione return to Hogwarts for their second year, which proves to be more challenging than the last. The Chamber of Secrets has been opened, leaving students and ghosts petrified by an unleashed monster. Harry must face up to claims that he is the heir of Salazar Slytherin (founder of the Chamber), learn that he can speak Parseltongue, and also discover the properties of a mysterious diary only to find himself trapped within the Chamber of Secrets itself.
Harry Potter's third year sees the boy wizard, along with his friends, attending Hogwarts School once again. Professor R. J. Lupin joins the staff as Defence Against the Dark Arts teacher, while convicted murderer Sirius Black escapes from Azkaban Prison. The Ministry of Magic entrusts the Dementors of Azkaban to guard Hogwarts from Black. Harry learns more about his past and his connection with the escaped prisoner.
During Harry's fourth year, the Dark Mark appears in the sky after a Death Eater attack at the Quidditch World Cup, Hogwarts plays host to a legendary event: the Triwizard Tournament, there is a new Defence Against the Dark Arts professor Alastor Moody and frequent nightmares bother Harry all year. Three European schools participate in the tournament, with three 'champions' representing each school in the deadly tasks. The Goblet of Fire chooses Fleur Delacour, Viktor Krum and Cedric Diggory to compete against each other. However, curiously, Harry's name is also produced from the Goblet making him a fourth champion, which results in a terrifying encounter with a reborn Lord Voldemort.
Harry's fifth year begins with him being attacked by Dementors in Little Whinging. Later, he finds out that the Ministry of Magic is in denial of Lord Voldemort's return. Harry is also beset by disturbing and realistic nightmares, while Professor Umbridge, a representative of Minister for Magic Cornelius Fudge, is the new Defence Against the Dark Arts teacher. Harry becomes aware that Voldemort is after a prophecy which reveals: "neither can live while the other survives". Therefore, the rebellion involving the students of Hogwarts, secret organisation Order of the Phoenix, the Ministry of Magic, and the Death Eaters begins.
In Harry's sixth year at Hogwarts, Lord Voldemort and his Death Eaters are increasing their terror upon the Wizarding and Muggle worlds. Headmaster Albus Dumbledore persuades his old friend Horace Slughorn to return to Hogwarts as a professor as there is a vacancy to fill. There is a more important reason, however, for Slughorn's return. While in a Potions lesson, Harry takes possession of a strangely annotated school textbook, inscribed 'This is the property of the Half-Blood Prince'. As romance and hormones lurk within the castle's walls all year, Draco Malfoy struggles to carry out a deed presented to him by Voldemort. Meanwhile, Dumbledore and Harry secretly work together to discover the method on how to destroy the Dark Lord once and for all.
After unexpected events at the end of the previous year, Harry, Ron, and Hermione are entrusted with a quest to find and destroy Lord Voldemort's secret to immortality  the Horcruxes. It is supposed to be their final year at Hogwarts, but the collapse of the Ministry of Magic and Voldemort's rise to power prevents them from attending. The trio undergo a long adventure with many obstacles in their path including Death Eaters, Snatchers, the mysterious Deathly Hallows and Harry's connection with the Dark Lord's mind becoming ever stronger.
After destroying one Horcrux and discovering the significance of the three Deathly Hallows, Harry, Ron and Hermione continue to seek the other Horcruxes in an attempt to destroy Voldemort, who has now obtained the Elder Wand. The Dark Lord discovers Harry's hunt for Horcruxes and launches an attack on Hogwarts School, where the trio return for one last stand against the dark forces that threaten to rid the Wizarding World of non-magical heritage to achieve pure-blood dominance.
The rights for the first four novels in the series were sold to Warner Bros. for 1,000,000 by J. K. Rowling. After the release of the fourth book in July 2000, the first film, Harry Potter and the Philosopher's Stone, was released on 16 November 2001. The film grossed $90million in the United States alone which set a record opening worldwide. The succeeding three motion picture adaptations followed suit in financial success, while garnering positive reviews from fans and critics. The fifth film, Harry Potter and the Order of the Phoenix was released by Warner Bros. on 11 July 2007 in English-speaking countries, except for the UK and Ireland which released the movie on 12 July.[71] The sixth, Harry Potter and the Half-Blood Prince, was released on 15 July 2009 to critical acclaim and finished its theatrical run ranked as the number two grossing film of 2009 on the worldwide charts. The final novel, Harry Potter and the Deathly Hallows, was split into two cinematic parts: Part 1 was released on 19 November 2010 and Part 2, the conclusion to both the final film and the series, was released on 15 July 2011.[72] Part 1 was originally scheduled to be released in 3D and 2D,[73] but due to a delay in the 3D conversion process, Warner Bros. released the film only in 2D and IMAX cinemas. However, Part 2 was released in 2D and 3D cinemas as originally planned.[74]
All the films have been a success financially and critically, making the franchise one of the major Hollywood tent-poles akin to James Bond, Star Wars, Indiana Jones and Pirates of the Caribbean. The series is noted by audiences for growing visually darker and more mature as each film was released.[33][75][76][77][78] However, opinions of the films generally divide book fans, with some preferring the more faithful approach of the first two films, and others preferring the more stylised character-driven approach of the later films. Some also feel the series has a "disjointed" feel due to the changes in directors, as well as Michael Gambon's interpretation of Albus Dumbledore differing from that of Richard Harris'. Author J. K. Rowling has been constantly supportive of the films,[79][80][81] and evaluated Deathly Hallows as her favourite one in the series. She wrote on her website of the changes in the book-to-film transition, "It is simply impossible to incorporate every one of my storylines into a film that has to be kept under four hours long. Obviously films have restrictions  novels do not have constraints of time and budget; I can create dazzling effects relying on nothing but the interaction of my own and my readers' imaginations".[82]
At the 64th British Academy Film Awards in February 2011, J. K. Rowling, David Heyman, David Barron, David Yates, Alfonso Cuarn, Mike Newell, Rupert Grint and Emma Watson collected the Michael Balcon Award for Outstanding British Contribution to Cinema for the series.[107][108] Harry Potter was also recognised by the BAFTA Los Angeles Britannia Awards, as David Yates won the Britannia Award for Artistic Excellence in Directing for his four Harry Potter films.[109][110] Though the series did not win any Academy Awards, six of the eight films were nominated for a total of 12 Oscars.
Some critics, fans and general audiences have expressed disappointment that the series has not gained any Academy Awards for its achievements.[111][112] Despite this, the series has gained success in many other award ceremonies including the annual Saturn Awards, Art Directors Guild Awards and Grammy Awards. The series has also gained a total of 30 nominations at the British Academy Film Awards presented at the annual BAFTAs.
Philosopher's Stone achieved seven BAFTA Award nominations including Best British Film and Best Supporting Actor for Robbie Coltrane.[113] The film was also nominated for eight Saturn Awards and won for its costumes design.[114] It was also nominated at the Art Directors Guild Awards for its production design[115] and received the Broadcast Film Critics Award for Best Live Action Family Film along with gaining two other nominations.[116] Chamber of Secrets won the award for Best Live Action Family Film in the Phoenix Film Critics Society. It was nominated for seven Saturn Awards including Best Director and Best Fantasy Film. The film was nominated for four BAFTA Awards and a Grammy Award for John Williams' score. Prisoner of Azkaban won an Audience Award at the BAFTA Awards, as well as Best Feature Film. The film also won a BMI Film Music award along with being nominated at the Grammy Awards, Visual Effect Society Awards and the Amanda Awards. Goblet of Fire won a BAFTA award for Best Production Design as well as being nominated at the Saturn Awards, Critic's Choice Awards and the Visual Effects Society Awards.
Order of the Phoenix picked up three awards at the inaugural ITV National Movie Awards.[117] At the Empire Awards, David Yates won Best Director.[118] Composer Nicholas Hooper received a nomination for a World Soundtrack Discovery Award.[119] The film was nominated at the BAFTA Awards, but did not win for Best Production Design or Best Special Visual Effects.[120] Half-Blood Prince was nominated for BAFTA Awards in Production Design and Visual Effects,[121] and was in the longlists for several other categories, including Best Supporting Actor for Alan Rickman.[122] Amongst other nominations and wins, the film also achieved Best Family Movie at the National Movie Awards as well as Best Live Action Family Film at the Phoenix Film Critics Society Awards, along with being nominated for Best Motion Picture at the Satellite Awards. Deathly Hallows  Part 1 gained two nominations at the BAFTA Awards for Best Make-Up and Hair and Best Visual Effects, along with receiving nominations for the same categories at the Broadcast Film Critics Association Awards. Eduardo Serra's cinematography and Stuart Craig's production design were also nominated in various award ceremonies and David Yates attained his second win at the Empire Awards, this time for Best Fantasy Film. He also obtained another Best Director nomination at the annual Saturn Awards, which also saw the film gain a Best Fantasy Film nomination.[123][124] Deathly Hallows  Part 2 was released to critical acclaim, gaining a mix of audience awards. Part 2 of Deathly Hallows was also recognised at the Saturn Awards as well as the BAFTA Awards where the film achieved a win for Best Special Visual Effects.[125]
As of 2012[update], the Harry Potter film franchise is the highest grossing film franchise of all time, with the eight films released grossing over $7.7billion worldwide. Without adjusting for inflation, this is higher than the first 22 James Bond films and the six films in the Star Wars franchise.[126] Chris Columbus' Philosopher's Stone became the highest-grossing Harry Potter film worldwide upon completing its theatrical run in 2002, but it was eventually topped by David Yates' Deathly Hallows  Part 2, while Alfonso Cuarn's Prisoner of Azkaban grossed the least.[127][128][129][130]
(55,913,000)
(45,093,000)
(40,184,000)
(45,244,000)
(42,443,000)
(40,261,000)
(37,500,000)
(56,000,000)
1 Origins

1.1 Casting the roles of Harry, Ron and Hermione


1.1 Casting the roles of Harry, Ron and Hermione
2 Production

2.1 Directors
2.2 Scripts
2.3 Cast and crew
2.4 Set design
2.5 Cinematography
2.6 Editing
2.7 Music
2.8 Visual effects
2.9 Final filming


2.1 Directors
2.2 Scripts
2.3 Cast and crew
2.4 Set design
2.5 Cinematography
2.6 Editing
2.7 Music
2.8 Visual effects
2.9 Final filming
3 Plot

3.1 Harry Potter and the Philosopher's Stone (2001)
3.2 Harry Potter and the Chamber of Secrets (2002)
3.3 Harry Potter and the Prisoner of Azkaban (2004)
3.4 Harry Potter and the Goblet of Fire (2005)
3.5 Harry Potter and the Order of the Phoenix (2007)
3.6 Harry Potter and the Half-Blood Prince (2009)
3.7 Harry Potter and the Deathly Hallows  Part 1 (2010)
3.8 Harry Potter and the Deathly Hallows  Part 2 (2011)


3.1 Harry Potter and the Philosopher's Stone (2001)
3.2 Harry Potter and the Chamber of Secrets (2002)
3.3 Harry Potter and the Prisoner of Azkaban (2004)
3.4 Harry Potter and the Goblet of Fire (2005)
3.5 Harry Potter and the Order of the Phoenix (2007)
3.6 Harry Potter and the Half-Blood Prince (2009)
3.7 Harry Potter and the Deathly Hallows  Part 1 (2010)
3.8 Harry Potter and the Deathly Hallows  Part 2 (2011)
4 Release
5 Reaction

5.1 Critical response

5.1.1 Review aggregate results


5.2 Accolades
5.3 Box office

5.3.1 All-time ranks




5.1 Critical response

5.1.1 Review aggregate results


5.1.1 Review aggregate results
5.2 Accolades
5.3 Box office

5.3.1 All-time ranks


5.3.1 All-time ranks
6 Notes
7 References
8 External links
1.1 Casting the roles of Harry, Ron and Hermione
2.1 Directors
2.2 Scripts
2.3 Cast and crew
2.4 Set design
2.5 Cinematography
2.6 Editing
2.7 Music
2.8 Visual effects
2.9 Final filming
3.1 Harry Potter and the Philosopher's Stone (2001)
3.2 Harry Potter and the Chamber of Secrets (2002)
3.3 Harry Potter and the Prisoner of Azkaban (2004)
3.4 Harry Potter and the Goblet of Fire (2005)
3.5 Harry Potter and the Order of the Phoenix (2007)
3.6 Harry Potter and the Half-Blood Prince (2009)
3.7 Harry Potter and the Deathly Hallows  Part 1 (2010)
3.8 Harry Potter and the Deathly Hallows  Part 2 (2011)
5.1 Critical response

5.1.1 Review aggregate results


5.1.1 Review aggregate results
5.2 Accolades
5.3 Box office

5.3.1 All-time ranks


5.3.1 All-time ranks
5.1.1 Review aggregate results
5.3.1 All-time ranks
Book: Harry Potter
Official website
Growing Up with Harry Potter  photo essay by Time
.
#(`*To Kill a Mockingbird*`)#.
To Kill a Mockingbird is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature. The plot and characters are loosely based on the author's observations of her family and neighbors, as well as on an event that occurred near her hometown in 1936, when she was 10years old.
The novel is renowned for its warmth and humor, despite dealing with the serious issues of rape and racial inequality. The narrator's father, Atticus Finch, has served as a moral hero for many readers and as a model of integrity for lawyers. One critic explains the novel's impact by writing, "In the twentieth century, To Kill a Mockingbird is probably the most widely read book dealing with race in America, and its protagonist, Atticus Finch, the most enduring fictional image of racial heroism."[1]
As a Southern Gothic novel and a Bildungsroman, the primary themes of To Kill a Mockingbird involve racial injustice and the destruction of innocence. Scholars have noted that Lee also addresses issues of class, courage, compassion, and gender roles in the American Deep South. The book is widely taught in schools in English-speaking countries with lessons that emphasize tolerance and decry prejudice. Despite its themes, To Kill a Mockingbird has been subject to campaigns for removal from public classrooms, often challenged for its use of racial epithets. Scholars also note the black characters in the novel are not fully explored, and some black readers receive it ambivalently, although it has an often profound effect on many white readers.
Reception to the novel varied widely upon publication. Literary analysis of it is considerably sparse compared to the number of copies sold and its use in education. Author Mary McDonough Murphy, who collected individual impressions of the book by several authors and public figures, calls To Kill a Mockingbird "an astonishing phenomenon".[2] In 2006, British librarians ranked the book ahead of the Bible as one "every adult should read before they die".[3] It was adapted into an Oscar-winning film in 1962 by director Robert Mulligan, with a screenplay by Horton Foote. Since 1990, a play based on the novel has been performed annually in Harper Lee's hometown of Monroeville, Alabama. To date, it is Lee's only published novel, and although she continues to respond to the book's impact, she has refused any personal publicity for herself or the novel since 1964.
Born in 1926, Harper Lee grew up in the Southern town of Monroeville, Alabama, where she became close friends with soon-to-be famous writer Truman Capote. She attended Huntingdon College in Montgomery (194445), and then studied law at the University of Alabama (194549). While attending college, she wrote for campus literary magazines: Huntress at Huntingdon and the humor magazine Rammer Jammer at the University of Alabama. At both colleges, she wrote short stories and other works about racial injustice, a rarely mentioned topic on such campuses at the time.[4] In 1950, Lee moved to New York City, where she worked as a reservation clerk for British Overseas Airways Corporation; there, she began writing a collection of essays and short stories about people in Monroeville. Hoping to be published, Lee presented her writing in 1957 to a literary agent recommended by Capote. An editor at J. B. Lippincott advised her to quit the airline and concentrate on writing. Donations from friends allowed her to write uninterrupted for a year.[5]
Ultimately, Lee spent two and a half years writing To Kill a Mockingbird. A description of the book's creation by the National Endowment for the Arts relates an episode when Lee became so frustrated that she tossed the manuscript out the window into the snow. Her agent made her retrieve it.[6] The book was published on July 11, 1960. It was initially titled Atticus, but Lee renamed it to reflect a story that went beyond a character portrait.[7] The editorial team at Lippincott warned Lee that she would probably sell only several thousand copies.[8] In 1964, Lee recalled her hopes for the book when she said, "I never expected any sort of success with 'Mockingbird.'...I was hoping for a quick and merciful death at the hands of the reviewers but, at the same time, I sort of hoped someone would like it enough to give me encouragement. Public encouragement. I hoped for a little, as I said, but I got rather a whole lot, and in some ways this was just about as frightening as the quick, merciful death I'd expected."[9] Instead of a "quick and merciful death", Reader's Digest Condensed Books chose the book for reprinting in part, which gave it a wide readership immediately.[10] Since the original publication, the book has never been out of print.
The book opens with the Finch family's ancestor, Simon Finch, a Cornish Methodist fleeing religious intolerance in England, settling in Alabama, becoming wealthy and, contrary to his religious beliefs, buying several slaves.
The main story takes place during three years of the Great Depression in the fictional "tired old town" of Maycomb, Alabama. It focuses on six-year-old Scout Finch, who lives with her older brother Jem and their widowed father Atticus, a middle-aged lawyer. Jem and Scout befriend a boy named Dill who visits Maycomb to stay with his aunt each summer. The three children are terrified of, and fascinated by, their neighbor, the reclusive "Boo" Radley. The adults of Maycomb are hesitant to talk about Boo and, for many years, few have seen him. The children feed each other's imagination with rumors about his appearance and reasons for remaining hidden, and they fantasize about how to get him out of his house. Following two summers of friendship with Dill, Scout and Jem find that someone is leaving them small gifts in a tree outside the Radley place. Several times, the mysterious Boo makes gestures of affection to the children, but, to their disappointment, never appears in person.
Atticus is appointed by the court to defend Tom Robinson, a black man who has been accused of raping a young white woman, Mayella Ewell. Although many of Maycomb's citizens disapprove, Atticus agrees to defend Tom. Other children taunt Jem and Scout for Atticus' actions, calling him a "nigger-lover". Scout is tempted to stand up for her father's honor by fighting even though he has told her not to. For his part, Atticus faces a group of men intent on lynching Tom. This danger is averted when Scout, Jem, and Dill shame the mob into dispersing by forcing them to view the situation from Atticus' and Tom's points of view.
Because Atticus does not want them to be present at Tom Robinson's trial, Scout, Jem and Dill watch in secret from the colored balcony. Atticus establishes that the accusersMayella and her father, Bob Ewell, the town drunkare lying. It also becomes clear that the friendless Mayella was making sexual advances towards Tom and her father caught her and beat her. Despite significant evidence of Tom's innocence, the jury convicts him. Jem's faith in justice is badly shaken, as is Atticus', when a hopeless Tom is shot and killed while trying to escape from prison.
Humiliated by the trial Bob Ewell vows revenge. He spits in Atticus' face on the street, tries to break into the presiding judge's house and menaces Tom Robinson's widow. Finally, he attacks the defenseless Jem and Scout as they walk home on a dark night from the school Halloween pageant. Jem's arm is broken in the struggle, but amid the confusion, someone comes to the children's rescue. The mysterious man carries Jem home, where Scout realizes that he is Boo Radley.
Maycomb's sheriff arrives and discovers that Bob Ewell has been killed in the struggle. The sheriff argues with Atticus about the prudence and ethics of holding Jem or Boo responsible. Atticus eventually accepts the sheriff's story that Ewell simply fell on his own knife. Boo asks Scout to walk him home, and after she says goodbye to him at his front door, he disappears again. While standing on the Radley porch, Scout imagines life from Boo's perspective and regrets that they never repaid him for the gifts he had given them.
Lee has said that To Kill a Mockingbird is not an autobiography, but rather an example of how an author "should write about what he knows and write truthfully".[11] Nevertheless, several people and events from Lee's childhood parallel those of the fictional Scout. Lee's father, Amasa Coleman Lee, was an attorney, similar to Atticus Finch, and in 1919, he defended two black men accused of murder. After they were convicted, hanged and mutilated,[12] he never tried another criminal case. Lee's father was also the editor and publisher of the Monroeville newspaper. Although more of a proponent of racial segregation than Atticus, he gradually became more liberal in his later years.[13] Though Scout's mother died when she was a baby, Lee was 25 when her mother, Frances Cunningham Finch, died. Lee's mother was prone to a nervous condition that rendered her mentally and emotionally absent.[14] Lee had a brother named Edwin, who like the fictional Jem was four years older than his sister. As in the novel, a black housekeeper came daily to care for the Lee house and family.
Lee modeled the character of Dill on her childhood friend, Truman Capote, known then as Truman Persons.[15][16] Just as Dill lived next door to Scout during the summer, Capote lived next door to Lee with his aunts while his mother visited New York City.[17] Like Dill, Capote had an impressive imagination and a gift for fascinating stories. Both Lee and Capote were atypical children: both loved to read. Lee was a scrappy tomboy who was quick to fight, but Capote was ridiculed for his advanced vocabulary and lisp. She and Capote made up and acted out stories they wrote on an old Underwood typewriter Lee's father gave them. They became good friends when both felt alienated from their peers; Capote called the two of them "apart people".[18] In 1960, Capote and Lee traveled to Kansas together to investigate the multiple murders that were the basis for Capote's nonfiction novel In Cold Blood.
Down the street from the Lees lived a family whose house was always boarded up; they served as the models for the fictional Radleys. The son of the family got into some legal trouble and the father kept him at home for 24years out of shame. He was hidden until virtually forgotten; he died in 1952.[19]
The origin of Tom Robinson is less clear although many have speculated that his character was inspired by several models. When Lee was 10years old, a white woman near Monroeville accused a black man named Walter Lett of raping her. The story and the trial were covered by her father's newspaper which reported that Lett was convicted and sentenced to death. After a series of letters appeared claiming Lett had been falsely accused, his sentence was commuted to life in prison. He died there of tuberculosis in 1937.[20] Scholars believe that Robinson's difficulties reflect the notorious case of the Scottsboro Boys,[21][22] in which nine black men were convicted of raping two white women on negligible evidence. However, in 2005, Lee stated that she had in mind something less sensational, although the Scottsboro case served "the same purpose" to display Southern prejudices.[23] Emmett Till, a black teenager who was murdered for flirting with a white woman in Mississippi in 1955, and whose death is credited as a catalyst for the Civil Rights Movement, is also considered a model for Tom Robinson.[24]
The narrative is very tough, because [Lee] has to both be a kid on the street and aware of the mad dogs and the spooky houses, and have this beautiful vision of how justice works and all the creaking mechanisms of the courthouse. Part of the beauty is that she... trusts the visual to lead her, and the sensory.
The strongest element of style noted by critics and reviewers is Lee's talent for narration, which in an early review in Time was called "tactile brilliance".[26] Writing a decade later, another scholar noted, "Harper Lee has a remarkable gift of story-telling. Her art is visual, and with cinematographic fluidity and subtlety we see a scene melting into another scene without jolts of transition."[27] Lee combines the narrator's voice of a child observing her surroundings with a grown woman's reflecting on her childhood, using the ambiguity of this voice combined with the narrative technique of flashback to play intricately with perspectives.[28] This narrative method allows Lee to tell a "delightfully deceptive" story that mixes the simplicity of childhood observation with adult situations complicated by hidden motivations and unquestioned tradition.[29] However, at times the blending causes reviewers to question Scout's preternatural vocabulary and depth of understanding.[30] Both Harding LeMay and the novelist and literary critic Granville Hicks expressed doubt that children as sheltered as Scout and Jem could understand the complexities and horrors involved in the trial for Tom Robinson's life.[31][32]
Writing about Lee's style and use of humor in a tragic story, scholar Jacqueline Tavernier-Courbin states: "Laughter... [exposes] the gangrene under the beautiful surface but also by demeaning it; one can hardly... be controlled by what one is able to laugh at."[33] Scout's precocious observations about her neighbors and behavior inspire National Endowment of the Arts director David Kipen to call her "hysterically funny".[34] To address complex issues, however, Tavernier-Courbin notes that Lee uses parody, satire, and irony effectively by using a child's perspective. After Dill promises to marry her, then spends too much time with Jem, Scout reasons the best way to get him to pay attention to her is to beat him up, which she does several times.[35] Scout's first day in school is a satirical treatment of education; her teacher says she must undo the damage Atticus has wrought in teaching her to read and write, and forbids Atticus from teaching her further.[36] Lee treats the most unfunny situations with irony, however, as Jem and Scout try to understand how Maycomb embraces racism and still tries sincerely to remain a decent society. Satire and irony are used to such an extent that Tavernier-Courbin suggests one interpretation for the book's title: Lee is doing the mockingof education, the justice system, and her own society by using them as subjects of her humorous disapproval.[33]
Critics also note the entertaining methods used to drive the plot.[37] When Atticus is out of town, Jem locks a Sunday school classmate in the church basement with the furnace during a game of Shadrach. This prompts their black housekeeper Calpurnia to escort Scout and Jem to her church, which allows the children a glimpse into her personal life, as well as Tom Robinson's.[38] Scout falls asleep during the Halloween pageant and makes a tardy entrance onstage, causing the audience to laugh uproariously. She is so distracted and embarrassed that she prefers to go home in her ham costume, which saves her life.[39]
Scholars have characterized To Kill a Mockingbird as both a Southern Gothic and coming-of-age or Bildungsroman novel. The grotesque and near-supernatural qualities of Boo Radley and his house, and the element of racial injustice involving Tom Robinson contribute to the aura of the Gothic in the novel.[40][41] Lee used the term "Gothic" to describe the architecture of Maycomb's courthouse and in regard to Dill's exaggeratedly morbid performances as Boo Radley.[42] Outsiders are also an important element of Southern Gothic texts and Scout and Jem's questions about the hierarchy in the town cause scholars to compare the novel to Catcher in the Rye and Adventures of Huckleberry Finn.[43] Despite challenging the town's systems, Scout reveres Atticus as an authority above all others, because he believes that following one's conscience is the highest priority, even when the result is social ostracism.[44] However, scholars debate about the Southern Gothic classification, noting that Boo Radley is in fact human, protective, and benevolent. Furthermore, in addressing themes such as alcoholism, incest, rape, and racial violence, Lee wrote about her small town realistically rather than melodramatically. She portrays the problems of individual characters as universal underlying issues in every society.[41]
As children coming of age, Scout and Jem face hard realities and learn from them. Lee seems to examine Jem's sense of loss about how his neighbors have disappointed him more than Scout's. Jem says to their neighbor Miss Maudie the day after the trial, "It's like bein' a caterpillar wrapped in a cocoon... I always thought Maycomb folks were the best folks in the world, least that's what they seemed like".[45] This leads him to struggle with understanding the separations of race and class. Just as the novel is an illustration of the changes Jem faces, it is also an exploration of the realities Scout must face as an atypical girl on the verge of womanhood. As one scholar writes, "To Kill a Mockingbird can be read as a feminist Bildungsroman, for Scout emerges from her childhood experiences with a clear sense of her place in her community and an awareness of her potential power as the woman she will one day be."[46]
In the 33 years since its publication, [To Kill a Mockingbird] has never been the focus of a dissertation, and it has been the subject of only six literary studies, several of them no more than a couple of pages long.
Despite the novel's immense popularity upon publication, it has not received the close critical attention paid to other modern American classics. Don Noble, editor of a book of essays about the novel, estimates that the ratio of sales to analytical essays may be a million to one. Christopher Metress writes that the book is "an icon whose emotive sway remains strangely powerful because it also remains unexamined".[48] Noble suggests it does not receive academic attention because of its consistent status as a best-seller ("If that many people like it, it can't be any good.") and that general readers seem to feel they do not require analytical interpretation.[49]
Harper Lee has remained famously detached from interpreting the novel since the mid-1960s. However, she gave some insight into her themes when, in a rare letter to the editor, she wrote in response to the passionate reaction her book caused: "Surely it is plain to the simplest intelligence that To Kill a Mockingbird spells out in words of seldom more than two syllables a code of honor and conduct, Christian in its ethic, that is the heritage of all Southerners."[50]
When the book was released, reviewers noted that it was divided into two parts, and opinion was mixed about Lee's ability to connect them.[51] The first part of the novel concerns the children's fascination with Boo Radley and their feelings of safety and comfort in the neighborhood. Reviewers were generally charmed by Scout and Jem's observations of their quirky neighbors. One writer was so impressed by Lee's detailed explanations of the people of Maycomb that he categorized the book as Southern romantic regionalism.[52] This sentimentalism can be seen in Lee's representation of the Southern caste system to explain almost every character's behavior in the novel. Scout's Aunt Alexandra attributes Maycomb's inhabitants' faults and advantages to genealogy (families that have gambling streaks and drinking streaks),[53] and the narrator sets the action and characters amid a finely detailed background of the Finch family history and the history of Maycomb. This regionalist theme is further reflected in Mayella Ewell's apparent powerlessness to admit her advances toward Tom Robinson, and Scout's definition of "fine folks" being people with good sense who do the best they can with what they have. The South itself, with its traditions and taboos, seems to drive the plot more than the characters.[52]
The second part of the novel deals with what book reviewer Harding LeMay termed "the spirit-corroding shame of the civilized white Southerner in the treatment of the Negro".[31] In the years following its release, many reviewers considered To Kill a Mockingbird a novel primarily concerned with race relations.[54] Claudia Durst Johnson considers it "reasonable to believe" that the novel was shaped by two events involving racial issues in Alabama: Rosa Parks' refusal to yield her seat on a city bus to a white person, which sparked the 1955 Montgomery Bus Boycott, and the 1956 riots at the University of Alabama after Autherine Lucy and Polly Myers were admitted (Myers eventually withdrew her application and Lucy was expelled, but reinstated in 1980).[55] In writing about the historical context of the novel's construction, two other literary scholars remark: "To Kill a Mockingbird was written and published amidst the most significant and conflict-ridden social change in the South since the Civil War and Reconstruction. Inevitably, despite its mid-1930s setting, the story told from the perspective of the 1950s voices the conflicts, tensions, and fears induced by this transition."[56]
Scholar Patrick Chura, who suggests Emmett Till was a model for Tom Robinson, enumerates the injustices endured by the fictional Tom that Till also faced. Chura notes the icon of the black rapist causing harm to the representation of the "mythologized vulnerable and sacred Southern womanhood".[24] Any transgressions by black males that merely hinted at sexual contact with white females during the time the novel was set often resulted in a punishment of death for the accused. Tom Robinson's trial was juried by poor white farmers who convicted him despite overwhelming evidence of his innocence, as more educated and moderate white townspeople supported the jury's decision. Furthermore, the victim of racial injustice in To Kill a Mockingbird was physically impaired, which made him unable to commit the act he was accused of, but also crippled him in other ways.[24] Roslyn Siegel includes Tom Robinson as an example of the recurring motif among white Southern writers of the black man as "stupid, pathetic, defenseless, and dependent upon the fair dealing of the whites, rather than his own intelligence to save him".[57] Although Tom is spared from being lynched, he is killed with excessive violence during an attempted escape from prison, when he is shot seventeen times.
The theme of racial injustice appears symbolically in the novel as well. For example, Atticus must shoot a rabid dog, even though it is not his job to do so.[58] Carolyn Jones argues that the dog represents prejudice within the town of Maycomb, and Atticus, who waits on a deserted street to shoot the dog,[59] must fight against the town's racism without help from other white citizens. He is also alone when he faces a group intending to lynch Tom Robinson and once more in the courthouse during Tom's trial. Lee even uses dreamlike imagery from the mad dog incident to describe some of the courtroom scenes. Jones writes, "[t]he real mad dog in Maycomb is the racism that denies the humanity of Tom Robinson.... When Atticus makes his summation to the jury, he literally bares himself to the jury's and the town's anger."[59]
One of the amazing things about the writing in To Kill a Mockingbird is the economy with which Harper Lee delineates not only racewhite and black within a small communitybut class. I mean different kinds of black people and white people both, from poor white trash to the upper crustthe whole social fabric.
In a 1964 interview, Lee remarked that her aspiration was "to be... the Jane Austen of South Alabama."[41] Both Austen and Lee challenged the social status quo and valued individual worth over social standing. When Scout embarrasses her poorer classmate, Walter Cunningham, at the Finch home one day, Calpurnia, their black cook, chastises and punishes her for doing so.[61] Atticus respects Calpurnia's judgment, and later in the book even stands up to his sister, the formidable Aunt Alexandra, when she strongly suggests they fire Calpurnia.[62] One writer notes that Scout, "in Austenian fashion", satirizes women with whom she does not wish to identify.[63] Literary critic Jean Blackall lists the priorities shared by the two authors: "affirmation of order in society, obedience, courtesy, and respect for the individual without regard for status".[41]
Scholars argue that Lee's approach to class and race was more complex "than ascribing racial prejudice primarily to 'poor white trash'... Lee demonstrates how issues of gender and class intensify prejudice, silence the voices that might challenge the existing order, and greatly complicate many Americans' conception of the causes of racism and segregation."[56] Lee's use of the middle-class narrative voice is a literary device that allows an intimacy with the reader, regardless of class or cultural background, and fosters a sense of nostalgia. Sharing Scout and Jem's perspective, the reader is allowed to engage in relationships with the conservative antebellum Mrs. Dubose; the lower-class Ewells, and the Cunninghams who are equally poor but behave in vastly different ways; the wealthy but ostracized Mr. Dolphus Raymond; and Calpurnia and other members of the black community. The children internalize Atticus' admonition not to judge someone until they have walked around in that person's skin, gaining a greater understanding of people's motives and behavior.[56]
The novel has been noted for its poignant exploration of different forms of courage.[64][65] Scout's impulsive inclination to fight students who insult Atticus reflects her attempt to stand up for him and defend him. Atticus is the moral center of the novel, however, and he teaches Jem one of the most significant lessons of courage.[66] In a statement that foreshadows Atticus' motivation for defending Tom Robinson and describes Mrs. Dubose, who is determined to break herself of a morphine addiction, Atticus tells Jem that courage is "when you're licked before you begin but you begin anyway and you see it through no matter what".[67]
Charles Shields, who has written the only book-length biography of Harper Lee to date, offers the reason for the novel's enduring popularity and impact is that "its lessons of human dignity and respect for others remain fundamental and universal".[68] Atticus' lesson to Scout that "you never really understand a person until you consider things from his point of view until you climb around in his skin and walk around in it" exemplifies his compassion.[65][69] She ponders the comment when listening to Mayella Ewell's testimony. When Mayella reacts with confusion to Atticus' question if she has any friends, Scout offers that she must be lonelier than Boo Radley. Having walked Boo home after he saves their lives, Scout stands on the Radley porch and considers the events of the previous three years from Boo's perspective. One writer remarks, "...[w]hile the novel concerns tragedy and injustice, heartache and loss, it also carries with it a strong sense [of] courage, compassion, and an awareness of history to be better human beings."[65]
Just as Lee explores Jem's development in coming to grips with a racist and unjust society, Scout realizes what being female means, and several female characters influence her development. Scout's primary identification with her father and older brother allows her to describe the variety and depth of female characters in the novel both as one of them and as an outsider.[46] Scout's primary female models are Calpurnia and her neighbor Miss Maudie, both of whom are strong willed, independent, and protective. Mayella Ewell also has an influence; Scout watches her destroy an innocent man in order to hide her own desire for him. The female characters who comment the most on Scout's lack of willingness to adhere to a more feminine role are also those who promote the most racist and classist points of view.[63] For example, Mrs. Dubose chastises Scout for not wearing a dress and camisole, and indicates she is ruining the family name by not doing so, in addition to insulting Atticus' intentions to defend Tom Robinson. By balancing the masculine influences of Atticus and Jem with the feminine influences of Calpurnia and Miss Maudie, one scholar writes, "Lee gradually demonstrates that Scout is becoming a feminist in the South, for with the use of first-person narration, she indicates that Scout/ Jean Louise still maintains the ambivalence about being a Southern lady she possessed as a child."[63]
Absent mothers and abusive fathers are another theme in the novel. Scout and Jem's mother died before Scout could remember her, Mayella's mother is dead, and Mrs. Radley is silent about Boo's confinement to the house. Apart from Atticus, the fathers described are abusers.[70] Bob Ewell, it is hinted, molested his daughter,[71] and Mr. Radley imprisons his son in his house until Boo is remembered only as a phantom. Bob Ewell and Mr. Radley represent a form of masculinity that Atticus does not, and the novel suggests that such men as well as the traditionally feminine hypocrites at the Missionary Society can lead society astray. Atticus stands apart from other men as a unique model of masculinity; as one scholar explains: "It is the job of real men who embody the traditional masculine qualities of heroic individualism, bravery, and an unshrinking knowledge of and dedication to social justice and morality, to set the society straight."[70]
Allusions to legal issues in To Kill a Mockingbird, particularly in scenes outside of the courtroom, has drawn the attention from legal scholars. Claudia Durst Johnson writes that "a greater volume of critical readings has been amassed by two legal scholars in law journals than by all the literary scholars in literary journals".[72] The opening quote by the 19th-century essayist Charles Lamb reads: "Lawyers, I suppose, were children once." Johnson notes that even in Scout and Jem's childhood world, compromises and treaties are struck with each other by spitting on one's palm and laws are discussed by Atticus and his children: is it right that Bob Ewell hunts and traps out of season? Many social codes are broken by people in symbolic courtrooms: Mr. Dolphus Raymond has been exiled by society for taking a black woman as his common-law wife and having interracial children; Mayella Ewell is beaten by her father in punishment for kissing Tom Robinson; by being turned into a non-person, Boo Radley receives a punishment far greater than any court could have given him.[55] Scout repeatedly breaks codes and laws and reacts to her punishment for them. For example, she refuses to wear frilly clothes, saying that Aunt Alexandra's "fanatical" attempts to place her in them made her feel "a pink cotton penitentiary closing in on [her]".[73] Johnson states, "[t]he novel is a study of how Jem and Scout begin to perceive the complexity of social codes and how the configuration of relationships dictated by or set off by those codes fails or nurtures the inhabitants of (their) small worlds."[55]
Songbirds and their associated symbolism appear throughout the novel. The family's last name of Finch also shares Lee's mother's maiden name. The titular mockingbird is a key motif of this theme, which first appears when Atticus, having given his children air-rifles for Christmas, allows their Uncle Jack to teach them to shoot. Atticus warns them that, although they can "shoot all the bluejays they want", they must remember that "it's a sin to kill a mockingbird".[74] Confused, Scout approaches her neighbor Miss Maudie, who explains that mockingbirds never harm other living creatures. She points out that mockingbirds simply provide pleasure with their songs, saying, "They don't do one thing but sing their hearts out for us."[74] Writer Edwin Bruell summarized the symbolism when he wrote in 1964, "'To kill a mockingbird' is to kill that which is innocent and harmlesslike Tom Robinson."[53] Scholars have noted that Lee often returns to the mockingbird theme when trying to make a moral point.[27][75][76]
Tom Robinson is the chief example among several innocents destroyed carelessly or deliberately throughout the novel. However, scholar Christopher Metress connects the mockingbird to Boo Radley: "Instead of wanting to exploit Boo for her own fun (as she does in the beginning of the novel by putting on gothic plays about his history), Scout comes to see him as a 'mockingbird'that is, as someone with an inner goodness that must be cherished."[77] The last pages of the book illustrate this as Scout relates the moral of a story Atticus has been reading to her, and in allusions to both Boo Radley and Tom Robinson[24] states about a character who was misunderstood, "when they finally saw him, why he hadn't done any of those things... Atticus, he was real nice," to which he responds, "Most people are, Scout, when you finally see them."[78]
The novel exposes the loss of innocence so frequently that reviewer R. A. Dave claims that because every character has to face, or even suffer defeat, the book takes on elements of a classical tragedy.[27] In exploring how each character deals with his or her own personal defeat, Lee builds a framework to judge whether the characters are heroes or fools. She guides the reader in such judgments, alternating between unabashed adoration and biting irony. Scout's experience with the Missionary Society is an ironic juxtaposition of women who mock her, gossip, and "reflect a smug, colonialist attitude toward other races" while giving the "appearance of gentility, piety, and morality".[63] Conversely, when Atticus loses Tom's case, he is last to leave the courtroom, except for his children and the black spectators in the colored balcony, who rise silently as he walks underneath them, to honor his efforts.[79]
Despite her editors' warnings that the book might not sell well, it quickly became a sensation, bringing acclaim to Lee in literary circles, in her hometown of Monroeville, and throughout Alabama.[80] The book went through numerous subsequent printings and became widely available through its inclusion in the Book of the Month Club and editions released by Reader's Digest Condensed Books.[81]
Initial reactions to the novel were varied. The New Yorker declared it "skilled, unpretentious, and totally ingenious",[82] and The Atlantic Monthly's reviewer rated it as "pleasant, undemanding reading", but found the narrative voice"a six-year-old girl with the prose style of a well-educated adult"to be implausible.[30] Time magazine's 1960 review of the book states that it "teaches the reader an astonishing number of useful truths about little girls and about Southern life" and calls Scout Finch "the most appealing child since Carson McCullers' Frankie got left behind at the wedding".[26] The Chicago Sunday Tribune noted the even-handed approach to the narration of the novel's events, writing: "This is in no way a sociological novel. It underlines no cause...To Kill a Mockingbird is a novel of strong contemporary national significance."[83]
Not all comments were enthusiastic, however. Some reviews lamented the use of poor white Southerners, and one-dimensional black victims,[84] and Granville Hicks labeled the book "melodramatic and contrived".[32] When the book was first released, Southern writer Flannery O'Connor commented, "I think for a child's book it does all right. It's interesting that all the folks that are buying it don't know they're reading a child's book. Somebody ought to say what it is."[48] Carson McCullers apparently agreed with the Time magazine review, writing to a cousin: "Well, honey, one thing we know is that she's been poaching on my literary preserves."[85]
One year after being published, To Kill a Mockingbird had been translated into ten languages. In the years since, it has sold over 30million copies and been translated into over 40 languages.[86] To Kill a Mockingbird has never been out of print in hardcover or paperback and has become part of the standard literature curriculum. A 2008 survey of secondary books read by students between grades 912 in the U.S. indicates the novel is the most widely read book in these grades.[87] A 1991 survey by the Book of the Month Club and the Library of Congress Center for the Book found that To Kill a Mockingbird was rated behind only the Bible in books that are "most often cited as making a difference".[88][note 1]
The 50th anniversary of the novel's release was met with celebrations and reflections on its impact.[89] Eric Zorn of the Chicago Tribune praises Lee's "rich use of language" but writes that the central lesson is that "courage isn't always flashy, isn't always enough, but is always in style".[90] Jane Sullivan in the Sydney Morning Herald agrees, stating that the book "still rouses fresh and horrified indignation" as it examines morality, a topic that has recently become unfashionable.[91] Chimamanda Ngozi Adichie writing in The Guardian states that Lee, rare among American novelists, writes with "a fiercely progressive ink, in which there is nothing inevitable about racism and its very foundation is open to question", comparing her to William Faulkner, who wrote about racism as an inevitability.[92] Literary critic Rosemary Goring in Scotland's The Herald notes the connections between Lee and Jane Austen, stating the book's central theme, that "ones moral convictions are worth fighting for, even at the risk of being reviled" is eloquently discussed.[93]
Native Alabamian Allen Barra sharply criticized Lee and the novel in The Wall Street Journal calling Atticus a "repository of cracker-barrel epigrams" and the novel represents a "sugar-coated myth" of Alabama history. Barra writes, "It's time to stop pretending that To Kill a Mockingbird is some kind of timeless classic that ranks with the great works of American literature. Its bloodless liberal humanism is sadly dated..."[94] Thomas Mallon in the New Yorker criticizes Atticus' stiff and self-righteous demeanor, and calls Scout "a kind of highly constructed doll" whose speech and actions are improbable. Although acknowledging that the novel works, Mallon blasts Lee's "wildly unstable" narrative voice for developing a story about a content neighborhood until it begins to impart morals in the courtroom drama, following with his observation that "the book has begun to cherish its own goodness" by the time the case is over.[95][note 2] Defending the book, Akin Ajayi writes that justice "is often complicated, but must always be founded upon the notion of equality and fairness for all." Ajayi states that the book forces readers to question issues about race, class, and society, but that it was not written to resolve them.[96]
Many writers compare their perceptions of To Kill a Mockingbird as adults with when they first read it as children. Mary McDonagh Murphy interviewed celebrities including Oprah Winfrey, Rosanne Cash, Tom Brokaw, and Harper's sister Alice Lee, who read the novel and compiled their impressions of it as children and adults into a book titled Scout, Atticus, and Boo.[97]
I promised myself that when I grew up and I was a man, I would try to do things just as good and noble as what Atticus had done for Tom Robinson.
One of the most significant impacts To Kill a Mockingbird has had is Atticus Finch's model of integrity for the legal profession. As scholar Alice Petry explains, "Atticus has become something of a folk hero in legal circles and is treated almost as if he were an actual person."[99] Morris Dees of the Southern Poverty Law Center cites Atticus Finch as the reason he became a lawyer, and Richard Matsch, the federal judge who presided over the Timothy McVeigh trial, counts Atticus as a major judicial influence.[100] One law professor at the University of Notre Dame stated that the most influential textbook he taught from was To Kill a Mockingbird, and an article in the Michigan Law Review claims, "No real-life lawyer has done more for the self-image or public perception of the legal profession," before questioning whether, "Atticus Finch is a paragon of honor or an especially slick hired gun".[101]
In 1992, an Alabama editorial called for the death of Atticus, saying that as liberal as Atticus was, he still worked within a system of institutionalized racism and sexism and should not be revered. The editorial sparked a flurry of responses from attorneys who entered the profession because of him and esteemed him as a hero.[102] Critics of Atticus maintain he is morally ambiguous and does not use his legal skills to challenge the racist status quo in Maycomb.[48] However, in 1997, the Alabama State Bar erected a monument to Atticus in Monroeville, marking his existence as the "first commemorative milestone in the state's judicial history".[103] In 2008, Lee herself received an honorary special membership to the Alabama State Bar for creating Atticus who "has become the personification of the exemplary lawyer in serving the legal needs of the poor".[104]
To Kill a Mockingbird has been a source of significant controversy since its being the subject of classroom study as early as 1963. The book's racial slurs, profanity, and frank discussion of rape have led people to challenge its appropriateness in libraries and classrooms across the United States. The American Library Association reported that To Kill a Mockingbird was number 21 of the 100 most frequently challenged books of 20002009.[105]
One of the first incidents of the book being challenged was in Hanover, Virginia, in 1966: a parent protested that the use of rape as a plot device was immoral. Johnson cites examples of letters to local newspapers, which ranged from amusement to fury; those letters expressing the most outrage, however, complained about Mayella Ewell's attraction to Tom Robinson over the depictions of rape.[106] Upon learning the school administrators were holding hearings to decide the book's appropriateness for the classroom, Harper Lee sent $10 to The Richmond News Leader suggesting it to be used toward the enrollment of "the Hanover County School Board in any first grade of its choice".[50] The National Education Association in 1968 placed the novel second on a list of books receiving the most complaints from private organizationsafter Little Black Sambo.[107]
With a shift of attitudes about race in the 1970s, To Kill a Mockingbird faced challenges of a different sort: the treatment of racism in Maycomb was not condemned harshly enough. This has led to disparate perceptions that the novel has a generally positive impact on race relations for white readers, but a more ambiguous reception by black readers. In one high-profile case outside the U.S., school districts in the Canadian provinces of New Brunswick and Nova Scotia attempted to have the book removed from standard teaching curricula in the 1990s,[note 3] stating:
The terminology in this novel subjects students to humiliating experiences that rob them of their self-respect and the respect of their peers. The word 'Nigger' is used 48 times [in] the novel...We believe that the English Language Arts curriculum in Nova Scotia must enable all students to feel comfortable with ideas, feelings and experiences presented without fear of humiliation...To Kill a Mockingbird is clearly a book that no longer meets these goals and therefore must no longer be used for classroom instruction.[108]
Furthermore, despite the novel's thematic focus on racial injustice, its black characters are not fully examined.[71] In its use of racial epithets, stereotyped depictions of superstitious blacks, and Calpurnia, who to some critics is an updated version of the "contented slave" motif and to others simply unexplored, the book is viewed as marginalizing black characters.[109][110] One writer asserts that the use of Scout's narration serves as a convenient mechanism for readers to be innocent and detached from the racial conflict. Scout's voice "functions as the not-me which allows the rest of usblack and white, male and femaleto find our relative position in society".[71] A teaching guide for the novel published by The English Journal cautions, "what seems wonderful or powerful to one group of students may seem degrading to another".[111] A Canadian language arts consultant found that the novel resonated well with white students, but that black students found it "demoralizing".[112]
However, the novel is cited as a major reason for the success of civil rights in the 1960s, that it "arrived at the right moment to help the South and the nation grapple with the racial tensions (of) the accelerating civil rights movement".[113] Its publication is so closely associated with the Civil Rights Movement that many studies of the book and biographies of Harper Lee include descriptions of important moments in the movement, despite the fact that she had no direct involvement in any of them.[114][115][116] Civil Rights leader Andrew Young comments that part of the book's effectiveness is that it "inspires hope in the midst of chaos and confusion" and by using racial epithets portrays the reality of the times in which it was set. Young views the novel as "an act of humanity" in showing the possibility of people rising above their prejudices.[117] Alabama author Mark Childress compares it to the impact of Uncle Tom's Cabin, a book that is popularly implicated in starting the U.S. Civil War. Childress states the novel "gives white Southerners a way to understand the racism that they've been brought up with and to find another way. And most white people in the South were good people. Most white people in the South were not throwing bombs and causing havoc... I think the book really helped them come to understand what was wrong with the system in the way that any number of treatises could never do, because it was popular art, because it was told from a child's point of view." [118]
Diane McWhorter, Pultizer Prize-winning historian of the Birmingham civil rights campaign, asserts that To Kill a Mockingbird condemns racism instead of racists, and states that every child in the South has moments of racial cognitive dissonance when they are faced with the harsh reality of inequality. This feeling causes them to question the beliefs with which they have been raised, which for many children is what the novel does. McWhorter writes of Lee, "...for a white person from the South to write a book like this in the late 1950s is really unusualby its very existence an act of protest."[119][note 4] Author James McBride calls Lee brilliant but stops short of calling her brave: "I think by calling Harper Lee brave you kind of absolve yourself of your own racism... She certainly set the standards in terms of how these issues need to be discussed, but in many ways I feel ... the moral bar's been lowered. And that's really distressing. We need a thousand Atticus Finches." McBride, however, defends the book's sentimentality, and the way Lee approaches the story with "honesty and integrity".[120]
Lee's childhood friend, author Truman Capote, wrote on the dust jacket of the first edition, "Someone rare has written this very fine first novel: a writer with the liveliest sense of life, and the warmest, most authentic sense of humor. A touching book; and so funny, so likeable."[121] This comment has been construed to suggest that Capote wrote the book or edited it heavily.[6] The only supporting evidence for this rumor is the 2003 report of a Tuscaloosa newspaper, which quoted Capote's biological father, Archulus Persons, as claiming that Capote had written "almost all" of the book.[122] The rumors were put to rest in 2006 when a Capote letter was donated to Monroeville's literary heritage museum. Writing to a neighbor in Monroeville in 1959, Capote mentioned that Lee was writing a book that was to be published soon. Extensive notes between Lee and her editor at Lippincott also refute the rumor of Capote's authorship.[123] Lee's older sister Alice has responded to the rumor, saying: "That's the biggest lie ever told."[20]
During the years immediately following the novel's publication, Harper Lee enjoyed the attention its popularity garnered her, granting interviews, visiting schools, and attending events honoring the book. In 1961, when To Kill a Mockingbird was in its 41st week on the bestseller list, it was awarded the Pulitzer Prize, stunning Lee.[124] It also won the Brotherhood Award of the National Conference of Christians and Jews in the same year, and the Paperback of the Year award from Bestsellers magazine in 1962.[81][125] Starting in 1964, Lee began to turn down interviews, complaining that the questions were monotonous, and grew concerned that attention she received bordered on the kind of publicity celebrities sought.[126] She has declined ever since to talk with reporters about the book. She has also steadfastly refused to provide an introduction, writing in 1995: "Introductions inhibit pleasure, they kill the joy of anticipation, they frustrate curiosity. The only good thing about Introductions is that in some cases they delay the dose to come. Mockingbird still says what it has to say; it has managed to survive the years without preamble."[127]
In 2001, Lee was inducted into the Alabama Academy of Honor.[128] In the same year, Chicago mayor Richard M. Daley initiated a reading program throughout the city's libraries, and chose his favorite book, To Kill a Mockingbird, as the first title of the One City, One Book program. Lee declared that "there is no greater honor the novel could receive".[129] By 2004, the novel had been chosen by 25communities for variations of the citywide reading program, more than any other novel.[130] David Kipen of the National Endowment of the Arts, who supervised The Big Read, states "...people just seem to connect with it. It dredges up things in their own lives, their interactions across racial lines, legal encounters, and childhood. It's just this skeleton key to so many different parts of people's lives, and they cherish it."[131]
In 2006, Lee was awarded an honorary doctorate from the University of Notre Dame. During the ceremony, the students and audience gave Lee a standing ovation, and the entire graduating class held up copies of To Kill a Mockingbird to honor her.[132][note 5] Lee was awarded the Presidential Medal of Freedom on November 5, 2007 by President George W. Bush. In his remarks, Bush stated, "One reason To Kill a Mockingbird succeeded is the wise and kind heart of the author, which comes through on every page...To Kill a Mockingbird has influenced the character of our country for the better. It's been a gift to the entire world. As a model of good writing and humane sensibility, this book will be read and studied forever."[133]
The book was made into the well-received 1962 film with the same title, starring Gregory Peck as Atticus Finch. The film's producer, Alan J. Pakula, remembered Paramount Studios executives questioning him about a potential script: "They said, 'What story do you plan to tell for the film?' I said, 'Have you read the book?' They said, 'Yes.' I said, 'That's the story.'"[134] The movie was a hit at the box office, quickly grossing more than $20 million from a $2 million budget. It won three Oscars: Best Actor for Gregory Peck, Best Art Direction-Set Decoration, Black-and-White, and Best Writing, Screenplay Based on Material from Another Medium for Horton Foote. It was nominated for five more Oscars including Best Actress in a Supporting Role for Mary Badham, the actress who played Scout.[135]
Harper Lee was pleased with the movie, saying: "In that film the man and the part met... I've had many, many offers to turn it into musicals, into TV or stage plays, but I've always refused. That film was a work of art."[136] Peck met Lee's father, the model for Atticus, before the filming. Lee's father died before the film's release, and Lee was so impressed with Peck's performance that she gave him her father's pocketwatch, which he had with him the evening he was awarded the Oscar for best actor.[137] Years later, he was reluctant to tell Lee that the watch was stolen out of his luggage in London Heathrow Airport. When Peck eventually did tell Lee, he said she responded, "'Well, it's only a watch.' Harpershe feels deeply, but she's not a sentimental person about things."[138] Lee and Peck shared a friendship long after the movie was made. Peck's grandson was named "Harper" in her honor.[139]
In May 2005, Lee made an uncharacteristic appearance at the Los Angeles Public Library at the request of Peck's widow Veronique, who said of Lee: "She's like a national treasure. She's someone who has made a difference...with this book. The book is still as strong as it ever was, and so is the film. All the kids in the United States read this book and see the film in the seventh and eighth grades and write papers and essays. My husband used to get thousands and thousands of letters from teachers who would send them to him."[9]
The book has also been adapted as a play by Christopher Sergel. It debuted in 1990 in Monroeville, a town that labels itself "The Literary Capital of Alabama". The play runs every May on the county courthouse grounds and townspeople make up the cast. White male audience members are chosen at the intermission to make up the jury. During the courtroom scene the production moves into the Monroe County Courthouse and the audience is racially segregated.[140] Author Albert Murray said of the relationship of the town to the novel (and the annual performance): "It becomes part of the town ritual, like the religious underpinning of Mardi Gras. With the whole town crowded around the actual courthouse, it's part of a central, civic educationwhat Monroeville aspires to be."[141]
According to a National Geographic article, the novel is so revered in Monroeville that people quote lines from it like Scripture; yet Harper Lee herself has refused to attend any performances, because "she abhors anything that trades on the book's fame".[142] To underscore this sentiment, Lee demanded that a book of recipes named Calpurnia's Cookbook not be published and sold out of the Monroe County Heritage Museum.[143] David Lister in The Independent states that Lee's refusal to speak to reporters makes them desire to interview her all the more, and her silence "makes Bob Dylan look like a media tart".[144] Despite her discouragement, a rising number of tourists have come to Monroeville, hoping to see Lee's inspiration for the book, or Lee herself. Local residents call them "Mockingbird groupies", and although Lee is not reclusive, she refuses publicity and interviews with an emphatic "Hell, no!"[145]
 Quotations related to To Kill a Mockingbird (novel) at Wikiquote
 
1 Biographical background and publication
2 Plot summary
3 Autobiographical elements
4 Style

4.1 Genres


4.1 Genres
5 Themes

5.1 Southern life and racial injustice
5.2 Class
5.3 Courage and compassion
5.4 Gender roles
5.5 Laws, written and unwritten
5.6 Loss of innocence


5.1 Southern life and racial injustice
5.2 Class
5.3 Courage and compassion
5.4 Gender roles
5.5 Laws, written and unwritten
5.6 Loss of innocence
6 Reception

6.1 Atticus Finch and the legal profession
6.2 Social commentary and challenges
6.3 Inaccurate rumor regarding potential Capote authorship
6.4 Honors


6.1 Atticus Finch and the legal profession
6.2 Social commentary and challenges
6.3 Inaccurate rumor regarding potential Capote authorship
6.4 Honors
7 Adaptations

7.1 1962 film
7.2 Play


7.1 1962 film
7.2 Play
8 See also
9 Notes
10 References
11 Bibliography
12 External links
4.1 Genres
5.1 Southern life and racial injustice
5.2 Class
5.3 Courage and compassion
5.4 Gender roles
5.5 Laws, written and unwritten
5.6 Loss of innocence
6.1 Atticus Finch and the legal profession
6.2 Social commentary and challenges
6.3 Inaccurate rumor regarding potential Capote authorship
6.4 Honors
7.1 1962 film
7.2 Play
Southern literature
Timeline of the African American Civil Rights Movement
To Kill a Mockingbird in popular culture
Johnson, Claudia. To Kill a Mockingbird: Threatening Boundaries. Twayne Publishers: 1994. ISBN 0-8057-8068-8
Johnson, Claudia. Understanding To Kill a Mockingbird: A Student Casebook to Issues, Sources, and Historic Documents. Greenwood Press: 1994. ISBN 0-313-29193-4
Lee, Harper. To Kill a Mockingbird. HarperCollins: 1960 (Perennial Classics edition: 2002). ISBN 0-06-093546-4
Mancini, Candice, (ed.) (2008). Racism in Harper Lee's To Kill a Mockingbird, The Gale Group. ISBN 0-7377-3904-5
Murphy, Mary M. (ed.) Scout, Atticus, and Boo: A Celebration of Fifty Years of To Kill a Mockingbird, HarperCollins Publishers: 2010. ISBN 978-0-06-192407-1
Noble, Don (ed.). Critical Insights: To Kill a Mockingbird by Harper Lee, Salem Press: 2010. ISBN 978-1-58765-618-7
Petry, Alice. "Introduction" in On Harper Lee: Essays and Reflections. University of Tennessee Press: 1994. ISBN 1-57233-578-5
Shields, Charles. Mockingbird: A Portrait of Harper Lee. Henry Holt and Co.: 2006. ISBN 0-8050-7919-X
To Kill a Mockingbird in the Encyclopedia of Alabama
.
#(`*Super Size Me*`)#.
Super Size Me is a 2004 American documentary film directed by and starring Morgan Spurlock, an American independent filmmaker. Spurlock's film follows a 30-day period from February 1 to March 2, 2003 during which he ate only McDonald's food. The film documents this lifestyle's drastic effect on Spurlock's physical and psychological well-being, and explores the fast food industry's corporate influence, including how it encourages poor nutrition for its own profit.
Spurlock dined at McDonald's restaurants three times per day, eating every item on the chain's menu at least once. Spurlock consumed an average of 20.92 megajoules or 5,000 kcal (the equivalent of 9.26 Big Macs) per day during the experiment.
As a result, the then-32-year-old Spurlock gained 24 lbs. (11.1kg), a 13% body mass increase, a cholesterol level of 230, and experienced mood swings, sexual dysfunction, and fat accumulation in his liver. It took Spurlock fourteen months to lose the weight gained from his experiment using a vegan diet supervised by his future wife, a chef who specializes in gourmet vegan dishes.
The reason for Spurlock's investigation was the increasing spread of obesity throughout U.S. society, which the Surgeon General has declared "epidemic," and the corresponding lawsuit brought against McDonald's on behalf of two overweight girls, who, it was alleged, became obese as a result of eating McDonald's food [Pelman v. McDonald's Corp., 237 F. Supp. 2d 512].[3] Spurlock points out that although the lawsuit against McDonald's failed (and subsequently many state legislatures have legislated against product liability actions against producers and distributors of "fast food"), much of the same criticism leveled against the tobacco companies applies to fast food franchises whose product is both physiologically addictive and physically harmful.[4][5]
The documentary was nominated for an Academy Award for Documentary Feature.[6]
A comic book related to the movie has been made with Dark Horse as the publisher. It contains stories about various cases of fast food health scares.[7]
As the film begins, Spurlock is in physically above average shape according to his personal trainer. He is seen by three doctors (a cardiologist, a gastroenterologist, and a general practitioner), as well as a nutritionist and a personal trainer. All of the health professionals predict the "McDiet" will have unwelcome effects on his body, but none expected anything too drastic, one citing the human body as being "extremely adaptable." Prior to the experiment, Spurlock ate a varied diet but always had vegan evening meals to appease his then-girlfriend, Alexandra, a vegan chef. At the beginning of the experiment, Spurlock, who stood 6 feet 2inches (188cm) tall, had a body weight of 185.5 lbs (84 kg).
Spurlock has specific rules governing his eating habits:
On February 1, Spurlock starts the month with breakfast near his home in Manhattan, where there is an average of four McDonald's locations (and 66,950 residents, with twice as many commuters) per square mile (2.6km). He aims to keep the distances he walks in line with the 5,000 steps (approximately two miles) walked per day by the average American.
Day 2 brings Spurlock's first (of nine) Super Size meal, at the McDonald's on 34th Street and Tenth Avenue, which happens to be a meal made of a Double Quarter Pounder with Cheese, Super Size French fries, and a 42 ounce Coke, which takes 22 minutes to eat. He experiences steadily increasing stomach discomfort during the process, and promptly vomits in the McDonald's parking lot.
After five days Spurlock has gained 9.5 pounds (4.5kg) (from 185.5 to about 195 pounds). It is not long before he finds himself experiencing depression, and he claims that his bouts of depression, lethargy, and headaches could be relieved by eating a McDonald's meal. His general practitioner describes him as being "addicted". At his second weigh-in, he had gained another 8 pounds (3.5kg), putting his weight at 203.5lb (92kg). By the end of the month he weighs about 210 pounds (95.5kg), an increase of about 24.5 pounds (about 11kg). Because he could only eat McDonald's food for a month, Spurlock refused to take any medication at all. At one weigh-in Morgan lost 1lb. from the previous weigh-in, and a nutritionist hypothesized that he had lost muscle mass, which weighs more than an identical volume of fat. At another weigh-in, a nutritionist said that he gained 17 pounds (8.5kg) in 12 days.
Spurlock's girlfriend, Alexandra Jamieson, attests to the fact that Spurlock lost much of his energy and sex drive during his experiment. It was not clear at the time whether or not Spurlock would be able to complete the full month of the high-fat, high-carbohydrate diet, and family and friends began to express concern.
On Day 21, Spurlock has heart palpitations. His internist, Dr. Daryl Isaacs, advises him to stop what he is doing immediately to avoid any serious health problems. He compares Spurlock with the protagonist played by Nicolas Cage in the movie Leaving Las Vegas, who intentionally drinks himself to death in a matter of weeks. Despite this warning, Spurlock decides to continue the experiment.
On March 2, Spurlock makes it to day 30 and achieves his goal. In thirty days, he has "Supersized" his meals nine times along the way (five of which were in Texas, four in New York City). His doctors are surprised at the degree of deterioration in Spurlock's health. He notes that he has eaten as many McDonald's meals as most nutritionists say the ordinary person should eat in 8 years (he ate 90 meals, which is close to the number of meals consumed once a month in an 8-year period).
An end text states that it took Spurlock 5 months to lose 20.1 pounds (9kg) and another 9 months to lose the last 4.5 pounds (2kg). His girlfriend Alexandra, now his ex-wife, began supervising his recovery with her "detox diet," which became the basis for her book, The Great American Detox Diet.[9]
The movie ends with a rhetorical question, "Who do you want to see go first, you or them?" This is accompanied by a cartoon tombstone, which reads "Ronald McDonald (19542012)", which originally appeared in The Economist in an article addressing the ethics of marketing to children.[10]
A short epilogue was added to the film. Although it showed that the salads can contain even more calories than burgers, if the customer adds liberal amounts of cheese and dressing prior to consumption, it also described McDonald's discontinuation of the Super Size option six weeks after the movie's premiere, as well as its recent emphasis on healthier menu items such as salads, and the release of the new adult Happy Meal. However, McDonald's claimed that these changes had nothing to do with the film.
Super Size Me first premiered at the 2004 Sundance Film Festival, where Morgan Spurlock won the Grand Jury Prize for directing the film.[11] The film opened in the U.S. on May 7, 2004, and grossed a total of $20,641,054 worldwide, making it the 12th highest-grossing documentary film of all time.[12] It was nominated for an Academy Award for Best Documentary but lost to the film Born into Brothels. Super Size Me received two thumbs up on At the Movies with Ebert and Roeper. The film overall received positive reviews from other critics, as well as movie-goers, and holds a 93% "Certified Fresh" rating on the film review aggregator Rotten Tomatoes.
Caroline Westbrook for BBC News stated that the hype for the documentary was proper "to a certain extent", because of its serious message, and that, overall, the film's "high comedy factor and over-familiarity of the subject matter render it less powerful than other recent documentaries - but it still makes for enjoyable, thought-provoking viewing."[13]
Critics of the film, including McDonald's, argue that the author intentionally consumed an average of 5,000 calories per day and did not exercise, and that the results would have been the same regardless of the source of overeating.[14] One reviewer pointed out "he's telling us something everyone already knows: Fast food is bad for you."[15] Robert Davis of Paste implied the film is an example of "how the ignorance of, or willful distortion of, basic scientific methods is used to manipulate public opinion."[16]
In the comedic documentary reply Fat Head, Tom Naughton "suggests that Spurlock's calorie and fat counts don't add up" and criticizes Spurlock's refusal to publish the Super Size Me food log; The Houston Chronicle reports: "Unlike Spurlock, Naughton has a page on his Web site that lists every item (including nutritional information) he ate during his fast-food month."[17] The film addresses such objections by highlighting that a part of the reason for Spurlock's deteriorating health was not just the high calorie intake but also the high quantity of sugar relative to vitamins and minerals in the McDonald's menu, which is similar in that regard to the nutritional content of the menus of most other U.S. fast-food chains.[citation needed] About 1/3 of Spurlock's calories came from sugar. His nutritionist, Bridget Bennett RD, cited him about his excess intake of sugar from "milkshakes and cokes". It is revealed toward the end of the movie that over the course of the diet, he consumed "over 30 pounds of sugar, and over 12 lbs. of fat from their food."[18]
Soso Whaley, an independent film producer, made a YouTube movie reply titled Me and Mickey D's, in which she also ate all meals at McDonald's, yet lost weight20 pounds over 60 days; 30 pounds in 90 days. Whaley's results were quite different because of the reduced calorie diet, and inclusion of exercise. Some of Whaley's requirements for her meals were the same as Spurlock's (had to eat everything on the menu over the course of the experiment, etc.); but some were different (she didn't have to clean the plateSpurlock required himself to do so). Whaley also documented her meals by saving the receipts. Whaley's film has been criticized by Sourcewatch,[19] as before the project began the teaser asked, "Will Eat at McDonald's for 30 Days and Lose Weight?", although the advertising by Spurlock's film said the same thing, but only reversed.[20]
In the United Kingdom, McDonald's placed a brief ad in the trailers of showings of the film, pointing to the website www.supersizeme-thedebate.co.uk.[21] The ads stated, "See what we disagree with. See what we agree with."
The film was the inspiration for the BBC television series The Supersizers... in which the presenters dine on historical meals and take medical tests to ascertain the impact on their health.[22]
1 Content

1.1 Experiment
1.2 Findings


1.1 Experiment
1.2 Findings
2 Reaction

2.1 Reception
2.2 Criticism and statistical notes


2.1 Reception
2.2 Criticism and statistical notes
3 Impact
4 See also

4.1 Media and publications


4.1 Media and publications
5 References
6 External links
1.1 Experiment
1.2 Findings
2.1 Reception
2.2 Criticism and statistical notes
4.1 Media and publications
He must fully eat three McDonald's meals per day: breakfast, lunch, and dinner.
He must consume every item on the McDonald's menu at least once over the course of the 30 days (he managed this in nine days).
He must only ingest items that are offered on the McDonald's menu, including bottled water. All outside consumption of food is prohibited
He must Super Size the meal when offered, but only when offered (i.e., he is not able to Super Size items himself).
He will attempt to walk about as much as a typical U.S citizen, based on a suggested figure of 5,000 standardized distance steps per day,[8] but he did not closely adhere to this, as he walked more while in New York than Houston.
Criticism of fast food
National Weight Control Registry
New York State Restaurant Association v. New York City Board of Health
Food, Inc.
Fast Food Nation
Million Calorie March: The Movie
The Omnivore's Dilemma
Super Size Me at the Internet Movie Database
Super Size Me at AllRovi
Super Size Me at Rotten Tomatoes
Super Size Me at Box Office Mojo
.
#(`*An Inconvenient Truth*`)#.
An Inconvenient Truth is a 2006 documentary film directed by Davis Guggenheim about former United States Vice President Al Gore's campaign to educate citizens about global warming via a comprehensive slide show that, by his own estimate made in the film, he has given more than a thousand times.
Premiering at the 2006 Sundance Film Festival and opening in New York City and Los Angeles on May 24, 2006, the documentary was a critical and box-office success, winning 2 Academy Awards for Best Documentary Feature and Best Original Song.[4] The film grossed $24 million in the U.S. and $26 million in the foreign box office, becoming the 9th highest grossing documentary film to date in the United States.[5]
The idea to document his efforts came from producer Laurie David who saw his presentation at a town-hall meeting on global warming which coincided with the opening of The Day After Tomorrow. David was so inspired by Gore's slide show that she, with producer Lawrence Bender, met with Guggenheim to adapt the presentation into a film.
Since the film's release, An Inconvenient Truth has been credited for raising international public awareness of climate change and reenergizing the environmental movement. The documentary has also been included in science curricula in schools around the world, which has spurred some controversy.
An Inconvenient Truth focuses on Al Gore and on his travels in support of his efforts to educate the public about the severity of the climate crisis. Gore says, "I've been trying to tell this story for a long time and I feel as if I've failed to get the message across."[6] The film documents a Keynote presentation (which Gore refers to as "the slide show") that Gore has presented throughout the world. It intersperses Gore's exploration of data and predictions regarding climate change and its potential for disaster with his own life story.
The former vice president opens the film by greeting an audience with a joke: "I am Al Gore; I used to be the next President of the United States."[7] Gore then begins his slide show on climate change; a comprehensive presentation replete with detailed graphs, flow charts and stark visuals. Gore shows off several majestic photographs of the Earth taken from multiple space missions, Earthrise and The Blue Marble.[8] Gore notes that these photos dramatically transformed the way we see the Earth, helping spark modern environmentalism.
Following this, Gore shares anecdotes that inspired his interest in the issue, including his college education with early climate expert Roger Revelle at Harvard University, his sister's death from lung cancer and his young son's near-fatal car accident. Gore recalls a story from his grade-school years, where a fellow student asked his geography teacher about continental drift; in response, the teacher called the concept the "most ridiculous thing [he'd] ever heard." Gore ties this conclusion to the assumption that "the Earth is so big, we can't possibly have any lasting, harmful impact on the Earth's environment." For comic effect, Gore uses a clip from the Futurama episode "Crimes of the Hot" to describe the greenhouse effect. Gore refers to his loss to George W. Bush in the 2000 United States presidential election as a "hard blow" yet one which subsequently "brought into clear focus, the mission [he] had been pursuing for all these years."
Throughout the movie, Gore discusses the scientific opinion on climate change, as well as the present and future effects of global warming and stresses that climate change "is really not a political issue, so much as a moral one", describing the consequences he believes global climate change will produce if the amount of human-generated greenhouse gases is not significantly reduced in the very near future. Gore also presents Antarctic ice coring data showing CO2 levels higher now than in the past 650,000 years.
The film includes segments intended to refute critics who say that global warming is unproven or that warming will be insignificant. For example, Gore discusses the possibility of the collapse of a major ice sheet in Greenland or in West Antarctica, either of which could raise global sea levels by approximately 20 feet, flooding coastal areas and producing 100 million refugees. Melt water from Greenland, because of its lower salinity, could then halt the currents that keep northern Europe warm and quickly trigger dramatic local cooling there. It also contains various short animated projections of what could happen to different animals more vulnerable to climate change.
The documentary ends with Gore arguing that if appropriate actions are taken soon, the effects of global warming can be successfully reversed by releasing less CO2 and planting more vegetation to consume existing CO2. Gore calls upon his viewers to learn how they can help him in these efforts. Gore concludes the film by saying:
"Each one of us is a cause of global warming, but each one of us can make choices to change that with the things we buy, the electricity we use, the cars we drive; we can make choices to bring our individual carbon emissions to zero. The solutions are in our hands, we just have to have the determination to make it happen. We have everything that we need to reduce carbon emissions, everything but political will. But in America, the will to act is a renewable resource."[9]
During the film's end credits, a diaporama pops up on screen suggesting to viewers things at home they can do to combat climate change, including "recycle", "speak up in your community", "try to buy a hybrid vehicle" and "encourage everyone you know to watch this movie."[10]
Gore's book of the same title was published concurrently with the theatrical release of the documentary. The book contains additional information, scientific analysis, and Gore's commentary on the issues presented in the documentary.[11][12] A 2007 documentary entitled An Update with Former Vice President Al Gore features Gore discussing additional information that came to light after the film was completed, such as Hurricane Katrina, coral reef depletion, glacial earthquake activity on the Greenland ice sheet, wildfires, and trapped methane gas release associated with permafrost melting.[13]
Gore became interested in global warming when he took a course at Harvard University with Professor Roger Revelle, one of the first scientists to measure carbon dioxide in the atmosphere.[14] Later, when Gore was in Congress, he initiated the first congressional hearing on the subject in 1981.[15] Gore's 1992 book, Earth in the Balance, dealing with a number of environmental topics, reached the New York Times bestseller list.[16]
As Vice President during the Clinton Administration, Gore pushed for the implementation of a carbon tax to encourage energy efficiency and diversify the choices of fuel better reflecting the true environmental costs of energy use; it was partially implemented in 1993.[17] He helped broker the 1997 Kyoto Protocol, an international treaty designed to curb greenhouse gas emissions.[18][19] The treaty was not ratified in the United States after a 95 to 0 vote in the Senate. The primary objections stemmed from the exemptions the treaty gave to China and India, whose industrial base and carbon footprint have grown rapidly, and fears that the exemptions would lead to further trade imbalances and offshoring arrangement with those countries.[20][21]
Gore also supported the funding of the ill-fated satellite called Triana, which would have provided an image of the Earth 24 hours a day, over the internet and would've acted as a barometer measuring the process of global warming.[22] During his 2000 presidential campaign, Gore ran, in part, on a pledge to ratify the Kyoto Protocol.[23]
After his defeat in the 2000 presidential election by George W. Bush, Gore returned his focus to the topic. He edited and adapted a slide show he had compiled years earlier, and began featuring the slide show in presentations on global warming across the U.S. and around the world. At the time of the film, Gore estimated he had shown the presentation more than one thousand times.[24]
Producer David saw Gore's slide show in New York City at a global warming town-hall meeting after the May 27, 2004 premiere of The Day After Tomorrow.[25] Gore was one of several panelists and he showed a ten-minute version of his slide show. [26]
Inspired, David assembled a team, including producer Lawrence Bender and former president of eBay Jeffrey Skoll, who met with Gore about the possibility of making the slide show into a movie. It took some convincing. The slide show, she says, "was his baby, and he felt proprietary about it and it was hard for him to let go." [25]
David said the box office returns weren't important to her. "None of us are going to make a dime." What is at stake, she says, "is, you know, the planet."[25]
David and Bender later met with director Davis Guggenheim, to have him direct the film adaptation of his slide show. Guggenheim, who was skeptical at first, later saw the presentation for himself, stating that he was "blown away," and "left after an hour and a half thinking that global warming [was] the most important issue...I had no idea how youd make a film out of it, but I wanted to try," he said.[27]
In 2004 Gore enlisted Duarte Design to condense and update his material and add video and animation.[28] As designer Ted Boda described the work: "As a designer for the presentation, Keynote was the first choice to help create such an engaging presentation.
Initially reluctant of the film adaptation, Gore said after he and the crew were into the production of the movie, the director, Guggenheim, earned his trust.[29]
When Bender first saw Gore's visual presentation he had concerns about connection with viewers, citing a "need to find a personal way in." In the string of interviews with Gore that followed, Gore himself felt like they "were making Kill Al Vol. 3".[30] Bender had other issues including a time frame that was "grueling" and needed to be done in "a very short period of time" despite many filming locations planned. These included many locations throughout the United States and also included China. "It was a lot of travel in a very short period of time. And they had to get this thing edited and cut starting in January, and ready to screen in May. Thats like a seriously tight schedule. So the logistics of pulling it off with a low budget were really difficult, and if theres one person who gets credit, its Leslie Chilcott, because she really pulled it together."[30]
The majority of the movie exhibits Gore delivering his lecture to an audience at a relatively small theater in Los Angeles. Gore's presentation was delivered on a 70-foot digital screen that Bender commissioned specifically for the movie.[30]
While the bulk of the film was shot on 4:4:4 HDCAM, according to director Guggenheim, a vast array of different film formats were used: "Theres 35mm and 16mm. A lot of the stuff on the farm I just shot myself on 8mm film. We used four Sony F950 HDCAMs for the presentation. We shot three different kinds of prosumer HD, both 30 and 24. Theres MiniDV, theres 3200 black-and-white stills, theres digital stills, some of them emailed on the day they were taken from as far off as Greenland. There was three or four different types of animation. One of the animators is from New Zealand and emailed me his work. Theres JPEG stuff."[31]
Guggenheim says while it would've been a lot easier to use one format, it would not have had the same impact. "Each format has its own feel and texture and touch. For the storytelling of what Gores memory was like of growing up on the farm, some of this 8mm stuff that I shot is very impressionistic. And for some of his memories of his sons accident, these grainy black-and-white stills...have a feel that contrasted very beautifully with the crisp hi-def HD that we shot. Every format was used to its best potential. Some of the footage of Katrina has this blown-out video, where the chroma is just blasted, and it looks real muddy, but that too has its own kind of powerful, impactful feeling."[31]
The film's thesis is that global warming is real, potentially catastrophic, and human-caused. Gore presents specific data that supports the thesis, including:
The Associated Press contacted more than 100 climate researchers and questioned them about the film's veracity. All 19 climate scientists who had seen the movie said that Gore accurately conveyed the science, with few errors.[35]
William H. Schlesinger, dean of the Nicholas School of Environment and Earth Sciences at Duke University said "[Gore] got all the important material and got it right." Robert Corell, chairman of the Arctic Climate Impact Assessment was also impressed. "I sat there and I'm amazed at how thorough and accurate. After the presentation I said, 'Al, I'm absolutely blown away. There's a lot of details you could get wrong.'...I could find no error."[35] Michael Shermer, scientific author and founder of The Skeptics Society, wrote in Scientific American that Gore's slide show "shocked me out of my doubting stance."[36] Eric Steig, a climate scientist writing on RealClimate, lauded the film's science as "remarkably up to date, with reference to some of the very latest research."[37] Ted Scambos, lead scientist from the National Snow and Ice Data Center, said the film "does an excellent job of outlining the science behind global warming and the challenges society faces in the coming century because of it." [38]
One concern among scientists in the film was the connection between hurricanes and global warming, which remains contentious in the science community. Gore cited five recent scientific studies to support his view.[35] "I thought the use of imagery from Hurricane Katrina was inappropriate and unnecessary in this regard, as there are plenty of disturbing impacts associated with global warming for which there is much greater scientific consensus," said Brian Soden, professor of meteorology and oceanography at the University of Miami.[35] Gavin Schmidt, climate modeler for NASA, thought Gore appropriately addressed the issue.[39] "Gore talked about 2005 and 2004 being very strong seasons, and if you weren't paying attention, you could be left with the impression that there was a direct cause and effect, but he was very careful to not say there's a direct correlation," Schmidt said.[39] "There is a difference between saying 'we are confident that they will increase' and 'we are confident that they have increased due to this effect,'" added Steig. "Never in the movie does he say: 'This particular event is caused by global warming.'"[39]
Gore's use of long ice core records of CO2 and temperature (from oxygen isotope measurements) in Antarctic ice cores to illustrate the correlation between the two drew some scrutiny; Schmidt, Steig and Michael E. Mann back up Gore's data. "Gore stated that the greenhouse gas levels and temperature changes over ice age signals had a complex relationship but that they 'fit'. Both of these statements are true," said Schmidt and Mann.[40] "The complexity though is actually quite fascinating...a full understanding of why CO2 changes in precisely the pattern that it does during ice ages is elusive, but among the most plausible explanations is that increased received solar radiation in the southern hemisphere due to changes in Earths orbital geometry warms the southern ocean, releasing CO2 into the atmosphere, which then leads to further warming through an enhanced greenhouse effect. Gores terse explanation of course does not mention such complexities, but the crux of his pointthat the observed long-term relationship between CO2 and temperature in Antarctica supports our understanding of the warming impact of increased CO2 concentrationsis correct. Moreover, our knowledge of why CO2 is changing now (fossil fuel burning) is solid. We also know that CO2 is a greenhouse gas, and that the carbon cycle feedback is positive (increasing temperatures lead to increasing CO2 and CH4), implying that future changes in CO2 will be larger than we might anticipate."[40] "Gore is careful not to state what the temperature/CO2 scaling is," said Steig. "He is making a qualitative point, which is entirely accurate. The fact is that it would be difficult or impossible to explain past changes in temperature during the ice age cycles without CO2 changes. In that sense, the ice core CO2-temperature correlation remains an appropriate demonstration of the influence of CO2 on climate."[37]
Steig disputed Gore's statement that you can visibly see the effect that the United States Clean Air Act has had on ice cores in Antarctica. "One can neither see, nor even detect using sensitive chemical methods any evidence in Antarctica of the Clean Air Act," he said, but did note that they are "clearly recorded in ice core records from Greenland."[41] Despite these flaws, Steig said that the film got the fundamental science right and the minor factual errors did not undermine the main message of the film,[41] adding "An Inconvenient Truth rests on a solid scientific foundation."[41]
Lonnie Thompson, Earth Science professor at Ohio State University, whose work on retreating glaciers was featured in the film, was pleased with how his research was presented. "It's so hard given the breadth of this topic to be factually correct, and make sure you don't lose your audience," Thompson said. "As scientists, we publish our papers in Science and Nature, but very few people read those. Here's another way to get this message out. To me, it's an excellent overview for an introductory class at a university. What are the issues and what are the possible consequences of not doing anything about those changes? To me, it has tremendous value. It will reach people that scientists will never reach."[39]
John Nielsen-Gammon from Texas A&M University said the "main scientific argument presented in the movie is for the most part consistent with the weight of scientific evidence, but with some of the main points needing updating, correction, or qualification."[42] Nielsen-Gammon thought the film neglected information gained from computer models, and instead relied entirely on past and current observational evidence, "perhaps because such information would be difficult for a lay audience to grasp, believe, or connect with emotionally."[42]
Steven Quiring, climatologist from Texas A&M University added that "whether scientists like it or not, An Inconvenient Truth has had a much greater impact on public opinion and public awareness of global climate change than any scientific paper or report."[43]
The film opened in New York City and Los Angeles on May 24, 2006. On Memorial Day weekend, it grossed an average of $91,447 per theater, the highest of any movie that weekend and a record for a documentary, though it was only playing on four screens at the time.[44]
At the 2006 Sundance Film Festival, the movie received three standing ovations.[45] It was also screened at the 2006 Cannes Film Festival[46] and was the opening night film at the 27th Durban International Film Festival on June 14, 2006.[47] An Inconvenient Truth was the most popular documentary at the 2006 Brisbane International Film Festival.[48]
The film has grossed over $24 million in the U.S., making it the ninth-highest-grossing documentary in the U.S. to date, (from 1982 to the present).[49] It grossed nearly $26 million in foreign countries, the highest being France, where it grossed $5 million.[50] According to Gore, "Tipper and I are devoting 100 percent of the profits from the book and the movie to a new bipartisan educational campaign to further spread the message about global warming."[51] Paramount Classics committed 5% of their domestic theatrical gross from the film to form a new bipartisan climate action group, Alliance for Climate Protection, dedicated to awareness and grassroots organizing.[52]
The film received a generally positive reaction from film critics. It garnered a "certified fresh" 93% rating at Rotten Tomatoes (as of May 21, 2007), with a 94% rating from the "Cream of the Crop" reviewers. At Metacritic, which assigns a weighted average score out of 100 to reviews from mainstream critics, the film has received an average score of 75%, based on 32 reviews.[53] Film critics Roger Ebert and Richard Roeper gave the film "two thumbs up". Ebert said, "In 39 years, I have never written these words in a movie review, but here they are: You owe it to yourself to see this film. If you do not, and you have grandchildren, you should explain to them why you decided not to,"[54] calling the film "horrifying, enthralling and [having] the potential, I believe, to actually change public policy and begin a process which could save the Earth."[8]
New York Magazine critic David Edelstein called the film "One of the most realistic documentaries I've ever seenand, dry as it is, one of the most devastating in its implications."[55] The New Yorker's David Remnick added that while it was "not the most entertaining film of the year...it might be the most important" and a "brilliantly lucid, often riveting attempt to warn Americans off our hellbent path to global suicide."[56] New York Times reviewer A.O. Scott thought the film was "edited crisply enough to keep it from feeling like 90 minutes of C-SPAN and shaped to give Mr. Gore's argument a real sense of drama," and "as unsettling as it can be," Scott continued, "it is also intellectually exhilarating, and, like any good piece of pedagogy, whets the appetite for further study."[57] Bright Lights Film Journal critic Jayson Harsin declared the film's aesthetic qualities groundbreaking, as a new genre of slideshow film.[58] NASA climatologist James E. Hansen said that with An Inconvenient Truth, "Al Gore may have done for global warming what Silent Spring did for pesticides. He will be attacked, but the public will have the information needed to distinguish our long-term well-being from short-term special interests."[59]
Several reviews criticized the film on scientific and political grounds. Journalist Ronald Bailey argued in the libertarian magazine Reason that although "Gore gets [the science] more right than wrong," he exaggerates the risks.[60] Global warming skeptics were vocally critical of the film, such as MIT physicist Richard S. Lindzen, who wrote in a June 26, 2006 op-ed in the Wall Street Journal that Gore was using a biased presentation to exploit the fears of the public for his own political gain.[61] Libertarian political activist Christopher Monckton blogged what he claims is a point-by-point refutation of the documentary.[62]
Some reviewers were also skeptical of Gore's intent, wondering whether he was setting himself for another Presidential run. Boston Globe writer Peter Canello criticized the "gauzy biographical material that seems to have been culled from old Gore campaign commercials."[63] Phil Hall of Film Threat gave the film a negative review, saying "An Inconvenient Truth is something you rarely see in movies today: a blatant intellectual fraud."[64] Conservative commentator Glenn Beck aired a one-hour special, Exposed: Climate of Fear, as a counterpoint to Gore's film with viewpoints of scientists opposing the mainstream scientific assessment of global warming.[65]
An Inconvenient Truth has received many different awards worldwide. The film won the 2007 Academy Awards for Best Documentary Feature[66] and Best Original Song for Melissa Etheridge's "I Need to Wake Up".[67] It is the first documentary to win 2 Oscars and the first to win a best original song Oscar.[68][69]
After winning the 2007 Academy Award for Documentary Feature,[70] the Oscar was awarded to director Guggenheim, who asked Gore to join him and other members of the crew on stage. Gore then gave a brief speech, saying:
My fellow Americans, people all over the world, we need to solve the climate crisis. It's not a political issue; it's a moral issue. We have everything we need to get started, with the possible exception of the will to act. That's a renewable resource. Let's renew it.[71]
In "extensive exit polling" of An Inconvenient Truth in "conservative suburban markets like Plano and Irvine (Orange County), as well as Dallas and Long Island", 92 percent rated "Truth" highly and 87 percent of the respondents said they'd recommend the film to a friend.[72]
In addition, the film received numerous other accolades, including a special recognition from the Humanitas Prize, the first time the organization had handed out a Special Award in over 10 years,[73] the 2007 Stanley Kramer Award from The Producers Guild of America, which recognizes "work that dramatically illustrates provocative social issues"[74] and the Presidents Award 2007 from the Society for Technical Communication "for demonstrating that effective and understandable technical communication, when coupled with passion and vision, has the power to educateand changethe world."[75] For Gore's wide-reaching efforts to draw the worlds attention to the dangers of global warming which is centerpieced in the film, Al Gore, along with the Intergovernmental Panel on Climate Change (IPCC), won the 2007 Nobel Peace Prize.[76] The related album, which featured the voices of Beau Bridges, Cynthia Nixon and Blair Underwood, also won Best Spoken Word Album at the 51st Grammy Awards.[77]
The film won many other awards for Best Documentary:[78]
The documentary has been generally well-received politically in many parts of the world and is credited for raising further awareness of global warming internationally.[96] The film inspired producer Kevin Wall to conceive the 2007 Live Earth festival[97] and influenced Italian composer Giorgio Battistelli to write an operatic adaptation, scheduled to premiere at La Scala in Milan in 2013.[98]
Following the film, Gore founded The Climate Project in 2006 which trained 1,000 activists to give Gores presentation in their communities. Presently, the group has 3,500 presenters worldwide.[99]
An additional initiative was launched in 2010, called "Inconvenient Youth". "'Inconvenient Youth' is built on the belief that teens can help lead efforts to solve the climate crisis, said Gore. The project was inspired by Mary Doerr, a 16-year-old who trained as presenter for the organization.[99]
In a July 2007 47-country Internet survey conducted by The Nielsen Company and Oxford University, 66% of those respondents who said they had seen An Inconvenient Truth stated that it had changed their mind about global warming and 89% said it had made them more aware of the problem. Three out of four (74%) said they had changed some of their habits because of seeing the film.[96]
Then-president George W. Bush, when asked whether he would watch the film, responded: "Doubt it." He later stated that "And in my judgment we need to set aside whether or not greenhouse gases have been caused by mankind or because of natural effects, and focus on the technologies that will enable us to live better lives and at the same time protect the environment." Gore responded that "The entire global scientific community has a consensus on the question that human beings are responsible for global warming and he [Bush] has today again expressed personal doubt that that is true." White House deputy press secretary Dana Perino stated that "The president noted in 2001 the increase in temperatures over the past 100 years and that the increase in greenhouse gases was due to a certain extent to human activity".[100]
Several United States Senators screened the film. New Mexico Democratic Senator Jeff Bingaman and Nevada Democratic Senator Harry Reid saw the movie at its Washington premiere at the National Geographic Society.[101][102] New Mexico Democratic Senator Tom Udall planned to see the film saying "It's such a powerful statement because of the way the movie is put together, I tell everybody, Democrat or Republican, they've got to go see this movie."[102] Former New Mexico Republican Senator Pete Domenici thought Gore's prominence on the global warming issue made it more difficult to get a consensus in Congress. Bingaman disputed this saying, "It seems to me we were having great difficulty recruiting Republican members of Congress to support a bill before Al Gore came up with this movie."[102]
Oklahoma Republican Senator Jim Inhofe, then-chairman of the Senate Environment and Public Works Committee, didn't plan to see the film (which he appears in), and compared it to Adolf Hitler's book "Mein Kampf"."If you say the same lie over and over again, and particularly if you have the media's support, people will believe it," Inhofe said, adding that he thought Gore was trying to use the issue to run for president again in 2008.[102]
In contrast to Inhofe, Arizona Republican Senator John McCain, did not criticize Gore's efforts or the movie, which he planned to see.[102]
Tennessee Republican Senator Lamar Alexander, said "Because (Gore) was a former vice president and presidential nominee, he brings a lot of visibility to (the issue)," Alexander said. "On the other hand it may be seen as political by some, and they may be less eager to be a part of it." Alexander also criticized the omission of nuclear power in the film. "Maybe it needs a sequel: 'An Inconvenient Truth 2: Nuclear Power.'"[102]
In September 2006, Gore traveled to Sydney, Australia to promote the film. Then-Australian Prime Minister John Howard said he would not meet with Gore or agree to Kyoto because of the movie: "I don't take policy advice from films." Former Opposition Leader Kim Beazley joined Gore for a viewing and other MPs attended a special screening at Parliament House earlier in the week.[103] After winning the general election a year later, Prime Minister Kevin Rudd ratified Kyoto in his first week of office, leaving the United States the only industrialized nation in the world not to have signed the treaty.[103]
In the United Kingdom, Conservative Prime Minister David Cameron urged people to watch the film in order to understand climate change.[104]
In Belgium, Margaretha Guidone persuaded the entire Belgian government to see the film.[105] 200 politicians and political staff accepted her invitation, among whom were Belgian prime minister Guy Verhofstadt and Minister-President of Flanders, Yves Leterme.[106] Gore received the Prince of Asturias Prize in 2007 for international cooperation.[107][108]
In Costa Rica, the film was screened by president Oscar Arias.[109] Arias's subsequent championing of the climate change issue was greatly influenced by the film.[110]
The Competitive Enterprise Institute released pro-carbon dioxide television ads in preparation for the film's release in May 2006. The ads featured a little girl blowing a dandelion with the tagline, "Carbon dioxide. They call it pollution. We call it life."[111]
In August 2006, the Wall Street Journal[112][113] revealed that a YouTube video lampooning Gore and the movie, titled Al Gore's Penguin Army, appeared to be "astroturfing" by DCI Group, a Washington public relations firm.
Several colleges and high schools have begun to use the film in science curricula. [114] In Germany, German Environment Minister Sigmar Gabriel bought 6,000 DVDs of An Inconvenient Truth to make it available to German schools.[115] In Spain, after a meeting with Gore, prime minister Jos Luis Rodrguez Zapatero said the government will make An Inconvenient Truth available to schools. In Burlington, Ontario, Canada, the Halton District School Board made An Inconvenient Truth available at schools and as an educational resource.[116]
As part of a nationwide "Sustainable Schools Year of Action" launched in late 2006, the UK Government, Welsh Assembly Government and Scottish Executive announced between JanuaryMarch 2007 that copies of An Inconvenient Truth would be sent to all secondary schools in England, Wales and Scotland. The film was placed into the science curriculum for fourth and sixth-year students in Scotland as a joint initiative between Learning and Teaching Scotland and ScottishPower.[117]
In May 2007, a group of global warming skeptics led by Stewart Dimmock, a lorry (HGV) driver and school governor from Kent, England, challenged the UK Government's distribution of the film in a lawsuit, Dimmock v Secretary of State for Education and Skills with help from political ally Viscount Monckton.[118][119] The plaintiffs sought an injunction preventing the screening of the film in English schools, arguing that schools are legally forbidden to promote partisan political views and, when dealing with political issues, are required to provide a balanced presentation of opposing views.
On 10 October 2007, Mr Justice Burton, after explaining that the requirement for a balanced presentation does not warrant that equal weight be given to alternative views of a mainstream view, ruled that it was clear that the film was substantially founded upon scientific research and fact, albeit that the science is used, in the hands of a "talented politician and communicator", to make a political statement and to support a political program.[120] The judge ruled that An Inconvenient Truth contained nine scientific errors and thus must be accompanied by an explanation of those errors before being shown to school children. The judge said that showing the film without the explanations of error would be a violation of education laws.[121]
The judge concluded "I have no doubt that Dr Stott, the Defendant's expert, is right when he says that: 'Al Gore's presentation of the causes and likely effects of climate change in the film was broadly accurate.'" On the basis of testimony from Robert M. Carter and the arguments put forth by the claimant's lawyers, the judge also pointed to nine "errors", i.e. statements the truth of which he did not rule on, but that he found to depart from the mainstream scientific positions on global warming.[122][123][124] He also found that some of these departures from the mainstream arose in the context of alarmism and exaggeration in support of political theses.[125][126] Since the government had already accepted to amend the guidance notes to address these along with other points in a fashion that the judge found satisfactory, no order was made on the application.
Each side declared victory. Government Minister of Children, Young People and Families, Kevin Brennan stated: "We have updated the accompanying guidance, as requested by the judge to make it clearer for teachers as to the stated Intergovernmental Panel on Climate Change position on a number of scientific points raised in the film."[127] Plaintiff Dimmock complained that "no amount of turgid guidance" could change his view that the film was unsuitable for the classroom.[128] A spokesman for Gore said: "Of the thousands of facts in the film, the judge only took issue with just a handful. And of that handful, we have the studies to back those pieces up."[129]
In the United States, 50,000 free copies of An Inconvenient Truth were offered to the National Science Teachers Association (NSTA), which declined to take them. Producer David provided an email correspondence from the NSTA detailing that their reasoning was that the DVDs would place "unnecessary risk upon the [NSTA] capital campaign, especially certain targeted supporters," and that they saw "little, if any, benefit to NSTA or its members" in accepting the free DVDs.[130] A Washington Post editorial called the decision "Science a la Joe Camel", citing for example that the NSTA had received $6 million since 1996 from Exxon Mobil, which had a representative on the organization's corporate board.[131] In public, the NSTA argued that distributing this film to its members would have been contrary to a long-standing NSTA policy against distributing unsolicited materials to its members. The NSTA also said that they had offered several other options for distributing the film but ultimately "[it] appears that these alternative distribution mechanisms were unsatisfactory."[132] David has said that NSTA Executive Director Gerry Wheeler promised in a telephone conversation to explore alternatives with NSTA's board for advertising the film but she had not yet received an alternative offer at the time of NSTA's public claim. She also said that she rejected their subsequent offers because they were nothing more than offers to sell their "commercially available member mailing list" and advertising space in their magazine and newsletter, which are available to anyone. She noted that in the past, NSTA had shipped out 20,000 copies of a 10-part video produced by Wheeler with funding provided by ConocoPhillips in 2003. NSTA indicated that they retained editorial control over the content, which David questioned based on the point of view portrayed in the global warming section of the video.[133]
The American Association for the Advancement of Science publication ScienceNOW published an assessment discussing both sides of the NSTA decision in which it was reported that "David says NSTA's imprimatur [i.e. endorsement or sanction] was essential and that buying a mailing list is a nonstarter. 'You don't want to send out a cold letter, and it costs a lot of money,' she says. 'There are a thousand reasons why that wouldn't work.'"[134]
In January 2007, the Federal Way (Washington State) School Board voted to require an approval by the principal and the superintendent for teachers to show the film to students and that the teachers must include the presentation of an approved "opposing view".[135] The moratorium was repealed, at a meeting on January 23, after a predominantly negative community reaction.[136] Shortly thereafter, the school board in Yakima, Washington, calling the film a "controversial issue", prevented the Environmental Club of Eisenhower High School from showing it, pending review by the school board, teachers, principal, and parents.[137] It lifted the stay a month later, upon the approval by a review panel.[138]
Former ACT New Zealand Member of Parliament Muriel Newman filed a petition to have New Zealand schoolchildren be protected from political indoctrination by putting provisions that resembled those in the UK to the Education Act. The petition was in response to concerned parents talked with Newman after An Inconvenient Truth was shown in schools in 2007. The parents were worried that teachers were not pointing out supposed inaccuracies in the film and were not explaining differing viewpoints.[139]
An Inconvenient Truth was scored by Michael Brook with an accompanying theme song played during the end credits by Melissa Etheridge. Brook explained that he wanted to bring out the emotion expressed in the film: "...in Inconvenient Truth, there's a lot of information and it's kind of a lecture, in a way, and very well organized and very well presented, but it's a lot to absorb. And the director, Guggenheim, wanted to have  sort of give people a little break every once in a while and say, okay, you don't have to absorb this information, you can just sort of  and it was more the personal side of Al Gore's life or how it connected to the theme of the film. And that's when there's music."[141]
Etheridge agreed to write An Inconvenient Truth's theme song, "I Need to Wake Up" after viewing Gore's slide show.[142] "I was so honored he would ask me to contribute to a project that is so powerful and so important, I felt such a huge responsibility," she said. "Then I went, 'What am I going to write? What am I going to say?' " Etheridge's former partner, Tammy Lynn Michaels, told her: "Write what you feel, because that's what people are going to feel."[142] Of Etheridge's commitment to the project, Gore said, "Melissa is a rare soul who gives a lot of time and effort to causes in which she strongly believes."[142] Etheridge received the 2006 Academy Award for Best Original Song for "I Need to Wake Up." Upon receiving the award, she noted in her acceptance speech:
Mostly I have to thank Al Gore, for inspiring us, for inspiring me, showing that caring about the Earth is not Republican or Democrat; it's not red or blue, it's all green.[71]



May 24, 2006(2006-05-24)
1 Synopsis 
2 Background

2.1 Origins
2.2 The slide show
2.3 Production

2.3.1 Technical aspects




2.1 Origins
2.2 The slide show
2.3 Production

2.3.1 Technical aspects


2.3.1 Technical aspects
3 Scientific basis
4 Reception

4.1 Box office
4.2 Reviews


4.1 Box office
4.2 Reviews
5 Criticism
6 Accolades 
7 Impact

7.1 Activism
7.2 Public opinion
7.3 Governmental reactions
7.4 Industry and business


7.1 Activism
7.2 Public opinion
7.3 Governmental reactions
7.4 Industry and business
8 Use in education

8.1 In the United Kingdom

8.1.1 Dimmock case


8.2 In the United States
8.3 In New Zealand


8.1 In the United Kingdom

8.1.1 Dimmock case


8.1.1 Dimmock case
8.2 In the United States
8.3 In New Zealand
9 Music
10 See also
11 References
12 External links
2.1 Origins
2.2 The slide show
2.3 Production

2.3.1 Technical aspects


2.3.1 Technical aspects
2.3.1 Technical aspects
4.1 Box office
4.2 Reviews
7.1 Activism
7.2 Public opinion
7.3 Governmental reactions
7.4 Industry and business
8.1 In the United Kingdom

8.1.1 Dimmock case


8.1.1 Dimmock case
8.2 In the United States
8.3 In New Zealand
8.1.1 Dimmock case
The Keeling curve, measuring CO2 from the Mauna Loa Observatory.
The retreat of numerous glaciers is shown in before-and-after photographs.
A study by researchers at the Physics Institute at the University of Bern and the European Project for Ice Coring in Antarctica presenting data from Antarctic ice cores showing carbon dioxide concentrations higher than at any time during the past 650,000 years.[32]
Temperature record since 1880 showing that the ten hottest years ever measured in this atmospheric record had all occurred in the previous fourteen years.
A 2004 survey, by Naomi Oreskes of 928 peer-reviewed scientific articles on global climate change published between 1993 and 2003. The survey, published as an editorial in the journal Science, found that every article either supported the human-caused global warming consensus or did not comment on it.[33] Gore also presented a 2004 study by Max and Jules Boykoff showing 53% of articles that appeared in major US newspapers over a fourteen year period gave roughly equal attention to scientists who expressed views that global warming was caused by humans as they did to global warming skeptics, creating a false balance.[34]
Chicago Film Critics Association 2006[79]
Dallas-Fort Worth Film Critics Association 2006[80][81]
Florida Film Critics 2006[82]
Kansas City Film Critics Awards 2006[83]
Las Vegas Film Critics Society 2006[84]
National Board of Review 2006[85]
National Society of Film Critics 2006[86]
New York Film Critics Online 2006[87]
Ohio Film Critics Awards 2006[88]
Oklahoma Film Critics Circle Awards 2006[89]
Online Film Critics Society 2006[90]
Phoenix Film Critics Circle 2006[91]
Satellite Awards 2006 (Nominated)[92]
St. Louis Film Critics Awards 2006[93]
Utah Film Critics Awards 2006[94]
Washington D.C. Film Critics Association 2006[95]
An Inconvenient Truth (opera)
An Inconvenient Truth 2 (TV series)
An Inconvenient Truth...Or Convenient Fiction? (2007 film)
An Inconvenient Truth
An Inconvenient Truth at AllRovi
An Inconvenient Truth at the Internet Movie Database
An Inconvenient Truth at Rotten Tomatoes
Interview: Davis Guggenheim and An Inconvenient Truth by Alex Steffen, 4 May 2006
New York Times, An Inconvenient Truth: Full credits, 2006
.
#(`*The Terminator*`)#.
The Terminator is a 1984 American science fiction action film directed by James Cameron, co-written by Cameron, Gale Anne Hurd and William Wisher Jr. and starring Arnold Schwarzenegger, Michael Biehn, and Linda Hamilton. The film was produced by Hemdale Film Corporation and distributed by Orion Pictures, and filmed in Los Angeles. Schwarzenegger plays the Terminator, a cyborg assassin sent back in time from the year 2029 to 1984 to kill Sarah Connor, played by Hamilton. Biehn plays Kyle Reese, a soldier from the future sent back in time to protect Sarah.
Though not expected to be either a commercial or critical success, The Terminator topped the American box office for two weeks and helped launch the film career of Cameron and solidify that of Schwarzenegger. Three sequels have been produced: Terminator 2: Judgment Day (1991), Terminator 3: Rise of the Machines (2003), and Terminator Salvation (2009), as well as a television series, Terminator: The Sarah Connor Chronicles (20082009). In 2008, The Terminator was selected by the Library of Congress for preservation in the United States National Film Registry, being deemed "culturally, historically, or aesthetically significant."
In a post-apocalyptic 2029, artificially intelligent machines seek to exterminate what is left of the human race. Two beings from this era travel back in time to 1984 Los Angeles: One is a Terminator (Arnold Schwarzenegger), a cyborg assassin programmed to kill Sarah Connor (Linda Hamilton); the other is Kyle Reese (Michael Biehn), a human resistance fighter sent to protect her. After killing two other Sarah Connors listed in the telephone directory, the Terminator tracks its target to a nightclub. Kyle saves Sarah from the Terminator's attack and the two make an escape.
Kyle explains that in the near future an artificial intelligence network called Skynet will become self-aware and initiate a nuclear holocaust of mankind. Sarah's yet-unborn son John will rally the survivors and lead a resistance movement against Skynet and its army of machines. With the Resistance on the verge of victory, Skynet has sent a Terminator back in time to kill Sarah before John can be born, as a last-ditch effort to avert the formation of the Resistance. The Terminator is an emotionless and efficient killing machine with a powerful metal endoskeleton, but with an external layer of living tissue that makes it resemble a human being.
Kyle and Sarah are again attacked by the Terminator, leading to a car chase and their arrest. Lieutenant Ed Traxler (Paul Winfield) and Detective Hal Vukovich (Lance Henriksen) tell Sarah that Kyle is insane. Kyle is questioned by psychologist Dr. Silberman (Earl Boen), who concludes that he is paranoid and delusional. The Terminator attacks the police station and kills many police officers, including Traxler and Vukovich, in its attempt to locate Sarah, but she and Kyle escape and seek refuge in a motel. Kyle confesses that he has long been in love with Sarah, having been given a photograph of her by her son John. Sarah reciprocates Kyle's feelings and they have sex.
The Terminator tracks them to the motel and wounds Kyle. In the ensuing chase the Terminator is caught in the blast of an exploding gasoline tank truck. With its flesh coating burned away, it pursues Sarah and Kyle into a factory. Kyle jams a pipe bomb into its abdomen, causing an explosion which severely damages it, but at the cost of his own life. Still partially functional, the Terminator tries to kill Sarah. She leads it into a hydraulic press and crushes it, causing it to deactivate.
Later, a pregnant Sarah is traveling through Mexico. Along the way she records audio tapes which she intends to pass on to her unborn son, John. She debates whether to tell him that Kyle is his father. A Mexican boy takes a photograph of her which she purchases  it is the photograph that John will later give to Kyle. She drives on towards approaching storm clouds.
Additional actors included Bess Motta as Ginger Ventura; Rick Rossovich as Matt Buchanan; Dick Miller as the gun shop clerk; Shawn Schepps as Nancy; Bruce M. Kerner as the desk sergeant; Schwarzenegger's friend and workout partner, professional bodybuilder Franco Columbu as a Terminator in 2029; and Bill Paxton, Brad Rearden, and Brian Thompson as punks who are confronted by the Terminator; Marianne Muellerleile as one of the people with the name "Sarah Connor" who get shot by the Terminator.
In Rome, during the release of Piranha II: The Spawning director James Cameron grew ill and had a dream about a metallic torso dragging itself from an explosion while holding kitchen knives.[3] When Cameron returned to Pomona, California, he stayed at Randall Frakes' home where he wrote a draft for The Terminator.[4] Cameron later stated that his influences while writing the script were 1950s science fiction films and episodes of The Outer Limits as well as contemporary films including The Driver and The Road Warrior.[5][6] To translate the draft into a script, Cameron enlisted his friend Bill Wisher, who had a similar approach to storytelling. Cameron gave Wisher the early scenes involving Sarah Connor and the police department scenes to write. As Wisher lived far away from Cameron, the two communicated script ideas by recording tapes of what they wrote by telephone. Cameron's agent hated the idea for The Terminator and told him to work on something else. After this, Cameron fired his agent.[7] The initial outline of the script involved two Terminators sent to the past. The first was similar to the Terminator in the film, while the second was a liquid metal cyborg that could not be destroyed with conventional weaponry.[8] Cameron could not think of a good way to depict this robot, stating that he "was seeing things in his head that couldn't be done with existing technology."[8][9] The story of the cyborgs in the film was cut down to a single robot idea.[9] The liquid metal Terminator would be revisited with the T-1000 character in the 1991 sequel Terminator 2: Judgment Day.[10]
Gale Anne Hurd, who had worked at New World Pictures as Roger Corman's assistant, showed interest in the film project.[7] Cameron sold the rights for The Terminator to Hurd for one dollar with the promise that she would produce it only if Cameron was to direct it. As a producer, Hurd had suggested edits to the script and took a screen writing credit in the film. Cameron has stated that Hurd "did no actual writing at all".[11] Cameron and Hurd had friends who worked with Roger Corman previously and who were now working at Orion Pictures. Orion agreed to distribute the film if Cameron could get financial backing elsewhere. The script was picked up by John Daly at Hemdale Pictures. Cameron wanted his pitch for Daly to finalize the deal and had his friend Lance Henriksen show up to the meeting early dressed and acting like the Terminator. Henriksen showed up at the office kicking open the door wearing a leather jacket and had gold foil smothered on his teeth and fake cuts on his face and then sat in a chair. Cameron arrived shortly after which relieved the staff from Henriksen's act. Daly was impressed by the screenplay and Cameron's sketches and passion for the film.[12] In late 1982 Daly agreed to back the film with help from HBO and Orion.[12][13] The Terminator was originally budgeted at $4 million and later raised to $6.5 million.[14]
"Casting Arnold Schwarzenegger as our Terminator, on the other hand, shouldn't have worked. The guy is supposed to be an infiltration unit, and there's no way you wouldn't spot a Terminator in a crowd instantly if they all looked like Arnold. It made no sense whatsoever. But the beauty of movies is that they don't have to be logical. They just have to have plausibility. If there's a visceral, cinematic thing happening that the audience likes, they don't care if it goes against what's likely."[15]
One of Cameron's first tasks was to find someone to play Kyle Reese. Orion wanted a star whose popularity was rising in the United States but who also would have foreign appeal. Orion's co-founder Mike Medavoy had met Arnold Schwarzenegger and sent his agent the script for The Terminator.[13] Cameron was dubious about casting Schwarzenegger as Reese as he felt he would need someone even bigger to play the Terminator. The studio had suggested O. J. Simpson for the role of the Terminator, but Cameron did not feel that Simpson would be believable as a killer.[16][17] Cameron still agreed to meet with Schwarzenegger about the film and devised a plan to avoid casting him. Cameron planned to pick a fight with him and return to Hemdale and find him unfit for the role. Upon meeting with Schwarzenegger, Cameron was entertained by Schwarzenegger who would talk about how the villain should be played. Cameron began sketching his face on a notepad and asked Schwarzenegger to stop talking and remain still.[17] After the meeting, Cameron returned to Daly saying Schwarzenegger would not play Reese but that "he'd make a hell of a Terminator".[18] Schwarzenegger was not as excited by the film; during an interview on the set of Conan the Barbarian, an interviewer asked him about a pair of shoes he had (which were for The Terminator). Schwarzenegger responded, "Oh some shit movie I'm doing, take a couple weeks."[19] In preparation for the role, Schwarzenegger spent three months training with weapons to be able to use them and feel comfortable around them.[18]
For the role of Reese, various other suggestions were made for the role including rock musician Sting.[20] Cameron chose Michael Biehn for the role. Biehn was originally skeptical about the part, feeling that the film was silly. After meeting with Cameron, Biehn stated his "feelings about the project changed".[20] Hurd stated that "almost everyone else who came in from the audition was so tough that you just never believed that there was gonna be this human connection between [Sarah Connor and Kyle Reese]. They have very little time to fall in love. A lot of people came in and just could not pull it off."[21] In the first few pages of the script, the character of Sarah Connor is written as "19, small and delicate features. Pretty in a flawed, accessible way. She doesn't stop the party when she walks in, but you'd like to get to know her. Her vulnerable quality masks a strength even she doesn't know exists."[22] For the role, Cameron chose Linda Hamilton, who had just finished filming Children of the Corn.[23] Rosanna Arquette had previously auditioned.[24] Cameron found a role for Lance Henriksen as Detective Hal Vukovich, as Henriksen had been essential to finding finances for the film.[25] For the special effects shots in the film, Cameron wanted Dick Smith who had previously worked on The Godfather and Taxi Driver. Smith did not take Cameron's offer and suggested his friend Stan Winston for the job.[26]
Filming for The Terminator was set to begin in early 1983 in Toronto. Production was halted when producer Dino De Laurentiis applied an option in Schwarzenegger's contract that would make him unattainable for nine months while he was filming Conan the Destroyer. During the waiting period, Cameron was contracted to write the script for Rambo: First Blood Part II. He also used this time to refine parts of The Terminator's script and meet with producers David Giler and Walter Hill to discuss a sequel to Alien.[25][27]
There was limited interference from Orion Pictures. Two suggestions Orion put forward included the addition of a canine cyborg for Reese which Cameron turned down and the second was to strengthen the love interest between Sarah and Reese which Cameron accepted.[28] On creating the Terminator's look, Winston and Cameron passed their sketches back and forth. They eventually decided on a design that was nearly identical to the original one Cameron drew in Rome.[26][29] Winston had a team of seven artists work for six months to create a puppet of the Terminator. It was first molded in clay, then plaster reinforced with steel ribbing. These pieces were then sanded, painted and then chrome-plated. Winston sculpted a reproduction of Schwarzenegger's face in several poses out of silicone, clay and plaster.[29] Both the sequences in 2029 and stop motion scenes in the film were developed by Fantasy II, a special effects company headed by Gene Warren Junior.[30] A stop motion model is used in several scenes in the film involving the skeletal frame. Cameron wanted to convince the audience that the model of the structure was capable of doing what they saw Schwarzenegger doing. To allow this, a scene was filmed of Schwarzenegger injured and limping away. This limp made it easier for the model to imitate Schwarzenegger.[31][32]
One of the guns seen in the film and on the film's poster was an AMT Hardballer Longslide modified by Ed Reynolds from SureFire to include a laser sight. Both non-functioning and functioning versions of the prop were created. Due to cost considerations, the laser sights used an external power supply that Arnold Schwarzenegger had to activate manually. Reynolds states that his only compensation for the project was promotional material for the film.[33]
In March 1984, the film began production in Los Angeles.[29][34] Cameron felt that with Schwarzenegger on the set, the style of the film changed, explaining that "...the movie took on a larger than life sheen. I just found myself on the set doing things I didn't think I would do  scenes that were just purely horrific that just couldn't be, because now they were too flamboyant."[35][36] Most of The Terminator's action scenes were filmed at night, which led to tight filming schedules before sunrise. A week before filming started, Linda Hamilton sprained her ankle, leading to a production change whereby the scenes in which Hamilton needed to run occurred as late as the filming schedule allowed. Hamilton's ankle was taped every day and she spent most of the film production in pain.[37]
Schwarzenegger tried to have the iconic movie line delivered by his character in the film, "I'll be back", changed due to language barriers. In an October 1, 2012 interview on Good Morning America, Schwarzenegger revealed that he had difficulty pronouncing the word I'll properly, and asked director James Cameron if it could be changed to "I will be back". Cameron refused, so Schwarzenegger worked to say the line as written as best he could. He would later say the line in numerous subsequent films throughout his career.[38]
After production finished on The Terminator, some post-production shots were needed.[39] These included scenes showing the Terminator outside Sarah Connor's apartment, Reese being zipped into a body bag, and the Terminator's head being crushed in a press.[16][34][39] The film's soundtrack was synthesizer music composed by Brad Fiedel.[40] Fiedel described the film's music as being about "a mechanical man and his heartbeat".[41] Almost all the music in the film was performed live.[7][41] The Terminator's theme is played over the opening credits and is played in various points in the film in sped up versions: a slowed down version when Reese dies, and a piano version during the love scene.[42] Fiedel created music for when Reese and Connor escape from the police station that would be appropriate for a "heroic moment". Cameron turned down this theme, as he believed it would lose the audience's excitement.[41] The soundtrack to the film was released in 1984.[40]
Orion Pictures did not have faith in The Terminator performing well at the box office and feared a negative critical reception.[43] At an early screening of the film, the actors' agents insisted to the producers that the film should be screened for critics.[16] Orion only held one press screening for the film.[43] The film was premiered on October 26, 1984. On its opening week, The Terminator played at 1,005 theaters and grossed $4,020,663 making it number one in the box office. The film remained at number one in its second week. It lost its number one spot in the third week to Oh, God! You Devil.[44][45] Cameron noted that The Terminator was a hit "relative to its market, which is between the summer and the Christmas blockbusters. But it's better to be a big fish in a small pond than the other way around."[46]
Writer Harlan Ellison stated that he "loved the movie, was just blown away by it",[47] but believed that the screenplay was based on an episode of The Outer Limits he had written, titled "Soldier".[48] Orion gave Ellison an undisclosed amount of money and an acknowledgment credit in later prints of the film.[48] Some accounts of the settlement state that "Demon with a Glass Hand", another Outer Limits episode written by Ellison, was also claimed to have been plagiarized by the film,[47][49][50][51][52] but Ellison has explicitly stated that The Terminator "was a ripoff" of "Soldier" rather than "Demon with a Glass Hand".[48]
Cameron was against Orion's decision and was told that if he did not agree with the settlement, they would have Cameron pay for any damages if Orion lost Ellison's suit.[49] Cameron replied that he "had no choice but to agree with the settlement. Of course there was a gag order as well, so I couldn't tell this story, but now I frankly don't care. It's the truth. Harlan Ellison is a parasite who can kiss my ass."[49][53]
Around and shortly after The Terminator's release in theaters, a number of merchandise items and media were released and sold to coincide with the film. Shaun Hutson wrote a novelization of the film which was published in 1984.[54] In September 1988, NOW Comics released a comic based on the film. Dark Horse Comics published a comic in 1990 that took place 39 years after the film.[55] Several video games based on The Terminator were released between 1991 and 1993 for various Nintendo and Sega systems.[56] A soundtrack to the film was released in 1984 which included the score by Brad Fiedel and the pop and rock songs used in the club scenes.[40]
The Terminator was released on VHS and Betamax in 1985.[57] The film performed well financially on its initial release. The Terminator premiered at number 35 on the top video cassette rentals and number 20 on top video cassette sales charts. In its second week, The Terminator reached number 4 on the top video cassette rentals and number 12 on top video cassette sales charts.[58][59] In March 1995, The Terminator was released as a letter boxed edition on Laserdisc.[60] The film premiered on DVD on September 3, 1997.[44][61] IGN referred to this DVD as "pretty bare-bones...released with just a mono soundtrack and a kind of poor transfer."[62] A special edition of the film was released on October 2, 2001 which included documentaries, the script and advertisements for the film.[63][64] On 23 January 2001 a VCD version was released for the Hong Kong market.[65] On June 20, 2006, the film was released on Blu-ray in the United States.[66]
Positive reviews of The Terminator focused on the action scenes and rapid pacing. Variety praised the film, calling it a "blazing, cinematic comic book, full of virtuoso moviemaking, terrific momentum, solid performances and a compelling story...Schwarzenegger is perfectly cast in a machine-like portrayal that requires only a few lines of dialog."[67] Richard Corliss of Time magazine said that the film has "Plenty of tech-noir savvy to keep infidels and action fans satisfied."[68] Time placed The Terminator on its "10 Best" list for 1984.[43] The Los Angeles Times called the film "a crackling thriller full of all sorts of gory treats...loaded with fuel-injected chase scenes, clever special effects and a sly humor."[43] The Milwaukee Journal gave the film 3 stars, calling it "the most chilling science fiction thriller since Alien."[69] A review in Orange Coast magazine stated that "the distinguishing virtue of The Terminator is its relentless tension. Right from the start it's all action and violence with no time taken to set up the story...It's like a streamlined Dirty Harry movie  no exposition at all; just guns, guns and more guns."[70] In the May 1985 issue of Cinefantastique it was referred to as a film that "manages to be both derivative and original at the same time...not since the Road Warrior has the genre exhibited so much exuberant carnage" and "an example of science fiction/horror at its best...Cameron's no-nonsense approach will make him a sought-after commodity".[71] In the United Kingdom the Monthly Film Bulletin praised the film's script, special effects, design and Schwarzenegger's performance.[71][72]
Other reviews focused on the film's level of violence and story-telling quality. The New York Times opined that the film was a "B-movie with flair. Much of it...has suspense and personality, and only the obligatory mayhem becomes dull. There is far too much of the latter, in the form of car chases, messy shootouts and Mr. Schwarzenegger's slamming brutally into anything that gets in his way."[73] The Pittsburgh Press wrote a negative review, calling the film "just another of the films drenched in artsy ugliness like Streets of Fire and Blade Runner."[74] The Chicago Tribune gave the film two stars, adding that "at times it's horrifyingly violent and suspenseful at others it giggles at itself. This schizoid style actually helps, providing a little humor just when the sci-fi plot turns too sluggish or the dialogue too hokey."[75] The Newhouse News Service called the film a "lurid, violent, pretentious piece of claptrap".[76] The film won three Saturn Awards for Best Science Fiction Film, best make-up and best writing.[77]
In 1991, Richard Schickel of Entertainment Weekly reviewed the film giving it an "A" rating, writing that "what originally seemed a somewhat inflated, if generous and energetic, big picture, now seems quite a good little film" and called it "one of the most original movies of the 1980s and seems likely to remain one of the best sci-fi films ever made."[78] Film4 gave the film five stars, calling it the "sci-fi action-thriller that launched the careers of James Cameron and Arnold Schwarzenegger into the stratosphere. Still endlessly entertaining."[79] TV Guide gave the film four stars referring to it as an "amazingly effective picture that becomes doubly impressive when one considers its small budget...For our money, this film is far superior to its mega-grossing mega-budgeted sequel."[80] Empire gave the film five stars calling it "As chillingly efficient in exacting thrills from its audience as its titular character is in executing its targets."[81] The film database Allmovie gave the film five stars, saying that it "established James Cameron as a master of action, special effects, and quasi-mythic narrative intrigue, while turning Arnold Schwarzenegger into the hard-body star of the 1980s."[82] Halliwell's Film Guide described the film as "slick, rather nasty but undeniably compelling comic book adventures."[83] The film holds a 100% "Certified Fresh" rating and a score of 84/100 ("universal acclaim"), respectively, on the review aggregate websites Rotten Tomatoes and Metacritic.[84][85]
The Terminator has received recognition from the American Film Institute. The film ranked 42nd on AFI's 100 Years... 100 Thrills, a list of America's most heart-pounding films.[86] The character of the Terminator was selected as the 22nd-greatest movie villain on AFI's 100 Years... 100 Heroes and Villains.[87] Arnold's catch phrase "I'll be back" was voted the 37th-greatest movie quote by the AFI.[88] In 2005, Total Film named The Terminator the 72nd-best film ever made.[89] In 2008, Empire magazine selected The Terminator as one of The 500 Greatest Movies of All Time.[90] Empire also placed the T-800 14th on their list of The 100 Greatest Movie Characters.[91] In 2008, The Terminator was deemed "culturally, historically, or aesthetically significant" by the Library of Congress and selected for preservation in the United States National Film Registry.[92]
Darian Leader considers The Terminator as an example of how the cinema has dealt with the problem of masculinity, stating that it illustrates that to be a man requires more than having the body of a man, and that something symbolic that is not ultimately human must be added. He sees The Terminator as similar to The Six Million Dollar Man and RoboCop in this respect.[93]
October 26, 1984(1984-10-26)
1 Plot
2 Cast
3 Production

3.1 Development
3.2 Pre-production
3.3 Production
3.4 Post-production


3.1 Development
3.2 Pre-production
3.3 Production
3.4 Post-production
4 Release

4.1 Marketing
4.2 Home video


4.1 Marketing
4.2 Home video
5 Reception and legacy
6 Themes
7 See also
8 References
9 External links
3.1 Development
3.2 Pre-production
3.3 Production
3.4 Post-production
4.1 Marketing
4.2 Home video
Arnold Schwarzenegger as The Terminator, a cyborg sent back in time to assassinate Sarah Connor. Schwarzenegger speaks only 18 lines in the film. James Cameron said that "Somehow, even his accent worked...It had a strange synthesized quality, like they hadn't gotten the voice thing quite worked out."[2]
Linda Hamilton as Sarah Connor, the Terminator's target.
Michael Biehn as Kyle Reese, a human Resistance fighter sent back in time to protect Sarah.
Paul Winfield as Ed Traxler, a police Lieutenant who questions Sarah.
Lance Henriksen as Hal Vukovich, a police detective who questions Sarah.
Earl Boen as Dr. Silberman, a police psychologist.
Arnold Schwarzenegger filmography
James Cameron filmography
List of action films of the 1980s
List of American films of 1984
List of science-fiction films of the 1980s
Andrews, Nigel (2003). True Myths: The Life and Times of Arnold Schwarzenegger. Carol Publishers. ISBN1-55972-364-5. http://books.google.ca/books?id=ONphNAAACAAJ. Retrieved September 18, 2010.
French, Sean (1996). The Terminator. British Film Institute. ISBN0-85170-553-7.
Hayward, Philip (2004). Off the planet: music, sound and science fiction cinema. Indiana University Press. ISBN0-86196-644-9. http://books.google.ca/books?id=9E8H8rRRj9IC. Retrieved September 19, 2010.
Keegan, Rebecca Winters (2009). The Futurist: The Life and Films of James Cameron. New York, United States: Crown Publishers. ISBN978-0-307-46031-8. http://books.google.ca/books?id=g8hOUZ99h8cC. Retrieved September 18, 2010.
Heard, Christopher (1997). Dreaming Aloud: The Life and Films of James Cameron. Toronto, Canada: Doubleday Canada. ISBN0-385-25680-9.
Overstreet, Robert M. (2010). The Official Overstreet Comic Book Companion (11 ed.). Random House of Canada. ISBN0-375-72308-0. http://books.google.ca/books?id=32mWSyAYyOAC. Retrieved September 19, 2010.
The Terminator at AllRovi
The Terminator at the Internet Movie Database
The Terminator at Rotten Tomatoes
The Terminator at Metacritic
The Terminator at Box Office Mojo
.
#(`*Up (2009 film)*`)#.
Up is a 2009 American 3D computer-animated comedy-adventure film produced by Pixar Animation Studios and directed by Pete Docter. The film centers on an elderly widower named Carl Fredricksen (voiced by Edward Asner) and an earnest young Wilderness Explorer named Russell (Jordan Nagai). By tying thousands of balloons to his home, 78-year-old Carl sets out to fulfill his lifelong dream to see the wilds of South America and to complete a promise made to his lifelong love. The film was co-directed by Bob Peterson, with music composed by Michael Giacchino.
Docter began working on the story in 2004, which was based on fantasies of escaping from life when it becomes too irritating. He and eleven other Pixar artists spent three days in Venezuela gathering research and inspiration. The design of the characters were caricatured and stylized considerably, and animators were challenged with creating realistic cloth. The floating house is attached by a varying number between 10-20,000 balloons in the film's sequences. Up was Pixar's first film to be presented in Disney Digital 3-D.[3]
Up was released on May 29, 2009 and opened the 2009 Cannes Film Festival, becoming the first animated and 3D film to do so.[4] The film became a great financial success, accumulating over $731 million in its theatrical release. Up received critical acclaim, with most reviewers commending the humor and heart of the film. Edward Asner was praised for his portrayal of Carl, and a montage of Carl and his wife Ellie aging together was widely lauded. The film received five Academy Award nominations, including Best Picture, making it the second animated film in history to receive such a nomination, following Beauty and the Beast (1991).[5]
Young Carl Fredricksen is a shy, quiet boy who idolizes renowned explorer Charles F. Muntz. He is saddened to learn, however, that Muntz has been accused of fabricating the skeleton of a giant bird he had claimed to have discovered in Paradise Falls, Venezuela, and was publicly disgraced. Muntz vowed to return to Paradise Falls and not leave until he had captured a specimen alive to clear his name.
One day, Carl befriends an energetic and somewhat eccentric tomboy named Ellie, who is also a Muntz fan. She confides to Carl her desire to move her "clubhouse"an abandoned house in the neighborhoodto a cliff overlooking Paradise Falls, making him promise to help her. Carl and Ellie eventually get married and grow old together in the restored house, working as a toy balloon vendor and a zookeeper, respectively. After being told they can't conceive children, the two decide to realize their dream of visiting Paradise Falls. They try to save up for the trip, but repeatedly end up spending the money on more pressing needs. Finally, elderly Carl Fredricksen arranges for the trip, but Ellie suddenly becomes ill and dies, leaving him alone.
Some time later, Carl is still living in their house, now surrounded by urban development, but he refuses to sell his house. He ends up injuring a construction worker over damage done to his mailbox. He is evicted from the house by court order due to being deemed a "public menace", and is ordered to move to a retirement home. However, Carl comes up with a scheme to keep his promise to Ellie: he turns his house into a makeshift airship, using thousands of helium balloons to lift it off its foundation. A young member of the "Wilderness Explorers" (a fictional youth organization, based on the Boy Scouts of America) named Russell becomes an accidental passenger, having pestered Carl earlier in an attempt to earn his final merit badge, "Assisting the Elderly".
After surviving a thunderstorm, the house lands near a large ravine facing Paradise Falls. Carl and Russell harness themselves to the still-buoyant house and begin to walk it around the ravine, hoping to reach the falls before the balloons deflate. They later befriend a tall, colorful flightless bird (whom Russell names "Kevin") and then a dog named Dug, who wears a special collar that allows him to speak.
Carl and Russell encounter a pack of dogs led by Alpha, and are taken to Dug's master, who turns out to be an elderly Charles Muntz. Muntz invites Carl and Russell aboard his airship, where he explains that he has spent the years since his disgrace searching Paradise Falls for the giant bird. The time he has spent alone and concentrating only on his mission has made him extremely paranoid, mentally unstable and dangerous. When Russell innocently reveals his friendship with Kevin, Muntz becomes disturbingly hostile and starts showing the flight helmets of explorers whom he has apparently eliminated, believing they were all after the bird. This prompts Carl, Russell, Kevin and Dug to flee, chased by Muntz's dogs. Muntz eventually catches up with them and starts a fire beneath Carl's house, forcing Carl to choose between saving his home or Kevin. Carl rushes to put out the fire, allowing Muntz to take the bird. Carl and Russell eventually reach the falls, but Russell is angry with Carl.
Settling into his home, Carl discovers photos of their married life in Ellie's childhood scrapbook and a final note from his wife thanking him for the "adventure" and encouraging him to go on a new one. Reinvigorated, he goes to find Russell, only to see him sailing off on some balloons to rescue Kevin. Because many balloons have popped or deflated from Muntz's attack, Carl is forced to empty the house of furniture so it can lift off again so that Carl can pursue Russell.
Russell is captured by Muntz, but Carl boards the airship in flight and frees both Russell and Kevin. Muntz pursues them around the airship, finally cornering Dug, Kevin, and Russell inside Carl's tethered house. Carl lures Kevin out through a window and back onto the airship with Dug and Russell clinging to her back, just as Muntz is about to close in; the insane hunter leaps after them, only to snag his foot on some balloon lines and fall to his death. Snapped from its tether, the house descends out of sight through the clouds, which Carl accepts as being for the best.
Carl, Russell and Dug reunite Kevin with her chicks, then fly the airship back to the city. When Russell's father misses his son's Senior Explorer ceremony, Carl proudly presents Russell with his final badge for assisting the elderly, as well as a personal addition: the grape soda cap that Ellie gave to Carl when they first met (which he dubs the "Ellie Badge"). Meanwhile, Carl's house is shown to have landed on the cliff beside Paradise Falls, as promised to Ellie.
During the credits, a series of photographs shows Carl enjoying his latest adventure: living an active life as a surrogate grandfather to Russell.
Writing for Up first began in 2004 by director Pete Docter. The fantasy of a flying house was developed on the idea of escaping from life when it becomes too irritating,[15][12] which stemmed from his difficulty with social situations growing up.[22] Actor and writer Thomas McCarthy aided Docter and Bob Peterson in shaping the story for about three months.[17] Docter selected an old man for the main character after drawing a picture of a grumpy old man with smiling balloons.[17] The two men thought an old man was a good idea for a protagonist because they felt their experiences and the way they affect their view of the world was a rich source of humor. Docter was not concerned with an elderly protagonist, stating children would relate to Carl in the way they relate to their grandparents.[12]
Docter noted the film reflects his friendships with Disney veterans Frank Thomas, Ollie Johnston, and Joe Grant (who all died before the film's release and thus the film was dedicated to them). Grant gave the script his approval as well as some advice before his death in 2005.[23] Docter recalled Grant would remind him the audience needed an "emotional bedrock" because of how wacky the adventure would become; in this case it is Carl mourning for his wife.[17] Docter felt Grant's personality influenced Carl's deceased wife Ellie more than the grouchy main character,[23] and Carl was primarily based on Spencer Tracy, Walter Matthau, James Whitmore, and their own grandparents, because there was "something sweet about these grumpy old guys".[7] Docter and Jonas Rivera noted Carl's charming nature in spite of his grumpiness derives from the elderly "hav[ing] this charm and almost this 'old man license' to say things that other people couldn't get away with [...] It's like how we would go to eat with Joe Grant and he would call the waitresses 'honey'. I wish I could call a waitress 'honey'."[24]
Docter revealed that the filmmakers' first story outline had Carl "just want[ing] to join his wife up in the sky. It was almost a kind of strange suicide mission or something. And obviously that's [a problem]. Once he gets airborne, then what? So we had to have some goal for him to achieve that he had not yet gotten."[20] As a result, they added the plot of going to South America. The location was chosen due to both Docter's love of tropical locations, but also in wanting a location that Carl could be stuck with a kid due to the inability to leave him with an authority such as a police officer or social worker. They implemented a child character as a way to help Carl stop being "stuck in his ways".[25]
Docter created Dug as he felt it would be refreshing to show what a dog thinks, rather than what people assume it thinks.[26] Knowledge of canine communication, body language and pack behaviors for the artists and animators to portray such thoughts came from consultant Dr. Ian Dunbar, veterinarian, dog behaviorist and trainer.[27] The idea for Alpha's voice derived from thinking about what would happen if someone broke a record player and it always played at a high pitch.[17] Russell was added to the story at a later date than Dug and Kevin;[17] his presence, as well as the construction workers, helped to make the story feel less "episodic".[20]
Carl's relationship with Russell reflects how "he's not really ready for the whirlwind that a kid is, as few of us are".[23] Docter added he saw Up as a "coming of age" tale and an "unfinished love story", with Carl still dealing with the loss of his wife.[28] He cited inspiration from Casablanca and A Christmas Carol, which are both "resurrection" stories about men who lose something, and regain purpose during their journey.[29] Docter and Rivera cited inspiration from the Muppets, Hayao Miyazaki, Dumbo, and Peter Pan. They also saw parallels to The Wizard of Oz and tried to make Up not feel too similar.[30] There is a scene where Carl and Russell haul the floating house through the jungle. A Pixar employee compared the scene to Fitzcarraldo, and Docter watched that film and The Mission for further inspiration.[31] The character Charles Muntz comes from Howard Hughes and Errol Flynn.[32]
Docter made Venezuela the film's setting after Ralph Eggleston gave him a video of the tepui mountains;[12][23] Venezuela and tepuis were already featured in a previous Disney film, Dinosaur. In 2004, Docter and eleven other Pixar artists spent three days reaching Monte Roraima by airplane, jeep, and helicopter.[11] They spent three nights there painting and sketching,[33] and encountering ants, mosquitoes, scorpions, frogs, and snakes. They also flew to Matawi Tepui and climbed to Angel Falls.[11] Docter felt "we couldn't use [the rocks and plants we saw]. Reality is so far out, if we put it in the movie you wouldn't believe it."[7] The film's creatures were also challenging to design because they had to fit in the surreal environment of the tepuis, but also be realistic because those mountains exist in real life.[23] The filmmakers visited Sacramento Zoo to observe a Himalayan Monal for Kevin's animation.[1] The animators designed Russell as an Asian-American, and modeled Russell after similar looking Peter Sohn, a Pixar storyboarder who voiced Emile in Ratatouille and directed the short Partly Cloudy, because of his energetic nature.[15][34]
While Pixar usually designs their characters to be caricatured, Carl was even more so, being only three heads high.[35] He was not given elderly features such as liver spots or hair in his ears to keep him appealing, yet giving him wrinkles, pockmarks on his nose, a hearing aid, and a cane to make him appear elderly. Docter wanted to push a stylized feel, particularly the way Carl's head is proportioned: he has a squarish appearance to symbolize his containment within his house, while Russell is rounded like a balloon.[8] The challenge on Up was making these stylized characters feel natural,[12] although Docter remarked the effect came across better than animating the realistic humans from Toy Story, who suffered from the "uncanny valley".[23] Cartoonists Al Hirschfeld, Hank Ketcham, and George Booth influenced the human designs.[17][29][36] Simulating realistic cloth on caricatured humans was harder than creating the 10,000 balloons flying the house.[22] New programs were made to simulate the cloth and for Kevin's iridescent feathers.[37] To animate old people, Pixar animators would study their own parents or grandparents and also watched footage of the Senior Olympics.[6] The directors had various rules for Carl's movements: he could not turn his head more than 1520degrees without turning his torso as well, nor could he raise his arms very high. However, they also wanted him to grow more flexible near the end of the film, transforming into an "action hero".
A technical director worked out that in order to make Carl's house fly, he would require 23million balloons, but Docter realized that number made the balloons look like small dots. Instead, the balloons created were made to be twice Carl's size. There are 10,927balloons for shots of the house just flying, 20,622balloons for the lift-off sequence, and a varying number in other scenes.[11]
Up is the third Pixar film to be scored by Michael Giacchino, after The Incredibles and Ratatouille. What Pete Docter wanted more importantly out of the music was the emotion, so Giacchino wrote a character theme-based score that producer Jonas Rivera thought enhanced the story. At the beginning of the movie, when young Carl is in the movie theater watching a newsreel about Muntz, the first piece of music heard is "Muntz's Theme", which starts out as a celebratory theme, and echoes through the film when Muntz reappears 70years later. "Ellie's Theme" is first heard when she is introduced as a little kid and plays several times during the film in different versions; for instance, during the sequence where Carl lifts his house with the balloons, the theme is changed from a simple piano melody to a full orchestral arrangement. Giacchino has compared the film to opera since each character has a unique theme that changes during a particular moment in the story.[38]
The score was released as a digital download on May26,2009, three days before the film opened in theaters. It won the Academy Award for Best Original Score,[39] the Grammy Award for Best Score Soundtrack Album,[40] the Golden Globe Award for Best Original Score,[41] and the 2010 BAFTA Award for Best Film Music.[42] It is the first score for a Pixar film to win the Oscar (Randy Newman also won for Monsters, Inc. and Toy Story 3, but in the category of Best Original Song).
When the film screened at the El Capitan Theatre in Hollywood, California from May29 to July23,2009, it was accompanied by Lighten Up!, a live show featuring Disney characters.[43] Other tie-ins included children's books such as My Name is Dug, illustrated by screenwriter Ronnie del Carmen.[44] Despite Pixar's track record, Target Corporation and Walmart stocked few Up items, while Pixar's regular collaborator Thinkway Toys did not produce merchandise, claiming its story is unusual and would be hard to promote. Disney acknowledged not every Pixar film would have to become a franchise.[1] Promotional partners include Aflac,[45] NASCAR, and Airship Ventures,[46][47] while Cluster Balloons promoted the film with a replica of Carl's couch lifted by hot air balloons for journalists to sit in.[48]
Director Pete Docter intended for audiences to take a specific point from the film, saying:
Prior to its theatrical release, Disney Pixar created three small animated vignettes called UPisodes to promote its film UP on the internet.[50] These UPisodes chronicled Carl Fredricksen and Russell's journey through the jungle, not seen in the movie. Fans were able to view the vignettes on Apple iTunes movie trailer site and YouTube.
Up was released on Blu-ray Disc and DVD in North America on November10,2009,[51] and in the United Kingdom on February15,2010.[52] It features the film plus the theatrical short Partly Cloudy and the new short Dug's Special Mission, as well as an audio commentary by director Pete Docter, the documentary Adventure is Out There on the filmmakers' research journey to South America, The Many Endings of Muntz (an alternate ending of sorts), and a digital copy. The Blu-ray edition has a four-disc pack that adds Cine-Explore with BonusView, Global Guardian Badge and Geography games, eight documentaries, and BD-Live to the Deluxe DVD and digital copy platters. A Limited Edition is also available called the Luxo Jr. Premium Pack that includes a collectible lamp modeled after Pixar's bouncy short star that is designed to hold a complete Pixar Blu-ray collection.[53]
In addition, Pixar also created a short film titled George & A.J., written and directed by Up storyboard artist Josh Cooley, that shows what the two Shady Oaks retirement home workers did after Carl left with his house. It was initially available for purchase at the iTunes Store, and then was later posted to DisneyPixar's Facebook and YouTube pages.[54][55]
In its first week it sold 3,969,792 units ($66,057,339). It eventually reached 10,811,453 units ($182,591,149),[56] becoming the best-selling DVD among those released in 2009, in terms of units sold. It also became the third in terms of sales revenue behind Transformers: Revenge of the Fallen and Twilight.[57]
The rental release of the film to Netflix, Blockbuster, and Redbox was controversial since it failed to include closed captioning.[58] Disney faced a consumer backlash over this[59] and quickly released a statement that this removal was an unfortunate error and that it was moving to correct the issue.[60]
Since its release, Up has received extremely positive reviews. As of September 3, 2011(2011 -09-03)[update], Rotten Tomatoes reports that 98% of critics have given the film a "Certified Fresh" positive review, based on 270reviews, with an 8.6/10 review average. The site's consensus states: "Another masterful work of art from Pixar, Up is an exciting, hilarious, and heartfelt adventure impeccably crafted and told with wit and depth."[61] The film also holds a score of 88 on the review aggregator website Metacritic as of September 3, 2011(2011 -09-03)[update].[62] Audiences gave the film an "A+" CinemaScore.[63]
Film critic Roger Ebert gave the film four out of four stars and called it "a wonderful film".[64][65] The Hollywood Reporter lauded the film as "Winsome, touching and arguably the funniest Pixar effort ever, this gorgeously rendered, high-flying adventure is a tidy 90-minute distillation of all the signature touches that came before it."[66] Although the San Francisco Chronicle noted that the film "contains many boring stretches of mindless freneticism and bland character interaction," it also declared that there are scenes in Up of "Such beauty, economy and poetic wisdom that they belong in any anthology of great movie moments...to watch Up with any attention is to be moved and astonished by the economy with which specific visuals are invested with emotion throughout [the film]..."[67] Variety enthused that "Up is an exceptionally refined picture; unlike so many animated films, it's not all about sensory bombardment and volume...Unsurprisingly, no one puts a foot wrong here. Vocal performances...exude a warm enthusiasm, and tech specifications could not be better. Michel Giacchino's full-bodied, traditional score is superlative..."[68] The Globe claimed that Up! is "the kind of movie that leaves you asking 'How do people come up [with] this stuff?'" along with an overall positive review on the film, despite it being predictable.[69]
The character of Carl Fredricksen has received mostly positive reception. Bill Capodagli, author of Innovate the Pixar Way, praised Carl for his ability to be a jerk and likable at the same time.[70] Wall Street Journal editor Joe Morgenstern described Carl as gruff, comparing him to Buster Keaton, but adds that this begins to wear thin as the movie progresses.[71] He has been compared with Spencer Tracy, an influence on the character, by The Washington Post editor Ann Hornaday[72] and Empire Online editor Ian Freer, who describes him as similar to a "Guess Who's Coming to Dinner-era" Tracy.[73] Entertainment Weekly editor Lisa Schwarzbaum described his appearance as a cross between Tracy and an eccentric out of a George Booth cartoon.[74] TIME editor Richard Corliss also makes the comparison, calling him a "trash compacted version" of Tracy.[75] He has also been compared to Walter Matthau, another inspiration for the character's design, by LA Weekly editor Scott Foundas, suggesting that actor Ed Asner was channeling him while performing the role of Carl.[76] Variety editor Todd McCarthy described Carl as a combination of both Tracy and Matthau.[68]
The relationship between Carl and his wife Ellie has been praised in several media outlets. In his book Disney, Pixar, and the Hidden Message of Children's Films, author M. Keith Booker described the love between Carl and Ellie as touching. While also describing the scene of the two of them aging as a "masterpiece of its own kind", he was not sure how much children would appreciate the scene, commenting that his son was squirming in his seat during the scene.[77] Reelviews editor James Berardinelli praised their relationship, stating that it brought a tear to his eye in a way no animated film has done, including anything by famed anime director Hayao Miyazaki.[78] Ann Hornaday praised the prologue, describing it as "worthy of Chaplin in its heartbreaking poignancy".[72] Chicago Tribune editor Michael Phillips praised the scene, describing it as an emotional and cinematic powerhouse, and that he also was nearly moved to tears. However, Salon.com editor Stephanie Zacharek criticized the love between Carl and Ellie, describing their marriage as resembling a dental adhesive commercial more than a real relationship.[79]
Edward Asner was praised in several media outlets for his portrayal of Carl. San Francisco Chronicle editor Mick LaSelle praised Asner as a great choice due to having a grumpiness to his voice that is not truly grumpy, but rather coming from a protective stance.[80] Entertainment Weekly editor Lisa Schwarzbaum praised Asner's acting, stating that he has a "Lou Grant authority" to his voice.[74] Time editor Richard Corliss stated that Asner had the "gruffness and deadpan comic timing to bring Carl to life".[8] The Boston Globe editor Ty Burr concurred with this, stating that his Lou Grant-like voice had not diminished with time.[81] USA Today editor Claudia Puig praised Asner's delivery, describing it as superb.[82]
Up earned $293,004,164 in the United States and Canada, and $438,338,580 in other territories, for a worldwide total of $731,342,744.[2] Worldwide, it is the 50th highest-grossing film, the 10th highest-grossing animated film,[83] the sixth highest-grossing film of 2009,[84] and the third highest-grossing Pixar film.[85]
In the United States and Canada, Up is the 45th highest-grossing film, the tenth highest-grossing Disney film,[86] the seventh highest-grossing 3-D film,[87] the sixth highest-grossing animated film,[88] the fifth highest-grossing film of 2009,[89] and the third highest-grossing Pixar film.[85] On its opening weekend, it performed stronger than analysts had been expecting, ranking number one with $68,108,790.[90] This is the fourth highest-grossing opening for Pixar[91] and the third-largest post-Memorial Day opening. It set a record for opening-weekend grosses originating from 3-D showings with $35.4 million (first surpassed by Avatar).[92] The opening-weekend audience was 53% female and 47% under 17 years old.[93] The film experienced small drop-offs on subsequent weekends, but lost first place to The Hangover.[94][95]
Outside the US and Canada, it is the 43rd highest-grossing film,[96] the tenth highest-grossing animated film, the fifth highest-grossing film of 2009,[97] and the third highest-grossing Pixar film.[85] It was on top of the overseas box office for three consecutive weekends and four in total.[98] Its highest-grossing opening weekends were recorded in France and the Maghreb region ($8.88 million), the UK, Ireland and Malta, ($8.44 million) and Japan ($7.24 million). These three were also its highest-grossing countries in total earnings.[99] Among major countries, it was the highest-grossing animated film of 2009 only in Spain ($37.1 million)[100] and Australia ($25.3 million).[101]

Up won two awards at the 82nd Academy Awards, for "Best Animated Feature" and "Academy Award for Best Original Score".[39] It is the second of three animated features to have been nominated for the Academy Award for Best Picture. Beauty and the Beast and Toy Story 3 were also nominated for Best Picture in their respective years. 'Up' also won "Best Original Score", and "Best Animated Feature Film" at the 67th Golden Globe Awards.[41] It was nominated for nine Annie Awards in eight categories, winning two awards for "Best Animated Feature" and "Best Directing in a Feature Production".[102] Up also received the Golden Tomato from Rotten Tomatoes for highest rating feature in 2009, and best reviewed animated film,[103] with an approval of 98percent from film critics, based on 259 reviews.[104] At the 2010 Kids' Choice Awards the film won "Favorite Animated Movie".[105] Dug, the talking canine, was awarded the Palm Dog Award by the British film critics as the best canine performance at Cannes Film Festival, winning over the fox from Antichrist and the black poodle from Inglourious Basterds.[106]
A video game, Kinect Rush: A Disney Pixar Adventure, was released on March 20, 2012, for Xbox 360. It features characters from five of Pixar's films: Up, The Incredibles, Cars, Ratatouille, and Toy Story.[107]
May 29, 2009(2009-05-29)
1 Plot
2 Cast
3 Production

3.1 Animation
3.2 Music


3.1 Animation
3.2 Music
4 Release

4.1 UPisodes
4.2 Home media


4.1 UPisodes
4.2 Home media
5 Reception

5.1 Critical response
5.2 Box office
5.3 Accolades


5.1 Critical response
5.2 Box office
5.3 Accolades
6 Video game
7 See also
8 References
9 External links
3.1 Animation
3.2 Music
4.1 UPisodes
4.2 Home media
5.1 Critical response
5.2 Box office
5.3 Accolades
Edward Asner as Carl Fredricksen (Jeremy Leary voiced Carl as a younger child). Docter and Rivera noted Asner's television alter ego, Lou Grant, had been helpful in writing for Carl, because it guided them in balancing likable and unlikable aspects of the curmudgeonly character.[6] When they met Asner and presented him with a model of his character, he joked, "I don't look anything like that." (The appearance of Carl is meant to resemble Spencer Tracy as he appeared in his final film, Guess Who's Coming to Dinner.[7]) They tailored his dialogue for him, with short sentences and more consonants, which "cemented the notion that Carl, post-Ellie, is a disgruntled bear that's been poked awake during hibernation".[8] In Colombia, unexpected publicity for the film was generated due to the uncanny similarity of Carl with Colombian ex-president Julio Csar Turbay Ayala.[9][10]
Christopher Plummer as Charles F. Muntz. Muntz is an old explorer looking for the beast of Paradise Falls; he vowed not to return to North America until he had captured the creature. He uses a group of dogs to aid him in his hunt. The name of his airship, Spirit of Adventure, may have been inspired by Charles Lindbergh's airplane, Spirit of St. Louis.[11] In various interviews, Pete Docter has mentioned Howard Hughes and real life adventurers Charles Lindbergh and Percy Fawcett as inspirations for Muntz.[12]
Jordan Nagai as Russell. On their journey, Russell makes several comments to Carl that suggest that Russell's father and mother are no longer together.[13] Russell's design was based on Pixar animator Peter Sohn.[14] Docter auditioned 400 boys in a nationwide casting call for the part.[15] Nagai, who is Japanese-American,[16] showed up to an audition with his brother, who was actually the one auditioning. Docter realized Nagai behaved and spoke non-stop like Russell and chose him for the part.[17] Nagai was 8 years old when cast.[15] Docter encouraged Nagai to act physically as well as vocally when recording the role, lifting him upside down and tickling him for the scene where Russell encounters Kevin.[8] Asian Americans have positively noted Pixar's first casting of an Asian lead character,[18] in contrast to the common practice of casting non-Asians in Asian parts.[19]
Bob Peterson as Dug, a Golden Retriever who can talk.[11] He is the misfit of a pack of talking dogs owned by Muntz. Peterson knew he would voice Dug when he wrote his line "I have just met you, and I love you," which was based on what a child told him when he was a camp counselor in the 1980s. The DVD release of the film features a short called Dug's Special Mission, which follows Dug just prior to his first meeting with Carl and Russell. Dug previously appeared in Ratatouille as a shadow on a wall that barks at Remy.[11]

Peterson also voices Alpha, a talking Doberman Pinscher[11] and the leader of Muntz's pack of dogs. Pete Docter has stated that Alpha "thinks of himself as Clint Eastwood". Despite his menacing appearance, a frequent malfunction in Alpha's translating collar causes his voice to sound comically high-pitched and squeaky, as if he had been breathing helium. The normal voice for his translator is a resonant, intimidating bass. With both voices, Alpha has a roundabout speech pattern that causes his sentences to be longer than necessary.


Peterson also voices Alpha, a talking Doberman Pinscher[11] and the leader of Muntz's pack of dogs. Pete Docter has stated that Alpha "thinks of himself as Clint Eastwood". Despite his menacing appearance, a frequent malfunction in Alpha's translating collar causes his voice to sound comically high-pitched and squeaky, as if he had been breathing helium. The normal voice for his translator is a resonant, intimidating bass. With both voices, Alpha has a roundabout speech pattern that causes his sentences to be longer than necessary.
Pete Docter as Kevin, a large colorful prehistoric bird. Other than voicing Kevin, Docter also voices Campmaster Strauch, Russell's camp master, seen at the end of the film.
Elizabeth Docter as Ellie Fredricksen as a younger child. The voice actor is the director's daughter,[20] who also provided some of the drawings shown by Ellie.[21]
Delroy Lindo as Beta, a Rottweiler[11] and one of Muntz's dogs.
Jerome Ranft as Gamma, a Bulldog[11] and one of Muntz's dogs.
John Ratzenberger as Tom, a construction worker who asks if Carl is ready to sell his house.[11]
David Kaye as the newsreel announcer.
Peterson also voices Alpha, a talking Doberman Pinscher[11] and the leader of Muntz's pack of dogs. Pete Docter has stated that Alpha "thinks of himself as Clint Eastwood". Despite his menacing appearance, a frequent malfunction in Alpha's translating collar causes his voice to sound comically high-pitched and squeaky, as if he had been breathing helium. The normal voice for his translator is a resonant, intimidating bass. With both voices, Alpha has a roundabout speech pattern that causes his sentences to be longer than necessary.
UPisode One: Animal Calls - in the first episode, Russell demonstrates his ability to mimic animal calls.
UPisode Two: First Aid - in the second episode, Russell tries to relieve a minor injury that Carl received.
UPisode Three: Snipe Trap - in the third episode, Russell attempts to capture the elusive snipe.
Up (video game)
Cluster ballooning (The activity of flying using clusters of helium balloons)
Official website
Up at the Internet Movie Database
Up at AllRovi
Up at the Big Cartoon DataBase
Up at Rotten Tomatoes
Up at Metacritic
Up at Box Office Mojo
.
#(`*WALL-E*`)#.
WALL-E (stylized with an interpunct as WALLE) is a 2008 American computer-animated romantic science fiction film produced by Pixar Animation Studios and directed by Andrew Stanton. The story follows a robot named WALL-E, who is designed to clean up a waste-covered Earth far in the future. He falls in love with another robot named EVE, who also has a programmed task, and follows her into outer space on an adventure that changes the destiny of both his kind and humanity. Both robots exhibit an appearance of free will and emotions similar to humans, which develop further as the film progresses.
After directing Finding Nemo, Stanton felt Pixar had created believable simulations of underwater physics and was willing to direct a film largely set in space. Most of the characters do not have actual human voices, but instead communicate with body language and robotic sounds, designed by Ben Burtt, that resemble voices. In addition, it is the first animated feature by Pixar to have segments featuring live-action characters.
Walt Disney Pictures released it in the United States and Canada on June 27, 2008. The film grossed $23.2million on its opening day, and $63.1million during its opening weekend in 3,992 theaters, ranking number one at the box office. This ranks as the fifth highest-grossing opening weekend for a Pixar film. Following Pixar tradition, WALL-E was paired with a short film, Presto, for its theatrical release.
WALL-E has been met with overwhelmingly positive reviews among critics, scoring an approval rating of 96% on the review aggregator Rotten Tomatoes. It grossed $521.3million worldwide, won the 2008 Golden Globe Award for Best Animated Feature Film, the 2009 Hugo Award for Best Dramatic Presentation, Long Form,[3] the final Nebula Award for Best Script,[4] the Saturn Award for Best Animated Film, and the Academy Award for Best Animated Feature as well as being nominated for five other Academy Awards at the 81st Academy Awards. WALL-E ranks first in TIME's "Best Movies of the Decade".[5]
WALL-E, besides being entertaining, is also seen as a critique on larger societal issues. This film addresses consumerism, nostalgia, environmental problems, waste management, and the immense impact that humans have on the earth. There is also a rather major overarching theme through this movie, which is issues of human nature, and the direction that the human race is headed. WALL-E is seen as a movie that appeals to both conservative and liberal audiences. [6]

In 2805, Earth is covered in garbage due to decades of mass consumerism facilitated by the megacorporation Buy n Large. BnL evacuated Earth's population in fully automated starliners in 2105. Left behind were trash compactor Waste Allocation Load Lifter  Earth Class (WALL-E) robots, to clean the planet. Through the years they eventually broke down. One WALL-E unit has managed to remain active by repairing itself using parts from other broken units. Wall-E also developed sentience. As with his regular duties, he inquisitively collected artifacts of human civilization. He kept these items in his storage truck home. He befriended a cockroach, and enjoys listening to Hello, Dolly!
One day, WALL-E discovers and collects a growing seedling plant. Later a spaceship lands and deploys Extraterrestrial Vegetation Evaluator (EVE). She is an advanced robot sent from the BnL starliner Axiom to search for vegetation on Earth. Inspired by Hello, Dolly!, WALL-E falls in love with the initially cold and hostile EVE. Wall-E wishes to hold hands with her. EVE gradually softens and befriends him. When WALL-E brings EVE to his truck and showcases his collection, she sees the plant and automatically stores it, inside herself. She goes into standby mode waiting for retrieval, from her ship. WALL-E spends time with EVE while she is in standby mode. He then clings to the hull of EVE's ship as it collects and returns her to the starliner Axiom.
On the Axiom, the ship's original human passengers and their descendants have suffered from severe bone loss and become morbidly obese after centuries of living in microgravity and relying on the ship's automated systems for most tasks. Captain B. McCrea, in charge of the ship, mostly leaves control to the robotic autopilot, AUTO. WALL-E follows EVE to the bridge of the Axiom, where the Captain learns that the earth is habitable again by putting the plant in the spaceship holo-detector for verification. The captain plans for the Axiom to make a hyperjump back to Earth so the passengers can recolonize. However, AUTO orders McCrea's robotic assistant GO-4 to steal the plant as part of AUTO's own no return directive. This directive was secretly issued to autopilots after BnL incorrectly concluded in 2110 that the planet could not be saved and that humanity should remain in space.
With the plant missing, EVE is considered defective and taken to the repair ward along with WALL-E. WALL-E mistakes the process on EVE for torture and tries to save her. He accidentally releases a horde of malfunctioning robots. The on-board security systems then designate both WALL-E and EVE as rogue. Angry with WALL-E's disruptions, EVE brings him to the escape pod bay to send him home. There they witness GO-4 dispose of the missing plant by placing it inside a pod which is set to self-destruct. WALL-E enters the pod, which is then jettisoned into space. Fortunately he escapes with the plant before the pod explodes. Reconciling with EVE, they celebrate with a dance in space outside the Axiom. Meanwhile the Captain, learning from the ship's computer, has become fascinated about life on Earth before it was polluted and abandoned.
The plant is brought to the captain. He surveys EVE's recordings of Earth and concludes that mankind must return to restore their home. However, AUTO reveals his directive, staging a mutiny and tasering WALL-E, severely damaging him. He also incapacitates EVE and confines the captain to his quarters. EVE realizes the only parts for repairing WALL-E are in his truck back on Earth. She helps him bring the plant to the holo-detector. The presence of a plant in the detector will automatically activate the Axiom's hyperjump. Captain McCrea opens the holo-detector. While fighting with AUTO, chaos on the ship ensues. AUTO partially crushes WALL-E by closing the holo-detector on him. AUTO is eventually disabled by McCrea, who now takes control. After freeing the severely damaged WALL-E, EVE places the plant in the holo-detector, this sets the Axiom on the instant hyperjump to Earth. The human population finally land back on Earth, after being away for hundreds of years.
EVE brings WALL-E back to his home where she repairs and reactivates him. After the repair WALL-E no longer recognises EVE, reverting to his original programming - an unfeeling waste compactor. Heartbroken, EVE gives WALL-E a farewell kiss. This jolts WALL-E's memory, and his personality returns. WALL-E and EVE happily reunite as the humans and robots of the Axiom begin to restore Earth and its environment. Viewed through a series of artworks WALL-E and EVE are seen holding hands in front of a large tree, which is revealed to have grown from the tiny plant that brought humankind home.
BACK ON M-O AND WALLY [sic]
M-O just finishes cleaning the floor.
Wally is fascinated.
Impishly makes another mark.
M-O compulsively cleans it. Cant resist.
M-O (bleeps): [Look, it stays clean. You got that?]
Wally wipes the bottom of his tread on M-Os face.
M-O loses it.
Scrubs his own face.
Andrew Stanton conceived WALL-E during a lunch with fellow writers John Lasseter, Pete Docter, and Joe Ranft in 1994. Toy Story was nearing completion and the writers brainstormed ideas for their next projects  A Bug's Life, Monsters, Inc., and Finding Nemo  at this lunch. Stanton asked, "What if mankind had to leave Earth and somebody forgot to turn off the last robot?"[7] Having struggled for many years with making the characters in Toy Story appealing, Stanton found his simple Robinson Crusoe-esque idea of a lonely robot on a deserted planet very strong.[10][11] Stanton made WALL-E a waste collector as the idea was instantly understandable, and because it was a low-status menial job that made him sympathetic.[12] Stanton also liked the imagery of stacked cubes of garbage.[13] He did not find the idea dark because having a planet covered in garbage was for him a childish imagining of disaster.[14]
Stanton and Pete Docter developed the film under the title of Trash Planet for two months in 1995, but they did not know how to develop the story and Docter chose to direct Monsters, Inc. instead.[15][16] Stanton came up with the idea of WALL-E finding a plant, because his life as the sole inhabitant on a deserted world reminded Stanton of a plant growing among pavements.[17] Before they turned their attention to other projects, Stanton and Lasseter thought about having WALL-E fall in love, as it was the necessary progression away from loneliness.[14] Stanton started writing WALL-E again in 2002 while completing Finding Nemo.[18] Stanton formatted his script in a manner reminiscent of Dan O'Bannon's Alien. O'Bannon wrote his script in a manner Stanton found reminded him of haiku, where visual descriptions were done in continuous lines of a few words. Stanton wrote his robot dialogue conventionally, but placed them in brackets.[11] In late 2003, Stanton and a few others created a story reel of the first twenty minutes of the film. Lasseter and Steve Jobs were impressed and officially began development,[19] though Jobs stated he did not like the title, originally spelled "W.A.L.-E."[20]
While the first act of WALL-E "fell out of the sky" for Stanton,[14] he had originally wanted aliens to plant EVE to explore Earth and the rest of the film was very different. When WALL-E comes to the Axiom, he incites a Spartacus-style rebellion by the robots against the remnants of the human race, which were cruel alien Gels (completely devolved, gelantinous, boneless, legless, see-through, green creatures that resemble Jell-O). James Hicks, a physiologist, mentioned to Stanton the concept of atrophy and the effects prolonged weightlessness would have on humans living in space for an inordinately extended time period.[7][21][22] Therefore, this was the inspiration of the humans degenerating into the alien Gels,[23] and their ancestry would have been revealed in a Planet of the Apes-style ending.[24] The Gels also spoke a made-up gibberish language, but Stanton scrapped this idea because he thought it would be too complicated for the audience to understand and they could easily be driven off from the storyline.[25] The Gels had a royal family, who host a dance in a castle on a lake in the back of the ship, and the Axiom curled up into a ball when returning to Earth in this incarnation of the story.[25] Stanton decided this was too bizarre and unengaging, and conceived humanity as "big babies".[24] Stanton developed the metaphorical theme of the humans learning to stand again and "grow[ing] up",[24][26] wanting WALL-E and EVE's relationship to inspire humanity because he felt very few films explore how utopian societies come to exist.[27] The process of depicting the descendants of humanity as the way they appear in the movie was very slow. Stanton first decided to put a nose and ears on the Gels so the audience could recognize them. Eventually, fingers, legs, clothes, and other characteristics were added until they arrived at the concept of being fetus-like to allow the audience to see themselves in the characters.[25]
In a later version of the film, Auto comes to the docking bay to retrieve EVE's plant. The film would have its first cutaway to the captain, but Stanton moved that as he found it too early to begin moving away from WALL-E's point-of-view. As a homage to Get Smart,[28] Auto takes the plant and goes into the bowels of the ship into a room resembling a brain where he watches videos of Buy n Large's scheme to clean up the Earth falling apart through the years. Stanton removed this to keep some mystery as to why the plant is taken from EVE. The captain appears to be unintelligent, but Stanton wanted him to just be unchallenged; otherwise he would have been unempathetic.[23] One example of how unintelligent the captain was depicted initially is that he was seen to wear his hat upside-down, only to fix it before he challenges Auto. In the finished film, he merely wears it casually atop his head, tightening it when he assumes real command of the Axiom.[25]
Originally, EVE would have been electrocuted by Auto, and then be quickly saved from ejection at the hands of the WALL-A robots by WALL-E. He would have then revived her by replacing her power unit with a cigarette lighter he brought from Earth. Stanton reversed this following a 2007 test screening, as he wanted to show EVE replacing her directive of bringing the plant to the captain with repairing WALL-E, and it made WALL-E even more heroic if he held the holo-detector open despite being badly hurt. Stanton also moved the moment where WALL-E reveals his plant (which he had snatched from the self-destructing escape pod) from producing it from a closet to immediately after his escape, as it made EVE happier and gave them stronger motivation to dance around the ship.[23] Stanton felt half the audience at the screening believed the humans would be unable to cope with living on Earth and would have died out after the film's end. Jim Capobianco, director of the short film Your Friend the Rat, created an end credits animation that continued the story  and stylized in different artistic movements throughout history  to clarify an optimistic tone.[29]
WALL-E was the most complex Pixar production since Monsters, Inc. because of the world and the history that had to be conveyed.[10] Whereas most Pixar films have up to 75,000 storyboards, WALL-E required 125,000.[30] Production designer Ralph Eggleston wanted the lighting of the first act on Earth to be romantic, while the second act on the Axiom to be cold and sterile. During the third act, the romantic lighting is slowly introduced into the Axiom environment.[7] Pixar studied Chernobyl and the city of Sofia to create the ruined world; art director Anthony Christov was from Bulgaria and recalled Sofia used to have problems storing its garbage.[31][32] Eggleston bleached out the whites on Earth to make WALL-E feel vulnerable. The overexposed light makes the location look more vast. Because of the haziness, the cubes making up the towers of garbage had to be very large, otherwise they would have lost shape (in turn, this helped save rendering time). The dull tans of Earth subtly become soft pinks and blues when EVE arrives. When WALL-E shows EVE all his collected items, all the lights he has collected light up to give an inviting atmosphere, like a Christmas tree. Eggleston tried to avoid the colors yellow and green so WALL-E  who was made yellow to emulate a tractor  would not blend into the deserted Earth, and to make the plant more prominent.[33]
Stanton also wanted the lighting to look realistic and evoke the science fiction films of his youth. He felt Pixar had captured the physics of being underwater with Finding Nemo, so for WALL-E he wanted to push that for air. It was while rewatching some of his favorite science fiction films he realized Pixar's films lacked the look of 70 mm film and its barrel distortion, lens flare and racking focus.[10] Producer Jim Morris invited Roger Deakins and Dennis Muren to advise on lighting and atmosphere. Muren spent several months with Pixar, while Deakins hosted one talk and was requested to stay on for another two weeks. Stanton said Muren's experience came from integrating computer animation into live-action settings, while Deakins helped them understand not to overly complicate their camerawork and lighting.[27] 1970s Panavision cameras were used to help the animators understand and replicate handheld imperfections like unfocused backgrounds in digital environments.[7] The first lighting test consisted of building a three-dimensional replica of WALL-E, filming it with a 70mm camera, and then trying to replicate that in the computer.[34] Stanton cited the shallow lens work of Gus Van Sant's films as an influence, as it created intimacy in each close-up. Stanton chose angles for the virtual cameras that a live-action filmmaker would choose if filming on a set.[14]
Stanton wanted the Axiom's interior to resemble Shanghai and Dubai.[10] Eggleston studied 1960s NASA paintings and the original concept art for Tomorrowland for the Axiom, to reflect that era's sense of optimism.[7] Stanton remarked "We are all probably very similar in our backgrounds here [at Pixar] in that we all miss the Tomorrowland that was promised us from the heyday of Disneyland," and wanted a "jet pack" feel.[10] Pixar also studied the Disney Cruise Line and visited Las Vegas, which was helpful in understanding artificial lighting.[7] Eggleston based his Axiom designs on the futuristic architecture of Santiago Calatrava. Eggleston divided the inside of the ship into three sections; the rear's economy class has a basic gray concrete texture with graphics keeping to the red, blue and white of the BnL logo. The coach class with living/shopping spaces has 'S' shapes as people are always looking for "what's around the corner". Stanton intended to have many colorful signs, but he realized this would overwhelm the audience and went with Eggleston's original idea of a small number of larger signs. The premier class is a large Zen-like spa with colors limited to turquoise, cream and tan, and leads on to the captain's warm carpeted and wooded quarters and the sleek dark bridge.[33] In keeping with the artificial Axiom, camera movements were modeled after those of the steadicam.[35]
The use of live action was a stepping stone for Pixar, as Stanton was planning to make John Carter of Mars his next project.[10] Storyboarder Derek Thompson noted introducing live action meant they had to make the rest of the film look even more realistic.[36] Eggleston added that if the historical humans had been animated and slightly caricaturized, then the audience would not have recognized how serious their devolution was.[33] Stanton cast Fred Willard as the historical Buy n Large CEO because "He's the most friendly and insincere car salesman I could think of."[24] The CEO says "stay the course," which Stanton used because he thought it was funny.[37] Industrial Light & Magic did the visual effects for these shots.[7]
WALL-E went undeveloped during the 1990s partly because Stanton and Pixar were not confident enough yet to have a feature length film with a main character that behaved like Luxo Jr. or R2-D2.[11] Stanton explained there are two types of robots in cinema: "human[s] with metal skin", like the Tin Man, or "machine[s] with function" like Luxo and R2. He found the latter idea "powerful" because it allowed the audience to project personalities onto the characters, as they do with babies and pets: "You're compelled ... you almost can't stop yourself from finishing the sentence 'Oh, I think it likes me! I think it's hungry! I think it wants to go for a walk!'"[38] He added, "We wanted the audience to believe they were witnessing a machine that has come to life."[7] The animators visited recycling stations to study machinery, and also met robot designers, visited NASA's Jet Propulsion Laboratory to study robots, watched a recording of a Mars rover,[18] and borrowed a bomb detecting robot from the San Francisco Police Department. Simplicity was preferred in their performances as giving them too many movements would make them feel human.[7]
Stanton wanted WALL-E to be a box and EVE to be like an egg.[39] WALL-E's eyes were inspired by a pair of binoculars Stanton was given when watching the Oakland Athletics play against the Boston Red Sox. He "missed the entire inning" because he was distracted by them.[40] The director was reminded of Buster Keaton and decided the robot would not need a nose or mouth.[41] Stanton added a zoom lens to make WALL-E more sympathetic.[41] Ralph Eggleston noted this feature gave the animators more to work with and gave the robot a child-like quality.[33] Pixar's studies of trash compactors during their visits to recycling stations inspired his body.[7] His tank treads were inspired by a wheelchair someone had developed that used treads instead of wheels.[39] The animators wanted him to have elbows, but realized this was unrealistic because he is only designed to pull garbage into his body.[7] His arms also looked very flimsy when they did a test of him waving.[39] Animation director Angus MacLane suggested they attach his arms to a track on the sides of his body to move them around, based on the inkjet printers his father designed. This arm design contributed to creating the character's posture, so if they wanted him to be nervous, they would lower them.[42] Stanton was unaware of the similarities between WALL-E and Johnny 5 from Short Circuit until others pointed it out to him.[11]
Stanton wanted EVE to be at the higher end of technology, and asked iPod designer Jonathan Ive to inspect her design. He was very impressed.[10] Her eyes are modelled on Lite-Brite toys,[41] but Pixar chose not to make them overly expressive as it would be too easy to have her eyes turn into hearts to express love or something similar.[39] Her limited design meant the animators had to treat her like a drawing, relying on posing her body to express emotion.[7] They also found her similar to a manatee or a narwhal because her floating body resembled an underwater creature.[39] Auto was a conscious homage to HAL 9000 from 2001: A Space Odyssey, and the usage of Also sprach Zarathustra for the showdown between the captain and Auto furthers that.[11][not in citation given] The manner in which he hangs from a wall gives him a threatening feel, like a spider.[43] Originally, Auto was designed entirely differently, resembling EVE, but masculine and authoritative; the Steward robots were also more aggressive Patrol-bots.[23] The majority of the robot cast were formed with the Build-a-bot program, where different heads, arms and treads were combined together in over a hundred variations.[7] The humans were modelled on sea lions due to their blubbery bodies,[33] as well as babies. The filmmakers noticed baby fat is a lot tighter than adult fat and copied that texture for the film's humans.[44]
To animate their robots, the film's story crew and animation crew watched a Keaton and a Charlie Chaplin film every day for almost a year, and occasionally a Harold Lloyd picture.[11] Afterwards, the filmmakers knew all emotions could be conveyed silently. Stanton cited Keaton's "great stone face" as giving them perseverance in animating a character with an unchanging expression.[41] As he rewatched these, Stanton felt that filmmakers  since the advent of sound  relied on dialogue too much to convey exposition.[11] The filmmakers dubbed the cockroach WALL-E keeps as a pet "Hal", in reference to silent film producer Hal Roach (as well as being an additional reference to HAL 9000).[7] They also watched 2001: A Space Odyssey, The Black Stallion and Never Cry Wolf, films that had sound but were not reliant on dialogue.[36] Stanton acknowledged Silent Running as an influence because its silent robots were a forerunner to the likes of R2-D2,[27] and that the "hopeless romantic" Woody Allen also inspired WALL-E.[15]
Producer Jim Morris recommended Ben Burtt as sound designer for WALL-E because Stanton kept using R2-D2 as the benchmark for the robots.[28] Burtt had completed Star Wars Episode III: Revenge of the Sith and told his wife he would no longer work on films with robots, but found WALL-E and its substitution of voices with sound "fresh and exciting".[7] He recorded 2500 sounds for the film, which was twice the average number for a Star Wars film,[18] and a record in his career.[7] Burtt began work in 2005,[45] and experimented with filtering his voice for two years.[46] Burtt described the robot voices as "like a toddler [...] universal language of intonation. 'Oh', 'Hm?', 'Huh!', you know?"[47]
During production Burtt had the opportunity to look at the items used by Jimmy MacDonald, Disney's in-house sound designer for many of their classic films. Burtt used many of MacDonald's items on WALL-E. Because Burtt was not simply adding sound effects in post-production, the animators were always evaluating his new creations and ideas, which Burtt found an unusual experience.[48] He worked in sync with the animators, returning their animation after adding the sounds to give them more ideas.[7] Burtt would choose scientifically-accurate sounds for each character, but if he could not find one that worked, he would choose a dramatic if unrealistic noise.[48] Burtt would find hundreds of sounds by looking at concept art of characters, before he and Stanton pared it down to a distinct few for each robot.[10]
Burtt saw a hand-cranked electrical generator while watching Island in the Sky, and bought an identical, unpacked device from 1950 on eBay to use for WALL-E moving around.[49] Burtt also used an automobile self starter for when WALL-E goes fast,[48] and the sound of cars being wrecked at a demolition derby provided for WALL-E's compressing trash in his body.[50] The Macintosh computer chime was used to signify when WALL-E has fully recharged his battery. For EVE, Burtt wanted her humming to have a musical quality.[48] Burtt was only able to provide neutral or masculine voices, so Pixar employee Elissa Knight was asked to provide her voice for Burtt to electronically modify. Stanton deemed the sound effect good enough to properly cast her in the role.[37] Burtt recorded a flying 10-foot-long (3.0m) radio-controlled jet plane for EVE's flying,[7] and for her plasma cannon, Burtt hit a slinky hung from a ladder with a timpani stick. He described it as a "cousin" to the blaster noise from Star Wars.[51]
MacInTalk was used because Stanton "wanted Auto to be the epitome of a robot, cold, zeros & ones, calculating, and soulless [and] Stephen Hawking's kind of voice I thought was perfect."[27] Additional sounds for the character were meant to give him a clockwork feel, to show he is always thinking and calculating.[48]
Burtt had visited Niagara Falls in 1987 and used his recordings from his trip for the sounds of wind.[50] He ran around a hall with a canvas bag up to record the sandstorm though.[7] For the scene where WALL-E runs from falling shopping carts, Burtt and his daughter went to a supermarket and placed a recorder in their cart. They crashed it around the parking lot and then let it tumble down a hill.[52] To create Hal (WALL-E's pet cockroach)'s skittering, he recorded the clicking caused by taking apart and reassembling handcuffs.[7]
Thomas Newman recollaborated with Stanton on WALL-E since the two got along well on Nemo, which gave Newman the Annie Award for Best Music in an Animated Feature. He began writing the score in 2005, in the hope that starting this task early would make him more involved with the finished film. But, Newman remarked that animation is so dependent on scheduling he should have begun work earlier on when Stanton and Reardon were writing the script. EVE's theme was arranged for the first time in October 2007. Her theme when played as she first flies around Earth originally used more orchestral elements, and Newman was encouraged to make it sound more feminine.[53] Newman said Stanton had thought up of many ideas for how he wanted the music to sound, and he generally followed them as he found scoring a partially silent film difficult. Stanton wanted the whole score to be orchestral, but Newman felt limited by this idea especially in scenes aboard the Axiom, and used electronics too.[54]
Stanton originally wanted to juxtapose the opening shots of space with 1930s French swing music, but he saw The Triplets of Belleville (2003) and did not want to appear as if he were copying it. Stanton then thought about the song "Put On Your Sunday Clothes" from Hello, Dolly!, since he had portrayed the sidekick Barnaby Tucker in a 1980 high school production.[55] Stanton found that the song was about two naive young men looking for love, which was similar to WALL-E's own hope for companionship. Jim Reardon suggested WALL-E find the film on video, and Stanton included "It Only Takes a Moment" and the clip of the actors holding hands, because he wanted a visual way to show how WALL-E understands love and conveys it to EVE. Hello Dolly! composer Jerry Herman allowed the songs to be used without knowing what for; when he saw the film, he found its incorporation into the story "genius".[56] Coincidentally, Newman's uncle Lionel worked on Hello, Dolly![7]
Newman travelled to London to compose the end credits song "Down to Earth" with Peter Gabriel, who was one of Stanton's favorite musicians. Afterwards, Newman rescored some of the film to include the song's composition, so it would not sound intrusive when played.[7] Louis Armstrong's rendition of "La Vie en rose" was used for a montage where WALL-E does not get EVE's attention on Earth. The script also specified using Bing Crosby's "Stardust" for when the two robots dance around the Axiom,[9] but Newman asked if he could score the scene himself. A similar switch occurred for the sequence in which WALL-E attempts to wake EVE up through various means; originally, the montage would play with the instrumental version of "Raindrops Keep Fallin' on My Head", but Newman wanted to challenge himself and scored an original piece for the sequence.[57]
This movie is widely recognized as a critique on society. It brings up very real issues that the world, and especially densely populated areas, are dealing with today and even more so in the future. Katherine Ellison asserts that Americans produce nearly 400 million tons of solid waste per year but recycle less than a third of it, according to a recent Columbia University study. Landfills are filling up so quickly that the UK may run out of landfill space by the year 2017. [58]
Because WALL-E overtly critiques consumerism, it also critiques Disneys production values and aesthetic, without being too obvious. [6]
In WALL-E: from environmental adaption to sentimental nostalgia, Robin Murray and Joseph Heuman explain the important theme of nostalgia in this film. Nostalgia is clearly represented by human artifacts, left behind, that WALL-E collects and cherishes, i.e. Zippo lighters, hubcaps, and plastic sporks. These modern items that we use out of necessity, are made sentimental through the lens of the bleak future of Earth. Nostalgia is also expressed through the musical score, as the film opens with a camera shot of outer space that slowly zooms in to a waste filled Earth while playing Put on Your Sunday Clothes, reflecting on simpler and happier times in human history. This film also expresses nostalgia through the longing of nature and the natural world, as it is the site and feeling of soil, and the plant brought back to the space ship by EVE, that make the captain decide it is time for humans to move back to Earth. WALL-E expresses nostalgia also, by reflecting on heterosexual romantic themes of older Disney and silent films. [6]
Stanton describes the theme of the film as "irrational love defeats life's programming":[24]
I realized the point I was trying to push with these two programmed robots was the desire for them to try and figure out what the point of living was... It took these really irrational acts of love to sort of discover them against how they were built... I realized that that's a perfect metaphor for real life. We all fall into our habits, our routines and our ruts, consciously or unconsciously to avoid living. To avoid having to do the messy part. To avoid having relationships with other people. of dealing with the person next to us. That's why we can all get on our cell phones and not have to deal with one another. I thought, 'That's a perfect amplification of the whole point of the movie.' I wanted to run with science in a way that would sort of logically project that.[24]
Stanton noted many commentators placed emphasis on the environmental aspect of humanity's complacency in the film, because "that disconnection is going to be the cause, indirectly, of anything that happens in life that's bad for humanity or the planet".[59] Stanton said that by taking away effort to work, the robots also take away humanity's need to put effort into relationships.[43] Christian journalist Rod Dreher saw technology as the complicated villain of the film. The humans' artificial lifestyle on the Axiom has separated them from nature, making them "slaves of both technology and their own base appetites, and have lost what makes them human". Dreher contrasted the hardworking, dirt covered WALL-E with the sleek clean robots on the ship. However, it is the humans and not the robots who make themselves redundant, and during the end credits humans and robots are shown working alongside each other to renew the Earth. "WALL-E is not a Luddite film," he said. "It doesn't demonize technology. It only argues that technology is properly used to help humans cultivate their true nature  that it must be subordinate to human flourishing, and help move that along."[60]
Stanton, who is Christian,[12] named EVE after the Biblical character because WALL-E's loneliness reminded him of Adam, before God created his wife.[61] Dreher noted EVE's biblical namesake and saw her directive as an inversion of that story; EVE uses the plant to tell humanity to return to Earth and move away from the "false god" of BnL and the lazy lifestyle it offers. Dreher also noted this departure from classical Christian viewpoints, where Adam is cursed to labor, in that WALL-E argues hard work is what makes humans human. Dreher emphasized the false god parallels to BnL in a scene where a robot teaches infants "B is for Buy n Large, your very best friend", which he compared to modern corporations such as McDonald's creating brand loyalty in children.[60] Megan Basham of World magazine felt the film criticizes the pursuit of leisure, whereas WALL-E in his stewardship learns to truly appreciate God's creation.[12]
During writing, a Pixar employee noted to Jim Reardon that EVE was reminiscent of the dove with the olive branch from the story of Noah's Ark, and the story was reworked with EVE finding a plant to return humanity from its voyage.[62] WALL-E himself has been compared to Prometheus,[28] Sisyphus,[60] and Butades: in an essay discussing WALL-E as representative of the artistic strive of Pixar itself, Hrag Vartanian compared WALL-E to Butades in a scene where the robot expresses his love for EVE by making a sculpture of her from spare parts. "The Ancient Greek tradition associates the birth of art with a Corinthian maiden who longing to preserve her lovers shadow traces it on the wall before he departed for war. The myth reminds us that art was born out of longing and often means more for the creator than the muse. In the same way Stanton and his Pixar team have told us a deeply personal story about their love of cinema and their vision for animation through the prism of all types of relationships."[63]
Continuing a Pixar tradition, WALL-E was paired with a short film for its theatrical release, Presto. The film was dedicated to Justin Wright (19812008), a Pixar animator who had worked on Ratatouille and died of a heart attack before WALL-E's release.[7]
Walt Disney Imagineering (WDI) built animatronic WALL-Es to promote the picture, which made appearances at Disneyland Resort;[64] the Franklin Institute; the Miami Science Museum; the Seattle Center; and the Tokyo International Film Festival.[65] Due to safety concerns, the 318kg robots were always strictly controlled and WDI always needed to know exactly what they were required to interact with. For this reason, they generally refused to have their puppets meet and greet children at the theme parks in case a WALL-E trod on a child's foot. Those who wanted to take a photograph with the character had to make do with a cardboard cutout.[66]
Very small quantities of merchandise were sold for WALL-E, as Cars items were still popular, and many manufacturers were more interested in Speed Racer, which was a successful line despite the film's failure at the box office. Thinkway, which created the WALL-E toys, had previously made Toy Story dolls when other toy producers had not shown an interest.[65] Among Thinkway's items were a WALL-E that danced when connected to a music player, a toy that could be taken apart and reassembled, and a groundbreaking remote control toy of him and EVE that had motion sensors that allowed them to interact with players.[67] There were even plushies.[68] The "Ultimate WALL-E" figures were not in stores until the film's home release in November 2008,[65] at a retail price of almost $200, leading The Patriot-News to deem it an item for "hard-core fans and collectors only".[67]
WALL-E grossed $223,808,164 in the USA and Canada and $297,503,696 overseas for a worldwide total of $521,311,860, marking it the ninth highest grossing film of 2008.[2]
The film premiered at the Greek Theatre in Los Angeles on June 23, 2008.[69]
In the USA and Canada, it opened in 3,992 theaters on June 27, 2008. During its opening weekend, it topped the box office with $63,087,526[70] which is currently the fifth-best opening weekend for a Pixar film[71] and the fourth-best opening among films released in June.[72] The movie earned $94.7 million in its first week and crossed the $200 million mark during its sixth weekend.[73]
Countries where it grossed over $10 million are the following: Japan ($44,005,222), UK, Ireland and Malta ($41,215,600), France and the Maghreb region ($27,984,103), Germany ($24,130,400), Mexico ($17,679,805), Spain ($14,973,097), Australia ($14,165,390), Italy ($12,210,993) and Russia and the CIS ($11,694,482).[74]
The film was released by Walt Disney Studios Home Entertainment on DVD and Blu-ray Disc on November 18, 2008. The various editions included Presto, a new short film BURN-E, the Leslie Iwerks documentary film The Pixar Story, shorts about the history of Buy n Large, the behind-the-scenes special features and a Digital Copy of the film that can be played through iTunes or Windows Media and compatible devices.[75] It sold 9,042,054 DVD units ($142,633,974) in total becoming the second best-selling animated DVD among those released in 2008 in terms of units sold (behind Kung Fu Panda), the best-selling animated feature in terms of sales revenue and the 3rd best-selling among all 2008 DVDs.[76]
WALL-E was met with overwhelmingly positive reviews from critics.[77] Rotten Tomatoes reported that 96% of critics gave the film positive reviews, based upon a sample of 233 reviews, with an average rating of 8.5/10.[78] At Metacritic, which assigns a normalized rating out of 100 to reviews from mainstream critics, the film has received an average score of 94, based on 39 reviews, which represents "universal acclaim".[77] indieWire named WALL-E the third best film of the year, based on their annual survey of 100 film critics, while Movie City News shows that WALL-E appeared in 162 different top ten lists, out of 286 different critics lists surveyed, the most mentions on a top ten list of any film released in 2008.[79]
Richard Corliss of Time named WALL-E as his favorite film of 2008 (and later of the decade), noting the film succeeded in "connect[ing] with a huge audience" despite the main characters' lack of speech and "emotional signifiers like a mouth, eyebrows, shoulders [and] elbows". It "evoke[d] the splendor of the movie past" and he also compared WALL-E and EVE's relationship to the chemistry of Spencer Tracy and Katharine Hepburn.[80] Other critics who named WALL-E as their favorite film of 2008 included Tom Charity of CNN,[81] Michael Phillips of the Chicago Tribune, Lisa Schwarzbaum of Entertainment Weekly, A. O. Scott of The New York Times, Christopher Orr of The New Republic, Ty Burr and Wesley Morris of The Boston Globe, Joe Morgenstern of The Wall Street Journal, and Anthony Lane of The New Yorker.[82]
Todd McCarthy of Variety called the film "Pixar's ninth consecutive wonder", saying it was imaginative yet straightforward. He said it pushed the boundaries of animation by balancing esoteric ideas with more immediately accessible ones, and that the main difference between the film and other science fiction projects rooted in an apocalypse was its optimism.[83] Kirk Honeycutt of The Hollywood Reporter declared that WALL-E surpassed the achievements of Pixar's previous eight features and probably their most original film to date. He said it had the "heart, soul, spirit and romance" of the best silent films. Honeycutt said the film's definitive stroke of brilliance was in using a mix of archive film footage and computer graphics to trigger WALL-E's romantic leanings. He praised Burtt's sound design, saying "If there is such a thing as an aural sleight of hand, this is it."[84]
Roger Ebert writing in the Chicago Sun-Times found WALL-E "an enthralling animated film, a visual wonderment, and a decent science-fiction story". Ebert said the scarcity of dialogue would allow it to "cross language barriers" in a manner appropriate to the global theme, and noted it would appeal to adults and children. He praised the animation, saying the color palette was "bright and cheerful [...] and a little bit realistic", and that Pixar managed to generate a "curious" regard for the WALL-E, comparing his "rusty and hard-working and plucky" design favorably to more obvious attempts at creating "lovable" lead characters. He said WALL-E was concerned with ideas rather than spectacle, saying it would trigger stimulating "little thoughts for the younger viewers."[85] He named it as one of his twenty favorite films of 2008 and argued it was "the best science-fiction movie in years".[86]
The film was interpreted as tackling a topical, ecologically-minded agenda,[78] though McCarthy said it did so with a lightness of touch that granted the viewer the ability to accept or ignore the message.[83] Kyle Smith of the New York Post, wrote that by depicting future humans as "a flabby mass of peabrained idiots who are literally too fat to walk", WALL-E was darker and more cynical than any major Disney feature film he could recall. He compared the humans to the patrons of Disney's Parks and Resorts, adding, "I'm also not sure I've ever seen a major corporation spend so much money to issue an insult to its customers."[87] Maura Judkis of U.S. News & World Report questioned whether this depiction of "frighteningly obese humans" would resonate with children and make them prefer to "play outside rather than in front of the computer, to avoid a similar fate".[88] The interpretation led to criticism of the film by conservative commentators such as Glenn Beck, and contributors to National Review Online including Shannen W. Coffin and Jonah Goldberg (although he admitted it was a "fascinating" and occasionally "brilliant" production).[89]
A few notable critics have argued that the film is vastly overrated,[90] claiming it failed to "live up to such blinding, high-wattage enthusiasm",[91] and that there were "chasms of boredom watching it", in particular "the second and third acts spiraled into the expected".[92] Other labels included "preachy"[90] and "too long".[91]
Child reviews sent into CBBC were mixed, some citing boredom and an inadequate storyline.[93]
Patrick J. Ford of The American Conservative said WALL-E's conservative critics missed lessons in the film that he felt appealed to traditional conservatism. He argued that the mass consumerism in the film was not shown to be a product of big business, but of too close a tie between big business and big government: "The government unilaterally provided its citizens with everything they needed, and this lack of variety led to Earth's downfall." Responding to Coffin's claim that the film points out the evils of mankind, Ford argued the only evils depicted were those that resulted from losing touch with our own humanity and that fundamental conservative representations such as the farm, the family unit, and wholesome entertainment were in the end held aloft by the human characters. He concluded, "By steering conservative families away from WALL-E, these commentators are doing their readers a great disservice."[94]
Director Terry Gilliam praised the film as "A stunning bit of work. The scenes on what was left of planet Earth are just so beautiful: one of the great silent movies. And the most stunning artwork! It says more about ecology and society than any live action film  all the people on their loungers floating around, brilliant stuff. Their social comment was so smart and right on the button."[95]
WALL-E won the Academy Award for Best Animated Feature and was nominated for Best Original Screenplay, Best Original Score, Best Original Song, Sound Editing, and Sound Mixing at the 81st Academy Awards, which it lost to Slumdog Millionaire, The Dark Knight and Milk, respectively.[96][97] Walt Disney Pictures also pushed for an Academy Award for Best Picture nomination,[98] but it was not nominated, provoking controversy as to whether the Academy deliberately restricted WALL-E to the Best Animated Feature category.[99] Peter Travers commented that "If there was ever a time where an animated feature deserved to be nominated for best picture it's Wall-E."[100] Only three animated films, 1991's Beauty and the Beast and Pixar's next two films, 2009's Up and 2010's Toy Story 3, have ever been nominated for the Academy Award for Best Picture. A reflective Stanton stated he was not disappointed the film was restricted to the Best Animated Film nomination because he was overwhelmed by the film's positive reception, and eventually "The line [between live-action and animation] is just getting so blurry that I think with each proceeding year, it's going to be tougher and tougher to say what's an animated movie and what's not an animated movie."[17]
WALL-E made a healthy appearance at the various 2008 end-of-the-year awards circles, particularly in the Best Picture category, where animated films are often overlooked. It has won the award, or the equivalent of it, from the Boston Society of Film Critics (tied with Slumdog Millionaire),[101] the Chicago Film Critics Association,[102] the Central Ohio Film Critics awards,[103] the Online Film Critics Society,[104] and most notably the Los Angeles Film Critics Association, where it became the first animated feature to win the prestigious award.[105] It was named as one of 2008's ten best films by the American Film Institute and the National Board of Review of Motion Pictures.[106][107]
It won Best Animated Feature Film at the 66th Golden Globe Awards, 81st Academy Awards and the Broadcast Film Critics Association Awards 2008.[108][109] It was nominated for several awards at the 2009 Annie Awards, including Best Feature Film, Animated Effects, Character Animation, Direction, Production design, Storyboarding and Voice acting (for Ben Burtt);[110] but was beaten out by Kung Fu Panda in every category.[111] It won Best Animated Feature at the 62nd British Academy Film Awards, and was also nominated there for Best Music and Sound.[112] Thomas Newman and Peter Gabriel won two Grammy Awards for "Down to Earth" and "Define Dancing".[113] It won all three awards it was nominated for by the Visual Effects Society: Best Animation, Best Character Animation (for WALL-E and EVE in the truck) and Best Effects in the Animated Motion Picture categories.[114] It became the first animated film to win Best Editing for a Comedy or Musical from the American Cinema Editors.[115] In 2009, Stanton, Reardon and Docter won Nebula Award, beating The Dark Knight and the Stargate Atlantis episode "The Shrine".[116][117] It won Best Animated Film and was nominated for Best Director at the Saturn Awards.[118]
At the British National Movie Awards, which is voted for by the public, it won Best Family Film.[119] It was also voted Best Feature Film at the British Academy Children's Awards.[120] WALL-E was listed at #63 on Empire's online poll of the 100 greatest movie characters, conducted in 2008.[121] In early 2010, TIME ranked WALL-E #1 in "Best Movies of the Decade".[5]
 
June 23, 2008(2008-06-23) (Los Angeles)
June 27, 2008(2008-06-27)
1 Plot
2 Cast and characters
3 Production

3.1 Writing
3.2 Design
3.3 Animation
3.4 Sound
3.5 Music


3.1 Writing
3.2 Design
3.3 Animation
3.4 Sound
3.5 Music
4 Themes

4.1 Environment and Waste
4.2 Technology
4.3 Christianity


4.1 Environment and Waste
4.2 Technology
4.3 Christianity
5 Reception

5.1 Release
5.2 Box-office performance
5.3 Home media
5.4 Reviews


5.1 Release
5.2 Box-office performance
5.3 Home media
5.4 Reviews
6 Accolades
7 See also
8 References
9 Further reading
10 External links
3.1 Writing
3.2 Design
3.3 Animation
3.4 Sound
3.5 Music
4.1 Environment and Waste
4.2 Technology
4.3 Christianity
5.1 Release
5.2 Box-office performance
5.3 Home media
5.4 Reviews
Ben Burtt produced the voice of WALL-E (Waste Allocation Load Lifter  Earth Class), the title character. WALL-E, a robot who has developed sentience, is the only robot of "his" kind shown to be still functioning on Earth. He is a small mobile compactor box with all-terrain treads, three-fingered shovel hands, binocular eyes, and retractable solar cells for power. He collects spare parts for himself, which becomes pivotal to the plot, and replaces broken and/or worn out parts on-the-fly by cannibalizing "dead" WALL-Es. Although working diligently to fulfill his directive to clean up the garbage (all the while accompanied by his cockroach friend Hal and music playing from his on-board recorder) he is distracted by his curiosity, collecting trinkets of interest. He stores and displays these "treasures" such as a birdcage full of rubber ducks, a Rubik's Cube, Zippo lighters, disposable cups filled with plastic cutlery and a golden trophy at his home where he examines and categorizes his finds while watching video cassettes of musicals via an iPod viewed through a large Fresnel lens.

Burtt is also credited for the voice of M-O (Microbe Obliterator), as well as most of the other robots. M-O is a tiny, obsessive compulsive maintenance robot with rollers for hands who keeps Axiom clean. When M-O meets WALL-E and sees how filthy he is, he deviates from his normal routine and follows WALL-E, cleaning up behind him. When he follows WALL-E to the garbage bay, he inadvertently but fortuitously saves WALL-E and EVE from being blown into the vacuum of space. He then forms a close friendship with Wall-E and aids the two in retrieving the plant, most notably through using his contaminant detecting vision when Wall-E drops the plant. Back on Earth, he ushers the other robots into giving WALLE and EVE some privacy as they share a tender moment.


Burtt is also credited for the voice of M-O (Microbe Obliterator), as well as most of the other robots. M-O is a tiny, obsessive compulsive maintenance robot with rollers for hands who keeps Axiom clean. When M-O meets WALL-E and sees how filthy he is, he deviates from his normal routine and follows WALL-E, cleaning up behind him. When he follows WALL-E to the garbage bay, he inadvertently but fortuitously saves WALL-E and EVE from being blown into the vacuum of space. He then forms a close friendship with Wall-E and aids the two in retrieving the plant, most notably through using his contaminant detecting vision when Wall-E drops the plant. Back on Earth, he ushers the other robots into giving WALLE and EVE some privacy as they share a tender moment.
Elissa Knight as EVE (Extraterrestrial Vegetation Evaluator), a sleek robot probe whose directive is to locate vegetation on Earth and verify habitability. She has a glossy white egg-shaped body and blue LED eyes. She moves using antigravity technology and is equipped with scanners, specimen storage and a plasma cannon in her arm, which she is quick to use. When first deployed on Earth she appears devoid of feeling but as the craft that delivered her blasts off and away she springs to life with gleeful flight. Watching her, WALL-E accidentally draws her attention as she sets about following her directive growing ever more impatient with both her lack of success and with WALL-E's constant monitoring. This shared strength of feeling soon connects the two characters.
Jeff Garlin as Captain B. McCrea, the commander, and apparently only, officer on the Axiom. His duties as captain are boring daily routines, with the ship's autopilot handling all true command functions. Meeting WALL-E, however, sparks his interest in Earth and he becomes engrossed in researching the home planet, paving the way for his retaking control of the ship back from the Autopilot.
Fred Willard as Shelby Forthright, historical CEO of the Buy n Large Corporation, shown only in videos recorded around the time of the Axiom's initial launch. Constantly optimistic, Forthright proposed the evacuation plans, then to clean up and recolonize the planet. However, the corporation gave up after realizing how toxic Earth had become. Forthright is the only live action character with a speaking role, the first in any Pixar film.
MacInTalk, the text-to-speech program for the Apple Macintosh, was used for the voice of Auto, the rogue autopilot artificial intelligence built into the ship. Unlike other robots in the film, Auto is not influenced by WALL-E, instead following directive A113, which is to prevent the Axiom and the humans from returning to Earth because of the toxicity, and he will prevent anyone from deviating from it. The robot's design is a homage to HAL 9000 from 2001: A Space Odyssey, featuring a HAL-style red "eye" in the center of his body.[citation needed]
John Ratzenberger and Kathy Najimy as John and Mary, respectively. John and Mary both live on the Axiom and are so dependent on their personal video screens and automatic services that they are oblivious to their surroundings, for instance not noticing that the ship features a giant swimming pool. However, they are brought out of their trances after separate encounters with WALL-E, eventually meeting face-to-face for the first time.
Sigourney Weaver as the voice of the Axiom's computer. Stanton joked about the role with Weaver, saying, "You realize you get to be 'Mother' now?"[7][8] referring to the name of the ship's computer in the film Alien, which also starred Weaver.[8]
Burtt is also credited for the voice of M-O (Microbe Obliterator), as well as most of the other robots. M-O is a tiny, obsessive compulsive maintenance robot with rollers for hands who keeps Axiom clean. When M-O meets WALL-E and sees how filthy he is, he deviates from his normal routine and follows WALL-E, cleaning up behind him. When he follows WALL-E to the garbage bay, he inadvertently but fortuitously saves WALL-E and EVE from being blown into the vacuum of space. He then forms a close friendship with Wall-E and aids the two in retrieving the plant, most notably through using his contaminant detecting vision when Wall-E drops the plant. Back on Earth, he ushers the other robots into giving WALLE and EVE some privacy as they share a tender moment.
Freedom Project
Hauser, Tim (2008). The Art of WALL-E. Chronicle Books. p.160. ISBN978-0-8118-6235-6.
Official website
WALL-E at the Internet Movie Database
WALL-E at the Big Cartoon DataBase
WALL-E at AllRovi
WALL-E at Box Office Mojo
WALL-E at Rotten Tomatoes
WALL-E at Metacritic
.
#(`*Automobile*`)#.
An automobile, autocar, motor car or car is a wheeled motor vehicle used for transporting passengers, which also carries its own engine or motor. Most definitions of the term specify that automobiles are designed to run primarily on roads, to have seating for one to eight people, to typically have four wheels, and to be constructed principally for the transport of people rather than goods.[3]
The term motorcar has also been used in the context of electrified rail systems to denote a car which functions as a small locomotive but also provides space for passengers and baggage. These locomotive cars were often used on suburban routes by both interurban and intercity railroad systems.[4]
It was estimated in 2010 that the number of automobiles had risen to over 1 billion vehicles, with 500 million reached in 1986.[5] The numbers are increasing rapidly, especially in China and India.[6]
The word automobile comes, via the French automobile from the Ancient Greek word  (auts, "self") and the Latin mobilis ("movable"); meaning a vehicle that moves itself. The alternative name car is believed to originate from the Latin word carrus or carrum ("wheeled vehicle"), or the Middle English word carre ("cart") (from Old North French), in turn these are said to have originated from the Gaulish word karros (a Gallic Chariot).[7][8]
The first working steam-powered vehicle was designed - and possibly built - by Ferdinand Verbiest, a Flemish member of a Jesuit mission in China around 1672. It was a 65cm-long scale-model toy for the Chinese Emperor, that was unable to carry a driver or a passenger.[9][10][11] It is not known if Verbiest's model was ever built.[10]
Nicolas-Joseph Cugnot is widely credited with building the first full-scale, self-propelled mechanical vehicle or automobile in about 1769; he created a steam-powered tricycle.[12] He also constructed two steam tractors for the French Army, one of which is preserved in the French National Conservatory of Arts and Crafts.[13] His inventions were however handicapped by problems with water supply and maintaining steam pressure.[13] In 1801, Richard Trevithick built and demonstrated his Puffing Devil road locomotive, believed by many to be the first demonstration of a steam-powered road vehicle. It was unable to maintain sufficient steam pressure for long periods, and was of little practical use.
In 1807 Nicphore Nipce and his brother Claude probably created the world's first internal combustion engine which they called a Pyrolophore, but they chose to install it in a boat on the river Saone in France.[14] Coincidentally, in 1807 the Swiss inventor Franois Isaac de Rivaz designed his own 'de Rivaz internal combustion engine' and used it to develop the world's first vehicle to be powered by such an engine. The Nipces' Pyrolophore was fuelled by a mixture of Lycopodium powder (dried spores of the Lycopodium plant), finely crushed coal dust and resin that were mixed with oil, whereas de Rivaz used a mixture of hydrogen and oxygen.[14] Neither design was very successful, as was the case with others, such as Samuel Brown, Samuel Morey, and Etienne Lenoir with his hippomobile, who each produced vehicles (usually adapted carriages or carts) powered by clumsy internal combustion engines.[15]
In November 1881, French inventor Gustave Trouv demonstrated a working three-wheeled automobile powered by electricity at the International Exposition of Electricity, Paris.[16]
Although several other German engineers (including Gottlieb Daimler, Wilhelm Maybach, and Siegfried Marcus) were working on the problem at about the same time, Karl Benz generally is acknowledged as the inventor of the modern automobile.[15]
In 1879, Benz was granted a patent for his first engine, which had been designed in 1878. Many of his other inventions made the use of the internal combustion engine feasible for powering a vehicle. His first Motorwagen was built in 1885 in Mannheim, Germany. He was awarded the patent for its invention as of his application on 29 January 1886 (under the auspices of his major company, Benz & Cie., which was founded in 1883). Benz began promotion of the vehicle on 3 July 1886, and about 25 Benz vehicles were sold between 1888 and 1893, when his first four-wheeler was introduced along with a model intended for affordability. They also were powered with four-stroke engines of his own design. Emile Roger of France, already producing Benz engines under license, now added the Benz automobile to his line of products. Because France was more open to the early automobiles, initially more were built and sold in France through Roger than Benz sold in Germany. In August 1888 Bertha Benz, the wife of Karl Benz, undertook the first road trip by car, to prove the road-worthiness of her husband's invention.
In 1896, Benz designed and patented the first internal-combustion flat engine, called boxermotor. During the last years of the nineteenth century, Benz was the largest automobile company in the world with 572 units produced in 1899 and, because of its size, Benz & Cie., became a joint-stock company.
The first motor car in central Europe and one of the first factory-made cars in the world, was produced by Czech company Nesselsdorfer Wagenbau (later renamed to Tatra) in 1897, the Prsident automobil.
Daimler and Maybach founded Daimler Motoren Gesellschaft (DMG) in Cannstatt in 1890, and sold their first automobile in 1892 under the brand name, Daimler. It was a horse-drawn stagecoach built by another manufacturer, that they retrofitted with an engine of their design. By 1895 about 30 vehicles had been built by Daimler and Maybach, either at the Daimler works or in the Hotel Hermann, where they set up shop after disputes with their backers. Benz, Maybach and the Daimler team seem to have been unaware of each other's early work. They never worked together; by the time of the merger of the two companies, Daimler and Maybach were no longer part of DMG.
Daimler died in 1900 and later that year, Maybach designed an engine named Daimler-Mercedes, that was placed in a specially ordered model built to specifications set by Emil Jellinek. This was a production of a small number of vehicles for Jellinek to race and market in his country. Two years later, in 1902, a new model DMG automobile was produced and the model was named Mercedes after the Maybach engine which generated 35hp. Maybach quit DMG shortly thereafter and opened a business of his own. Rights to the Daimler brand name were sold to other manufacturers.
Karl Benz proposed co-operation between DMG and Benz & Cie. when economic conditions began to deteriorate in Germany following the First World War, but the directors of DMG refused to consider it initially. Negotiations between the two companies resumed several years later when these conditions worsened and, in 1924 they signed an Agreement of Mutual Interest, valid until the year 2000. Both enterprises standardized design, production, purchasing, and sales and they advertised or marketed their automobile models jointly, although keeping their respective brands. On 28 June 1926, Benz & Cie. and DMG finally merged as the Daimler-Benz company, baptizing all of its automobiles Mercedes Benz, as a brand honoring the most important model of the DMG automobiles, the Maybach design later referred to as the 1902 Mercedes-35hp, along with the Benz name. Karl Benz remained a member of the board of directors of Daimler-Benz until his death in 1929, and at times, his two sons participated in the management of the company as well.
In 1890, mile Levassor and Armand Peugeot of France began producing vehicles with Daimler engines, and so laid the foundation of the automobile industry in France.
The first design for an American automobile with a gasoline internal combustion engine was made in 1877 by George Selden of Rochester, New York. Selden applied for a patent for an automobile in 1879, but the patent application expired because the vehicle was never built. After a delay of sixteen years and a series of attachments to his application, on 5 November 1895, Selden was granted a United States patent (U.S. Patent 549,160) for a two-stroke automobile engine, which hindered, more than encouraged, development of automobiles in the United States. His patent was challenged by Henry Ford and others, and overturned in 1911.
In 1893, the first running, gasoline-powered American car was built and road-tested by the Duryea brothers of Springfield, Massachusetts. The first public run of the Duryea Motor Wagon took place on 21 September 1893, on Taylor Street in Metro Center Springfield.[17][18] To construct the Duryea Motor Wagon, the brothers had purchased a used horse-drawn buggy for $70 and then installed a 4 HP, single cylinder gasoline engine.[17] The car had a friction transmission, spray carburetor, and low tension ignition. It was road-tested again on 10 November, when the The Springfield Republican newspaper made the announcement.[17] This particular car was put into storage in 1894 and stayed there until 1920 when it was rescued by Inglis M. Uppercu and presented to the United States National Museum.[17]
In Britain, there had been several attempts to build steam cars with varying degrees of success, with Thomas Rickett even attempting a production run in 1860.[19] Santler from Malvern is recognized by the Veteran Car Club of Great Britain as having made the first petrol-powered car in the country in 1894[20] followed by Frederick William Lanchester in 1895, but these were both one-offs.[20] The first production vehicles in Great Britain came from the Daimler Motor Company, a company founded by Harry J. Lawson in 1896, after purchasing the right to use the name of the engines. Lawson's company made its first automobiles in 1897, and they bore the name Daimler.[20]
In 1892, German engineer Rudolf Diesel was granted a patent for a "New Rational Combustion Engine". In 1897, he built the first Diesel Engine.[15] Steam-, electric-, and gasoline-powered vehicles competed for decades, with gasoline internal combustion engines achieving dominance in the 1910s.
Although various pistonless rotary engine designs have attempted to compete with the conventional piston and crankshaft design, only Mazda's version of the Wankel engine has had more than very limited success.
The large-scale, production-line manufacturing of affordable automobiles was debuted by Ransom Olds in 1902 at his Oldsmobile factory located in Lansing, Michigan and based upon the assembly line techniques pioneered by Marc Isambard Brunel at the Portsmouth Block Mills, England in 1802. The assembly line style of mass production and interchangeable parts had been pioneered in the U.S. by Thomas Blanchard in 1821, at the Springfield Armory in Springfield, Massachusetts.[21] This concept was greatly expanded by Henry Ford, beginning in 1914.
As a result, Ford's cars came off the line in fifteen minute intervals, much faster than previous methods, increasing productivity eightfold (requiring 12.5-man-hours before, 1-hour 33 minutes after), while using less manpower.[22] It was so successful, paint became a bottleneck. Only Japan black would dry fast enough, forcing the company to drop the variety of colors available before 1914, until fast-drying Duco lacquer was developed in 1926. This is the source of Ford's apocryphal remark, "any color as long as it's black".[22] In 1914, an assembly line worker could buy a Model T with four months' pay.[22]
Ford's complex safety proceduresespecially assigning each worker to a specific location instead of allowing them to roam aboutdramatically reduced the rate of injury. The combination of high wages and high efficiency is called "Fordism," and was copied by most major industries. The efficiency gains from the assembly line also coincided with the economic rise of the United States. The assembly line forced workers to work at a certain pace with very repetitive motions which led to more output per worker while other countries were using less productive methods.
In the automotive industry, its success was dominating, and quickly spread worldwide seeing the founding of Ford France and Ford Britain in 1911, Ford Denmark 1923, Ford Germany 1925; in 1921, Citroen was the first native European manufacturer to adopt the production method. Soon, companies had to have assembly lines, or risk going broke; by 1930, 250 companies which did not, had disappeared.[22]
Development of automotive technology was rapid, due in part to the hundreds of small manufacturers competing to gain the world's attention. Key developments included electric ignition and the electric self-starter (both by Charles Kettering, for the Cadillac Motor Company in 19101911), independent suspension, and four-wheel brakes.
Since the 1920s, nearly all cars have been mass-produced to meet market needs, so marketing plans often have heavily influenced automobile design. It was Alfred P. Sloan who established the idea of different makes of cars produced by one company, so buyers could "move up" as their fortunes improved.
Reflecting the rapid pace of change, makes shared parts with one another so larger production volume resulted in lower costs for each price range. For example, in the 1930s, LaSalles, sold by Cadillac, used cheaper mechanical parts made by Oldsmobile; in the 1950s, Chevrolet shared hood, doors, roof, and windows with Pontiac; by the 1990s, corporate powertrains and shared platforms (with interchangeable brakes, suspension, and other parts) were common. Even so, only major makers could afford high costs, and even companies with decades of production, such as Apperson, Cole, Dorris, Haynes, or Premier, could not manage: of some two hundred American car makers in existence in 1920, only 43 survived in 1930, and with the Great Depression, by 1940, only 17 of those were left.[22]
In Europe much the same would happen. Morris set up its production line at Cowley in 1924, and soon outsold Ford, while beginning in 1923 to follow Ford's practise of vertical integration, buying Hotchkiss (engines), Wrigley (gearboxes), and Osberton (radiators), for instance, as well as competitors, such as Wolseley: in 1925, Morris had 41% of total British car production. Most British small-car assemblers, from Abbey to Xtra had gone under. Citroen did the same in France, coming to cars in 1919; between them and other cheap cars in reply such as Renault's 10CV and Peugeot's 5CV, they produced 550,000 cars in 1925, and Mors, Hurtu, and others could not compete.[22] Germany's first mass-manufactured car, the Opel 4PS Laubfrosch (Tree Frog), came off the line at Russelsheim in 1924, soon making Opel the top car builder in Germany, with 37.5% of the market.[22]
The weight of a car influences fuel consumption and performance, with more weight resulting in increased fuel consumption and decreased performance. According to a research conducted by Julian Allwood of the University of Cambridge, global energy use could be heavily reduced by using lighter cars, and an average weight of 500kg has been said to be well achievable.[23]
In some competitions such as the Shell Eco Marathon, average car weights of 45kg have also been achieved.[24][25] These cars are only single-seaters (still falling within the definition of a car, although 4-seater cars are more common), but it nevertheless demonstrates the huge degree in which car weights can still be reduced, and the subsequent lower fuel use (i.e. up to a fuel use of 2560km/l).[26]
Most cars are designed to carry multiple occupants, often with four or five seats. Larger cars can often carry six, seven or more occupants depending in the internal arrange of seats. Sports cars are often designed with only two seats, and very occasionally three seats. The differing needs for passenger capacity and their luggage has resulted in a large variety of body styles to suit personal requirements such as the sedan/saloon, hatchback, station wagon/estate and Multi-Purpose Vehicle/Minivan.
Most automobiles in use today are propelled by an internal combustion engine, fueled by deflagration of gasoline (also known as petrol) or diesel. Both fuels are known to cause air pollution and are also blamed for contributing to climate change and global warming.[27] Rapidly increasing oil prices, concerns about oil dependence, tightening environmental laws and restrictions on greenhouse gas emissions are propelling work on alternative power systems for automobiles. Efforts to improve or replace existing technologies include the development of hybrid vehicles, plug-in electric vehicles and hydrogen vehicles. Vehicles using alternative fuels such as ethanol flexible-fuel vehicles and natural gas vehicles are also gaining popularity in some countries.
While road traffic injuries represent the leading cause in worldwide injury-related deaths,[28] their popularity undermines this statistic.
Mary Ward became one of the first documented automobile fatalities in 1869 in Parsonstown, Ireland[29] and Henry Bliss one of the United States' first pedestrian automobile casualties in 1899 in New York City.[30] There are now standard tests for safety in new automobiles, like the EuroNCAP and the US NCAP tests,[31] and insurance industry-backed tests by the Insurance Institute for Highway Safety (IIHS).[32]
The costs of automobile usage, which may include the cost of: acquiring the vehicle, repairs and maintenance, fuel, depreciation, driving time, parking fees, taxes, and insurance,[33] are weighed against the cost of the alternatives, and the value of the benefits perceived and real of vehicle usage. The benefits may include on-demand transportation, mobility, independence and convenience.[11]
Similarly the costs to society of encompassing automobile use, which may include those of: maintaining roads, land use, pollution, public health, health care, and of disposing of the vehicle at the end of its life, can be balanced against the value of the benefits to society that automobile use generates. The societal benefits may include: economy benefits, such as job and wealth creation, of automobile production and maintenance, transportation provision, society wellbeing derived from leisure and travel opportunities, and revenue generation from the tax opportunities. The ability for humans to move flexibly from place to place has far reaching implications for the nature of societies.[34]
Transportation is a major contributor to air pollution in most industrialised nations. According to the American Surface Transportation Policy Project nearly half of all Americans are breathing unhealthy air. Their study showed air quality in dozens of metropolitan areas has worsened over the last decade.[35] In the United States the average passenger car emits 11,450 pounds (5,190kg) of the greenhouse gas, carbon dioxide annually, along with smaller amounts of carbon monoxide, hydrocarbons, and nitrogen.[36]
Animals and plants are often negatively impacted by automobiles via habitat destruction and pollution. Over the lifetime of the average automobile the "loss of habitat potential" may be over 50,000 square meters (540,000sqft) based on primary production correlations.[37] Animals are also killed every year on roads by automobiles, referred to as Roadkill.
Growth in the popularity of vehicles and commuting has led to traffic congestion. Brussels is considered Europe's most congested city.[38]
Fuel taxes may act as an incentive for the production of more efficient, hence less polluting, car designs (e.g. hybrid vehicles) and the development of alternative fuels. High fuel taxes may provide a strong incentive for consumers to purchase lighter, smaller, more fuel-efficient cars, or to not drive. On average, today's automobiles are about 75 percent recyclable, and using recycled steel helps reduce energy use and pollution.[39] In the United States Congress, federally mandated fuel efficiency standards have been debated regularly, passenger car standards have not risen above the 27.5miles per US gallon (8.55L/100km; 33.0mpg-imp) standard set in 1985. Light truck standards have changed more frequently, and were set at 22.2miles per US gallon (10.6L/100km; 26.7mpg-imp) in 2007.[40]
Oil consumption in the twentieth and twenty-first centuries has been abundantly pushed by automobile growth; the 19852003 oil glut even fuelled the sales of low economy vehicles in OECD countries. The BRIC countries might also kick in, as China briefly was the first automobile market in December 2009.[41]
Residents of low-density, residential-only sprawling communities are also more likely to die in car collisions[original research?] which kill 1.2 million people worldwide each year, and injure about forty times this number.[28] Sprawl is more broadly a factor in inactivity and obesity, which in turn can lead to increased risk of a variety of diseases.[42]
Automobile propulsion technology under development include gasoline/electric and plug-in hybrids, battery electric vehicles, hydrogen cars, biofuels, and various alternative fuels. Research into future alternative forms of power include the development of fuel cells, Homogeneous Charge Compression Ignition (HCCI), Stirling engines,[43] and even using the stored energy of compressed air or liquid nitrogen.
New materials which may replace steel car bodies include duraluminum, fiberglass, carbon fiber, and carbon nanotubes.
Telematics technology is allowing more and more people to share cars, on a pay-as-you-go basis, through car share and carpool schemes.
Communication is also evolving due to connected car systems.
Fully autonomous vehicles, also known as robotic cars, or driverless cars, already exist in prototype, and are expected to be commercially available around 2020. According to urban designer and futurist Michael E. Arth, driverless electric vehiclesin conjunction with the increased use of virtual reality for work, travel, and pleasurecould reduce the world's 800 million vehicles to a fraction of that number within a few decades.[44] This would be possible if almost all private cars requiring drivers, which are not in use and parked 90% of the time, would be traded for public self-driving taxis that would be in near constant use. This would also allow for getting the appropriate vehicle for the particular needa bus could come for a group of people, a limousine could come for a special night out, and a Segway could come for a short trip down the street for one person. Children could be chauffeured in supervised safety, DUIs would no longer exist, and 41,000 lives could be saved each year in the US alone.[45][46]
There have been several projects aiming to develop a car on the principles of open design. The projects include OScar, Riversimple (through 40fires.org)[47] and c,mm,n.[48] None of the projects have reached significant success in terms of developing a car as a whole both from hardware and software perspective and no mass production ready open-source based design have been introduced as of late 2009. Some car hacking through on-board diagnostics (OBD) has been done so far.[49]
Established alternatives for some aspects of automobile use include public transit such as buses, trolleybuses, trains, subways, tramways light rail, cycling, and walking. Car-share arrangements and carpooling are also increasingly popularthe US market leader in car-sharing has experienced double-digit growth in revenue and membership growth between 2006 and 2007, offering a service that enables urban residents to "share" a vehicle rather than own a car in already congested neighborhoods.[50] Bike-share systems have been tried in some European cities, including Copenhagen and Amsterdam. Similar programs have been experimented with in a number of US Cities.[51] Additional individual modes of transport, such as personal rapid transit could serve as an alternative to automobiles if they prove to be socially accepted.[52]
The automotive industry designs, develops, manufactures, markets, and sells the world's motor vehicles. In 2008, more than 70 million motor vehicles, including cars and commercial vehicles were produced worldwide.[53]
In 2007, a total of 71.9 million new automobiles were sold worldwide: 22.9 million in Europe, 21.4 million in the Asia-Pacific Region, 19.4 million in the USA and Canada, 4.4 million in Latin America, 2.4 million in the Middle East and 1.4 million in Africa.[54] The markets in North America and Japan were stagnant, while those in South America and other parts of Asia grew strongly. Of the major markets, China, Russia, Brazil and India saw the most rapid growth.
About 250 million vehicles are in use in the United States. Around the world, there were about 806 million cars and light trucks on the road in 2007; they burn over 260 billion US gallons (980,000,000m3) of gasoline and diesel fuel yearly. The numbers are increasing rapidly, especially in China and India.[6] In the opinion of some, urban transport systems based around the car have proved unsustainable, consuming excessive energy, affecting the health of populations, and delivering a declining level of service despite increasing investments. Many of these negative impacts fall disproportionately on those social groups who are also least likely to own and drive cars.[55][56][57] The sustainable transport movement focuses on solutions to these problems.
In 2008, with rapidly rising oil prices, industries such as the automotive industry, are experiencing a combination of pricing pressures from raw material costs and changes in consumer buying habits. The industry is also facing increasing external competition from the public transport sector, as consumers re-evaluate their private vehicle usage.[58] Roughly half of the US's fifty-one light vehicle plants are projected to permanently close in the coming years, with the loss of another 200,000 jobs in the sector, on top of the 560,000 jobs lost this decade.[59] Combined with robust growth in China, in 2009, this resulted in China becoming the largest automobile producer and market in the world. China 2009 sales had increased to 13.6 million, a significant increase from one million of domestic car sales in 2000.[60]

1 Etymology
2 History
3 Mass production
4 Weight
5 Seating and body style
6 Fuel and propulsion technologies
7 Safety
8 Costs and benefits
9 Criticism
10 Future car technologies

10.1 Driverless cars
10.2 Open source development


10.1 Driverless cars
10.2 Open source development
11 Alternatives to the automobile
12 Industry
13 See also
14 References
15 Further reading
16 External links
10.1 Driverless cars
10.2 Open source development
Car classification
Carfree city
List of countries by automobile production
List of countries by vehicles per capita
Lists of automobiles
Motor vehicle theft
Noise pollution
Peak car
Steering
Traffic collision
Traffic congestion
U.S. Automobile Production Figures production figures for each make from 1899 to 2000
Halberstam, David, The Reckoning, New York, Morrow, 1986. ISBN 0-688-04838-2
Kay, Jane Holtz, Asphalt nation: how the automobile took over America, and how we can take it back, New York, Crown, 1997. ISBN 0-517-58702-5
Heathcote Williams, Autogeddon, New York, Arcade, 1991. ISBN 1-55970-176-5
Wolfgang Sachs: For love of the automobile: looking back into the history of our desires, Berkeley: University of California Press, 1992, ISBN 0-520-06878-5
 Media related to Automobile at Wikimedia Commons
 The dictionary definition of automobile at Wiktionary
Fdration Internationale de l'Automobile
Forum for the Automobile and Society
.
#(`*Star Wars*`)#.
Franchises:
Star Wars is an American epic space opera franchise consisting of a film series created by George Lucas. The film series has spawned a media franchise outside the film series called the Expanded Universe including books, television series, computer and video games, and comic books. These supplements to the film trilogies have resulted in significant development of the series' fictional universe. These media kept the franchise active in the interim between the film trilogies. The franchise portrays a universe which is in a galaxy that is described as far, far away. It commonly portrays Jedi as a representation of good, in conflict with the Sith, their evil counterpart. Their weapon of choice, the lightsaber, is commonly recognized in popular culture. The fictional universe also contains many themes, especially influences of philosophy and religion.
The first film in the series was originally released on May 25, 1977, under the title Star Wars, by 20th Century Fox, and became a worldwide pop culture phenomenon, followed by two sequels, released at three-year intervals. Sixteen years after the release of the trilogy's final film, the first in a new prequel trilogy of films was released. The three prequel films were also released at three-year intervals, with the final film of that particular series released on May 19, 2005. In October 2012, The Walt Disney Company acquired Lucasfilm for $4.05 billion and announced that it would produce three new films, with the first planned for release in 2015.[1] 20th Century Fox still retains the distribution rights to the first two Star Wars trilogies, owning permanent rights for the original film Episode IV: A New Hope, while holding the rights to Episodes I-III, V and VI until May 2020.[2]
Reactions to the original trilogy were mostly positive, with the last film being considered the weakest, while the prequel trilogy received a more mixed reaction, with most of the praise being for the final movie, according to most review aggregator websites. All six of the main films in the series were also nominated for or won Academy Awards.
All of the main films have been a box office success, with the overall box office revenue generated by the Star Wars films (including the theatrical Star Wars: The Clone Wars) totalling $4.49billion,[3] making it the third-highest-grossing film series.[4] The success has also led to multiple re-releases in theaters for the series.
The events depicted in Star Wars media take place in a fictional galaxy. Many species of alien creatures (often humanoid) are depicted. Robotic droids are also commonplace and are generally built to serve their owners. Space travel is common, and many planets in the galaxy are members of a Galactic Republic, later reorganized as the Galactic Empire.
One of the prominent elements of Star Wars is the "Force", an omnipresent energy that can be harnessed by those with that ability, known as Force-sensitives. It is described in the first produced film as "an energy field created by all living things [that] surrounds us, penetrates us, [and] binds the galaxy together."[5] The Force allows users to perform various supernatural feats (such as telekinesis, clairvoyance, precognition, and mind control) and can amplify certain physical traits, such as speed and reflexes; these abilities vary between characters and can be improved through training. While the Force can be used for good, it has a dark side that, when pursued, imbues users with hatred, aggression, and malevolence. The six films feature the Jedi, who use the Force for good, and the Sith, who use the dark side for evil in an attempt to take over the galaxy. In the Star Wars Expanded Universe, many dark side users are Dark Jedi rather than Sith, mainly because of the "Rule of Two" (see Sith Origin).[5][6][7][8][9][10]
The film series began with Star Wars, released on May 25, 1977. This was followed by two sequels: The Empire Strikes Back, released on May 21, 1980, and Return of the Jedi, released on May 25, 1983. The opening crawl of the sequels disclosed that they were numbered as "Episode V" and "Episode VI" respectively, though the films were generally advertised solely under their subtitles. Though the first film in the series was simply titled Star Wars, with its 1981 re-release it had the subtitle Episode IV: A New Hope added to remain consistent with its sequel, and to establish it as the middle chapter of a continuing saga.[11]
In 1997, to correspond with the 20th anniversary of A New Hope, Lucas released a "Special Edition" of the Star Wars trilogy to theaters. The re-release featured alterations to the three films, primarily motivated by the improvement of CGI and other special effects technologies, which allowed visuals that were not possible to achieve at the time of the original filmmaking. Lucas continued to make changes to the films for subsequent releases, such as the first ever DVD release of the original trilogy on September 21, 2004 and the first ever Blu-ray release of all six films on September 16, 2011.[12]
More than two decades after the release of the original film, the series continued with the long-awaited prequel trilogy; consisting of Episode I: The Phantom Menace, released on May 19, 1999; Episode II: Attack of the Clones, released on May 16, 2002; and Episode III: Revenge of the Sith, released on May 19, 2005.[13]
On August 15, 2008 Star Wars: The Clone Wars was released theatrically as a lead-in to the weekly animated TV series of the same name. Episode VII is scheduled to be released in 2015.
The prequel trilogy begins with the greedy Trade Federation setting up a blockade around the peaceful planet Naboo, under the orders of the Sith Lord Darth Sidious. It is revealed that Sidious secretly planned the blockade to give his alter ego, Senator Palpatine, a pretense to overthrow the Supreme Chancellor of the Galactic Republic and take his place. The Jedi Knight Qui-Gon Jinn and his apprentice Obi-Wan Kenobi are sent to Naboo to negotiate with the Federation, but are forced to instead help the planet's ruler, Queen Padm Amidala, escape from the blockade and plea her case before the Galactic Senate on Coruscant. When their spaceship is damaged during the escape, they land on the desert planet Tatooine for repairs, where Qui-Gon discovers a young slave named Anakin Skywalker. Qui-Gon comes to believe that Anakin is the "Chosen One" foretold by Jedi prophecy to bring balance to the Force, and he helps liberate the boy from slavery. The Jedi Council, led by Yoda, sense that Anakin's future is clouded by fear, but reluctantly allows Obi-Wan to train Anakin after Qui-Gon is killed by Palpatine's first apprentice, Darth Maul, during the Battle of Naboo.[6]
The remainder of the prequel trilogy chronicles Anakin's gradual fall to the dark side of the Force as he fights in the Clone Wars, which Palpatine secretly engineers in order to destroy the Republic and lure Anakin into his service.[7] Anakin and Padm fall in love and secretly wed, and eventually Padm becomes pregnant. Anakin has a prophetic vision of Padm dying in childbirth, and Palpatine convinces him that the dark side holds the power to save her life; desperate, Anakin submits to the dark side and takes the Sith name Darth Vader. While Palpatine re-organizes the Republic into the tyrannical Galactic Empireappointing himself Emperor for lifeVader participates in the extermination of the Jedi Order, culminating in a lightsaber battle between himself and Obi-Wan on the volcanic planet Mustafar.[8]
Obi-Wan ultimately defeats his former apprentice and friend, severing his limbs and leaving him for dead beside a lava flow. However, Palpatine arrives shortly afterward and saves Vader, putting him into a black, mechanical suit of armor that keeps him alive. At the same time, Padm dies while giving birth to twins Luke and Leia. The twins are hidden from Vader and are not told who their real parents are.[8]
The original trilogy begins 19 years later as Vader nears completion of the massive Death Star space station, which will allow the Empire to crush the Rebel Alliance, an organized resistance formed to combat Palpatine's tyranny. Vader captures Princess Leia, who has stolen the plans to the Death Star and hidden them in the astromech droid R2-D2. R2, along with his protocol droid counterpart C-3PO, escapes to Tatooine. There, the droids are purchased by Luke Skywalker and his step-uncle and aunt. While Luke is cleaning R2, he accidentally triggers a message put into the droid by Leia, who asks for assistance from Obi-Wan. Luke later assists the droids in finding the Jedi Knight, who is now passing as an old hermit under the alias Ben Kenobi. When Luke asks about his father, Obi-Wan tells him that Anakin was a great Jedi who was betrayed and murdered by Vader.[15]
Obi-Wan and Luke hire the smuggler Han Solo and his Wookiee co-pilot Chewbacca to take them to Alderaan, Leia's home world, which they eventually find has been destroyed by the Death Star. Once on board the space station, Obi-Wan allows himself to be killed during a lightsaber rematch with Vader; his sacrifice allows the group to escape with the plans that help the rebels destroy the Death Star. Luke himself fires the shot that destroys the deadly space station.[5]
Three years later, Luke travels to find Yoda, now living in exile on the swamp-infested world Dagobah, in order to start his Jedi training. However, Luke is interrupted when Vader lures him into a trap by capturing Han and the others. During a fierce lightsaber duel, Vader reveals that he is Luke's father and attempts to turn him to the dark side.[9] Luke escapes, and, after rescuing Han from the gangster Jabba the Hutt a year later, returns to Yoda to complete his training. However, now over 900 years old, Yoda is on his deathbed. Before he passes away, Yoda confirms that Vader is Luke's father; moments later, Obi-Wan's spirit tells Luke that he must face his father before he can become a Jedi, and that Leia is his twin sister. As the Rebels attack the second Death Star, Luke confronts Vader as Palpatine watches; both Sith Lords intend to turn Luke to the dark side and take him as their apprentice.[10]
During the subsequent lightsaber duel, Luke succumbs to his anger and brutally overpowers Vader, but controls himself at the last minute; realizing that he is about to suffer his father's fate, he spares Vader's life and proudly declares his allegiance to the Jedi. An enraged Palpatine then attempts to kill Luke with Force lightning, a sight that moves Vader to turn on and kill his master, suffering mortal wounds in the process. Redeemed, Anakin Skywalker dies in his son's arms. Luke becomes a full-fledged Jedi, and the Rebels destroy the second Death Star.[10]
Star Wars features elements such as knights, witches, and princesses that are related to archetypes of the fantasy genre.[16] The Star Wars world, unlike fantasy and science-fiction films that featured sleek and futuristic settings, was portrayed as dirty and grimy. Lucas' vision of a "used future" was further popularized in the science fiction-horror films Alien,[17] which was set on a dirty space freighter; Mad Max 2, which is set in a post-apocalyptic desert; and Blade Runner, which is set in a crumbling, dirty city of the future. Lucas made a conscious effort to parallel scenes and dialogue between films, and especially to parallel the journeys of Luke Skywalker with that of his father Anakin when making the prequels.[6]
All six films of the Star Wars series were shot in an aspect ratio of 2.40:1. The original trilogy was shot with anamorphic lenses. Episodes IV and V were shot in Panavision, while Episode VI was shot in Joe Dunton Camera (JDC) scope. Episode I was shot with Hawk anamorphic lenses on Arriflex cameras, and Episodes II and III were shot with Sony's CineAlta high-definition digital cameras.[18]
Lucas hired Ben Burtt to oversee the sound effects on A New Hope. Burtt's accomplishment was such that the Academy of Motion Picture Arts and Sciences presented him with a Special Achievement Award because it had no award at the time for the work he had done.[19] Lucasfilm developed the THX sound reproduction standard for Return of the Jedi.[20] John Williams composed the scores for all six films. Lucas' design for Star Wars involved a grand musical sound, with leitmotifs for different characters and important concepts. Williams' Star Wars title theme has become one of the most famous and well-known musical compositions in modern music history.[21]
The technical lightsaber choreography for the original trilogy was developed by leading filmmaking sword-master Bob Anderson. Anderson trained actor Mark Hamill (Luke Skywalker) and performed all the sword stunts as Darth Vader during the lightsaber duels in The Empire Strikes Back and Return of the Jedi, wearing Vader's costume. Anderson's role in the original Star Wars trilogy was highlighted in the film Reclaiming the Blade, where he shares his experiences as the fight choreographer developing the lightsaber techniques for the movies.[22]
In 1971, Universal Studios agreed to make American Graffiti and Star Wars in a two-picture contract, although Star Wars was later rejected in its early concept stages. American Graffiti was completed in 1973 and, a few months later, Lucas wrote a short summary called "The Journal of the Whills", which told the tale of the training of apprentice C.J. Thorpe as a "Jedi-Bendu" space commando by the legendary Mace Windy.[23] Frustrated that his story was too difficult to understand, Lucas then wrote a 13-page treatment called The Star Wars, which had thematic parallels with Akira Kurosawa's The Hidden Fortress.[24] By 1974, he had expanded the treatment into a rough draft screenplay, adding elements such as the Sith, the Death Star, and a protagonist named Annikin Starkiller. For the second draft, Lucas made heavy simplifications, and introduced the young hero on a farm as Luke Starkiller. Annikin became Luke's father, a wise Jedi knight. "The Force" was also introduced as a supernatural power. The next draft removed the father character and replaced him with a substitute named Ben Kenobi, and in 1976 a fourth draft had been prepared for principal photography. The film was titled Adventures of Luke Starkiller, as taken from the Journal of the Whills, Saga I: The Star Wars. During production, Lucas changed Luke's name to Skywalker and altered the title to simply The Star Wars and finally Star Wars.[25]
At that point, Lucas was not expecting the film to become part of a series. The fourth draft of the script underwent subtle changes that made it more satisfying as a self-contained film, ending with the destruction of the Empire itself by way of destroying the Death Star. However, Lucas had previously conceived of the film as the first in a series of adventures. Later, he realized the film would not in fact be the first in the sequence, but a film in the second trilogy in the saga. This is stated explicitly in George Lucas' preface to the 1994 reissue of Splinter of the Mind's Eye:
It wasn't long after I began writing Star Wars that I realized the story was more than a single film could hold. As the saga of the Skywalkers and Jedi Knights unfolded, I began to see it as a tale that could take at least nine films to tellthree trilogiesand I realized, in making my way through the back story and after story, that I was really setting out to write the middle story.
The second draft contained a teaser for a never-made sequel about "The Princess of Ondos," and by the time of the third draft some months later Lucas had negotiated a contract that gave him rights to make two sequels. Not long after, Lucas met with author Alan Dean Foster, and hired him to write these two sequels as novels.[26] The intention was that if Star Wars were successful, Lucas could adapt the novels into screenplays.[27] He had also by that point developed an elaborate backstory to aid his writing process.[28]
When Star Wars proved successful, Lucas decided to use the film as the basis for an elaborate serial, although at one point he considered walking away from the series altogether.[29] However, Lucas wanted to create an independent filmmaking centerwhat would become Skywalker Ranchand saw an opportunity to use the series as a financing agent.[30] Alan Dean Foster had already begun writing the first sequel novel, but Lucas decided to abandon his plan to adapt Foster's work; the book was released as Splinter of the Mind's Eye the following year. At first Lucas envisioned a series of films with no set number of entries, like the James Bond series. In an interview with Rolling Stone in August 1977, he said that he wanted his friends to each take a turn at directing the films and giving unique interpretations on the series. He also said that the backstory in which Darth Vader turns to the dark side, kills Luke's father and fights Ben Kenobi on a volcano as the Galactic Republic falls would make an excellent sequel.
Later that year, Lucas hired science fiction author Leigh Brackett to write Star Wars II with him. They held story conferences and, by late November 1977, Lucas had produced a handwritten treatment called The Empire Strikes Back. The treatment is very similar to the final film, except that Darth Vader does not reveal he is Luke's father. In the first draft that Brackett would write from this, Luke's father appears as a ghost to instruct Luke.[31]
Brackett finished her first draft in early 1978; Lucas has said he was disappointed with it, but before he could discuss it with her, she died of cancer.[32] With no writer available, Lucas had to write his next draft himself. It was this draft in which Lucas first made use of the "Episode" numbering for the films; Empire Strikes Back was listed as Episode II.[33] As Michael Kaminski argues in The Secret History of Star Wars, the disappointment with the first draft probably made Lucas consider different directions in which to take the story.[34] He made use of a new plot twist: Darth Vader claims to be Luke's father. According to Lucas, he found this draft enjoyable to write, as opposed to the yearlong struggles writing the first film, and quickly wrote two more drafts,[35] both in April 1978. He also took the script to a darker extreme by having Han Solo imprisoned in carbonite and left in limbo.[9]
This new story point of Darth Vader being Luke's father had drastic effects on the series. Michael Kaminski argues in his book that it is unlikely that the plot point had ever seriously been considered or even conceived of before 1978, and that the first film was clearly operating under an alternate storyline where Vader was separate from Luke's father;[36] there is not a single reference to this plot point before 1978. After writing the second and third drafts of Empire Strikes Back in which the point was introduced, Lucas reviewed the new backstory he had created: Anakin Skywalker was Ben Kenobi's brilliant student and had a child named Luke, but was swayed to the dark side by Emperor Palpatine (who became a Sith and not simply a politician). Anakin battled Ben Kenobi on the site of a volcano and was wounded, but then resurrected as Darth Vader. Meanwhile Kenobi hid Luke on Tatooine while the Republic became the Empire and Vader systematically hunted down and killed the Jedi.[37]
With this new backstory in place, Lucas decided that the series would be a trilogy, changing Empire Strikes Back from Episode II to Episode V in the next draft.[35] Lawrence Kasdan, who had just completed writing Raiders of the Lost Ark, was then hired to write the next drafts, and was given additional input from director Irvin Kershner. Kasdan, Kershner, and producer Gary Kurtz saw the film as a more serious and adult film, which was helped by the new, darker storyline, and developed the series from the light adventure roots of the first film.[38]
By the time he began writing Episode VI in 1981 (then titled Revenge of the Jedi), much had changed. Making Empire Strikes Back was stressful and costly, and Lucas' personal life was disintegrating. Burned out and not wanting to make any more Star Wars films, he vowed that he was done with the series in a May 1983 interview with Time magazine. Lucas' 1981 rough drafts had Darth Vader competing with the Emperor for possession of Lukeand in the second script, the "revised rough draft", Vader became a sympathetic character. Lawrence Kasdan was hired to take over once again and, in these final drafts, Vader was explicitly redeemed and finally unmasked. This change in character would provide a springboard to the "Tragedy of Darth Vader" storyline that underlies the prequels.[39]
After losing much of his fortune in a divorce settlement in 1987, Lucas had no desire to return to Star Wars, and had unofficially canceled his sequel trilogy by the time of Return of the Jedi.[40] Nevertheless, the prequels, which were quite developed at this point, continued to fascinate him. After Star Wars became popular once again, in the wake of Dark Horse's comic book line and Timothy Zahn's trilogy of novels, Lucas saw that there was still a large audience. His children were older, and with the explosion of CGI technology he was now considering returning to directing.[41] By 1993 it was announced, in Variety among other sources, that he would be making the prequels. He began outlining the story, now indicating the series would be a tragic one examining Anakin Skywalker's fall to the dark side. Lucas also began to change how the prequels would exist relative to the originals; at first they were supposed to be a "filling-in" of history tangential to the originals, but now he saw that they could form the beginning of one long story that started with Anakin's childhood and ended with his death. This was the final step towards turning the film series into a "Saga".[42]
In 1994, Lucas began writing the first screenplay titled Episode I: The Beginning. Following the release of that film, Lucas announced that he would also be directing the next two, and began working on Episode II at that time.[43] The first draft of Episode II was completed just weeks before principal photography, and Lucas hired Jonathan Hales, a writer from The Young Indiana Jones Chronicles, to polish it.[44] Unsure of a title, Lucas had jokingly called the film "Jar Jar's Great Adventure."[45] In writing The Empire Strikes Back, Lucas initially decided that Lando Calrissian was a clone and came from a planet of clones which caused the "Clone Wars" mentioned by Princess Leia in A New Hope;[46][47] he later came up with an alternate concept of an army of clone shocktroopers from a remote planet which attacked the Republic and were repelled by the Jedi.[48] The basic elements of that backstory became the plot basis for Episode II, with the new wrinkle added that Palpatine secretly orchestrated the crisis.[7]
Lucas began working on Episode III before Attack of the Clones was released, offering concept artists that the film would open with a montage of seven Clone War battles.[49] As he reviewed the storyline that summer, however, he says he radically re-organized the plot.[50] Michael Kaminski, in The Secret History of Star Wars, offers evidence that issues in Anakin's fall to the dark side prompted Lucas to make massive story changes, first revising the opening sequence to have Palpatine kidnapped and his apprentice, Count Dooku, murdered by Anakin as the first act in the latter's turn towards the dark side.[51] After principal photography was complete in 2003, Lucas made even more massive changes in Anakin's character, re-writing his entire turn to the dark side; he would now turn primarily in a quest to save Padm's life, rather than the previous version in which that reason was one of several, including that he genuinely believed that the Jedi were evil and plotting to take over the Republic. This fundamental re-write was accomplished both through editing the principal footage, and new and revised scenes filmed during pick-ups in 2004.[52]
Lucas often exaggerated the amount of material he wrote for the series; much of it stemmed from the post1978 period when the series grew into a phenomenon. Michael Kaminski explained that these exaggerations were both a publicity and security measure. Kaminski rationalized that since the series' story radically changed throughout the years, it was always Lucas' intention to change the original story retroactively because audiences would only view the material from his perspective.[8][53] When congratulating the producers of the TV series Lost in 2010, Lucas himself jokingly admitted, "when Star Wars first came out, I didn't know where it was going either. The trick is to pretend you've planned the whole thing out in advance. Throw in some father issues and references to other stories let's call them homages and you've got a series".[54]
The sequel trilogy was a reportedly planned trilogy of films (Episodes VII, VIII and IX) by Lucasfilm as a sequel to the original Star Wars trilogy (Episodes IV, V and VI) released between 1977 and 1983.[55] While the similarly discussed Star Wars prequel trilogy (Episodes I, II and III) was ultimately released between 1999 and 2005, Lucasfilm and George Lucas have for many years denied plans for a sequel trilogy, insisting that Star Wars is meant to be a six-part series.[56][57] In May 2008, speaking about the upcoming Star Wars: The Clone Wars, Lucas maintained his status on the sequel trilogy:
In January 2012, Lucas announced that he would step away from blockbuster films and instead produce smaller art-house films. In an interview regarding whether or not the scrutiny he received from the prequel trilogy and the alterations made on the original trilogy were a factor in his retirement, Lucas stated:
In October 2012, The Walt Disney Company agreed to buy Lucasfilm and announced that a new Star Wars Episode VII film will be released in 2015. The co-chairman of Lucasfilm, Kathleen Kennedy became president of the company, reporting to Walt Disney Studios chairman Alan Horn. In addition, Kennedy will serve as executive producer on new Star Wars feature films, with franchise creator and Lucasfilm founder Lucas serving as creative consultant.[60] The screenplay for Episode VII will be written by Michael Arndt.[61]
On November 20, 2012, The Hollywood Reporter reported that Lawrence Kasdan, writer of The Empire Strikes Back and Return of the Jedi, and Simon Kinberg will write and produce Episodes VIII and IX.[62]
At a ShoWest convention in 2005, Lucas demonstrated new technology and stated that he planned to release the six films in a new 3D film format, beginning with A New Hope in 2007.[63] However, by January 2007, Lucasfilm stated on StarWars.com that "there are no definitive plans or dates for releasing the Star Wars saga in 3-D." At Celebration Europe in July 2007, Rick McCallum confirmed that Lucasfilm is "planning to take all six films and turn them into 3-D," but they are "waiting for the companies out there that are developing this technology to bring it down to a cost level that makes it worthwhile for everybody".[64] In July 2008, Jeffrey Katzenberg, the CEO of DreamWorks Animation, revealed that Lucas plans to redo all six of the movies in 3D.[65] In late September 2010, it was announced that The Phantom Menace would be theatrically re-released in 3-D on February 10, 2012.[66][67] All six films would be re-released in order, with the 3-D conversion process taking at least a year to complete per film.[68]
The six films together were nominated for 25 Academy Awards, of which they won ten. Three of these were Special Achievement Awards.
The term Expanded Universe (EU) is an umbrella term for officially licensed Star Wars material outside of the six feature films. The material expands the stories told in the films, taking place anywhere from 25,000 years before The Phantom Menace to 140 years after Return of the Jedi. The first Expanded Universe story appeared in Marvel Comics' Star Wars #7 in January 1978 (the first six issues of the series having been an adaptation of the film), followed quickly by Alan Dean Foster's novel Splinter of the Mind's Eye the following month.[93]
George Lucas retains artistic control over the Star Wars universe. For example, the death of central characters and similar changes in the status quo must first pass his screening before authors are given the go-ahead. In addition, Lucasfilm Licensing devotes efforts to ensure continuity between the works of various authors across companies.[94] Elements of the Expanded Universe have been adopted by Lucas for use in the films, such as the name of capital planet Coruscant, which first appeared in Timothy Zahn's novel Heir to the Empire before being used in The Phantom Menace. Additionally, Lucas so liked the character Aayla Secura, who was introduced in Dark Horse Comics' Star Wars series, that he included her as a character in Attack of the Clones.[95]
Lucas has played a large role in the production of various television projects, usually serving as storywriter or executive producer.[96] Star Wars has had numerous radio adaptations. A radio adaptation of A New Hope was first broadcast on National Public Radio in 1981. The adaptation was written by science fiction author Brian Daley and directed by John Madden. It was followed by adaptations of The Empire Strikes Back in 1983 and Return of the Jedi in 1996. The adaptations included background material created by Lucas but not used in the films. Mark Hamill, Anthony Daniels, and Billy Dee Williams reprised their roles as Luke Skywalker, C-3PO, and Lando Calrissian, respectively, except in Return of the Jedi in which Luke was played by Joshua Fardon and Lando by Arye Gross. The series also used John Williams' original score from the films and Ben Burtt's original sound designs.[97]
In addition to the two trilogies and The Clone Wars film, several other authorized films have been produced:
Following the success of the Star Wars films and their subsequent merchandising, several animated television series have been created for the younger fan base:
Star Wars-based fiction predates the release of the first film, with the 1976 novelization of Star Wars (ghost-written by Alan Dean Foster and credited to Lucas). Foster's 1978 novel, Splinter of the Mind's Eye, was the first Expanded Universe work to be released. In addition to filling in the time between A New Hope and The Empire Strikes Back, this additional content greatly expanded the Star Wars timeline before and after the film series. Star Wars fiction flourished during the time of the original trilogy (19771983) but slowed to a trickle afterwards. In 1992, however, Timothy Zahn's Thrawn trilogy debuted, sparking a new interest in the Star Wars universe. Since then, several hundred tie-in novels have been published by Bantam and Del Rey. A similar resurgence in the Expanded Universe occurred in 1996 with the Steve Perry novel Shadows of the Empire, set in between The Empire Strikes Back and Return of the Jedi, and accompanying video game and comic book series.[100]
LucasBooks radically changed the face of the Star Wars universe with the introduction of the New Jedi Order series, which takes place some 20 years after Return of the Jedi and stars a host of new characters alongside series originals. For younger audiences, three series have been introduced. The Jedi Apprentice series follows the adventures of Qui-Gon Jinn and his apprentice Obi-Wan Kenobi prior to The Phantom Menace. The Jedi Quest series follows the adventures of Obi-Wan and his apprentice Anakin Skywalker in between The Phantom Menace and Attack of the Clones. The Last of the Jedi series follows the adventures of Obi-Wan and another surviving Jedi almost immediately, set in between Revenge of the Sith and A New Hope.
Marvel Comics published Star Wars comic book series and adaptations from 1977 to 1986. A wide variety of creators worked on this series, including Roy Thomas, Archie Goodwin, Howard Chaykin, Al Williamson, Carmine Infantino, Gene Day, Walt Simonson, Michael Golden, Chris Claremont, Whilce Portacio, Jo Duffy, and Ron Frenz. The Los Angeles Times Syndicate published a Star Wars newspaper strip by Russ Manning, Goodwin and Williamson[101][102] with Goodwin writing under a pseudonym. In the late 1980s, Marvel announced it would publish a new Star Wars comic by Tom Veitch and Cam Kennedy. However, in December 1991, Dark Horse Comics acquired the Star Wars license and used it to launch a number of ambitious sequels to the original trilogy instead, including the popular Dark Empire stories.[103] They have since gone on to publish a large number of original adventures set in the Star Wars universe. There have also been parody comics, including Tag and Bink.[104]
Since 1977, dozens of board, card, video, miniature, and tabletop role-playing games, among other types, have been published bearing the Star Wars name, beginning in 1977 with the board game Star Wars: Escape from the Death Star[105] (not to be confused with another board game with the same title, published in 1990).[106]
Star Wars video games commercialization started in 1982 with Star Wars: The Empire Strikes Back published for the Atari 2600 by Parker Brothers. Since then, Star Wars has opened the way to a myriad of space-flight simulation games, first-person shooter games, role-playing video games, RTS games, and others.
Three different official tabletop role-playing games have been developed for the Star Wars universe: a version by West End Games in the 1980s and 1990s, one by Wizards of the Coast in the 2000s and one by Fantasy Flight Games in the 2010s.
The best-selling games so far are the Lego Star Wars and the Battlefront series, with 12million and 10million units respectively [107][108] while the most critically acclaimed is the first Knights of the Old Republic.[109]
The most recently released games are Lego Star Wars: The Complete Saga, Lego Star Wars III: The Clone Wars, Star Wars: The Force Unleashed and Star Wars: The Force Unleashed II, for the PS3, PSP, PS2, Xbox 360, Nintendo DS and Wii. While The Complete Saga focuses on all six episodes of the series, The Force Unleashed, of the same name of the multimedia project which it is a part of, takes place in the largely unexplored time period between Revenge of the Sith and A New Hope and casts players as Darth Vader's "secret apprentice" hunting down the remaining Jedi. The game features a new game engine, and was released on September 16, 2008 in the United States.[110][111] There are three more titles based on the Clone Wars which were released for the Nintendo DS (Star Wars: The Clone Wars  Jedi Alliance) and Wii (Star Wars: The Clone Wars  Lightsaber Duels and Star Wars: The Clone Wars - Republic Heroes).
Star Wars trading cards have been published since the first 'blue' series, by Topps, in 1977.[112] Dozens of series have been produced, with Topps being the licensed creator in the United States. Some of the card series are of film stills, while others are original art. Many of the cards have become highly collectible with some very rare "promos", such as the 1993 Galaxy Series II "floating Yoda" P3 card often commanding US$1000 or more. While most "base" or "common card" sets are plentiful, many "insert" or "chase cards" are very rare.[113]
From 1995 until 2001 Decipher,_Inc had the license for, created and produced a collectible card game based on Star Wars; the Star Wars Collectible Card Game (also known as SWCCG).
The board game Risk has been adapted to the series in two editions by Hasbro: Risk Star Wars: The Original Trilogy Edition[114] (2006) and Risk Star Wars: Clone Wars Edition[115] (2005).
The Star Wars saga has inspired many fans to create their own non-canon material set in the Star Wars galaxy. In recent years, this has ranged from writing fan-fiction to creating fan films. In 2002, Lucasfilm sponsored the first annual Official Star Wars Fan Film Awards, officially recognizing filmmakers and the genre. Because of concerns over potential copyright and trademark issues, however, the contest was initially open only to parodies, mockumentaries, and documentaries. Fan-fiction films set in the Star Wars universe were originally ineligible, but in 2007 Lucasfilm changed the submission standards to allow in-universe fiction entries.[116]
While many fan films have used elements from the licensed Expanded Universe to tell their story, they are not considered an official part of the Star Wars canon. However, the lead character from the Pink Five series was incorporated into Timothy Zahn's 2007 novel Allegiance, marking the first time a fan-created Star Wars character has ever crossed into the official canon.[117] Lucasfilm, for the most part, has allowed but not endorsed the creation of these derivative fan-fiction works, so long as no such work attempts to make a profit from or tarnish the Star Wars franchise in any way.[118]
Before Disney's acquisition, George Lucas had established a partnership in 1986 with Disney and its Walt Disney Imagineering division to create Star Tours, an attraction that opened at Disneyland in 1987. The attraction also had subsequent incarnations at other Disney Parks worldwide, with the exception of Hong Kong Disneyland.
The attractions at Disneyland, Disney's Hollywood Studios and Tokyo Disneyland closed on July 27, 2010, September 7, 2010 and April 2, 2012, respectively, in order to allow the rides to be converted into Star Tours: The Adventures Continue. The successor attraction opened at Disney's Hollywood Studios on May 20, 2011 and June 3, at Disneyland. The Japan version is expected to open in 2013.
The Jedi Training Academy is a live show where children are selected to learn the teachings of the Jedi Knights and the Force in order to become Padawan learners. The show is present at the Rebels stage at Disney's Hollywood Studios and at the Tomorrowland Terrace at Disneyland.
The Walt Disney World Resort's Disney's Hollywood Studios park hosts an annual festival, Star Wars Weekends during specific dates from May to June. The event began in 1997.
The Star Wars saga has had a significant impact on modern American pop culture. Both the films and characters have been parodied in numerous films and television.
     
IV: A New Hope (1977)
V: The Empire Strikes Back (1980)
VI: Return of the Jedi (1983)
I: The Phantom Menace (1999)
II: Attack of the Clones (2002)
III: Revenge of the Sith (2005)
The Clone Wars (2008)
VII (2015)
List of Star Wars video games
X-Wing
Jedi Knight
Rogue Squadron
Knights of the Old Republic
Battlefront
Lego Star Wars
The Force Unleashed
1 Setting
2 Theatrical films

2.1 Plot overview

2.1.1 Cast and characters


2.2 Themes
2.3 Technical information
2.4 Production history

2.4.1 Original trilogy
2.4.2 Prequel trilogy
2.4.3 Sequel trilogy
2.4.4 3D releases




2.1 Plot overview

2.1.1 Cast and characters


2.1.1 Cast and characters
2.2 Themes
2.3 Technical information
2.4 Production history

2.4.1 Original trilogy
2.4.2 Prequel trilogy
2.4.3 Sequel trilogy
2.4.4 3D releases


2.4.1 Original trilogy
2.4.2 Prequel trilogy
2.4.3 Sequel trilogy
2.4.4 3D releases
3 Box office performance
4 Critical reaction

4.1 Academy Awards


4.1 Academy Awards
5 Expanded Universe

5.1 Other films
5.2 Animated series
5.3 Literature
5.4 Games
5.5 Fan works


5.1 Other films
5.2 Animated series
5.3 Literature
5.4 Games
5.5 Fan works
6 Attractions
7 Legacy
8 See also
9 Notes
10 References
11 Further reading
12 External links
2.1 Plot overview

2.1.1 Cast and characters


2.1.1 Cast and characters
2.2 Themes
2.3 Technical information
2.4 Production history

2.4.1 Original trilogy
2.4.2 Prequel trilogy
2.4.3 Sequel trilogy
2.4.4 3D releases


2.4.1 Original trilogy
2.4.2 Prequel trilogy
2.4.3 Sequel trilogy
2.4.4 3D releases
2.1.1 Cast and characters
2.4.1 Original trilogy
2.4.2 Prequel trilogy
2.4.3 Sequel trilogy
2.4.4 3D releases
4.1 Academy Awards
5.1 Other films
5.2 Animated series
5.3 Literature
5.4 Games
5.5 Fan works
The Star Wars Holiday Special, a 1978 two-hour television special, shown only once and never released on video. Notable for the introduction of Boba Fett.
Caravan of Courage: An Ewok Adventure, a 1984 American made-for-TV filmreleased theatrically overseas.
Ewoks: The Battle for Endor, a 1985 American made-for-TV filmreleased theatrically overseas.
The Great Heep, a 1986 animated television special from the Star Wars: Droids TV series.
Lego Star Wars: The Quest for R2-D2, a 2009 official comedy spoof primarily based on The Clone Wars film.
Star Wars: Droids, also known as Droids: The Adventures of R2-D2 and C-3PO, which premiered in September 1985, focused on the travels of R2-D2 and C-3PO as they shift through various owners/masters, and vaguely fills in the gaps between the events of Star Wars Episode III: Revenge of the Sith and Star Wars Episode IV: A New Hope.
Star Wars: Ewoks, also known as Ewoks, was simultaneously released in September 1985 and focused on the adventures of Wicket and various other recognizable Ewok characters from the original trilogy in the years leading up to Star Wars Episode VI: Return of the Jedi.
Star Wars: Clone Wars animated micro-series created by Genndy Tartakovsky, which aired on Cartoon Network from November 2003 to March 2005.
Star Wars: The Clone Wars CGI-animated series continuation of the animated movie of the same name, which has been airing on Cartoon Network since October 2008.
Star Wars: Detours:[98] an animated comedy series written by Brendan Hay, who is a writer for the comedy news show The Daily Show, and with creative consulting from the co-creators of Robot Chicken: Seth Green and Matthew Senreich. The series will take place during the original trilogy and the setting will be remote from the front line of war.[99]
Notable film parodies of Star Wars include Hardware Wars, a 13-minute 1977 spoof which Lucas has called his favorite Star Wars parody, and Spaceballs, a feature film by Mel Brooks which featured effects done by Lucas' Industrial Light & Magic.[119][120]
Lucasfilm itself made two mockumentaries: Return of the Ewok (1982), about Warwick Davis, who portrayed Wicket W. Warrick in Return of the Jedi; and R2-D2: Beneath the Dome (2002), which depicts R2-D2's "life story".[121][122]
There have also been many songs based on, and in, the Star Wars universe. "Weird Al" Yankovic recorded two parodies: "Yoda", a parody of "Lola" by The Kinks; and "The Saga Begins", a parody of Don McLean's song "American Pie" that retells of The Phantom Menace from Obi-Wan Kenobi's perspective.[123]
In television, the creators of the Robot Chicken series have produced three television specials satirizing the Star Wars films ("Robot Chicken: Star Wars", "Episode II", and "III"), and are developing an animated comedy series based in the Star Wars universe.[124] The creators of the Family Guy series have also produced three Star Wars specials titled "Blue Harvest", "Something, Something, Something, Dark Side", and "It's a Trap!".[125]
During the 2012 Emerald City Comicon in Seattle, Washington several prominent cartoon voice-over actors including Rob Paulsen, Jess Harnell, John DiMaggio, Maurice LaMarche Tara Strong and Kevin Conroy performed a parody reading of Star Wars Episode IV: A New Hope as a radio play in cartoon character voices that they perform on other animated series, i.e. Jess Harnell and Rob Paulsen as Wakko and Yakko Warner from Animaniacs, Tara Strong as Bubbles from The Powerpuff Girls and Princess Clara from Drawn Together, Maurice LaMarche and John DiMaggio as Kiff Kroker and Bender Rodriguez from Futurama and Kevin Conroy narrating as Batman.[126]
When Ronald Reagan proposed the Strategic Defense Initiative (SDI), a system of lasers and missiles meant to intercept incoming ICBMs, the plan was quickly labeled "Star Wars," implying that it was science fiction and linking it to Ronald Reagan's acting career. According to Frances FitzGerald, Reagan was annoyed by this, but Assistant Secretary of Defense Richard Perle told colleagues that he "thought the name was not so bad."; "'Why not?' he said. 'It's a good movie. Besides, the good guys won.'"[127] This gained further resonance when Reagan described the Soviet Union as an "evil empire".
Architecture of Star Wars
Empire of Dreams: The Story of the Star Wars Trilogy
List of Star Wars creatures
Comparison of Star Wars and Star Trek
Physics and Star Wars
Star Wars canon
Star Wars Day
The Story of Star Wars
Jedi census phenomenon
Jediism
Star Wars music
Arnold, Alan (1980). Once Upon a Galaxy: A Journal of the Making of The Empire Strikes Back. Ballantine Books. ISBN0-345-29075-5. http://www.amazon.com/Once-Upon-Galaxy-Journal-Strikes/dp/0345290755
Bouzereau, Laurent (1997). The Annotated Screenplays. Del Rey. ISBN0-345-40981-7. http://www.amazon.com/Star-Wars-Screenplays-Laurent-Bouzereau/dp/0345409817
Kaminski, Michael (2007). "The Secret History of Star Wars". http://secrethistoryofstarwars.com/book.html
Kaminski, Michael (2008). "The Secret History of Star Wars". http://secrethistoryofstarwars.com/book.html. Retrieved May 21, 2008
Rinzler, J.W. (2007). The Making of Star Wars: The Definitive Story Behind the Original Film (Star Wars). Del Rey. ISBN0-345-49476-8. http://www.amazon.com/Making-Star-Wars-Definitive-Original/dp/0345494768
Rinzler, Jonathan (2005). The Making of Star Wars, Episode III - Revenge of the Sith. Del Rey. ISBN0-345-43139-1. http://www.amazon.com/Making-Star-Wars-Episode-III/dp/0345431391.
Star Wars, religion, and philosophy

Bortolin, Matthew (April 25, 2005). The Dharma of Star Wars. Wisdom Publications. ISBN0-86171-497-0. http://www.amazon.com/Dharma-Star-Wars-Matthew-Bortolin/dp/0861714970.
Decker, Kevin S. (March 10, 2005). Star Wars and Philosophy. Open Court. ISBN0-8126-9583-6. http://www.amazon.com/Star-Wars-Philosophy-Popular-Culture/dp/0812695836.
Porter, John M. (January 31, 2003). The Tao of Star Wars. Humanics Trade Group. ISBN0-89334-385-4. http://www.amazon.com/Tao-Star-Wars-John-Porter/dp/0893343854.
Snodgrass, Jon (September 13, 2004). Peace Knights of the Soul. InnerCircle Publishing. ISBN0-9755214-7-0. http://www.amazon.com/Peace-Knights-Soul-Jon-Snodgrass/dp/0975521470.
Staub, Dick (March 25, 2005). Christian Wisdom of the Jedi Masters. Jossey-Bass. ISBN0-7879-7894-9. http://www.amazon.com/Christian-Wisdom-Jedi-Masters-Staub/dp/0787978949.


Bortolin, Matthew (April 25, 2005). The Dharma of Star Wars. Wisdom Publications. ISBN0-86171-497-0. http://www.amazon.com/Dharma-Star-Wars-Matthew-Bortolin/dp/0861714970.
Decker, Kevin S. (March 10, 2005). Star Wars and Philosophy. Open Court. ISBN0-8126-9583-6. http://www.amazon.com/Star-Wars-Philosophy-Popular-Culture/dp/0812695836.
Porter, John M. (January 31, 2003). The Tao of Star Wars. Humanics Trade Group. ISBN0-89334-385-4. http://www.amazon.com/Tao-Star-Wars-John-Porter/dp/0893343854.
Snodgrass, Jon (September 13, 2004). Peace Knights of the Soul. InnerCircle Publishing. ISBN0-9755214-7-0. http://www.amazon.com/Peace-Knights-Soul-Jon-Snodgrass/dp/0975521470.
Staub, Dick (March 25, 2005). Christian Wisdom of the Jedi Masters. Jossey-Bass. ISBN0-7879-7894-9. http://www.amazon.com/Christian-Wisdom-Jedi-Masters-Staub/dp/0787978949.
Joseph Campbell's influence on Star Wars

Campbell, Joseph (June 1, 1991). The Power of Myth. Anchor. ISBN0-385-41886-8. http://www.amazon.com/Power-Myth-Joseph-Campbell/dp/0385418868.
Henderson, Mary (November 3, 1997). Star Wars: The Magic of Myth. Bantam. ISBN0-553-10206-0. http://www.amazon.com/Star-Wars-Magic-Myth-Wars/dp/0553102060.
Larsen, Stephen (April 1, 2002). Joseph Campbell: A Fire in the Mind. Inner Traditions. ISBN0-89281-873-5. http://www.amazon.com/Joseph-Campbell-Fire-Stephen-Larsen/dp/0892818735.


Campbell, Joseph (June 1, 1991). The Power of Myth. Anchor. ISBN0-385-41886-8. http://www.amazon.com/Power-Myth-Joseph-Campbell/dp/0385418868.
Henderson, Mary (November 3, 1997). Star Wars: The Magic of Myth. Bantam. ISBN0-553-10206-0. http://www.amazon.com/Star-Wars-Magic-Myth-Wars/dp/0553102060.
Larsen, Stephen (April 1, 2002). Joseph Campbell: A Fire in the Mind. Inner Traditions. ISBN0-89281-873-5. http://www.amazon.com/Joseph-Campbell-Fire-Stephen-Larsen/dp/0892818735.
Bortolin, Matthew (April 25, 2005). The Dharma of Star Wars. Wisdom Publications. ISBN0-86171-497-0. http://www.amazon.com/Dharma-Star-Wars-Matthew-Bortolin/dp/0861714970.
Decker, Kevin S. (March 10, 2005). Star Wars and Philosophy. Open Court. ISBN0-8126-9583-6. http://www.amazon.com/Star-Wars-Philosophy-Popular-Culture/dp/0812695836.
Porter, John M. (January 31, 2003). The Tao of Star Wars. Humanics Trade Group. ISBN0-89334-385-4. http://www.amazon.com/Tao-Star-Wars-John-Porter/dp/0893343854.
Snodgrass, Jon (September 13, 2004). Peace Knights of the Soul. InnerCircle Publishing. ISBN0-9755214-7-0. http://www.amazon.com/Peace-Knights-Soul-Jon-Snodgrass/dp/0975521470.
Staub, Dick (March 25, 2005). Christian Wisdom of the Jedi Masters. Jossey-Bass. ISBN0-7879-7894-9. http://www.amazon.com/Christian-Wisdom-Jedi-Masters-Staub/dp/0787978949.
Campbell, Joseph (June 1, 1991). The Power of Myth. Anchor. ISBN0-385-41886-8. http://www.amazon.com/Power-Myth-Joseph-Campbell/dp/0385418868.
Henderson, Mary (November 3, 1997). Star Wars: The Magic of Myth. Bantam. ISBN0-553-10206-0. http://www.amazon.com/Star-Wars-Magic-Myth-Wars/dp/0553102060.
Larsen, Stephen (April 1, 2002). Joseph Campbell: A Fire in the Mind. Inner Traditions. ISBN0-89281-873-5. http://www.amazon.com/Joseph-Campbell-Fire-Stephen-Larsen/dp/0892818735.
Official website
Star Wars at the Open Directory Project
Star Wars - Wookieepedia A Wiki Devoted To Star Wars
.
#(`*Indiana Jones*`)#.
Dr. Henry Walton "Indiana" Jones, Jr.[12] is a title character and the protagonist of the Indiana Jones franchise. George Lucas and Steven Spielberg created the character in homage to the action heroes of 1930s film serials. The character first appeared in the 1981 film Raiders of the Lost Ark, to be followed by Indiana Jones and the Temple of Doom in 1984, Indiana Jones and the Last Crusade in 1989, The Young Indiana Jones Chronicles from 1992 to 1996, and Indiana Jones and the Kingdom of the Crystal Skull in 2008. Alongside the more widely known films and television programs, the character is also featured in novels, comics, video games, and other media. Jones is also featured in the Disney theme park attraction Indiana Jones Adventure, which exists in similar forms at Disneyland and Tokyo DisneySea.
Jones is most famously played by Harrison Ford and has also been portrayed by River Phoenix (as the young Jones in The Last Crusade), and in the television series The Young Indiana Jones Chronicles by Corey Carrier, Sean Patrick Flanery, and George Hall. Doug Lee has supplied Jones's voice to two LucasArts video games, Indiana Jones and the Fate of Atlantis and Indiana Jones and the Infernal Machine, while David Esch supplied his voice to Indiana Jones and the Emperor's Tomb and John Armstrong in Indiana Jones and the Staff of Kings.[13]
Particularly notable facets of the character include his iconic look (bullwhip, fedora, and leather jacket), sense of humor, deep knowledge of many ancient civilizations and languages, and fear of snakes.
Since his first appearance in Raiders of the Lost Ark, Indiana Jones has become a worldwide star and remains one of cinema's most revered movie characters. In 2003, he was ranked as the second greatest movie hero of all time by the American Film Institute.[14] He was also named the 6th Greatest Movie Character by Empire magazine.[15] Entertainment Weekly ranked Indy 2nd on their list of The All-Time Coolest Heroes in Pop Culture.[16] Premiere magazine also placed Indy at number 7 on their list of The 100 Greatest Movie Characters of All Time.[17] On their list of the 100 Greatest Fictional Characters, Fandomania.com ranked Indy at number 10.[18] In 2010, he ranked #2 on Time Magazine's list of The Greatest Fictional Characters of All Time, surpassed only by Sherlock Holmes.[citation needed]
Indiana Jones is from Princeton, NJ and was first introduced in the 1981 film Raiders of the Lost Ark, set in 1936. The character is an adventurer reminiscent of the 1930s film serial treasure hunters and pulp action heroes, whose research is funded by Marshall College (named after producer Frank Marshall)[19] a fictional college in Connecticut, where he is a professor of archaeology. In this first adventure, he is pitted against the Nazis, traveling the world to prevent them from recovering the Ark of the Covenant (see also Biblical archaeology). He is aided by Marion Ravenwood and Sallah. The Nazis are led by Jones's archrival, a Nazi-sympathizing French archaeologist named Ren Belloq, and Arnold Toht, a sinister Gestapo agent.
The 1984 prequel, Indiana Jones and the Temple of Doom, set in 1935, took the character into a more horror-oriented story, skipping his legitimate teaching job and globe trotting, and taking place almost entirely in India. This time, Jones attempts to recover children and the Sankara stones from the bloodthirsty Thuggee cult. He is aided by Shorty Short Round and accompanied by Willie Scott (Kate Capshaw).
The third film, 1989's Indiana Jones and the Last Crusade, set in 1938, returned to the formula of the original, reintroducing characters such as Sallah and Marcus Brody, a scene from Professor Jones's classroom (he now teaches at Barnett College), the globe trotting element of multiple locations, and the return of the infamous Nazi mystics, this time trying to find the Holy Grail. The film's introduction, set in 1912, provided some back story to the character, specifically the origin of his fear of snakes, his use of a bullwhip, the scar on his chin, and his hat; the film's epilogue also reveals that "Indiana" is not Jones's first name, but a nickname he took from the family dog. The film was a buddy movie of sorts, teaming Jones with his father, often to comical effect. Although Lucas intended at the time to do five films, this ended up being the last for over eighteen years, as Lucas could not think of a good plot element to drive the next installment.[20]
The 2008 film, Indiana Jones and the Kingdom of the Crystal Skull, became the latest film in the series. Set in 1957, 19 years after the third film, it pits an older, wiser Indiana Jones against Soviet agents bent on harnessing the power of a crystal skull associated with extraterrestrials discovered in South America by his former colleague Harold Oxley (John Hurt). He is aided in his adventure by an old lover, Marion Ravenwood (Karen Allen), and her sona young greaser named Henry "Mutt" Williams (Shia LaBeouf), later revealed to be his biological child, Henry Jones III. There were rumors that LaBeouf will take over the Indy franchise.[21] This film also reveals that Jones was recruited by the Office of Strategic Services (a predecessor department to the CIA) during World War II, attaining the rank of Colonel in the United States Army and running covert operations with MI6 agent George McHale on the Soviet Union.
From 1992 to 1996, George Lucas executive-produced a television series named The Young Indiana Jones Chronicles, aimed mainly at teenagers and older children, which showed many of the important events and historical figures of the early 20th century through the prism of Indiana Jones' life.
The show initially featured the formula of an elderly (93 to 94 years of age) Indiana Jones played by George Hall introducing a story from his youth by way of an anecdote: the main part of the episode then featured an adventure with either a young adult Indy (16 to 21 years of age) played by Sean Patrick Flanery or a child Indy (8 to 11 years) played by Corey Carrier. One episode, "Young Indiana Jones and the Mystery of the Blues", is bookended by Harrison Ford as Indiana Jones, rather than Hall. Later episodes and telemovies did not have this bookend format.
The bulk of the series centers around the young adult Indiana Jones and his activities during World War I as a 16-17 year old soldier in the French Foreign Legion and then as an intelligence officer and spy seconded to French intelligence. The child Indy episodes follow the boy's travels around the globe as he accompanies his parents on his father's worldwide lecture tour from 1908 to 1910.
The show provided some backstory for the films, as well as new information regarding the character. Indiana Jones was born July 1, 1899, and his middle name is Walton (Lucas's middle name). It is also mentioned that he had a sister called Suzie who died as an infant of fever, and that he eventually has a daughter and grandchildren who appear in some episode introductions and epilogues. His relationship with his father, first introduced in Indiana Jones and the Last Crusade, was further fleshed out with stories about his travels with his father as a young boy. Indy damages or loses his right eye sometime between the events in 1957 and the early 1990s, when the "Old Indy" segments take place, as the elderly Indiana Jones wears an eyepatch.
In 1999, Lucas removed the episode introductions and epilogues by George Hall for the VHS and DVD releases, and re-edited the episodes into chronologically ordered feature-length stories. The series title was also changed to The Adventures of Young Indiana Jones.
The character has appeared in several officially licensed games, including LEGO Indiana Jones video games, beginning with adaptations of Raiders of the Lost Ark, Temple of Doom, two adaptations of The Last Crusade (one with purely action mechanics, one with an adventure and puzzle based structure) and Indiana Jones' Greatest Adventures which included the storylines from all three of the original films.
Following this, the games branched off into original storylines with Indiana Jones in the Lost Kingdom, Indiana Jones and the Fate of Atlantis, Indiana Jones and the Infernal Machine, Indiana Jones and the Emperor's Tomb and Indiana Jones and the Staff of Kings.[22] Emperor's Tomb sets up Jones's companion Wu Han and the search for Nurhaci's ashes seen at the beginning of Temple of Doom. The first two games were developed by Hal Barwood and starred Doug Lee as the voice of Indiana Jones; Emperor's Tomb had David Esch fill the role and Staff of Kings starred John Armstrong.
Indiana Jones and the Infernal Machine was the first Indy-based game presented in three dimensions, as opposed to 8-bit graphics and side-scrolling games before.
There is also a small game from Lucas Arts Indiana Jones and His Desktop Adventures. A video game was made for young Indy called Young Indiana Jones and the Instruments of Chaos, as well as a video game version of The Young Indiana Jones Chronicles.
Two Lego Indiana Jones games have also been released. Lego Indiana Jones: The Original Adventures was released in 2008[23] and follows the plots of the first three films. It was followed by LEGO Indiana Jones 2: The Adventure Continues in late 2009. The sequel includes an abbreviated reprise of the first three films, but focuses on the plot of Indiana Jones and the Kingdom of the Crystal Skull.
Indiana Jones has also made cameo appearances as an unlockable character in the games Mercenaries: Playground of Destruction and Lego Star Wars: The Complete Saga.
Social gaming company Zynga introduced Indiana Jones to their "Adventure World" game in late 2011.[24]
Indiana Jones is featured at several Walt Disney theme park attractions. The Indiana Jones Adventure attractions at Disneyland and Tokyo DisneySea ("Temple of the Forbidden Eye" and "Temple of the Crystal Skull," respectively) place Indy at the forefront of two similar archaeological discoveries. These two temples each contain a wrathful deity who threatens the guests who ride through in World War II troop transports. The attractions, some of the most expensive of their kind at the time, opened in 1995 and 2001, respectively, with sole design credit attributed to Walt Disney Imagineering. Disney did not license Harrison Ford's likeness for the North American version; nevertheless, a differentiated Indiana Jones audio-animatronic character appears at three points in both attractions. However, the Indiana Jones featured in the DisneySea version does use Harrison Ford's likeness but uses Japanese audio for all of his speaking parts. In 2010, some of the Indy audio-animatronics at the Disneyland version have been replaced and now resemble Ford.[25]
Disneyland Paris also features an Indiana Jones-titled ride where people speed off through ancient ruins in a runaway mine wagon similar to that found in Indiana Jones and the Temple of Doom. Indiana Jones and the Temple of Peril is a looping roller coaster engineered by Intamin, designed by Walt Disney Imagineering, and opened in 1993.
The Indiana Jones Epic Stunt Spectacular! is a live show that has been presented in the Disney's Hollywood Studios theme park of the Walt Disney World Resort with few changes since the park's 1989 opening under a different name. The 25-minute show presents various stunts framed in the context of a feature film production, and recruits members of the audience to participate in the show. Stunt artists in the show re-create and ultimately reveal some of the secrets of the stunts of the Raiders of the Lost Ark films, including the well-known "running-from-the-boulder" scene. Stunt performer Anislav Varbanov was fatally injured in August 2009, while rehearsing the popular show.[26] Also at Disney's Hollywood Studios, an audio-animatronic Indiana Jones appears in another attraction; during the The Great Movie Ride's Raiders of the Lost Ark segment.
In his role as a college professor of archaeology, Henry Jones Jr. is scholarly and learned in a tweed suit, lecturing on ancient civilizations. In Indiana Jones and the Kingdom of the Crystal Skull, it is revealed that Jones is influenced by the Marxist Archaeologist, Vere Gordon Childe, whose qualified acceptance of cultural diffusionism theory he propounds. Ironically, though Childe loathes fieldwork,[27] Indy goes on to say, "If you want to be a good archaeologist, you gotta get out of the library." This is in tongue-in-cheek contrast to the previous film's comment, "Seventy percent of all archaeology is done in the library."
However, at the opportunity to recover important artifacts, Dr. Jones transforms into "Indiana," a "non-superhero superhero" image he has concocted for himself.[28] Producer Frank Marshall said, "Indy [is] a fallible character. He makes mistakes and gets hurt. [...] That's the other thing people like: He's a real character, not a character with superpowers."[29] Spielberg said there "was the willingness to allow our leading man to get hurt and to express his pain and to get his mad out and to take pratfalls and sometimes be the butt of his own jokes. I mean, Indiana Jones is not a perfect hero, and his imperfections, I think, make the audience feel that, with a little more exercise and a little more courage, they could be just like him."[30] According to Spielberg biographer Douglas Brode, Indiana created his heroic figure so as to escape the dullness of teaching at a school. Both of Indiana's personas reject one another in philosophy, creating a duality.[28] Harrison Ford said the fun of playing the character was because Indiana is both a romantic and a cynic,[31] while scholars have analyzed Indiana as having traits of a lone wolf; a man on a quest; a noble treasure hunter; a hardboiled detective; a human superhero; and an American patriot.[32]
Like many characters in his films, Jones has some autobiographical elements of Spielberg. Indiana lacks a proper father figure because of his strained relationship with his father, Henry Senior. His own contained anger is misdirected at the likes of Professor Abner Ravenwood, his mentor at the University of Chicago, leading to a strained relationship with Marion Ravenwood.[28] The teenage Indiana bases his own look on a figure from the prologue of Indiana Jones and the Last Crusade, after being given his hat.[33] Marcus Brody acts as Indiana's positive role model at the college.[33] Indiana's own insecurities are made worse by the absence of his mother.[34] In Indiana Jones and the Temple of Doom, the character becomes the father in a temporary family unit with Willie Scott and Short Round to survive. Indiana is rescued from the evil of Kali by Short Round's dedication. Indiana also saves many children from slavery.[34]
Because of Indiana's strained relationship with his father, who was absent much of Indiana's youth searching for the Holy Grail, the character does not pursue the more spiritual aspects of the cultures he studies.[citation needed] Indiana uses his knowledge of Shiva to ultimately defeat Mola Ram.[34] In Raiders, however, he is wise enough to close his eyes in the presence of God in the Ark of the Covenant. By contrast, his rival Rene Belloq dies horribly for having the audacity to try to communicate directly with God.[28]
In Crusade's prologue, Indiana's intentions are revealed as prosocial, as he believes artifacts "belong in a museum." In the film's climax, Indiana undergoes "literal" tests of faith to retrieve the Grail and save his father's life. He also remembers Jesus as a historical figure  a humble carpenter  rather than an exalted figure when he recognizes the simple nature and tarnished appearance of the real Grail amongst a large assortment of much more ornately decorated ones. Henry Senior rescues his son from falling to his death when reaching for the fallen Grail, telling him to "let it go," overcoming his mercenary nature.[33] The Young Indiana Jones Chronicles explains how Indiana becomes solitary and less idealistic after fighting in World War I.[35] In Indiana Jones and the Kingdom of the Crystal Skull, Jones is older and wiser, whereas his sidekicks Mutt and Mac are youthfully arrogant and greedy, respectively.[36]
Indiana Jones is modeled after the strong-jawed heroes of the matine serials and pulp magazines that George Lucas and Steven Spielberg enjoyed in their childhoods (such as the Republic Pictures serials, and the Doc Savage series). Sir H. Rider Haggard's safari guide/big game hunter Allan Quatermain of King Solomon's Mines, who dates back to 1885, is a notable template for Jones.[37] The two friends first discussed the project in Hawaii around the time of the release of the first Star Wars film.[38] Spielberg told Lucas how he wanted his next project to be something fun, like perhaps a James Bond film. According to sources, Lucas responded to the effect that he had something "even better,"[38] or that he "got that beat."[39]
Two of the possible bases for Indiana Jones are Professor Challenger, created by Sir Arthur Conan Doyle in 1912 for his novel, The Lost World, who was in turn based on his Doyle physiology professor, Sir William Rutherford, an adventuring academic, albeit a zoologist/anthropologist.[citation needed]
The character was originally named Indiana Smith (perhaps in a nod to the 1966 Western film Nevada Smith), after an Alaskan Malamute Lucas owned in the 1970s (Indiana); however, Spielberg disliked the name Smith, and Lucas casually suggested Jones as an alternative.[38]
Lucas has said on various occasions that Sean Connery's portrayal of British secret agent James Bond was one of the primary inspirations for Jones, a reason Connery was chosen for the role of Indiana's father in Indiana Jones and the Last Crusade.[40][41]
Costume designer Deborah Nadoolman Landis said the inspiration for Indiana's outfit was Charlton Heston's Harry Steele in Secret of the Incas: "We did watch this film together as a crew several times, and I always thought it strange that the filmmakers did not credit it later as the inspiration for the series."[42]
Many people are said to be the real-life inspiration of the Indiana Jones characteralthough none of the following have been confirmed as inspirations by Lucas or Spielberg. There are some suggestions, listed here in alphabetical order by last name:
In general, when asking a member of the public to name an archaeologist, or even something about archaeology, Indiana Jones comes up. Students of archaeology and professionals alike often name Indiana Jones as one of their inspirations, or maybe what interested them in archaeology to begin with, despite some obvious issues with how Dr. Jones practices archaeology. No stranger to criticism when it comes to the practice of archaeology (treasure-hunter, looter, etc.) , Indiana Jones, as representative of archaeology and anthropology as a whole, has some deeper, core ethical issues as well. Cultural relativism, succinctly defined as regarding all cultures as equally valid, lies at the core of what archaeologists and anthropologists do. While far from perfect, it is at least something to strive for the ability to see outside ones own cultural biases, to be as un-ethnocentric as possible. Indiana Jones doesn't seem to be striving very hard. Relations with indigenous peoples is an important ethical debates in archaeology today, along with issues of ownership, who has the right to interpret the past, and of course, looting.[54]
Indiana Jones begins the first film immediately addressing ethical issues as in, showing us the wrong way to go about archaeology. He has a side-kick, or perhaps a hired guide, an obvious representative of the indigenous people there. The guide fumbles along, not once asked for advice by Indiana, eventually even attempting to take the artifact for himself. The audience of course sees this as wrong since Indiana found the artifact, it must be his. In reality, the guide may have held a more substantial claim. Ownership of archaeological sites or materials is notoriously slippery, but the Indiana Jones leads audiences to the incorrect assumption of finders-keepers, and that preservation and understandings lies solely with academics from the West.
The ethical dilemmas of Indiana Jones are still current today, but they also reflect the roots of the archaeological discipline. Archaeology dates back much further, where affluent and enthusiastic collectors kept artifacts to show off to friends, but the discipline itself is rooted in colonialism. The earliest development of archaeology then is the transformation from a hobby of those economically advantaged enough to pursue it, to a serious and highly regarded academic discipline.[55] Indiana Jones represents the beginnings of that discipline, still very much in its infant stages, and unfortunately leading modern audiences to adopt the ideals taught in the three, now four, films.
Upon requests by Spielberg and Lucas, the costume designer gave the character a distinctive silhouette through the styling of the hat; after examining many hats, the designers chose a tall-crowned, wide-brimmed fedora. As a documentary of Raiders pointed out, the hat served a practical purpose. Following the lead of the old "B"-movies that inspired the Indiana Jones series, the fedora hid the actor's face sufficiently to allow doubles to perform the more dangerous stunts seamlessly. Examples in Raiders include the wider-angle shot of Indy and Marion crashing a statue through a wall, and Indy sliding under a fast-moving vehicle from front to back. Thus it was necessary for the hat to stay in place much of the time.
The hat became so iconic that the filmmakers could only come up with very good reasons or jokes to remove it. If it ever fell off during a take, filming would have to stop to put it back on. In jest, Ford put a stapler against his head to stop his hat from falling off when a documentary crew visited during shooting of Indiana Jones and the Last Crusade. This created the urban legend that Ford stapled the hat to his head.[56] Although other hats were also used throughout the movies, the general style and profile remained the same. Elements of the outfit include:
The fedora and leather jacket from Indiana Jones and the Last Crusade are on display at the Smithsonian's American History Museum in Washington, D.C.[60] The collection of props and clothing from the films has become a thriving hobby for some aficionados of the franchise.[61] Jones' whip was the third most popular film weapon, as shown by a 2008 poll held by 20th Century Fox, which surveyed approximately two thousand film fans.[62]
Originally, Spielberg suggested Harrison Ford; Lucas resisted the idea, since he had already cast the actor in American Graffiti, Star Wars, and The Empire Strikes Back, and did not want Ford to become known as his "Bobby De Niro" (in reference to the fact that fellow director Martin Scorsese regularly cast Robert De Niro in his films).[38] During an intensive casting process, Lucas and Spielberg auditioned many actors, and finally cast then little-known actor Tom Selleck as Indiana Jones. Shortly afterward pre-production began in earnest on Raiders of the Lost Ark.[38] However, CBS refused to release Selleck from his contractual commitment to Magnum, P.I. (which was gradually gaining momentum in the ratings), forcing him to turn down the role.[38] One of CBS's concerns was that shooting for Magnum P.I. conflicted with shooting for Raiders, both of which were to begin about the same time. However, Selleck was to say later in an interview that shooting for Magnum P.I. was delayed and did not actually begin until shooting for Raiders had concluded.
After Spielberg suggested Ford again, Lucas gave in, and Ford was cast in the role less than three weeks before filming began.[38]
Though some archaeologists criticize Indy's methods as befitting a "looter" rather more than a careful worker of precious sites, many have adopted the popular figure as something of a standard-bearer for their profession.[63] The industry magazine Archaeology, believing that Jones, as one editor said, was "a horrible archaeologist but a great diplomat for archaeology,"[63] named eight past and current archaeologists who they felt "embodied [Jones'] spirit" as recipients of the "Indy Spirit Awards" in 2008.[64] That same year Ford himself was elected to the Board of Directors of the Archaeological Institute of America. Commenting that "understanding the past can only help us in dealing with the present and the future," Ford was praised by the association's president for his character's "significant role in stimulating the public's interest in archaeological exploration."[65]
While himself a homage to various prior adventurers, aspects of Indiana Jones also directly influenced some subsequent characterizations:
 
1 Appearances

1.1 Television
1.2 Video games
1.3 Theme parks


1.1 Television
1.2 Video games
1.3 Theme parks
2 Character description and formation
3 Origins and inspirations

3.1 Historical models
3.2 Ethical issues for archaeology


3.1 Historical models
3.2 Ethical issues for archaeology
4 Costume

4.1 Casting


4.1 Casting
5 Influence
6 References
7 External links
1.1 Television
1.2 Video games
1.3 Theme parks
3.1 Historical models
3.2 Ethical issues for archaeology
4.1 Casting
Beloit College professor and paleontologist Roy Chapman Andrews.[43]
Italian archaeologist and circus strongman Giovanni Battista Belzoni (17781823).[44]
Yale University professor, historian, and explorer Hiram Bingham III, who rediscovered and excavated the lost city of Machu Picchu,[45] and chronicled his find in the bestselling book The Lost City of the Incas in 1948.[46]
University of Chicago archaeologist Robert Braidwood.[47]
University of Chicago archaeologist James Henry Breasted.[48]
The British archaeologist Percy Fawcett, who spent much of his life exploring the jungles of northern Brazil, and who was last seen in 1925 returning to the Amazon Basin to look for the Lost City Of Z. A fictionalized version of Fawcett appears to Jones in the book Indiana Jones And The Seven Veils.[7]
The Harvard University paleontologist Farish Jenkins.[49]
British archaeologist and soldier T. E. Lawrence.[50]
The Northwestern University anthropologist, professor and adventurer William Montgomery McGovern.[51]
Frederick Albert Mitchell-Hedges.[52]
German archaeologist Otto Rahn.[53]
The fedora was supplied by Herbert Johnson Hatters in England for the first three films. It was referred to as "The Australian Model" by costume designer Deborah Landis and was fitted with a Petersham bow.[57] The fedora for Crystal Skull was made by Steve Delk and Marc Kitter of the Adventurebilt Hat Company of Columbus, Mississippi.
The leather jacket, a hybrid of the "Type 440" and the A-2 jacket, was made by Leather Concessionaires (now known as Wested Leather Co.) for Raiders of the Lost Ark and Indiana Jones and the Last Crusade. For Indiana Jones and the Temple of Doom, jackets were made in-house at Bermans & Nathans in London based on a stunt jacket they provided for Raiders of the Lost Ark. Tony Nowak made the jacket for Indiana Jones and the Kingdom of the Crystal Skull.
The bag was a modified Mark VII British gas mask bag, usually worn under the jacket with the exception of Crystal Skull where the bag was worn over the jacket.
The whip was a 8 to 10 foot (2.4 to 3.0 m) bullwhip crafted by David Morgan for the first three films. The whips for Crystal Skull were crafted by a variety of people, including Terry Jacka, Joe Strain and Morgan (different lengths and styles were likely used in specific stunts).
The pistol was usually a World War I-era revolver, including the Webley Government (Last Crusade and Crystal Skull), or a .45 ACP Smith & Wesson Hand Ejector 2nd model revolver (Raiders). He has also used an M1917 revolver (Temple of Doom), a Nagant M1895 (Young Indiana Jones), and a 9mm Browning Hi-Power (Raiders).[58] The weapon is carried in a military pattern flap holster.
The shoes were made by Alden. A stock style (model 405) that had been a favorite of Ford's before the movies, they are still sold today (though in a redder (brick) shade of brown than seen in the movies) and are popularly known as "Indy Boots."[59]
Lara Croft, the female archaeologist of the Tomb Raider series, was originally designed as a man, but was changed to a woman, partly because the developers felt that the original design was too similar to Indiana Jones.[66] Paramount Pictures, which distributed the Indiana Jones film series, would later make two films based on the Tomb Raider games.
Prince of Persia producer Ben Mattes explained that their "inspiration was anything Harrison Ford has ever done: Indiana Jones, Han Solo."[67]
The video game series Uncharted is also very heavily influenced and inspired by Indiana Jones. The main character, Nathan Drake, also shares many similarities with Indiana Jones himself, both visually and personality-wise. The design team felt the sources shared themes of mystery and "what-if scenarios" that romanticized adventure and aimed to include those in Uncharted.[68]
IndianaJones.com  the official Indiana Jones site
Indiana Jones at the Internet Movie Database
The Indiana Jones Wiki A wiki devoted to Indiana Jones
Indiana Jones on Rankopedia.com
.
#(`*Apple*`)#.
The apple is the pomaceous fruit of the apple tree, species Malus domestica in the rose family (Rosaceae). It is one of the most widely cultivated tree fruits, and the most widely known of the many members of genus Malus that are used by humans. Apples grow on small, deciduous trees. The tree originated in Western Asia, where its wild ancestor, Malus sieversii, is still found today. Apples have been grown for thousands of years in Asia and Europe, and were brought to North America by European colonists. Apples have been present in the mythology and religions of many cultures, including Norse, Greek and Christian traditions. In 2010, the fruit's genome was decoded, leading to new understandings of disease control and selective breeding in apple production.
There are more than 7,500 known cultivars of apples, resulting in a range of desired characteristics. Different cultivars are bred for various tastes and uses, including in cooking, fresh eating and cider production. Domestic apples are generally propagated by grafting, although wild apples grow readily from seed. Trees are prone to a number of fungal, bacterial and pest problems, which can be controlled by a number of organic and non-organic means.
About 69million tonnes of apples were grown worldwide in 2010, and China produced almost half of this total. The United States is the second-leading producer, with more than 6% of world production. Turkey is third, followed by Italy, India and Poland. Apples are often eaten raw, but can also be found in many foods (especially desserts) and drinks. Many beneficial health effects have been found from eating apples; however, the seeds are slightly poisonous as they contain cyanide and two forms of allergies are seen to various proteins found in the fruit.
The apple forms a tree that is small and deciduous, reaching 3 to 12 metres (9.8 to 39 ft) tall, with a broad, often densely twiggy crown.[2] The leaves are alternately arranged simple ovals 5 to 12cm long and 36 centimetres (1.22.4 in) broad on a 2 to 5 centimetres (0.79 to 2.0 in) petiole with an acute tip, serrated margin and a slightly downy underside. Blossoms are produced in spring simultaneously with the budding of the leaves. The flowers are white with a pink tinge that gradually fades, five petaled, and 2.5 to 3.5 centimetres (0.98 to 1.4 in) in diameter. The fruit matures in autumn, and is typically 5 to 9 centimetres (2.0 to 3.5 in) in diameter. The center of the fruit contains five carpels arranged in a five-point star, each carpel containing one to three seeds, called pips.[2]
The original wild ancestors of Malus domestica was Malus sieversii, found growing wild in the mountains of Central Asia in southern Kazakhstan, Kyrgyzstan, Tajikistan, and Xinjiang, China.[3] Cultivation of the species, most likely beginning on the forested flanks of the Tian Shan mountains, has progressed over a long period of time and permitted secondary introgression of genes from other species into the open-pollinated seeds, including such a large amount of gene exchange with Malus sylvestris, the crabapple, that current populations of apples are more related to those of crabapples than to the more morphologically similar progenitor Malus sieversii, though in pure strains without recent admixture the contribution of the latter predominates.[4][5][6]
In 2010, an Italian-led consortium announced they had decoded the complete genome of the apple in collaboration with horticultural genomicists at Washington State University,[7] using the Golden delicious variety.[8] It had about 57,000 genes, the highest number of any plant genome studied to date[9] and more genes than the human genome (about 30,000).[10] This new understanding of the apple genome will help scientists in identifying genes and gene variants that contribute to resistance to disease and drought, and other desirable characteristics. Understanding the genes behind these characteristics will allow scientists to perform more knowledgeable selective breeding. Decoding the genome also provided proof that Malus sieversii was the wild ancestor of the domestic applean issue that had been long-debated in the scientific community.[11]
The center of diversity of the genus Malus is in eastern Turkey. The apple tree was perhaps the earliest tree to be cultivated,[12] and its fruits have been improved through selection over thousands of years. Alexander the Great is credited with finding dwarfed apples in Kazakhstan in Asia in 328 BCE;[2] those he brought back to Macedonia might have been the progenitors of dwarfing root stocks. Winter apples, picked in late autumn and stored just above freezing, have been an important food in Asia and Europe for millennia, as well as in Argentina and in the United States since the arrival of Europeans.[12] Apples were brought to North America with colonists in the 17th century,[2] and the first apple orchard on the North American continent was planted in Boston by Reverend William Blaxton in 1625.[13] The only apples native to North America are crab apples, which were once called "common apples".[14] Apple varieties brought as seed from Europe were spread along Native American trade routes, as well as being cultivated on Colonial farms. An 1845 United States apples nursery catalogue sold 350 of the "best" varieties, showing the proliferation of new North American varieties by the early 19th century.[15] In the 20th century, irrigation projects in Washington state began and allowed the development of the multibillion dollar fruit industry, of which the apple is the leading species.[2]
Until the 20th century, farmers stored apples in frostproof cellars during the winter for their own use or for sale. Improved transportation of fresh apples by train and road replaced the necessity for storage.[16][17] In the 21st century, long-term storage again came into popularity, as "controlled atmosphere" facilities were used to keep apples fresh year-round. Controlled atmosphere facilities use high humidity and low oxygen and carbon dioxide levels to maintain fruit freshness.[18]
In Norse mythology, the goddess Iunn is portrayed in the Prose Edda (written in the 13th century by Snorri Sturluson) as providing apples to the gods that give them eternal youthfulness. English scholar H. R. Ellis Davidson links apples to religious practices in Germanic paganism, from which Norse paganism developed. She points out that buckets of apples were found in the Oseberg ship burial site in Norway, and that fruit and nuts (Iunn having been described as being transformed into a nut in Skldskaparml) have been found in the early graves of the Germanic peoples in England and elsewhere on the continent of Europe, which may have had a symbolic meaning, and that nuts are still a recognized symbol of fertility in southwest England.[19]
Davidson notes a connection between apples and the Vanir, a tribe of gods associated with fertility in Norse mythology, citing an instance of eleven "golden apples" being given to woo the beautiful Gerr by Skrnir, who was acting as messenger for the major Vanir god Freyr in stanzas 19 and 20 of Skrnisml. Davidson also notes a further connection between fertility and apples in Norse mythology in chapter 2 of the Vlsunga saga when the major goddess Frigg sends King Rerir an apple after he prays to Odin for a child, Frigg's messenger (in the guise of a crow) drops the apple in his lap as he sits atop a mound.[20] Rerir's wife's consumption of the apple results in a six-year pregnancy and the Caesarean section birth of their sonthe hero Vlsung.[21]
Further, Davidson points out the "strange" phrase "Apples of Hel" used in an 11th century poem by the skald Thorbiorn Brnarson. She states this may imply that the apple was thought of by the skald as the food of the dead. Further, Davidson notes that the potentially Germanic goddess Nehalennia is sometimes depicted with apples and that parallels exist in early Irish stories. Davidson asserts that while cultivation of the apple in Northern Europe extends back to at least the time of the Roman Empire and came to Europe from the Near East, the native varieties of apple trees growing in Northern Europe are small and bitter. Davidson concludes that in the figure of Iunn "we must have a dim reflection of an old symbol: that of the guardian goddess of the life-giving fruit of the other world."[19]
Apples appear in many religious traditions, often as a mystical or forbidden fruit. One of the problems identifying apples in religion, mythology and folktales is that the word "apple" was used as a generic term for all (foreign) fruit, other than berries, but including nuts, as late as the 17th century.[22] For instance, in Greek mythology, the Greek hero Heracles, as a part of his Twelve Labours, was required to travel to the Garden of the Hesperides and pick the golden apples off the Tree of Life growing at its center.[23][24][25]
The Greek goddess of discord, Eris, became disgruntled after she was excluded from the wedding of Peleus and Thetis.[26] In retaliation, she tossed a golden apple inscribed  (Kalliste, sometimes transliterated Kallisti, 'For the most beautiful one'), into the wedding party. Three goddesses claimed the apple: Hera, Athena, and Aphrodite. Paris of Troy was appointed to select the recipient. After being bribed by both Hera and Athena, Aphrodite tempted him with the most beautiful woman in the world, Helen of Sparta. He awarded the apple to Aphrodite, thus indirectly causing the Trojan War.
The apple was thus considered, in ancient Greece, to be sacred to Aphrodite, and to throw an apple at someone was to symbolically declare one's love; and similarly, to catch it was to symbolically show one's acceptance of that love.[27] An epigram claiming authorship by Plato states:
Atalanta, also of Greek mythology, raced all her suitors in an attempt to avoid marriage. She outran all but Hippomenes (a.k.a. Melanion, a name possibly derived from melon the Greek word for both "apple" and fruit in general),[24] who defeated her by cunning, not speed. Hippomenes knew that he could not win in a fair race, so he used three golden apples (gifts of Aphrodite, the goddess of love) to distract Atalanta. It took all three apples and all of his speed, but Hippomenes was finally successful, winning the race and Atalanta's hand.[23]
Though the forbidden fruit in the Book of Genesis is not identified, popular Christian tradition has held that it was an apple that Eve coaxed Adam to share with her.[29] This may have been the result of Renaissance painters adding elements of Greek mythology into biblical scenes (alternative interpretations also based on Greek mythology occasionally replace the apple with a pomegranate). In this case the unnamed fruit of Eden became an apple under the influence of story of the golden apples in the Garden of Hesperides. As a result, in the story of Adam and Eve, the apple became a symbol for knowledge, immortality, temptation, the fall of man into sin, and sin itself. In Latin, the words for "apple" and for "evil" are similar (mlum "an apple", mlum "an evil, a misfortune"). This may also have influenced the apple becoming interpreted as the biblical "forbidden fruit". The larynx in the human throat has been called Adam's apple because of a notion that it was caused by the forbidden fruit sticking in the throat of Adam.[29] The apple as symbol of sexual seduction has been used to imply sexuality between men, possibly in an ironic vein.[29]
There are more than 7,500 known cultivars of apples.[30] Cultivars vary in their yield and the ultimate size of the tree, even when grown on the same rootstock.[31] Different cultivars are available for temperate and subtropical climates. The UK's National Fruit Collection, which is the responsibility of the Department of Environment Food and Rural Affairs, has a collection of over 2,000 accessions in Kent[32]{{cite web url=http://www.ecpgr.cgiar.org/index.php?id=2501&tx_wfqbe_pi1[uid]=59 |accessdate=2 December 2012}} The UK's national fruit collection database contains a wealth of information on the characteristics and origin of many apples, including alternative names for what is essentially the same 'genetic' apple variety. Search for example for Court Pendu Plat to find alternative names including names in use across many other countries. Most of these cultivars are bred for eating fresh (dessert apples), though some are cultivated specifically for cooking (cooking apples) or producing cider. Cider apples are typically too tart and astringent to eat fresh, but they give the beverage a rich flavour that dessert apples cannot.[33]
Commercially popular apple cultivars are soft but crisp. Other desired qualities in modern commercial apple breeding are a colourful skin, absence of russeting, ease of shipping, lengthy storage ability, high yields, disease resistance, typical 'Red Delicious' apple shape, and popular flavour.[31] Modern apples are generally sweeter than older cultivars, as popular tastes in apples have varied over time. Most North Americans and Europeans favour sweet, subacid apples, but tart apples have a strong minority following.[34] Extremely sweet apples with barely any acid flavour are popular in Asia[34] and especially India.[33]
Old cultivars are often oddly shaped, russeted, and have a variety of textures and colours. Some find them to have a better flavour than modern cultivars,[35] but may have other problems which make them commercially unviable, such as low yield, disease susceptibility, or poor tolerance for storage or transport. A few old cultivars are still produced on a large scale, but many have been kept alive by home gardeners and farmers that sell directly to local markets. Many unusual and locally important cultivars with their own unique taste and appearance exist; apple conservation campaigns have sprung up around the world to preserve such local cultivars from extinction. In the United Kingdom, old cultivars such as 'Cox's Orange Pippin' and 'Egremont Russet' are still commercially important even though by modern standards they are low yielding and disease prone.[2]
In the wild, apples grow quite readily from seeds. However, like most perennial fruits, apples are ordinarily propagated asexually by grafting. This is because seedling apples are an example of "extreme heterozygotes", in that rather than inheriting DNA from their parents to create a new apple with those characteristics, they are instead different from their parents, sometimes radically.[36] Triploids have an additional reproductive barrier in that the 3 sets of chromosomes cannot be divided evenly during meiosis, yielding unequal segregation of the chromosomes (aneuploids). Even in the very unusual case when a triploid plant can produce a seed (apples are an example), it happens infrequently, and seedlings rarely survive.[37] Most new apple cultivars originate as seedlings, which either arise by chance or are bred by deliberately crossing cultivars with promising characteristics.[38] The words 'seedling', 'pippin', and 'kernel' in the name of an apple cultivar suggest that it originated as a seedling. Apples can also form bud sports (mutations on a single branch). Some bud sports turn out to be improved strains of the parent cultivar. Some differ sufficiently from the parent tree to be considered new cultivars.[39]
The Excelsior Experiment Station of the University of Minnesota has, since the 1930s, introduced a steady progression of important hardy apples that are widely grown, both commercially and by backyard orchardists, throughout Minnesota and Wisconsin. Its most important introductions have included 'Haralson' (which is the most widely cultivated apple in Minnesota), 'Wealthy', 'Honeygold', and 'Honeycrisp'.
Apples have been acclimatized in Ecuador at very high altitudes, where they provide crops twice per year because of constant temperate conditions year-round.[40]
Rootstocks used to control tree size have been used in apple cultivation for over 2,000 years. Dwarfing rootstocks were probably discovered by chance in Asia.[citation needed] Alexander the Great sent samples of dwarf apple trees back to his teacher, Aristotle, in Greece. They were maintained at the Lyceum, a center of learning in Greece.
Most modern apple rootstocks were bred in the 20th century. Much research into the existing rootstocks was begun at the East Malling Research Station in Kent, England. Following that research, Malling worked with the John Innes Institute and Long Ashton to produce a series of different rootstocks with disease resistance and a range of different sizes, which have been used all over the world.
Apples are self-incompatible; they must cross-pollinate to develop fruit. During the flowering each season, apple growers usually provide pollinators to carry the pollen. Honey bees are most commonly used. Orchard mason bees are also used as supplemental pollinators in commercial orchards. Bumblebee queens are sometimes present in orchards, but not usually in enough quantity to be significant pollinators.[39]
There are four to seven pollination groups in apples, depending on climate:
One cultivar can be pollinated by a compatible cultivar from the same group or close (A with A, or A with B, but not A with C or D).[41]
Varieties are sometimes classed as to the day of peak bloom in the average 30 day blossom period, with pollinizers selected from varieties within a 6 day overlap period.
Cultivars vary in their yield and the ultimate size of the tree, even when grown on the same rootstock. Some cultivars, if left unpruned, will grow very large, which allows them to bear much more fruit, but makes harvesting very difficult. Mature trees typically bear 40200 kilograms (88440 lb) of apples each year, though productivity can be close to zero in poor years. Apples are harvested using three-point ladders that are designed to fit amongst the branches. Dwarf trees will bear about 1080 kilograms (22180 lb) of fruit per year.[39]
Crops ripen at different times of the year according to the variety of apple. Varieties that yield their crop in the Summer include: Transparent, Primate, Sweet Bough, and Duchess; Fall producers include: Chenango, Gravenstein, Wealthy, McIntosh, Snow, and Blenheim; Winter producers include: King, Wagener, Swayzie, Greening, and Tolman Sweet.[42]
Commercially, apples can be stored for some months in controlled-atmosphere chambers to delay ethylene-induced onset of ripening. The apples are commonly stored in chambers with higher concentrations of carbon dioxide and high air filtration. This prevents ethylene concentrations from rising to higher amounts and preventing ripening from moving too quickly. Ripening continues when the fruit is removed.[43] For home storage, most varieties of apple can be held for approximately two weeks when kept at the coolest part of the refrigerator (i.e. below 5C). Some types, including the Granny Smith and Fuji, can be stored up to a year without significant degradation.[44][45]
The trees are susceptible to a number of fungal and bacterial diseases and insect pests. Many commercial orchards pursue an aggressive program of chemical sprays to maintain high fruit quality, tree health, and high yields. A trend in orchard management is the use of organic methods. These use a less aggressive and direct methods of conventional farming. Instead of spraying potent chemicals, often shown to be potentially dangerous and maleficent to the tree in the long run, organic methods include encouraging or discouraging certain cycles and pests. To control a specific pest, organic growers might encourage the prosperity of its natural predator instead of outright killing it, and with it the natural biochemistry around the tree. Organic apples generally have the same or greater taste than conventionally grown apples, with reduced cosmetic appearances.[46]
A wide range of pests and diseases can affect the plant; three of the more common diseases/pests are mildew, aphids and apple scab.
Among the most serious disease problems are fireblight, a bacterial disease; and Gymnosporangium rust, and black spot, two fungal diseases.[48] Codling moths and apple maggots are two other pests which affect apple trees. Young apple trees are also prone to mammal pests like mice and deer, which feed on the soft bark of the trees, especially in winter.[49]
About 69 million tonnes of apples were grown worldwide in 2010, and China produced almost half of this total. The United States is the second-leading producer, with more than 6% of world production. The largest exporters of apples in 2009 were China, the U.S., Poland, Italy, Chile, and France while the biggest importers in the same year were Russia, Germany, the UK and the Netherlands.[50]
In the United States, more than 60% of all the apples sold commercially are grown in Washington state.[51] Imported apples from New Zealand and other more temperate areas are competing with US production and increasing each year.[52]
Most of Australia's apple production is for domestic consumption. Imports from New Zealand have been disallowed under quarantine regulations for fireblight since 1921.[53]
Source: [1]
Other countries with a significant production are Russia, Ukraine, Argentina, Germany and Japan.
Apples are often eaten raw; except for the seeds, which are slightly poisonous (see below), the whole fruit including the skin is suitable for human consumption but the core is often not eaten, leaving an apple core as a residue. Varieties bred for this purpose are termed dessert or table apples.
Apples can be canned or juiced. They are milled to produce apple cider (non-alcoholic, sweet cider) and filtered for apple juice. The juice can be fermented to make cider (alcoholic, hard cider), ciderkin, and vinegar. Through distillation, various alcoholic beverages can be produced, such as applejack, Calvados,[54] and apfelwein. Apple seed oil[55] and pectin may also be produced.
Apples are an important ingredient in many desserts, such as apple pie, apple crumble, apple crisp and apple cake. They are often eaten baked or stewed, and they can also be dried and eaten or reconstituted (soaked in water, alcohol or some other liquid) for later use. Pured apples are generally known as apple sauce. Apples are also made into apple butter and apple jelly. They are also used (cooked) in meat dishes.
Sliced apples turn brown with exposure to air due to the conversion of natural phenolic substances into melanin upon exposure to oxygen.[56] Different cultivars vary in their propensity to brown after slicing.[57] Sliced fruit can be treated with acidulated water to prevent this effect.[56]
Organic apples are commonly produced in the United States.[58] Organic production is difficult in Europe, though a few orchards have done so with commercial success,[58] using disease-resistant cultivars and the very best cultural controls. The latest tool in the organic repertoire is a spray of a light coating of kaolin clay, which forms a physical barrier to some pests, and also helps prevent apple sun scald.[39][58]
One form of apple allergy, often found in northern Europe, is called birch-apple syndrome, and is found in people who are also allergic to birch pollen. The allergy is caused by a protein in apples that is similar to birch pollen, and people affected by this protein can also become allergic to other fruits, nuts and vegetables. Reactions, which are called oral allergy syndrome (OAS), generally involve itching and inflammation of the mouth and throat,[59] but in rare cases can also include life-threatening anaphylaxis.[60] This reaction only occurs when raw fruit is consumedthe allergen is neutralized in the cooking process. The variety of apple, maturity and storage conditions can change the amount of allergen present in individual fruits. Long storage times can increase the amount of proteins that cause birch-apple syndrome.[59]
In other areas, such as the Mediterranean, people have adverse reactions to apples because of their similarity to peaches, including a close relationship between the allergens of the two fruits. This form of apple allergy also includes OAS, but often has more severe symptoms, such as vomiting, abdominal pain and urticaria, and can be life-threatening. Individuals with this form of allergy can also develop reactions to other fruits and nuts. Cooking does not break down the protein causing this particular reaction, so affected individuals can eat neither raw nor cooked apples. Freshly harvested, over-ripe fruits tend to have the highest levels of the protein that causes this reaction.[59]
Breeding efforts have yet to produce a hypoallergenic fruit for either of the two types of apple allergy.[59]
The proverb "An apple a day keeps the doctor away.", addressing the health effects of the fruit, dates from 19th century Wales.[65] Research suggests that apples may reduce the risk of colon cancer, prostate cancer and lung cancer.[61] Compared to many other fruits and vegetables, apples contain relatively low amounts of vitamin C, but are a rich source of other antioxidant compounds.[56] Apple's antioxidant property prevents the damage to cells and tissues. The fiber content, while less than in most other fruits, helps regulate bowel movements and may thus reduce the risk of colon cancer. They may also help with heart disease,[66] weight loss,[66] and controlling cholesterol. The fiber contained in apples reduces cholesterol by preventing reabsorption, and (like most fruits and vegetables) they are bulky for their caloric content.[63][66] However, apple seeds are mildly poisonous, containing a small amount of amygdalin, a cyanogenic glycoside. It is usually not enough to be dangerous to humans, but can deter birds.[67]
There is evidence from laboratory experiments that apples possess phenolic compounds which may be cancer-protective and demonstrate antioxidant activity.[68] The predominant phenolic phytochemicals in apples are quercetin, epicatechin, and procyanidin B2.[69]
According to the United States Department of Agriculture, a small apple weighing roughly 149 grams contains roughly 77 calories.[70]
Apple juice concentrate has been found to increase the production of the neurotransmitter acetylcholine in mice, providing a potential mechanism for the "prevention of the decline in cognitive performance that accompanies dietary and genetic deficiencies and aging." Other studies have shown an "alleviation of oxidative damage and cognitive decline" in mice after the administration of apple juice.[64] Researchers at the Chinese University of Hong Kong discovered that fruit flies who were fed an apple extract lived 10% longer than other flies who were fed a normal diet.[71]
In the United States, it was once tradition to bring an apple to the teacher on the first day of elementary school.[citation needed] In the Disney film Pinocchio, Geppetto has Pinocchio bring an apple to his teacher.
 
1 Botanical information

1.1 Wild ancestors
1.2 Genome


1.1 Wild ancestors
1.2 Genome
2 History
3 Cultural aspects

3.1 Germanic paganism
3.2 Greek mythology
3.3 The forbidden fruit in the Garden of Eden


3.1 Germanic paganism
3.2 Greek mythology
3.3 The forbidden fruit in the Garden of Eden
4 Cultivars
5 Production

5.1 Breeding
5.2 Rootstocks
5.3 Pollination
5.4 Maturation and harvest
5.5 Storage
5.6 Pests and diseases


5.1 Breeding
5.2 Rootstocks
5.3 Pollination
5.4 Maturation and harvest
5.5 Storage
5.6 Pests and diseases
6 Production
7 Human consumption

7.1 Popular uses
7.2 Organic production
7.3 Allergy


7.1 Popular uses
7.2 Organic production
7.3 Allergy
8 Nutrition
9 Traditions
10 References
11 Further reading
12 External links
1.1 Wild ancestors
1.2 Genome
3.1 Germanic paganism
3.2 Greek mythology
3.3 The forbidden fruit in the Garden of Eden
5.1 Breeding
5.2 Rootstocks
5.3 Pollination
5.4 Maturation and harvest
5.5 Storage
5.6 Pests and diseases
7.1 Popular uses
7.2 Organic production
7.3 Allergy
Group A  Early flowering, 1 to 3 May in England (Gravenstein, Red Astrachan)
Group B  4 to 7 May (Idared, McIntosh)
Group C  Mid-season flowering, 8 to 11 May (Granny Smith, Cox's Orange Pippin)
Group D  Mid/late season flowering, 12 to 15 May (Golden Delicious, Calville blanc d'hiver)
Group E  Late flowering, 16 to 18 May (Braeburn, Reinette d'Orlans)
Group F  19 to 23 May (Suntan)
Group H  24 to 28 May (Court-Pendu Gris) (also called Court-Pendu plat)
Mildew: which is characterized by light grey powdery patches appearing on the leaves, shoots and flowers, normally in spring. The flowers will turn a creamy yellow colour and will not develop correctly. This can be treated in a manner not dissimilar from treating Botrytis; eliminating the conditions which caused the disease in the first place and burning the infected plants are among the recommended actions to take.[47]
Aphids: There are five species of aphids commonly found on apples: apple grain aphid, rosy apple aphid, apple aphid, spirea aphid and the woolly apple aphid. The aphid species can be identified by their colour, the time of year when they are present and by differences in the cornicles, which are small paired projections from the rear of aphids.[47] Aphids feed on foliage using needle-like mouth parts to suck out plant juices. When present in high numbers, certain species reduce tree growth and vigor.[48]
Apple scab: Apple scab causes leaves to develop olive-brown spots with a velvety texture that later turn brown and become cork-like in texture. The disease also affects the fruit, which also develops similar brown spots with velvety or cork-like textures. Apple scab is spread through fungus growing in old apple leaves on the ground and spreads during warm spring weather to infect the new year's growth.[49]
In the UK, a toffee apple is a traditional confection made by coating an apple in hot toffee and allowing it to cool. Similar treats in the US are candy apples (coated in a hard shell of crystallized sugar syrup), and caramel apples, coated with cooled caramel.
Apples are eaten with honey at the Jewish New Year of Rosh Hashanah to symbolize a sweet new year.[54]
Farms with apple orchards may open them to the public, so consumers may themselves pick the apples they will buy.[54]
Browning, F. (1999). Apples: The Story of the Fruit of Temptation. North Point Press. ISBN 978-0-86547-579-3
Mabberley, D.J. / Juniper, B.E. (2009). The Story of the Apple. Timber Press. ISBN 978-1-60469-172-6
Gerhauser, C. Cancer chemopreventive potential of apples, apple juice, and apple components, Planta Medica (2008),74(13):16081624.
Hyson, D.A. A Comprehensive Review of Apples and Apple Components and Their Relationship to Human Health, Advances in Nutrition. An International Review Journal (2011),2(5):408420.
Apples at the Open Directory Project
Apple Facts from the UK's Institute of Food Research
National Fruit Collection (UK)
Brogdale Farm (home of the UK's National Fruit Collection)
Grand Valley State University digital collections- diary of Ohio fruit farmer Theodore Peticolas, 1863
.
#(`*Microsoft*`)#.
Coordinates: 473822.55N 122742.42W / 47.6395972N 122.12845W / 47.6395972; -122.12845
Microsoft Corporation (NASDAQ:MSFT) is an American multinational corporation headquartered in Redmond, Washington that develops, manufactures, licenses and supports a wide range of products and services related to computing. The company was founded by Bill Gates and Paul Allen on April 4, 1975. Microsoft is the world's largest software maker measured by revenues.[3] It is also one of the world's most valuable companies.[4]
Microsoft was established to develop and sell BASIC interpreters for the Altair 8800. It rose to dominate the personal computer operating system market with MS-DOS in the mid-1980s, followed by the Microsoft Windows line of operating systems. The company's 1986 initial public offering, and subsequent rise in its share price, created an estimated three billionaires and 12,000 millionaires from Microsoft employees. Since the 1990s, it has increasingly diversified from the operating system market and has made a number of corporate acquisitions. In May 2011, Microsoft acquired Skype Technologies for $8.5 billion in its largest acquisition to date.[5]
As of 2012, Microsoft is market dominant in both the PC operating system and office suite markets (the latter with Microsoft Office). The company also produces a wide range of other software for desktops and servers, and is active in areas including internet search (with Bing), the video game industry (with the Xbox and Xbox 360 consoles), the digital services market (through MSN), and mobile phones (via the Windows Phone OS). In June 2012, Microsoft announced that it would be entering the PC vendor market for the first time, with the launch of the Microsoft Surface tablet computer.
In the 1990s, critics began to contend that Microsoft used monopolistic business practices and anti-competitive strategies including refusal to deal and tying, put unreasonable restrictions in the use of its software, and used misrepresentative marketing tactics; both the U.S. Department of Justice and European Commission found the company in violation of antitrust laws.
Paul Allen and Bill Gates, childhood friends with a passion in computer programming, were seeking to make a successful business utilizing their shared skills. The January 1975 issue of Popular Electronics featured Micro Instrumentation and Telemetry Systems's (MITS) Altair 8800 microcomputer. Allen noticed that they could program a BASIC interpreter for the device; after a call from Gates claiming to have a working interpreter, MITS requested a demonstration. Since they didn't actually have one, Allen worked on a simulator for the Altair while Gates developed the interpreter. Although they developed the interpreter on a simulator and not the actual device, the interpreter worked flawlessly when they demonstrated the interpreter to MITS in Albuquerque, New Mexico in March 1975; MITS agreed to distribute it, marketing it as Altair BASIC.[6]:108, 112114 They officially established Microsoft on April 4, 1975, with Gates as the CEO.[8] Allen came up with the original name of "Micro-Soft," as recounted in a 1995 Fortune magazine article. In August 1977 the company formed an agreement with ASCII Magazine in Japan, resulting in its first international office, "ASCII Microsoft".[9] The company moved to a new home in Bellevue, Washington in January 1979.[8]
Microsoft entered the OS business in 1980 with its own version of Unix, called Xenix.[16] However, it was MS-DOS that solidified the company's dominance. After negotiations with Digital Research failed, IBM awarded a contract to Microsoft in November 1980 to provide a version of the CP/M OS, which was set to be used in the upcoming IBM Personal Computer (IBM PC).[17] For this deal, Microsoft purchased a CP/M clone called 86-DOS from Seattle Computer Products, branding it as MS-DOS, which IBM rebranded to PC-DOS. Following the release of the IBM PC in August 1981, Microsoft retained ownership of MS-DOS. Since IBM copyrighted the IBM PC BIOS, other companies had to reverse engineer it in order for non-IBM hardware to run as IBM PC compatibles, but no such restriction applied to the operating systems. Due to various factors, such as MS-DOS's available software selection, Microsoft eventually became the leading PC operating systems vendor.[7][18]:210 The company expanded into new markets with the release of the Microsoft Mouse in 1983, as well as a publishing division named Microsoft Press.[6]:232 Paul Allen resigned from Microsoft in February after developing Hodgkin's disease.[6]:231
While jointly developing a new OS with IBM in 1984, OS/2, Microsoft released Microsoft Windows, a graphical extension for MS-DOS, on November 20.[6]:242243, 246 Microsoft moved its headquarters to Redmond on February 26, 1986, and on March 13 the company went public;[19] the ensuing rise in the stock would make an estimated four billionaires and 12,000 millionaires from Microsoft employees.[20] Due to the partnership with IBM, in 1990 the Federal Trade Commission set its eye on Microsoft for possible collusion; it marked the beginning of over a decade of legal clashes with the U.S. Government.[21] Microsoft announced the release of its version of OS/2 to original equipment manufacturers (OEMs) on April 2, 1987;[6]:243244 meanwhile, the company was at work on a 32-bit OS, Microsoft Windows NT, using ideas from OS/2; it shipped on July 21, 1993 with a new modular kernel and the Win32 application programming interface (API), making porting from 16-bit (MS-DOS-based) Windows easier. Once Microsoft informed IBM of NT, the OS/2 partnership deteriorated.[22]
In 1990, Microsoft introduced its office suite, Microsoft Office. The software bundled separate office productivity applications, such as Microsoft Word and Microsoft Excel.[6]:301 On May 22 Microsoft launched Windows 3.0 with a streamlined user interface graphics and improved protected mode capability for the Intel 386 processor.[23] Both Office and Windows became dominant in their respective areas.[24][25] Novell, a Word competitor from 19841986, filed a lawsuit years later claiming that Microsoft left part of its APIs undocumented in order to gain a competitive advantage.[26]
On July 27, 1994, the U.S. Department of Justice, Antitrust Division filed a Competitive Impact Statement that said, in part: "Beginning in 1988, and continuing until July 15, 1994, Microsoft induced many OEMs to execute anti-competitive "per processor" licenses. Under a per processor license, an OEM pays Microsoft a royalty for each computer it sells containing a particular microprocessor, whether the OEM sells the computer with a Microsoft operating system or a non-Microsoft operating system. In effect, the royalty payment to Microsoft when no Microsoft product is being used acts as a penalty, or tax, on the OEM's use of a competing PC operating system. Since 1988, Microsoft's use of per processor licenses has increased."[27]
Following Bill Gates's internal "Internet Tidal Wave memo" on May 26, 1995 Microsoft began to redefine its offerings and expand its product line into computer networking and the World Wide Web.[28] The company released Windows 95 on August 24, 1995, featuring pre-emptive multitasking, a completely new user interface with a novel start button, and 32-bit compatibility; similar to NT, it provided the Win32 API.[29][30]:20 Windows 95 came bundled with the online service MSN, and for OEMs Internet Explorer, a web browser. Internet Explorer was not bundled with the retail Windows 95 boxes because the boxes were printed before the team finished the web browser, and instead was included in the Windows 95 Plus! pack.[31] Branching out into new markets in 1996, Microsoft and NBC Universal created a new 24/7 cable news station, MSNBC.[32] Microsoft created Windows CE 1.0, a new OS designed for devices with low memory and other constraints, such as personal digital assistants.[33] In October 1997, the Justice Department filed a motion in the Federal District Court, stating that Microsoft violated an agreement signed in 1994 and asked the court to stop the bundling of Internet Explorer with Windows.[6]:323324
Bill Gates handed over the CEO position on January 13, 2000 to Steve Ballmer, an old college friend of Gates and employee of the company since 1980, creating a new position for himself as Chief Software Architect.[6]:111, 228[8] Various companies including Microsoft formed the Trusted Computing Platform Alliance in October 1999 to, among other things, increase security and protect intellectual property through identifying changes in hardware and software. Critics decry the alliance as a way to enforce indiscriminate restrictions over how consumers use software, and over how computers behave, a form of digital rights management; for example the scenario where a computer is not only secured for its owner, but also secured against its owner as well.[34][35] On April 3, 2000, a judgment was handed down in the case of United States v. Microsoft,[36] calling the company an "abusive monopoly";[37] it settled with the U.S. Department of Justice in 2004.[19] On October 25, 2001 Microsoft released Windows XP, unifying the mainstream and NT lines under the NT codebase.[38] The company released the Xbox later that year, entering the game console market dominated by Sony and Nintendo.[39] In March 2004 the European Union brought antitrust legal action against the company, citing it abused its dominance with the Windows OS, resulting in a judgment of 497million ($613million) and to produce new versions of Windows XP without Windows Media Player, Windows XP Home Edition N and Windows XP Professional N.[40][41]
Released in January 2007, the next version of Windows, Windows Vista, focused on features, security, and a redesigned user interface dubbed Aero.[43][44] Microsoft Office 2007, released at the same time, featured a "Ribbon" user interface which was a significant departure from its predecessors. Relatively strong sales of both titles helped to produce a record profit in 2007.[45] The European Union imposed another fine of 899million ($1.4billion) for Microsoft's lack of compliance with the March 2004 judgment on February 27, 2008, saying that the company charged rivals unreasonable prices for key information about its workgroup and backoffice servers. Microsoft stated that it was in compliance and that "these fines are about the past issues that have been resolved".[46]
Bill Gates retired from his role as Chief Software Architect on June 27, 2008 while retaining other positions related to the company in addition to being an advisor for the company on key projects.[47] Azure Services Platform, the company's entry into the cloud computing market for Windows, launched on October 27, 2008.[48] On February 12, 2009, Microsoft announced its intent to open a chain of Microsoft-branded retail stores, and on October 22, 2009 the first retail Microsoft Store opened in Scottsdale, Arizona; the same day the first store opened Windows 7 was officially released to the public. Windows 7's focus was on refining Vista with ease of use features and performance enhancements, rather than a large reworking of Windows.[49][50][51]
As the smartphone industry boomed beginning in 2007, Microsoft struggled to keep up with its rivals Apple and Google in providing a modern smartphone operating system. As a result, in 2010, Microsoft revamped their aging flagship mobile operating system, Windows Mobile, replacing it with the new Windows Phone OS; along with a new strategy in the smartphone industry that has Microsoft working more closely with smartphone manufactures, such as Nokia, and to provide a consistent user experience across all smartphones using Microsoft's Windows Phone OS. It used a new user interface design language, codenamed "Metro", which prominently used simple shapes, typography and iconography, and the concept of minimalism.
Microsoft is a founding member of the Open Networking Foundation started on March 23, 2011. Other founding companies include Google, HP Networking, Yahoo, Verizon, Deutsche Telekom and 17 other companies. The nonprofit organization is focused on providing support for a new cloud computing initiative called Software-Defined Networking.[52] The initiative is meant to speed innovation through simple software changes in telecommunications networks, wireless networks, data centers and other networking areas.[53]
Following the release of Windows Phone 7, Microsoft gradually began a rebranding of its entire product range between 2011 and 2012. Logos, products, services and websites were updated to use the principles and concepts of the Metro design language.[54]
In June 2011, Windows 8 was previewed for the first time in Taipei. A Developer Preview was released on September 13 of that year, which was replaced by a Consumer Preview on February 29, 2012.[55]
On May 31, 2012, Microsoft released the Release Preview version of its next generation Windows 8 software. It was designed to power devices ranging from tablets to desktop computers.[56]
On June 18, 2012, Microsoft announced the Microsoft Surface, the first computer in the company's history to have its hardware made by Microsoft themselves.[57][58]
On June 25, 2012, Microsoft announced that it was paying $1.2 billion to buy the social network Yammer.[59]
On July 31, 2012, Microsoft launched the Outlook.com webmail service to compete with Gmail.[60]
On September 4, 2012, Microsoft released Windows Server 2012.[61]
On October 1, 2012, Microsoft announced it is launching a news operation when Windows 8 launches later in the month. It is part of a new-look MSN.[62]
On October 26, Microsoft launched Windows 8 and the Microsoft Surface.[63][58] Three days later, it launched Windows Phone 8.[64]
To cope with the potential increase in demand for products and services, Microsoft opened a number of "holiday stores" across the US to compliment the ever-growing increase of bricks and mortar Microsoft Stores that have been sprouting up in 2012.[65]
For the 2010 fiscal year, Microsoft had five product divisions: Windows Division, Server and Tools, Online Services Division, Microsoft Business Division, and Entertainment and Devices Division.
The company's Client division produces the flagship Windows OS line such as Windows 7; it also produces the Windows Live family of products and services. Server and Tools produces the server versions of Windows, such as Windows Server 2008 R2 as well as a set of development tools called Microsoft Visual Studio, Microsoft Silverlight, a web application framework, and System Center Configuration Manager, a collection of tools providing remote-control abilities, patch management, software distribution and a hardware/software inventory. Other server products include: Microsoft SQL Server, a relational database management system, Microsoft Exchange Server, for certain business-oriented e-mail and scheduling features, Small Business Server, for messaging and other small business-oriented features; and Microsoft BizTalk Server, for business process management.
Microsoft provides IT consulting ("Microsoft Consulting Services") and produces a set of certification programs handled by the Server and Tools division designed to recognize individuals who have a minimal set of proficiencies in a specific role; this includes developers ("Microsoft Certified Solution Developer"), system/network analysts ("Microsoft Certified Systems Engineer"), trainers ("Microsoft Certified Trainers") and administrators ("Microsoft Certified Systems Administrator" and "Microsoft Certified Database Administrator"). Microsoft Press, which publishes books, is also managed by the division. The Online Services Business division handles the online service MSN and the search engine Bing. As of December 2009, the company also possesses an 18% ownership of the cable news channel MSNBC without any editorial control; however, the division develops the channel's website, msnbc.com, in a joint venture with the channel's co-owner, NBC Universal.[66]
The Microsoft Business Division produces Microsoft Office including Microsoft Office 2010, the company's line of office software. The software product includes Word (a word processor), Access (a relational database program), Excel (a spreadsheet program), Outlook (Groupware, frequently used with Exchange Server), PowerPoint (presentation software), Publisher (desktop publishing software) and Sharepoint. A number of other products were added later with the release of Office 2003 including Visio, Project, MapPoint, InfoPath and OneNote. The division also develops enterprise resource planning (ERP) software for companies under the Microsoft Dynamics brand. These include: Microsoft Dynamics AX, Microsoft Dynamics NAV, Microsoft Dynamics GP, and Microsoft Dynamics SL. They are targeted at varying company types and countries, and limited to organizations with under 7,500 employees.[67] Also included under the Dynamics brand is the customer relationship management software Microsoft Dynamics CRM, part of the Azure Services Platform.
The Entertainment and Devices Division produces the Windows CE OS for embedded systems and Windows Phone for smartphones.[68] Microsoft initially entered the mobile market through Windows CE for handheld devices, eventually developing into the Windows Mobile OS and now, Windows Phone. Windows CE is designed for devices where the OS may not directly be visible to the end user, in particular, appliances and cars. The division also produces computer games that run on Windows PCs and other systems including titles such as Age of Empires, Halo and the Microsoft Flight Simulator series, and houses the Macintosh Business Unit which produces Mac OS software including Microsoft Office 2011 for Mac. Microsoft's Entertainment and Devices Division designs, markets, and manufactures consumer electronics including the Xbox 360 game console, the handheld Zune media player, and the television-based Internet appliance MSN TV. Microsoft also markets personal computer hardware including mice, keyboards, and various game controllers such as joysticks and gamepads.
Technical reference for developers and articles for various Microsoft magazines such as Microsoft Systems Journal (or MSJ) are available through the Microsoft Developer Network (MSDN). MSDN also offers subscriptions for companies and individuals, and the more expensive subscriptions usually offer access to pre-release beta versions of Microsoft software.[69][70] In April 2004 Microsoft launched a community site for developers and users, titled Channel9, that provides a wiki and an Internet forum.[71] Another community site that provides daily videocasts and other services, On10.net, launched on March 3, 2006.[72] Free technical support is traditionally provided through online Usenet newsgroups, and CompuServe in the past, monitored by Microsoft employees; there can be several newsgroups for a single product. Helpful people can be elected by peers or Microsoft employees for Microsoft Most Valuable Professional (MVP) status, which entitles them to a sort of special social status and possibilities for awards and other benefits.[73]
Noted for its internal lexicon, the expression "eating our own dog food" is used to describe the policy of using prerelease and beta versions of products inside Microsoft in an effort to test them in "real-world" situations.[74] This is usually shortened to just "dog food" and is used as noun, verb, and adjective. Another bit of jargon, FYIFV or FYIV ("Fuck You, I'm [Fully] Vested"), is used by an employee to indicate they are financially independent and can avoid work anytime they wish.[75] The company is also known for its hiring process, mimicked in other organizations and dubbed the "Microsoft interview", which is notorious for off-the-wall questions such as "Why is a manhole cover round?".[76]
Microsoft is an outspoken opponent of the cap on H1B visas, which allow companies in the U.S. to employ certain foreign workers. Bill Gates claims the cap on H1B visas makes it difficult to hire employees for the company, stating "I'd certainly get rid of the H1B cap" in 2005.[77] Critics of H1B visas argue that relaxing the limits would result in increased unemployment for U.S. citizens due to H1B workers working for lower salaries.[78] The Human Rights Campaign Corporate Equality Index, a report of how progressive the organization deems company policies towards LGBT (lesbian, gay, bisexual and transsexual) employees, rated Microsoft as 87% from 2002 to 2004 and as 100% from 2005 to 2010 after they allowed gender expression.[79]
Criticism of Microsoft has followed the company's existence because of various aspects of its products and business practices. Ease of use, stability, and security of the company's software are common targets for critics. More recently, Trojan horses and other exploits have plagued numerous users due to faults in the security of Microsoft Windows and other programs. Microsoft is also accused of locking vendors into their products, and of not following and complying with existing standards in its software.[80] Total cost of ownership comparisons of Linux as well as Mac OS X to Windows are a continuous point of debate.
The company has been in numerous lawsuits by several governments and other companies for unlawful monopolistic practices. In 2004, the European Union found Microsoft guilty in a highly publicized anti-trust case. Additionally, Microsoft's EULA for some of its programs is often criticized as being too restrictive as well as being against open source software.
Microsoft has been criticized (along with Yahoo, AOL, and other companies) for its involvement in censorship in the People's Republic of China.[81] Microsoft has also come under criticism for outsourcing jobs to China and India.[82][83][84] There were reports of poor working conditions at a factory in southern China that makes some of Microsoft's products.[85]
The company is run by a board of directors made up of mostly company outsiders, as is customary for publicly traded companies. Members of the board of directors as of June 2010 are: Steve Ballmer, Dina Dublon, Bill Gates (chairman), Raymond Gilmartin, Reed Hastings, Maria Klawe, David Marquardt, Charles Noski, and Helmut Panke.[86] Board members are elected every year at the annual shareholders' meeting using a majority vote system. There are five committees within the board which oversee more specific matters. These committees include the Audit Committee, which handles accounting issues with the company including auditing and reporting; the Compensation Committee, which approves compensation for the CEO and other employees of the company; the Finance Committee, which handles financial matters such as proposing mergers and acquisitions; the Governance and Nominating Committee, which handles various corporate matters including nomination of the board; and the Antitrust Compliance Committee, which attempts to prevent company practices from violating antitrust laws.[87]
When Microsoft went public and launched its initial public offering (IPO) in 1986, the opening stock price was $21; after the trading day, the price closed at $27.75. As of July 2010, with the company's nine stock splits, any IPO shares would be multiplied by 288; if one was to buy the IPO today given the splits and other factors, it would cost about 9cents.[6]:235236[89][90] The stock price peaked in 1999 at around $119 ($60.928 adjusting for splits).[91] The company began to offer a dividend on January 16, 2003, starting at eight cents per share for the fiscal year followed by a dividend of sixteen cents per share the subsequent year, switching from yearly to quarterly dividends in 2005 with eight cents a share per quarter and a special one-time payout of three dollars per share for the second quarter of the fiscal year.[91][92] Though the company had subsequent increases in dividend payouts, the price of Microsoft's stock remained steady for years.[92][93]
One of Microsoft's business tactics, described by an executive as "embrace, extend and extinguish," initially embraces a competing standard or product, then extends it to produce their own version which is then incompatible with the standard, which in time extinguishes competition that does not or cannot use Microsoft's new version.[94] Various companies and governments sue Microsoft over this set of tactics, resulting in billions of dollars in rulings against the company.[95][36][41] Microsoft claims that the original strategy is not anti-competitive, but rather an exercise of its discretion to implement features it believes customers want.[96]
Standard and Poor's and Moody's have both given a AAA rating to Microsoft, whose assets were valued at $41billion as compared to only $8.5billion in unsecured debt. Consequently, in February 2011 Microsoft released a corporate bond amounting to $2.25billion with relatively low borrowing rates compared to government bonds.[97]
For the first time in 20 years Apple Inc. surpassed Microsoft in Q1 2011 quarterly profits and revenues due to a slowdown in PC sales and continuing huge losses in Microsoft's Online Services Division (which contains its search engine Bing). Microsoft profits were $5.2 billion, while Apple Inc. profits were $6 billion, on revenues of $14.5 billion and $24.7 billion respectively.[98]
Microsoft's Online Services Division has been continuously loss-making since 2006 and in Q1 2011 it lost $726 million. This follows a loss of $2.5 billion for the year 2010.[99]
On July 20, 2012, Microsoft posted its first quarterly loss ever, despite earning record revenues for the quarter and fiscal year. Microsoft reported a net loss of $492 million; the 2007 acquisition of advertising company aQuantive for $6.2 billion and problems associated with it have been cited as the cause.[100]
Microsoft is ranked on the 17th place in Greenpeaces Guide to Greener Electronics (16th Edition) that ranks 18 electronics manufacturers according to their policies on toxic chemicals, recycling and climate change.[101] Microsofts timeline for phasing out BFRs and phthalates in all products is 2012 but its commitment to phasing out PVC is not clear. As yet (January 2011) it has no products that are completely free from PVC and BFRs.[102]
Microsoft's main U.S. campus received a silver certification from the Leadership in Energy and Environmental Design (LEED) program in 2008, and it installed over 2,000 solar panels on top of its buildings in its Silicon Valley campus, generating approximately 15 percent of the total energy needed by the facilities in April 2005.[103]
Microsoft makes use of alternative forms of transit. It created one of the worlds largest private bus systems, the "Connector", to transport people from outside the company; for on-campus transportation, the "Shuttle Connect" uses a large fleet of hybrid cars to save fuel. The company also subsidises regional public transport as an incentive.[103][104] In February 2010 however, Microsoft took a stance against adding additional public transport and high-occupancy vehicle (HOV) lanes to a bridge connecting Redmond to Seattle; the company did not want to delay the construction any further.[105]
Microsoft was ranked number 1 in the list of the World's Best Multinational Workplaces by the Great Place to Work Institute in 2011.[106]
In 2004, Microsoft commissioned research firms to do independent studies comparing the total cost of ownership (TCO) of Windows Server 2003 to Linux; the firms concluded that companies found Windows easier to administrate than Linux, thus those using Windows would administrate faster resulting in lower costs for their company (i.e. lower TCO).[107] This spurred a wave of related studies; a study by the Yankee Group concluded that upgrading from one version of Windows Server to another costs a fraction of the switching costs from Windows Server to Linux, although companies surveyed noted the increased security and reliability of Linux servers and concern about being locked into using Microsoft products.[108] Another study, released by the Open Source Development Labs, claimed that the Microsoft studies were "simply outdated and one-sided" and their survey concluded that the TCO of Linux was lower due to Linux administrators managing more servers on average and other reasons.[109]
As part of the "Get the Facts" campaign Microsoft highlighted the .NET trading platform that it had developed in partnership with Accenture for the London Stock Exchange, claiming that it provided "five nines" reliability. After suffering extended downtime and unreliability[110][111] the LSE announced in 2009 that it was planning to drop its Microsoft solution and switch to a Linux based one in 2010.[112][113]
Microsoft adopted the so-called "Pac-Man Logo", designed by Scott Baker, in 1987. Baker stated "The new logo, in Helvetica italic typeface, has a slash between the o and s to emphasize the "soft" part of the name and convey motion and speed."[114] Dave Norris ran an internal joke campaign to save the old logo, which was green, in all uppercase, and featured a fanciful letter O, nicknamed the blibbet, but it was discarded.[115] Microsoft's logo with the "Your potential. Our passion." tagline below the main corporate name, is based on a slogan Microsoft used in 2008. In 2002, the company started using the logo in the United States and eventually started a TV campaign with the slogan, changed from the previous tagline of "Where do you want to go today?".[12][13][116] During the private MGX (Microsoft Global Exchange) conference in 2010, Microsoft unveiled the company's next tagline, "Be What's Next.".[14]
On August 23, 2012, Microsoft unveiled a new corporate logo at the opening of its 23rd Microsoft store in Boston indicating the company's shift of focus from the classic style to the tile-centric modern interface which it uses/will use on the Windows Phone platform, Xbox 360, Windows 8 and the upcoming Office Suites.[117] The new logo also includes four squares with the colors of the then-current Windows logo.[118] This logo is, however, not completely new - it was featured in Windows 95 commercials from the mid-90s.[119][120] Each color represents a major product of the company: blue for Windows (hence the Windows 8 logo), green for Xbox, red for Office, and yellow possibly for Bing.[121]


NASDAQ:MSFT
SEHK:4338
Dow Jones Industrial Average component
NASDAQ-100 component
S&P 500 component
Bill Gates (Chairman)
Steve Ballmer (CEO)
1 History

1.1 Early history
1.2 19841994: Windows and Office
1.3 19952005: Internet and the 32-bit era
1.4 20062012: Windows Vista, mobile, SaaS and Windows 7
1.5 2011-present: Mobile, Metro and Yammer


1.1 Early history
1.2 19841994: Windows and Office
1.3 19952005: Internet and the 32-bit era
1.4 20062012: Windows Vista, mobile, SaaS and Windows 7
1.5 2011-present: Mobile, Metro and Yammer
2 Product divisions

2.1 Windows Division, Server and Tools, Online Services Division
2.2 Business Division
2.3 Entertainment and Devices Division


2.1 Windows Division, Server and Tools, Online Services Division
2.2 Business Division
2.3 Entertainment and Devices Division
3 Culture
4 Criticism
5 Corporate affairs

5.1 Financial
5.2 Environment
5.3 Marketing
5.4 Logo


5.1 Financial
5.2 Environment
5.3 Marketing
5.4 Logo
6 See also
7 References
8 External links
1.1 Early history
1.2 19841994: Windows and Office
1.3 19952005: Internet and the 32-bit era
1.4 20062012: Windows Vista, mobile, SaaS and Windows 7
1.5 2011-present: Mobile, Metro and Yammer
2.1 Windows Division, Server and Tools, Online Services Division
2.2 Business Division
2.3 Entertainment and Devices Division
5.1 Financial
5.2 Environment
5.3 Marketing
5.4 Logo
List of Microsoft topics
Microsoftportal
Xboxportal
Companiesportal
Seattleportal
Official website
Official blog
Microsoft Corporation at Google Finance
Microsoft Corporation at Yahoo! Finance
Microsoft Corporation at Hoover's
Microsoft Corporation at Reuters
Microsoft Corporation SEC filings at EDGAR Online
Microsoft Corporation SEC filings at the Securities and Exchange Commission
.
#(`*Sony*`)#.
Sony Corporation (, Son Kabushiki Gaisha?) (TYO: 6758, NYSE:SNE), commonly referred to as Sony, is a Japanese multinational conglomerate corporation headquartered in Knan Minato, Tokyo, Japan.[4] It ranked 87th on the 2012 list of Fortune Global 500.[2][5] Sony is one of the leading manufacturers of electronics products for the consumer and professional markets.[6]
Sony Corporation is the electronics business unit and the parent company of the Sony Group, which is engaged in business through its four operating segments  Electronics (including video games, network services and medical business), Motion pictures, Music and Financial Services.[7][8][9] These make Sony one of the most comprehensive entertainment companies in the world. Sony's principal business operations include Sony Corporation (Sony Electronics in the U.S.), Sony Pictures Entertainment, Sony Computer Entertainment, Sony Music Entertainment, Sony Mobile Communications (formerly Sony Ericsson), and Sony Financial. Sony is among the Worldwide Top 20 Semiconductor Sales Leaders and third-largest television manufacturer in the world, after Samsung Electronics and LG Electronics.
The Sony Group (, Son Gurpu?) is a Japan-based corporate group primarily focused on the Electronics (such as AV/IT products and components), Game (such as PlayStation), Entertainment (such as motion pictures and music), and Financial Services (such as insurance and banking) sectors. The group consists of Sony Corporation (holding and electronics), Sony Computer Entertainment (game), Sony Pictures Entertainment (motion pictures), Sony Music Entertainment (music), Sony/ATV Music Publishing (music publishing), Sony Financial Holdings (financial services) and others.
Its founders Akio Morita and Masaru Ibuka derived the name from sonus, the Latin word for sound, and also from the English slang word "sonny", since they considered themselves to be "sonny boys", a loan word into Japanese which in the early 1950s connoted smart and presentable young men.[6]
Sony found its beginning in the wake of World War II. In 1946, Masaru Ibuka started an electronics shop in a bomb-damaged department store building in Tokyo. The company had $530 in capital and a total of eight employees.[10] The next year, he was joined by his colleague, Akio Morita, and they founded a company called Tokyo Tsushin Kogyo[11][12] (Tokyo Telecommunications Engineering Corporation). The company built Japan's first tape recorder, called the Type-G.[12] In 1958 the company name was changed to Sony (see also Origin of name, below).
In the early 1950s, Ibuka traveled in the United States and heard about Bell Labs' invention of the transistor.[12] He convinced Bell to license the transistor technology to his Japanese company, for use in communications. Ibuka's company made the first commercially successful transistor radios.[13][14] According to Schiffer, Sony's TR-63 radio "cracked open the U.S. market and launched the new industry of consumer microelectronics." By the mid 1950s, American teens had begun buying portable transistor radios in huge numbers, helping to propel the fledgling industry from an estimated 100,000 units in 1955 to 5 million units by the end of 1968.
Sony co-founder Akio Morita founded Sony Corporation of America in 1960.[10] In the process, he was struck by the mobility of employees between American companies, which was unheard of in Japan at that time.[10] When he returned to Japan, he encouraged experienced, middle-aged employees of other companies to reevaluate their careers and consider joining Sony.[10] The company filled many positions in this manner, and inspired other Japanese companies to do the same.[10] Moreover, Sony played a major role in the development of Japan as a powerful exporter during the 1960s, 70s, and 80s.[15] It also helped to significantly improve American perceptions of "made in Japan" products.[16] Known for its production quality, Sony was able to charge above-market prices for its consumer electronics and resisted lowering prices.[16]
In 1971, Masaru Ibuka handed the position of president over to his co-founder Akio Morita. Sony began a life insurance company in 1979, one of its many peripheral businesses. Amid a global recession in the early 1980s, electronics sales dropped and the company was forced to cut prices.[16] Sony's profits fell sharply. "It's over for Sony," one analyst concluded. "The company's best days are behind it."[16] Around that time, Norio Ohga took up the role of president. He encouraged the development of the Compact Disc in the 1970s and 80s, and of the PlayStation in the early 1990s. Ohga went on to purchase CBS Records in 1988 and Columbia Pictures in 1989, greatly expanding Sony's media presence. Ohga would succeed Morita as chief executive officer in 1989.
Under the vision of co-founder Akio Morita[17] and his successors, the company had aggressively expanded into new businesses.[15] Part of its motivation for doing so was the pursuit of "convergence," linking film, music, and digital electronics via the Internet.[15] This expansion proved unrewarding and unprofitable,[15] threatening Sony's ability to charge a premium on its products[17] as well as its brand name.[17] In 2005, Howard Stringer replaced Nobuyuki Idei as chief executive officer, marking the first time that a foreigner has run a major Japanese electronics firm. Stringer helped to reinvigorate the company's struggling media businesses, encouraging blockbusters such as Spider-Man while cutting 9,000 jobs.[15] He hoped to sell off peripheral business and focus the company again on electronics.[17] Furthermore, he aimed to increase cooperation between business units,[17] which he described as "silos" operating in isolation from one another.[18] In a bid to provide a unified brand for its global operations, Sony introduced a slogan known as "make.believe" in 2009.
Despite some successes, the company faced continued struggles in the mid- to late-2000s.[15] It became known for its stagnancy, with a fading brand name.[15] In 2012, Kazuo Hirai was promoted to president and CEO, replacing Sir Howard Stringer. Shortly thereafter, Hirai outlined his company-wide initiative, named "One Sony" to revive Sony from years of financial losses and bureaucratic management structure, which proved difficult for former CEO Stringer to accomplish, partly due to differences in business culture and native languages between Stringer and some of Sony's Japanese divisions and subsidiaries. Hirai outlined 3 major areas of focus for Sony's electronics business, which include imaging technology, gaming and mobile technology, as well as a focus on reducing the major losses from the television business.[19]
When Tokyo Tsushin Kogyo was looking for a romanized name to use to market themselves, they strongly considered using their initials, TTK. The primary reason they did not is that the railway company Tokyo Kyuko was known as TKK.[12] The company occasionally used the acronym "Totsuko" in Japan, but during his visit to the United States, Morita discovered that Americans had trouble pronouncing that name. Another early name that was tried out for a while was "Tokyo Teletech" until Akio Morita discovered that there was an American company already using Teletech as a brand name.[20]
The name "Sony" was chosen for the brand as a mix of two words. One was the Latin word "Sonus", which is the root of sonic and sound, and the other was "Sonny", a familiar term used in 1950s America to call a boy.[6] The first Sony-branded product, the TR-55 transistor radio, appeared in 1955 but the company name did not change to Sony until January 1958.[21]
At the time of the change, it was extremely unusual for a Japanese company to use Roman letters to spell its name instead of writing it in kanji. The move was not without opposition: TTK's principal bank at the time, Mitsui, had strong feelings about the name. They pushed for a name such as Sony Electronic Industries, or Sony Teletech. Akio Morita was firm, however, as he did not want the company name tied to any particular industry. Eventually, both Ibuka and Mitsui Bank's chairman gave their approval.[12]
Sony has historically been notable for creating its own in-house standards for new recording and storage technologies, instead of adopting those of other manufacturers and standards bodies. The most infamous of these was the videotape format war of the early 1980s, when Sony marketed the Betamax system for video cassette recorders against the VHS format developed by JVC. In the end, VHS gained critical mass in the marketbase and became the worldwide standard for consumer VCRs and Sony adopted the format. While Betamax is for all practical purposes an obsolete format, a professional-oriented component video format called Betacam that was derived from Betamax is still used today, especially in the television industry, although far less so in recent years with the introduction of digital and high definition.
Sony launched the Betamax videocassette recording format in 1975. In 1979 the Walkman brand was introduced, in the form of the world's first portable music player.
1982 saw the launch of Sony's professional Betacam videotape format and the collaborative Compact Disc (CD) format. In 1983 Sony introduced 90mm micro diskettes (better known as 3.5-inch (89mm) floppy disks), which it had developed at a time when there were 4" floppy disks and a lot of variations from different companies to replace the then on-going 5.25" floppy disks. Sony had great success and the format became dominant; 3.5" floppy disks gradually became obsolete as they were replaced by current media formats. In 1983 Sony launched the MSX, a home computer system, and introduced the world (with their counterpart Philips) to the Compact Disc (CD). In 1984 Sony launched the Discman series which extended their Walkman brand to portable CD products. In 1985 Sony launched their Handycam products and the Video8 format. Video8 and the follow-on hi-band Hi8 format became popular in the consumer camcorder market. In 1987 Sony launched the 4mm DAT or Digital Audio Tape as a new digital audio tape standard.
In addition to developing consumer-based recording media, after the launch of the CD Sony began development of commercially based recording media. In 1986 they launched Write-Once optical discs (WO) and in 1988 launched Magneto-optical discs which were around 125MB size for the specific use of archival data storage.[22]
In the early 1990s two high-density optical storage standards were being developed: one was the MultiMedia Compact Disc (MMCD), backed by Philips and Sony, and the other was the Super Density disc (SD), supported by Toshiba and many others. Philips and Sony abandoned their MMCD format and agreed upon Toshiba's SD format with only one modification based on MMCD technology, viz EFMPlus. The unified disc format was called DVD which was marketed in 1997.
Sony introduced the MiniDisc format in 1993 as an alternative to Philips DCC or Digital Compact Cassette. Since the introduction of MiniDisc, Sony has attempted to promote its own audio compression technologies under the ATRAC brand, against the more widely used MP3. Until late 2004, Sony's Network Walkman line of digital portable music players did not support the MP3 de facto standard natively, although the provided software SonicStage would convert MP3 files into the ATRAC or ATRAC3 formats.
In 1993, Sony challenged the industry standard Dolby Digital 5.1 surround sound format with a newer and more advanced proprietary motion picture digital audio format called SDDS (Sony Dynamic Digital Sound). This format employed eight channels (7.1) of audio opposed to just six used in Dolby Digital 5.1 at the time. Unlike Dolby Digital, SDDS utilized a method of backup by having mirrored arrays of bits on both sides of the film which acted as a measure of reliability in case the film was partially damaged. Ultimately, SDDS has been vastly overshadowed by the preferred DTS (Digital Theatre System) and Dolby Digital standards in the motion picture industry. SDDS was solely developed for use in the theatre circuit; Sony never intended to develop a home theatre version of SDDS.
In 1998, Sony launched their Memory Stick format; flash memory cards for use in Sony lines of digital cameras and portable music players. It has seen little support outside of Sony's own products with Secure Digital cards (SD) commanding considerably greater popularity. This is due in part to the SD format's greater throughput (which allows faster recording and access), higher capacities, and significantly lower price per unit capacity compared to Memory Sticks available at the same time. Sony has made updates to the Memory Stick format with Memory Stick Duo and Memory Stick Micro.
Sony and Philips jointly developed the Sony-Philips digital interface format (S/PDIF) and the high-fidelity audio system SACD. The latter has since been entrenched in a format war with DVD-Audio. At present, neither has gained a major foothold with the general public. CDs are preferred by consumers because of ubiquitous presence of CD drives in consumer devices.
In 2004, Sony built upon the MiniDisc format by releasing Hi-MD. Hi-MD allows the playback and recording of audio on newly introduced 1 GB Hi-MD discs in addition to playback and recording on regular MiniDiscs. Recordings on the Hi-MD Walkmans can be transferred to and from the computer virtually unrestricted, unlike earlier NetMD. In addition to saving audio on the discs, Hi-MD allows the storage of computer files such as documents, videos and photos. Hi-MD introduced the ability to record CD-quality audio with a linear PCM recording feature. It was the first time since MiniDisc's introduction in 1992 that the ATRAC codec could be bypassed and lossless CD-quality audio could be recorded on the small discs.
Sony was one of the leading developers of the Blu-ray Disc optical disc format, the newest standard for disc-based content delivery. The format emerged as the market leader over the competing standard, Toshiba's HD DVD, after a 2 year-long format war. The first Blu-ray players became commercially available in 2006. By the end of 2007 the format had the backing of every major motion picture studio except Universal, Paramount, and DreamWorks.[23][24][25] The Blu-ray format's popularity continued to increase, solidifying its position as the dominant HD media format, and Toshiba announced its decision to stop supporting HD DVD in 2008. Now, all major studios support Blu-ray and release their films on the format.
Sony offers a number of products in a variety of product lines around the world.[26] Sony has developed a music playing robot called Rolly, dog-shaped robots called AIBO, humanoids, and QRIO.
As of 1 April 2012, Sony is organized into the following business segments: Imaging Products & Solutions (IP&S), Game, Mobile Products & Communications (MP&C), Home Entertainment & Sound (HE&S), Devices, Pictures, Music, Financial Services and All Other.[27] The network and medical businesses are included in the All Other.
Sony Corporation is the electronics business unit and the parent company of the Sony Group. It primarily conducts strategic business planning of the group, research and development (R&D), planning, designing and marketing for electronics products. Its subsidiaries such as Sony EMCS Corporation, Sony Semiconductor Corporation and Sony Chemical & Information Device Corporation are responsible for manufacturing as well as product engineering and R&D for mass production (Sony EMCS is also responsible for customer service operations). In 2012, Sony incorporated rolled most of its consumer content services (including video, music, and gaming) into the Sony Entertainment Network.
Sony produced the world's first portable music player, the Walkman in 1979. This line fostered a fundamental change in music listening habits by allowing people to carry music with them and listen to music through lightweight headphones. Walkman originally referred to portable audio cassette players. The company now uses the Walkman brand to market its portable audio and video players as well as a line of former Sony Ericsson mobile phones.
Sony utilized a related brand, Discman, to refer to its CD players. It dropped this name in the late 1990s.
Sony sells many of its computer products using the VAIO brand.
Sony produced computers (MSX home computers and NEWS workstations) during the 1980s, exclusively for sale in the Japanese market. The company withdrew from the computer business around 1990. Sony entered again into the global computer market under the new VAIO brand, began in 1996. Short for "Video Audio Integrated Operation," the line was the first computer brand to highlight visual-audio features.[18]
Sony faced considerable controversy when some of its laptop batteries exploded and caught fire in 2006.,[28] resulting in the largest computer-related recall to that point in history.[29][30][31]
In a bid to join the tablet computer market, the company launched its Sony Tablet series in 2011. The machines run on Google Android software.
Sony offers a range of digital cameras. Point-and-shoot models adopt the Cyber-shot name, while digital single-lens reflex models are branded using Alpha.
The first Cyber-shot was introduced in 1996. At the time, digital cameras were a relative novelty. Sony's market share of the digital camera market fell from a high of 20% to 9% by 2005.[18]
Sony entered the market for digital single-lens reflex cameras in 2006 when it acquired the camera business of Konica Minolta. Sony rebranded the company's line of cameras as its Alpha line. Sony is the world's third largest manufacturer of the cameras, behind leaders Canon and Nikon.
In 1968 Sony introduced the Trinitron brand name for its lines of aperture grille cathode ray tube televisions and (later) computer monitors. Sony stopped production of Trinitron for most markets, but continued producing sets for markets such as Pakistan, Bangladesh and China. Sony discontinued its series of Trinitron computer monitors in 2005. The company discontinued the last Trinitron-based television set in the USA in early 2007. The end of Trinitron marked the end of Sony's analog television sets and monitors.
Sony used the LCD WEGA name for its LCD TVs until summer 2005. The company then introduced the BRAVIA name. BRAVIA is an in house brand owned by Sony which produces high-definition LCD televisions, projection TVs and front projectors, home cinemas and the BRAVIA home theatre range. All Sony high-definition flat-panel LCD televisions in North America have carried the logo for BRAVIA since 2005. Sony is the third-largest maker of televisions in the world.[32] As of 2012[update], Sony's television business has been unprofitable for eight years.[32]
In December 2011, Sony agreed to sell all stake in an LCD joint venture with Samsung Electronics for about $940million.[33] On 28 March 2012, Sony Corporation and Sharp Corporation announced that they have agreed to further amend the joint venture agreement originally executed by the parties in July 2009, as amended in April 2011, for the establishment and operation of Sharp Display Products Corporation ("SDP"), a joint venture to produce and sell large-sized LCD panels and modules[34]
Sony also sells a range of DVD players. It has shifted its focus in recent years to promoting the Blu-ray format, including discs and players.
Sony produces a wide range of semiconductors and electronic components including image sensors, laser diodes, system LSIs, mixed-signal LSIs, OLED panels, etc. The company has a strong presence in image sensor market. Sony-manufactured CCD and CMOS image sensors are widely used in digital cameras, smartphones, tablet computers.
Sony has targeted medical, healthcare and biotechnology business as a growth sector in the future. The company acquired iCyt Mission Technology, Inc. (renamed Sony Biotechnology Inc. in 2012), a manufacture of flow cytometers, in 2010 and Micronics, Inc., a developer of microfluidics-based diagnostic tools, in 2011.
In 2012, Sony announced that it will acquire all shares of So-net Entertainment Corporation, which is the majority shareholder of M3, Inc., an operator of portal sites (m3.com, MR-kun, MDLinx and MEDI:GATE) for healthcare professionals.
On September 28, 2012, Olympus and Sony announced that the two companies will establish a joint venture to develop new surgical endoscopes with 4K resolution (or higher) and 3D capability.[35]
Sony Mobile Communications AB (formerly Sony Ericsson Mobile Communications AB) is a multinational mobile phone manufacturing company headquartered in London, United Kingdom and a wholly owned subsidiary of Sony Corporation.
In 2001, Sony entered into a joint venture with Swedish telecommunications company Ericsson, forming Sony Ericsson.[36] Initial sales were rocky, and the company posted losses in 2001 and 2002. However, SMC reached a profit in 2003. Sony Ericsson distinguished itself with multimedia-capable mobile phones, which included features such as cameras. These were unusual for the time. Despite their innovations, SMC faced intense competition from Apple's iPhone, released in 2007. From 2008 to 2010, amid a global recession, SMC slashed its workforce by several thousand. Sony acquired Ericsson's share of the venture in 2012 for over US$1 billion.[36] In 2009, SMC was the fourth-largest mobile phone manufacturer in the world (after Nokia, Samsung and LG).[37] By 2010, its market share had fallen to sixth place.[38] Sony Mobile Communications now focuses exclusively on the smartphone market.
Sony Computer Entertainment is best known for producing the popular line of PlayStation consoles. The line grew out of a failed partnership with Nintendo. Originally, Nintendo requested for Sony to develop an add-on for its console that would play Compact Discs. In 1991 Sony announced the add-on, as well as a dedicated console known as the "Play Station." However, a disagreement over software licensing for the console caused the partnership to fall through. Sony then continued the project independently.
Launched in 1994, the first PlayStation gained 61% of global console sales and broke Nintendo's long-standing lead in the market.[39] Sony followed up with the PlayStation 2 in 2000, which was even more successful. The console has become the most successful of all time, selling over 150million units as of 2011[update]. Sony released the PlayStation 3, a high-definition console, in 2006. It was the first console to use the Blu-ray format, although its expensive[18] Cell processor made it considerably more expensive than competitors Xbox 360 and Wii. Early on, poor sales performance resulted in significant losses for the company, pushing it to sell the console at a loss.[40] The PlayStation 3 has generally sold more poorly than those competitors, although not by a large margin. It later introduced the PlayStation Move, an accessory that allows players to control video games using motion gestures.
Sony extended the brand to the portable games market in 2005 with the PlayStation Portable (PSP). The console has sold reasonably, but has taken a second place to a rival handheld, the Nintendo DS. Sony developed the Universal Media Disc (UMD) optical disc medium for use on the PlayStation Portable. Early on, the format was used for movies, but it has since lost major studio support. Sony released a disc-less version of its PlayStation Portable, the PSP Go. The company went on to release its second portable video game system, PlayStation Vita, in 2011 and 2012.
Sony Online Entertainment operates online services for PlayStation, as well as several other online games. In 2011 hackers broke into the PlayStation Network online service, stealing the personal information of 77million account holders.
Sony Pictures Entertainment, Inc. (SPE) is the television and film production/distribution unit of Sony. With 12.5% box office market share in 2011, the company was ranked 3rd among movie studios.[41] Its group sales in 2010 were US$7.2 billion.[8][42] The company has produced many notable movie franchises, including Spider-Man, The Karate Kid, and Men in Black. It has also produced popular television game shows Jeopardy! and Wheel of Fortune.
Sony entered the television and film production market when it acquired Columbia Pictures Entertainment in 1989 for $3.4 billion. Columbia lives on in the Columbia TriStar Motion Picture Group, a subsidiary of SPE which in turn owns TriStar Pictures and Columbia Pictures. SPE's television division is known as Sony Pictures Television.
For the first several years of its existence, Sony Pictures Entertainment performed poorly, leading many to suspect the company would sell off the division.[43] Sony Pictures Entertainment encountered controversy in the early 2000s. In July 2000, a marketing executive working for Sony Corporation created a fictitious film critic, David Manning, who gave consistently good reviews for releases from Sony subsidiary Columbia Pictures that generally received poor reviews amongst real critics.[44] Sony later pulled the ads, suspended Manning's creator and his supervisor and paid fines to the state of Connecticut[45] and to fans who saw the reviewed films in the US.[46] In 2006 Sony started using ARccOS Protection on some of their film DVDs, but later issued a recall.[47]
Sony Music Entertainment (also known as SME or Sony Music) is the second-largest global recorded music company of the "big four" record companies and is controlled by Sony Corporation of America, the United States subsidiary of Japan's Sony Corporation. The company owns full or partial rights to the catalogues of Michael Jackson, The Beatles, Usher, Eminem, Akon, and others.
In one of its largest-ever acquisitions, Sony purchased CBS Record Group in 1987 for US$2 billion. In the process, Sony gained the rights to the catalogue of Michael Jackson, considered by the Guinness Book of World Records to be the most successful entertainer of all time. The acquisition of CBS Records provided the foundation for the formation of Sony Music Entertainment, which Sony established in 1991.
In 2004, Sony entered into a joint venture with Bertelsmann AG, merging Sony Music Entertainment with Bertelsmann Music Group to create Sony BMG. In 2005, Sony BMG faced a copy protection scandal, because its music CDs had installed a controversial feature on users' computers[48] that posing a security risk to affected users.[citation needed] In 2007, the company acquired Famous Music for US$370 million, gaining the rights to the catalogues of Eminem and Akon, among others.
Sony bought out Bertelsmann's share in the company and formed a new Sony Music Entertainment in 2008. Since then, the company has undergone management changes.
Besides its record label, Sony operates other music businesses. In 1995, Sony purchased a 50% stake in ATV Music Publishing, forming Sony/ATV Music Publishing. At the time, the publishing company was the second-largest of its kind in the world. The company owns much of the publishing rights to the catalogue of The Beatles. Sony purchased digital music recognition company Gracenote for US$260 million in 2008.
Sony Financial Holdings is a holding company for Sony's financial services business. It owns and oversees the operation of Sony Life (in Japan and the Philippines), Sony Assurance, Sony Bank and Sony Bank Securities. The company is headquartered in Tokyo, Japan.
Sony Financial accounts for half of Sony's global earnings.[49] The unit proved the most profitable of Sony's businesses in fiscal year 2006, earning $1.7 billion in profit.[17] Sony Financial's low fees have aided the unit's popularity while threatening Sony's premium brand name.[17]
Sony is one of Japan's largest corporations by revenue. It had revenues of 6.395 trillion in 2012. It also maintains large reserves of cash, with 13.29 trillion on hand as of 2012. In May 2012, Sony shares were valued at about $15 billion.[50]
The company was immensely profitable throughout the 1990s and early 2000s, in part because of the success of its new PlayStation line. The company encountered financial difficulty in the mid- to late-2000s due to a number of factors: the global financial crisis, increased competition for PlayStation, and the Japanese earthquake. The company faced three consecutive years of losses leading up to 2011.[51] While noting the negative effects of intervening circumstances such as natural disasters and fluctuating currency exchange rates,[51] the Financial Times criticized the company for its "lack of resilience" and "inability to gauge the economy."[51] The newspaper voiced skepticism about Sony's revitalization efforts, given a lack of tangible results.[51]
In September 2000 Sony had a market capitalization of $100billion; but by December 2011 it had plunged to $18billion, reflecting falling prospects for Sony but also reflecting grossly inflated share prices of the 'dot.com' years.[52] Net worth, as measured by stockholder equity, has steadily grown from $17.9billion in March 2002 to $35.6billion through December 2011.[53] Earnings yield (inverse of the price to earnings ratio) has never been more than 5% and usually much less; thus Sony has always traded in over-priced ranges with the exception of the 2009 market bottom.
In April 2012, Sony announced that it would reduce its workforce by 10,000 (6% of its employee base) as part of CEO Hirai's effort to get the company back into the green. This came after a loss of 520billion yen (roughly US$6.36 billion) for fiscal 2012, the worst since the company was founded. Accumulation loss for the past four years was 919.32billion-yen.[54][55] Sony plans to increase its marketing expenses by 30% in 2012.[56] 1,000 of the jobs cut come from the company's mobile phone unit's workforce. 700 jobs will be cut in the 2012-2013 fiscal year and the remaining 300 in the following fiscal year.[57]
On 9 December 2008, Sony Corporation announced that it would be cutting 8,000 jobs, dropping 8,000 contractors and reducing its global manufacturing sites by 10% to save $1.1billion per year.[59]
In November 2011, Sony was ranked 9th (jointly with Panasonic) in Greenpeace's Guide to Greener Electronics. This chart grades major electronics companies on their environmental work. The company scored 3.6/10, incurring a penalty point for comments it has made in opposition to energy efficiency standards in California. It also risks a further penalty point in future editions for being a member of trade associations that have commented against energy efficiency standards.[60] Together with Philips, Sony receives the highest score for energy policy advocacy after calling on the EU to adopt an unconditional 30% reduction target for greenhouse gas emissions by 2020. Meanwhile, it receives full marks for the efficiency of its products.[60] In 2007, Sony ranked 14th on the Greenpeace guide. Sony fell from its earlier 11th place ranking due to Greenpeace's claims that Sony had double standards in their waste policies.[61]
Since 1976, Sony has had an Environmental Conference.[62] Sony's policies address their effects on global warming, the environment, and resources. They are taking steps to reduce the amount of greenhouse gases that they put out as well as regulating the products they get from their suppliers in a process that they call "green procurement".[63] Sony has said that they have signed on to have about 75 percent of their Sony Building running on geothermal power. The "Sony Take Back Recycling Program" allow consumers to recycle the electronics products that they buy from Sony by taking them to eCycle (Recycling) drop-off points around the U.S. The company has also developed a biobattery that runs on sugars and carbohydrates that works similarly to the way living creatures work. This is the most powerful small biobattery to date.[64]
In 2000, Sony faced criticism for a document entitled "NGO Strategy" that was leaked to the press. The document involved the company's surveillance of environmental activists in an attempt to plan how to counter their movements. It specifically mentioned environmental groups that were trying to pass laws that held electronics-producing companies responsible for the clean up of the toxic chemicals contained in their merchandise.[65]

1 History

1.1 Origin of name


1.1 Origin of name
2 Formats and technologies
3 Business units

3.1 Electronics

3.1.1 Sony Corporation

3.1.1.1 Audio
3.1.1.2 Computing
3.1.1.3 Photography
3.1.1.4 Video
3.1.1.5 Semiconductor and component
3.1.1.6 Medical-related business


3.1.2 Sony Mobile Communications
3.1.3 Sony Computer Entertainment


3.2 Entertainment

3.2.1 Sony Pictures Entertainment
3.2.2 Sony Music Entertainment
3.2.3 Sony/ATV Music Publishing


3.3 Finance

3.3.1 Sony Financial Services




3.1 Electronics

3.1.1 Sony Corporation

3.1.1.1 Audio
3.1.1.2 Computing
3.1.1.3 Photography
3.1.1.4 Video
3.1.1.5 Semiconductor and component
3.1.1.6 Medical-related business


3.1.2 Sony Mobile Communications
3.1.3 Sony Computer Entertainment


3.1.1 Sony Corporation

3.1.1.1 Audio
3.1.1.2 Computing
3.1.1.3 Photography
3.1.1.4 Video
3.1.1.5 Semiconductor and component
3.1.1.6 Medical-related business


3.1.1.1 Audio
3.1.1.2 Computing
3.1.1.3 Photography
3.1.1.4 Video
3.1.1.5 Semiconductor and component
3.1.1.6 Medical-related business
3.1.2 Sony Mobile Communications
3.1.3 Sony Computer Entertainment
3.2 Entertainment

3.2.1 Sony Pictures Entertainment
3.2.2 Sony Music Entertainment
3.2.3 Sony/ATV Music Publishing


3.2.1 Sony Pictures Entertainment
3.2.2 Sony Music Entertainment
3.2.3 Sony/ATV Music Publishing
3.3 Finance

3.3.1 Sony Financial Services


3.3.1 Sony Financial Services
4 Corporate information

4.1 Finances
4.2 Environmental record


4.1 Finances
4.2 Environmental record
5 See also
6 References
7 Further reading
8 External links
1.1 Origin of name
3.1 Electronics

3.1.1 Sony Corporation

3.1.1.1 Audio
3.1.1.2 Computing
3.1.1.3 Photography
3.1.1.4 Video
3.1.1.5 Semiconductor and component
3.1.1.6 Medical-related business


3.1.2 Sony Mobile Communications
3.1.3 Sony Computer Entertainment


3.1.1 Sony Corporation

3.1.1.1 Audio
3.1.1.2 Computing
3.1.1.3 Photography
3.1.1.4 Video
3.1.1.5 Semiconductor and component
3.1.1.6 Medical-related business


3.1.1.1 Audio
3.1.1.2 Computing
3.1.1.3 Photography
3.1.1.4 Video
3.1.1.5 Semiconductor and component
3.1.1.6 Medical-related business
3.1.2 Sony Mobile Communications
3.1.3 Sony Computer Entertainment
3.2 Entertainment

3.2.1 Sony Pictures Entertainment
3.2.2 Sony Music Entertainment
3.2.3 Sony/ATV Music Publishing


3.2.1 Sony Pictures Entertainment
3.2.2 Sony Music Entertainment
3.2.3 Sony/ATV Music Publishing
3.3 Finance

3.3.1 Sony Financial Services


3.3.1 Sony Financial Services
3.1.1 Sony Corporation

3.1.1.1 Audio
3.1.1.2 Computing
3.1.1.3 Photography
3.1.1.4 Video
3.1.1.5 Semiconductor and component
3.1.1.6 Medical-related business


3.1.1.1 Audio
3.1.1.2 Computing
3.1.1.3 Photography
3.1.1.4 Video
3.1.1.5 Semiconductor and component
3.1.1.6 Medical-related business
3.1.2 Sony Mobile Communications
3.1.3 Sony Computer Entertainment
3.1.1.1 Audio
3.1.1.2 Computing
3.1.1.3 Photography
3.1.1.4 Video
3.1.1.5 Semiconductor and component
3.1.1.6 Medical-related business
3.2.1 Sony Pictures Entertainment
3.2.2 Sony Music Entertainment
3.2.3 Sony/ATV Music Publishing
3.3.1 Sony Financial Services
4.1 Finances
4.2 Environmental record
Sony marketing
Sony rootkit
List of companies of Japan
Panasonic
PlayStation Division is Under Review
Made in Japan by Akio Morita and Sony, HarperCollins (1994)
Sony: The Private Life by John Nathan, Houghton Mifflin (1999)
Sony Radio, Sony Transistor Radio 35th Anniversary 19551990 information booklet (1990)
The Portable Radio in American Life by University of Arizona Professor Michael Brian Schiffer, PhD (The University of Arizona Press, 1991).
The Japan Project: Made in Japan a documentary about Sony's early history in the U.S. by Terry Sanders.
Official website
Sony Australia
Sony Canada
Sony Ireland
Sony New Zealand
Sony United Kingdom
Sony USA
.
#(`*Sony Mobile Communications*`)#.
Sony Mobile Communications AB (formerly Sony Ericsson Mobile Communications AB) is a multinational mobile phone manufacturing company headquartered in Tokyo, Japan, and a wholly owned subsidiary of Sony Corporation. It was founded on October 1, 2001 as a joint venture between Sony and the Swedish telecommunications equipment company Ericsson, under the name Sony Ericsson.[1] Sony acquired Ericsson's share in the venture on February 16, 2012.[5]
Sony Mobile Communications has research and development facilities in Tokyo, Japan; Chennai, India; Lund, Sweden; Beijing, China and Silicon Valley, United States.[6] Sony Mobile is the world's third-largest mobile phone manufacturer by market share in the third quarter of 2012.[7]
In the United States, Ericsson partnered with General Electric in the early nineties, primarily to establish a US presence and brand recognition.
Ericsson had decided to obtain chips for its phones from a single sourcea Philips facility in New Mexico. In March 2000, a fire at the Philips factory contaminated the sterile facility. Philips assured Ericsson and Nokia (their other major customer) that production would be delayed for no more than a week. When it became clear that production would actually be compromised for months, Ericsson was faced with a serious shortage.[8] Nokia had already begun to obtain parts from alternative sources, but Ericsson's position was much worse as production of current models and the launch of new ones was held up.[9]
Ericsson, which had been in the mobile phone market for decades, and was the world's third largest cellular telephone handset maker, was struggling with huge losses. This was mainly due to this fire and its inability to produce cheaper phones like Nokia. To curtail the losses, it considered outsourcing production to Asian companies that could produce the handsets for lower costs.[citation needed]
Speculation began about a possible sale by Ericsson of its mobile phone division, but the company's president said it had no plans to do so. "Mobile phones are really a core business for Ericsson. We wouldn't be as successful (in networks) if we didn't have phones", he said.[citation needed]
Sony was a marginal player in the worldwide mobile phone market with a share of less than 1 percent in 2000. By August 2001, the two companies had finalised the terms of the merger announced in April. The company was to have an initial workforce of 3,500 employees.
Following the creation of the joint venture, Ericsson's market share actually fell, and in August 2002, Ericsson announced that it would cease making mobile phones and end its partnership with Sony if the business continued to disappoint.[citation needed] However, in January 2003, both companies said they would inject more money into the joint venture in a bid to stem the losses.
Sony Ericsson's strategy was to release new models capable of digital photography as well as other multimedia capabilities such as downloading and viewing video clips and personal information management capabilities. To this end, it released several new models which had built-in digital camera and colour screen which were novelties at that time. The joint venture, however, continued to make bigger losses in spite of booming sales. The target date for making a profit from its first year to 2002 was postponed to 2003 to second half of 2003. It failed in its mission of becoming the top seller of multimedia handsets and was in fifth-place and struggling in 2005.
On March 1, 2005, Sony Ericsson introduced the K750i with a 2 megapixel camera, as well as its platform mate, the W800i, the first of the Walkman phones capable of 30 hours of music playback, and two low-end phones.
In 2007 the company's first 5-Megapixel camera phone, the Sony Ericsson K850i, was announced followed in 2008 by the Sony Ericsson C905, the world's first 8-Megapixel phone.[citation needed] At Mobile World Congress 2009, Sony Ericsson unveiled the first 12-Megapixel phone, named Satio, on May 28, 2009.
On May 1, 2005, Sony Ericsson agreed to become the global title sponsor for the WTA Tour in a deal worth $88 million US dollars over 6 years. The women's pro tennis circuit was renamed the Sony Ericsson WTA Tour. Just over a month later on June 7, it announced sponsorship of West Indian batsmen Chris Gayle and Ramnaresh Sarwan. In October 2005, Sony Ericsson presented the first mobile phone based on UIQ 3, the P990.
On January 2, 2007, Sony Ericsson announced in Stockholm that it would have some of its mobile phones made in India, and that its two outsourcing partners, Flextronics and Foxconn would manufacture ten million mobile phones per year by 2009. CEO Miles Flint announced at a press conference held with India's communications minister Dayanidhi Maran in Chennai that India was one of the fastest growing markets in the world and a priority market for Sony Ericsson with 105 million users of GSM mobile telephones.
Sony Ericsson struggled following the launch of Apple's iPhone in the third quarter of 2007. Its handset shipments fell from a high of 30.8m in Q4 2007 to only 8.1m in Q1 2011.[10] The company had made net losses in six of the 15 quarters and seen its cash reserves shrink from 2.2bn to 599m, after taking a 375m cash injection from its joint owners. The eclipse of the Symbian operating system, initially by Apple's iPhone, and then by Google's Android, has affected Sony Ericsson's position in the market.
Sony Ericsson was overtaken by its South Korean rival LG Electronics in Q1 2008. Sony Ericsson's company's profits fell significantly by 43% to 133million (approx. US$180million), sales falling by 8% and market share falling from 9.4% to 7.9%, despite favourable conditions that the handset market was expected to grow by 10% in 2008. Sony Ericsson announced another profit warning in June 2008[11] and saw net profit crash by 97% in Q2 2008, announcing that it would cut 2,000 jobs, leading to wide fear that Sony Ericsson is on the verge of decline along with its struggling rival, Motorola.[12] In Q3 the profits were much on the same level, however November and December saw increased profits along with new models being released such as the C905 being one of the top sellers across the United Kingdom.
In June 2008, Sony Ericsson had about 8,200 employees, it then launched a cost-cutting program and by the end of 2009 it had slashed its global workforce by around 5,000 people. It planned to cut another 1,500 jobs in 2010. It has also closed R&D centres in Chadwick House, Birchwood (Warrington) in the UK; Miami, Seattle, San Diego and RTP (Raleigh, NC) in the USA; The Chennai Unit (Tamil Nadu) in India; Hssleholm and Kista in Sweden and operations in the Netherlands. The UIQ centres in London and Budapest were also closed, UIQ was a joint venture with Motorola which began life in the 1990s.[13][14][15][16][17][18][19][20][21][22][23]
On October 27, 2011, Sony announced that it would acquire Ericsson's stake in Sony Ericsson for 1.05 billion ($1.47 billion), making the mobile handset business a wholly owned subsidiary of Sony. The transaction's completion was expected to occur in January 2012.[24][25][26] At their keynote at the 2012 Consumer Electronics Show, Sony's Kaz Hirai announced that Sony Ericsson would be known simply as Sony Mobile Communications pending completion of the transaction. On January 26, 2012, the European Union approved the buyout.[27] On February 16, 2012, Sony announced it had completed the full acquisition of Sony Ericsson.[5] On October 1, 2012, Sony Mobile will be moving its headquarters from Lund, Sweden to Tokyo, Japan in order to fully integrate with its parent company.[2] The first Sony-only mobile was the Sony Xperia S along with launch of Sony Xperia U and Sony Xperia P at the 2012 Consumer Electronics Show. Sony Mobile Communications has decided to phase out all the feature (non-smart) phones by September 2012 and focus on smartphones segment.[28] On July 2, 2012, Sony announced it is buying Gaikai, a cloud service to support its expansion into the cloud gaming realm. Sony is paying a reported $380 million to acquire Gaikai.[29]
In 2009, Sony Ericsson announced that it was moving its North American headquarters from Research Triangle Park, North Carolina to Atlanta. The headquarters move was part of a plan to reduce its workforce, then 10,000 employees, by 20%. As of that year, Sony Ericsson had 425 employees in Research Triangle Park; the staff had been reduced by hundreds due to layoffs.[30] Stacy Doster, a spokesperson of Sony Ericsson, said that the proximity to Hartsfield-Jackson Atlanta International Airport's flights to Latin America and the operations of AT&T Mobility influenced the decision to move the USA headquarters. Sony Ericsson will close the Research Triangle site.[30][31] On August 23, 2012, Sony announced heavy cuts of their work force in Lund, Sweden.
Sony concentrates on the categories of: business (web and email), design and all-rounder phones. Its two categories are:
Sony Ericsson posted its first profit in the second half of 2003. Since then, the sales figures from phones have been:
In the third-quarter of 2009, Sony Ericsson had a 5% global market share in mobile phone handsets, the fourth-largest after Nokia (37%), Samsung (21%) and LG (11%).
During 2010, in 11 months, Sony Ericsson's Facebook fan count rose from 300,000 to 4million to become the 40th-largest brand on the social networking site. The company aims to capitalise on this fanbase and increase engagement by profiling these fans and matching them to dedicated content. It will also analyse the top commenters on the Facebook page and ensure engagement through special content and offering these fans the chance to visit Sony Ericsson offices.[41]
As of 2011, Sony Ericsson sponsors the UEFA Champions League and the Sony Ericsson Open tennis tournament in Miami. According to the head of global marketing partnerships,[42] Stephan Croix, our sport sponsorships allow us to promote our phones in a subtle and authentic way to our fanbase. Our promise to fans is to enrich their experience during the game but also before and after.
Sony Ericsson ranks 6th out of 15 leading electronics makers in Greenpeaces Guide to Greener Electronics that assesses companies' policies on climate and energy, sustainability and how green their products are. The company scores 4.2/10 and is one of the top scorers in the Products category, gaining maximum points for the energy efficiency of its phones and doing well for its avoidance of hazardous substances in its products.[43] Sony Ericsson is ahead of many of its competitors in eliminating chemical substances in its products and is currently finalising the phase out of antimony, beryllium, phthalates and the very small remaining use of BFR.[44]
However, Greenpeace criticises Sony Ericsson for not having a plan to reduce its greenhouse gas emissions through energy efficiency or more use of renewable energy. The guide also states that the company still needs to report the amount of recycled plastic sourced as a percentage of all plastics used.[43]
In June 2009, Sony Ericsson launched its first GreenHeart series device, the C901, which indirectly emits 15% less CO2 during its fabrication and usage, compared to other SE phones. It is also packed in a small box without paper manual, includes an eco-charger, and its cover is made of recycled plastic.[45]
1 History

1.1 Origins
1.2 2001 to 2010
1.3 2010 to present


1.1 Origins
1.2 2001 to 2010
1.3 2010 to present
2 Operations
3 Products

3.1 2012 Categories
3.2 Categories prior to 2012


3.1 2012 Categories
3.2 Categories prior to 2012
4 Sales and market share
5 Marketing campaigns
6 Environmental record
7 See also
8 References
9 External links
1.1 Origins
1.2 2001 to 2010
1.3 2010 to present
3.1 2012 Categories
3.2 Categories prior to 2012
The BRAVIA-branded line of phones, launched 2007 in the Japanese market only.
Until now, five BRAVIA branded phones have been produced. Sony Ericsson (FOMA SO903iTV, FOMA SO906i, U1, S004, and S005[32]) uses the BRAVIA brand. BRAVIA branded phone are able to show 1seg terrestrial television.
The XPERIA range of mobile phones, heralded by the Sony Ericsson XPERIA X1 in February 2008 at the Mobile World Congress (formerly 3GSM) held in Barcelona Spain, was the first trademark promoted by Sony Ericsson as its own and is designated to provide technological convergence among its target user base. The first model, X1, carried the Windows Mobile operating system with a Sony Ericsson's panel interface. The Xperia X10 model features the Android operating system. Additionally, Yahoo! News reported that Sony would align with Google to run Android on its upcoming smartphone.[33][34]
The Walkman-branded W series music phones, launched in 2005.
The Sony Ericsson W-series music phones are notable for being the first music-centric series mobile phones, prompting a new market segment for portable music that was developing at the time. The main feature that can be seen in all of these Walkman phones is they all have a 'W' button, which when pressed opens the media centre. Sony Ericsson's Walkman phones have formerly been commercially endorsed by pop stars Christina Aguilera and Jason Kay across Europe. Walkman branded phones are also produced for the Japanese market.
The Cyber-shot-branded line of phones, launched in 2006 in newer models of the K series phones.
This range of phones are focused on the quality of the camera included with the phone. Cyber-shot phones always include a flash, some with a xenon flash, and also include auto-focus cameras. Sony Ericsson kicked off its global marketing campaign for Cyber-shot phone with the launch of 'Never Miss a Shot'. The campaign featured top female tennis players Ana Ivanovi and Daniela Hantuchov. On February 10, 2008, the series has been expanded with the announcement of C702, C902 and C905 phones. Cyber-shot branded phones are also produced for the Japanese market.
The UIQ smartphone range of mobiles, introduced with the P series in 2003 with the introduction of P800.
They are notable for their touchscreens, QWERTY keypads (on most models), and use of the UIQ interface platform for Symbian OS. This range has since expanded into the M series and G series phones.
The GreenHeart range of mobile phones, first introduced in 2009, heralded by the Sony Ericsson J105i Naite and C901 GreenHeart.
It is focused on an environmentally friendly theme, but still featured with recent mobile technology and multimedia capability. It mainly uses eco-friendly materials and features eco-apps.
MyPhoneExplorer full featured Sony Ericsson manager software for Microsoft Windows, able to backup, synch and editing almost all data on the phone via Bluetooth, infrared or USB
Media Go music, photo, video, and game management software made for Sony Ericsson phones
Disc2Phone music management software made for Sony Ericsson phones
SonicStage music management software made for Japan market phones
PlayNow global content distribution portal
List of Sony Ericsson products
Sony Mobile Communications
Sony Mobile Blog
.
#(`*Telephone*`)#.
The telephone, or phone, is a telecommunications device that transmits and receives sounds, usually the human voice. Telephones are a point-to-point communication system whose most basic function is to allow two people separated by large distances to talk to each other. Developed in the mid-1870s by Alexander Graham Bell and others, the telephone has long been considered indispensable to businesses, households and governments, is now one of the most common appliances in the developed world. The word "telephone" has been adapted to many languages and is now recognized around the world.
All modern telephones have a microphone to speak into, an earphone (or 'speaker') which reproduces the voice of the other person, a ringer which makes a sound to alert the owner when a call is coming in, and a keypad (or on older phones a telephone dial) to enter the telephone number of the telephone to be called. The microphone and earphone are usually built into a handset which is held up to the face to talk. The keypad may be part of the handset or of a base unit to which the handset would be connected. A landline telephone is connected by a pair of wires to the telephone network, while a mobile phone (also called a cell phone) is portable and communicates with the telephone network by radio. A cordless telephone has a portable handset which communicates by radio transmission with the handset owners base station which is connected by wire to the telephone network, and can only be used within about 50 feet from the base station.
The microphone converts the sound waves to electrical signals and then these are sent through the telephone network to the other phone and there converted by an earphone, or speaker, back into sound waves. Telephones are a duplex communications medium, meaning they allow the people on both ends to talk simultaneously. The telephone network, consisting of a worldwide net of telephone lines, fiberoptic cables, microwave transmission, cellular networks, communications satellites, and undersea telephone cables connected by switching centers, allows any telephone in the world to communicate with any other. Each telephone line has an identifying number called its telephone number. To initiate a telephone call the user enters the other telephone's number into a numeric keypad on the phone. Graphic symbols used to designate telephone service or phone-related information in print, signage, and other media include  (U+2121),  (U+260E),  (U+260F), and  (U+2706).
Although originally designed for simple voice communications, most modern telephones have many additional capabilities. They may be able to record spoken messages, send and receive text messages, take and display photographs or video, play music, and surf the Internet. A current trend is phones that integrate all mobile communication and computing needs; these are called smartphones.
Credit for the invention of the electric telephone is frequently disputed, and new controversies over the issue have arisen from time to time. As with other influential inventions such as radio, television, the light bulb, and the computer, there were several inventors who did pioneering experimental work on voice transmission over a wire and improved on each other's ideas. Innocenzo Manzetti, Antonio Meucci, Johann Philipp Reis, Elisha Gray, Alexander Graham Bell, and Thomas Edison, among others, have all been credited with pioneering work on the telephone. An undisputed fact is that Alexander Graham Bell was the first to be awarded a patent for the electric telephone by the United States Patent and Trademark Office (USPTO) in March 1876.[1] That first patent by Bell was the master patent of the telephone, from which other patents for electric telephone devices and features flowed.[2]
The early history of the telephone became and still remains a confusing morass of claims and counterclaims, which were not clarified by the large number of lawsuits that hoped to resolve the patent claims of many individuals and commercial competitors. The Bell and Edison patents, however, were forensically victorious and commercially decisive.
A Hungarian engineer, Tivadar Pusks, quickly invented the telephone switchboard in 1876, which allowed for the formation of telephone exchanges, and eventually networks.[3]
The word telephone comes from the Greek: , tle, "far" and , phn, "voice".
A traditional landline telephone system, also known as "plain old telephone service" (POTS), commonly carries both control and audio signals on the same twisted pair (C) of insulated wires: the telephone line. The signaling equipment, or ringer, (see figure 1) consists of a bell, beeper, light or other device (A7) to alert the user to incoming calls, and number buttons or a rotary dial (A4) to enter a telephone number for outgoing calls. Most of the expense of wire-line telephone service is the wires, so telephones transmit both the incoming and outgoing voice channels on a single pair of wires. A twisted pair line rejects electromagnetic interference (EMI) and crosstalk better than a single wire or an untwisted pair. The strong outgoing voice signal from the microphone does not overpower the weaker incoming speaker signal with a sidetone because a hybrid coil (A3) subtracts the microphone's signal from the signal sent to the local speaker. The junction box (B) arrests lightning (B2) and adjusts the line's resistance (B1) to maximize the signal power for the line's length. Telephones have similar adjustments for inside line lengths (A8). The wire's voltages are negative compared to earth, to reduce galvanic corrosion. Negative voltage attracts positive metal ions toward the wires.
The landline telephone contains a switchhook (A4) and an alerting device, usually a ringer (A7), that remains connected to the phone line whenever the phone is "on hook" (i.e. the switch (A4) is open), and other components which are connected when the phone is "off hook". The off-hook components include a transmitter (microphone, A2), a receiver (speaker, A1), and other circuits for dialing, filtering (A3), and amplification.
A calling party wishing to speak to another party will pick up the telephone's handset, thereby operating a lever which closes the switchhook (A4), which powers the telephone by connecting the transmitter (microphone), receiver (speaker), and related audio components to the line. The off-hook circuitry has a low resistance (less than 300 ohms) which causes a direct current (DC), which comes down the line (C) from the telephone exchange. The exchange detects this current, attaches a digit receiver circuit to the line, and sends a dial tone to indicate readiness. On a modern push-button telephone, the caller then presses the number keys to send the telephone number of the called party. The keys control a tone generator circuit (not shown) that makes DTMF tones that the exchange receives. A rotary-dial telephone uses pulse dialing, sending electrical pulses, that the exchange can count to get the telephone number (as of 2010 many exchanges were still equipped to handle pulse dialing). If the called party's line is available, the exchange sends an intermittent ringing signal (about 90 volts alternating current (AC) in North America and UK and 60 volts in Germany) to alert the called party to an incoming call. If the called party's line is in use, the exchange returns a busy signal to the calling party. However, if the called party's line is in use but has call waiting installed, the exchange sends an intermittent audible tone to the called party to indicate an incoming call.
The phone's ringer (A7) is connected to the line through a capacitor (A6), a device which blocks direct current but passes alternating current. So, the phone draws no current when it is on hook (a DC voltage is continually connected to the line), but exchange circuitry (D2) can send an AC voltage down the line to ring for an incoming call. (When there is no exchange, telephones often have hand-cranked magnetos to make the ringing voltage.) When a landline phone is inactive or "on hook", the circuitry at the telephone exchange detects the absence of direct current and therefore "knows" that the phone is on hook (therefore, only AC current will go through) with only the alerting device electrically connected to the line. When a party initiates a call to this line, the exchange sends the ringing signal. When the called party picks up the handset, they actuate a double-circuit switchhook (not shown) which simultaneously disconnects the alerting device and connects the audio circuitry to the line. This, in turn, draws direct current through the line, confirming that the called phone is now active. The exchange circuitry turns off the ring signal, and both phones are now active and connected through the exchange. The parties may now converse as long as both phones remain off hook. When a party "hangs up", placing the handset back on the cradle or hook, direct current ceases in that line, signaling the exchange to disconnect the call.
Calls to parties beyond the local exchange are carried over "trunk" lines which establish connections between exchanges. In modern telephone networks, fiber-optic cable and digital technology are often employed in such connections. Satellite technology may be used for communication over very long distances.
In most landline telephones, the transmitter and receiver (microphone and speaker) are located in the handset, although in a speakerphone these components may be located in the base or in a separate enclosure. Powered by the line, the microphone (A2) produces a modulated electrical current which varies its frequency and amplitude in response to the sound waves arriving at its diaphragm. The resulting current is transmitted along the telephone line to the local exchange then on to the other phone (via the local exchange or via a larger network), where it passes through the coil of the receiver (A3). The varying current in the coil produces a corresponding movement of the receiver's diaphragm, reproducing the original sound waves present at the transmitter.
Along with the microphone and speaker, additional circuitry is incorporated to prevent the incoming speaker signal and the outgoing microphone signal from interfering with each other. This is accomplished through a hybrid coil (A3). The incoming audio signal passes through a resistor (A8) and the primary winding of the coil (A3) which passes it to the speaker (A1). Since the current path A8 - A3 has a far lower impedance than the microphone (A2), virtually all of the incoming signal passes through it and bypasses the microphone.
At the same time the DC voltage across the line causes a DC current which is split between the resistor-coil (A8-A3) branch and the microphone-coil (A2-A3) branch. The DC current through the resistor-coil branch has no effect on the incoming audio signal. But the DC current passing through the microphone is turned into AC current (in response to voice sounds) which then passes through only the upper branch of the coil's (A3) primary winding, which has far fewer turns than the lower primary winding. This causes a small portion of the microphone output to be fed back to the speaker, while the rest of the AC current goes out through the phone line.
A Lineman's handset is a telephone designed for testing the telephone network, and may be attached directly to aerial lines and other infrastructure components.
Early telephones were technically diverse. Some used a liquid transmitter, some had a metal diaphragm that induced current in an electromagnet wound around a permanent magnet, and some were "dynamic" - their diaphragm vibrated a coil of wire in the field of a permanent magnet or the coil vibrated the diaphragm. The sound-powered dynamic kind survived in small numbers through the 20th century in military and maritime applications, where its ability to create its own electrical power was crucial. Most, however, used the Edison/Berliner carbon transmitter, which was much louder than the other kinds, even though it required an induction coil which was an impedance matching transformer to make it compatible with the impedance of the line. The Edison patents kept the Bell monopoly viable into the 20th century, by which time the network was more important than the instrument.
Early telephones were locally powered, using either a dynamic transmitter or by the powering of a transmitter with a local battery. One of the jobs of outside plant personnel was to visit each telephone periodically to inspect the battery. During the 20th century, "common battery" operation came to dominate, powered by "talk battery" from the telephone exchange over the same wires that carried the voice signals.
Early telephones used a single wire for the subscriber's line, with ground return used to complete the circuit (as used in telegraphs). The earliest dynamic telephones also had only one port opening for sound, with the user alternately listening and speaking (or rather, shouting) into the same hole. Sometimes the instruments were operated in pairs at each end, making conversation more convenient but also more expensive.
At first, the benefits of a telephone exchange were not exploited. Instead telephones were leased in pairs to a subscriber, who had to arrange for a telegraph contractor to construct a line between them, for example between a home and a shop. Users who wanted the ability to speak to several different locations would need to obtain and set up three or four pairs of telephones. Western Union, already using telegraph exchanges, quickly extended the principle to its telephones in New York City and San Francisco, and Bell was not slow in appreciating the potential.
Signalling began in an appropriately primitive manner. The user alerted the other end, or the exchange operator, by whistling into the transmitter. Exchange operation soon resulted in telephones being equipped with a bell in a ringer box, first operated over a second wire, and later over the same wire, but with a condenser (capacitor) in series with the bell coil to allow the AC ringer signal through while still blocking DC (keeping the phone "on hook"). Telephones connected to the earliest Strowger automatic exchanges had seven wires, one for the knife switch, one for each telegraph key, one for the bell, one for the push-button and two for speaking. Large wall telephones in the early 20th century usually incorporated the bell, and separate bell boxes for desk phones dwindled away in the middle of the century.
Rural and other telephones that were not on a common battery exchange had a magneto or hand-cranked generator to produce a high voltage alternating signal to ring the bells of other telephones on the line and to alert the operator. Some local farming communities that were not connected to the main networks set up barbed wire telephone lines that exploited the existing system of field fences to transmit the signal.
In the 1890s a new smaller style of telephone was introduced, packaged in three parts. The transmitter stood on a stand, known as a "candlestick" for its shape. When not in use, the receiver hung on a hook with a switch in it, known as a "switchhook." Previous telephones required the user to operate a separate switch to connect either the voice or the bell. With the new kind, the user was less likely to leave the phone "off the hook". In phones connected to magneto exchanges, the bell, induction coil, battery and magneto were in a separate bell box or "ringer box".[4] In phones connected to common battery exchanges, the ringer box was installed under a desk, or other out of the way place, since it did not need a battery or magneto.
Cradle designs were also used at this time, having a handle with the receiver and transmitter attached, now called a handset, separate from the cradle base that housed the magneto crank and other parts. They were larger than the "candlestick" and more popular.
Disadvantages of single wire operation such as crosstalk and hum from nearby AC power wires had already led to the use of twisted pairs and, for long distance telephones, four-wire circuits. Users at the beginning of the 20th century did not place long distance calls from their own telephones but made an appointment to use a special sound proofed long distance telephone booth furnished with the latest technology.
What turned out to be the most popular and longest lasting physical style of telephone was introduced in the early 20th century, including Bell's Model 102. A carbon granule transmitter and electromagnetic receiver were united in a single molded plastic handle, which when not in use sat in a cradle in the base unit. The circuit diagram of the Model 102 shows the direct connection of the receiver to the line, while the transmitter was induction coupled, with energy supplied by a local battery.[5] The coupling transformer, battery, and ringer were in a separate enclosure. The dial switch in the base interrupted the line current by repeatedly but very briefly disconnecting the line 110 times for each digit, and the hook switch (in the center of the circuit diagram) disconnected the line and the transmitter battery while the handset was on the cradle.
After the 1930s, the base also enclosed the bell and induction coil, obviating the old separate ringer box. Power was supplied to each subscriber line by central office batteries instead of a local battery, which required periodic service. For the next half century, the network behind the telephone became progressively larger and much more efficient, but after the telephone dial was added the instrument itself changed little until American Telephone & Telegraph Company (AT&T) introduced touch-tone dialing in the 1960s.
The Public Switched Telephone Network (PSTN) has gradually evolved towards digital telephony which has improved the capacity and quality of the network. End-to-end analog telephone networks were first modified in the early 1960s by upgrading transmission networks with T1 carrier systems, designed to support the basic 3kHz voice channel by sampling the bandwidth-limited analog voice signal and encoding using PCM. While digitization allows wideband voice on the same channel, the improved quality of a wider analog voice channel did not find a large market in the PSTN.
Later transmission methods such as SONET and fiber optic transmission further advanced digital transmission. Although analog carrier systems existed that multiplexed multiple analog voice channels onto a single transmission medium, digital transmission allowed lower cost and more channels multiplexed on the transmission medium. Today the end instrument often remains analog but the analog signals are typically converted to digital signals at the (serving area interface (SAI), central office (CO), or other aggregation point. Digital loop carriers (DLC) place the digital network ever closer to the customer premises, relegating the analog local loop to legacy status.
Internet Protocol (IP) telephony (also known as Voice over Internet Protocol, VoIP), is a disruptive technology that is rapidly gaining ground against traditional telephone network technologies. As of January 2005, up to 10% of telephone subscribers in Japan and South Korea have switched to this digital telephone service. A January 2005 Newsweek article suggested that Internet telephony may be "the next big thing."[6] As of 2006 many VoIP companies offer service to consumers and businesses.
IP telephony uses an Internet connection and hardware IP Phones or softphones installed on personal computers to transmit conversations encoded as data packets. In addition to replacing POTS (plain old telephone service), IP telephony services are also competing with mobile phone services by offering free or lower cost connections via WiFi hotspots. VoIP is also used on private networks which may or may not have a connection to the global telephone network.
IP telephones have two notable disadvantages compared to traditional telephones. Unless the IP telephone's components are backed up with an uninterruptible power supply or other emergency power source, the phone will cease to function during a power outage as can occur during an emergency or disaster, exactly when the phone is most needed. Traditional phones connected to the older PSTN network do not experience that problem since they are powered by the telephone company's battery supply, which will continue to function even if there's a prolonged power black-out. A second distinct problem for an IP phone is the lack of a 'fixed address' which can impact the provision of emergency services such as police, fire or ambulance, should someone call for them. Unless the registered user updates the IP phone's physical address location after moving to a new residence, emergency services can be, and have been, dispatched to the wrong location.
By the end of 2009, there were a total of nearly 6 billion mobile and fixed-line subscribers worldwide. This included 1.26 billion fixed-line subscribers and 4.6 billion mobile subscribers.[7]
In some countries, many telephone operating companies (commonly abbreviated to telco in American English) are in competition to provide telephone services. The above Main article lists only facilities based providers and not companies which lease services from facilities based providers in order to serve their customers.

1 History
2 Basic principles
3 Details of operation

3.1 Early development
3.2 Early commercial instruments


3.1 Early development
3.2 Early commercial instruments
4 Digital telephony
5 IP telephony
6 Usage
7 Telephone operating companies
8 Patents
9 See also
10 Notes
11 References
12 Further reading
13 External links
3.1 Early development
3.2 Early commercial instruments
1844  Innocenzo Manzetti first mooted the idea of a speaking telegraph or telephone. Use of the 'speaking telegraph' and 'sound telegraph' monikers would eventually be replaced by the newer, distinct name, 'telephone'.
26 August 1854  Charles Bourseul published an article in the magazine L'Illustration (Paris): "Transmission lectrique de la parole" (electric transmission of speech), describing a 'make-and-break' type telephone transmitter later created by Johann Reis.
26 October 1861  Johann Philipp Reis (18341874) publicly demonstrated the Reis telephone before the Physical Society of Frankfurt.
22 August 1865, La Feuille d'Aoste reported It is rumored that English technicians to whom Mr. Manzetti illustrated his method for transmitting spoken words on the telegraph wire intend to apply said invention in England on several private telegraph lines". However telephones would not be demonstrated there until 1876, with a set of telephones from Bell.
28 December 1871  Antonio Meucci files patent caveat No. 3335 in the U.S. Patent Office titled "Sound Telegraph", describing communication of voice between two people by wire. A 'patent caveat' was not an invention patent award, but only an unverified notice filed by an individual that he or she intends to file a regular patent application in the future.
1874  Meucci, after having renewed the caveat for two years does not renew it again, and the caveat lapses.
6 April 1875  Bell's U.S. Patent 161,739 "Transmitters and Receivers for Electric Telegraphs" is granted. This uses multiple vibrating steel reeds in make-break circuits.
11 February 1876  Gray invents a liquid transmitter for use with a telephone but does not build one.
14 February 1876  Elisha Gray files a patent caveat for transmitting the human voice through a telegraphic circuit.
14 February 1876  Alexander Bell applies for the patent "Improvements in Telegraphy", for electromagnetic telephones using what is now called amplitude modulation (oscillating current and voltage) but which he referred to as "undulating current".
19 February 1876  Gray is notified by the U.S. Patent Office of an interference between his caveat and Bell's patent application. Gray decides to abandon his caveat.
7 March 1876  Bell's U.S. patent 174,465 "Improvement in Telegraphy" is granted, covering "the method of, and apparatus for, transmitting vocal or other sounds telegraphically  by causing electrical undulations, similar in form to the vibrations of the air accompanying the said vocal or other sound."
10 March 1876  The first successful telephone transmission of clear speech using a liquid transmitter when Bell spoke into his device, Mr. Watson, come here, I want to see you. and Watson heard each word distinctly.
30 January 1877  Bell's U.S. patent 186,787 is granted for an electromagnetic telephone using permanent magnets, iron diaphragms, and a call bell.
27 April 1877  Edison files for a patent on a carbon (graphite) transmitter. The patent 474,230 was granted 3 May 1892, after a 15 year delay because of litigation. Edison was granted patent 222,390 for a carbon granules transmitter in 1879.
US 174,465 -- Telegraphy (Bell's first telephone patent) -- Alexander Graham Bell
US 186,787 -- Electric Telegraphy (permanent magnet receiver) -- Alexander Graham Bell
US 474,230 -- Speaking Telegraph (graphite transmitter) -- Thomas Edison
US 203,016 -- Speaking Telephone (carbon button transmitter) -- Thomas Edison
US 222,390 -- Carbon Telephone (carbon granules transmitter) -- Thomas Edison
US 485,311 -- Telephone (solid back carbon transmitter) -- Anthony C. White (Bell engineer) This design was used until 1925 and installed phones were used until the 1940s.
US 3,449,750 -- Duplex Radio Communication and Signalling AppartusG. H. Sweigert
US 3,663,762 -- Cellular Mobile Communication SystemAmos Edward Joel (Bell Labs)
US 3,906,166 -- Radio Telephone System (DynaTAC cell phone) -- Martin Cooper et al. (Motorola)
Bell System
Bell Telephone Memorial
Cordless telephone
Dual-tone multi-frequency signaling
Harvard sentences
Satellite phone
Sidetone
Telecommunications
Telephone keypad
Telephone plug
Telephone related articles
Telephone switchboard
Telephone tapping
Timeline of the telephone
Tip and ring (Wiring terminology)
Videophone
Coe, Lewis (1995), The Telephone and Its Several Inventors: A History. Jefferson, North Carolina: McFarland & Co., Inc. ISBN 0-7864-0138-9
Evenson, A. Edward (2000), The Telephone Patent Conspiracy of 1876: The Elisha Gray - Alexander Bell Controversy. Jefferson, North Carolina: McFarland & Co., Inc. ISBN 0-7864-0883-9
Baker, Burton H. (2000), The Gray Matter: The Forgotten Story of the Telephone. St. Joseph, Michigan: Telepress. ISBN 0-615-11329-X
Huurdeman, Anton A. (2003), The Worldwide History of Telecommunications. Hoboken: New Jersey: Wiley-IEEE Press. ISBN 978-0-471-20505-0
John, Richard R. Network Nation: Inventing American Telecommunications (Harvard University Press; 2010) 520 pages; traces the evolution of the country's telegraph and telephone networks.
Josephson, Matthew (1992), Edison: A Biography. Hoboken, New Jersey: John Wiley & Sons, Inc. ISBN 0-471-54806-5
Bruce, Robert V. (1990), Bell: Alexander Graham Bell and the Conquest of Solitude. Ithaca, New York: Cornell University Press. ISBN 0-8014-9691-8.
Sobel, Robert (1974), The Entrepreneurs: Explorations Within the American Business Tradition. Weybright & Talley. ISBN 0-679-40064-8.
Todd, Kenneth P. (1998), A Capsule History of the Bell System. American Telephone & Telegraph Company (AT&T).
Early U.S. Telephone Industry Data
1911 Britannica "Telephone" article
Virtual museum of early telephones
The Telephone, 1877
Telephone Citizendium
The short film "Now You're Talking (1927)" is available for free download at the Internet Archive [more]
The short film "Communication (1928)" is available for free download at the Internet Archive [more]
The short film "Telephone Memories (Reel 1 of 2) (1931)" is available for free download at the Internet Archive [more]
The short film "Telephone Memories (Reel 2 of 2) (1931)" is available for free download at the Internet Archive [more]
The short film "Far Speaking (ca. 1935)" is available for free download at the Internet Archive [more]
.
#(`*IPod*`)#.
The iPod is a line of portable media players designed and marketed by Apple Inc. The first line was released on November 10, 2001; its most recent redesigns were announced on September 12, 2012. There are four current generations of the iPod: the ultra-compact iPod Shuffle, the compact iPod Nano, the touchscreen iPod Touch, and the hard drive-based iPod Classic. Like other digital music players, iPods can serve as external data storage devices. Storage capacity varies by model, ranging from 2GB for the iPod Shuffle to 160GB for the iPod Classic. The devices are controlled by the Samsung ARM and the Apple A4 CPUs.
Apple's iTunes software (and other open source software) can be used to transfer music, photos, videos, games, contact information, e-mail settings, Web bookmarks, and calendars, to the devices supporting these features from computers using certain versions of Apple Macintosh and Microsoft Windows operating systems.[1][2]
Prior to iOS 5, the iPod branding was used for the media player included with the iPhone and iPad, a combination of the Music and Videos apps on the iPod Touch. As of iOS 5, separate apps named "Music" and "Video" are standardized across all iOS-powered products.[3] While the iPhone and iPad have essentially the same media-player capabilities as the iPod line, they are generally treated as separate products. In the last few years, iPhone and iPad sales have overtaken those of the iPod.
For some years, Apple and its manufacturing contractor Foxconn have received criticism due to poor working conditions at the assembly plant in China.[4][5][6][7][8][9]
The iPod line came from Apple's "digital hub" category,[10] when the company began creating software for the growing market of personal digital devices. Digital cameras, camcorders and organizers had well-established mainstream markets, but the company found existing digital music players "big and clunky or small and useless" with user interfaces that were "unbelievably awful,"[10] so Apple decided to develop its own. As ordered by CEO Steve Jobs, Apple's hardware engineering chief Jon Rubinstein assembled a team of engineers to design the iPod line, including hardware engineers Tony Fadell and Michael Dhuey,[11] and design engineer Jonathan Ive.[10] Rubinstein had already discovered the Toshiba disk drive when meeting with an Apple supplier in Japan, and purchased the rights to it for Apple, and had also already worked out how the screen, battery, and other key elements would work.[12] The product was developed in less than one year and unveiled on October 23, 2001. Jobs announced it as a Mac-compatible product with a 5GB hard drive that put "1,000 songs in your pocket."[13]
Apple did not develop the iPod software entirely in-house, instead using PortalPlayer's reference platform based on two ARM cores. The platform had rudimentary software running on a commercial microkernel embedded operating system. PortalPlayer had previously been working on an IBM-branded MP3 player with Bluetooth headphones.[10] Apple contracted another company, Pixo, to help design and implement the user interface under the direct supervision of Steve Jobs.[10] As development progressed, Apple continued to refine the software's look and feel. Starting with the iPod Mini, the Chicago font was replaced with Espy Sans. Later iPods switched fonts again to Podium Sansa font similar to Apple's corporate font, Myriad. iPods with color displays then adopted some Mac OS X themes like Aqua progress bars, and brushed metal meant to evoke a combination lock. In 2007, Apple modified the iPod interface again with the introduction of the sixth-generation iPod Classic and third-generation iPod Nano by changing the font to Helvetica and, in most cases, splitting the screen in half by displaying the menus on the left and album artwork, photos, or videos on the right (whichever was appropriate for the selected item).
In September 2007, during a lawsuit with patent holding company Burst.com, Apple drew attention to a patent for a similar device that was developed in 1979. Kane Kramer applied for a UK patent for his design of a "plastic music box" in 1981, which he called the IXI.[14] He was unable to secure funding to renew the US$ 120,000 worldwide patent, so it lapsed and Kramer never profited from his idea.[14]
The name iPod was proposed by Vinnie Chieco, a freelance copywriter, who (with others) was called by Apple to figure out how to introduce the new player to the public. After Chieco saw a prototype, he thought of the movie 2001: A Space Odyssey and the phrase "Open the pod bay door, Hal!", which refers to the white EVA Pods of the Discovery One spaceship. Chieco saw an analogy to the relationship between the spaceship and the smaller independent pods in the relationship between a personal computer and the music player.[10] Apple researched the trademark and found that it was already in use. Joseph N. Grasso of New Jersey had originally listed an "iPod" trademark with the U.S. Patent and Trademark Office (USPTO) in July 2000 for Internet kiosks. The first iPod kiosks had been demonstrated to the public in New Jersey in March 1998, and commercial use began in January 2000, but had apparently been discontinued by 2001. The trademark was registered by the USPTO in November 2003, and Grasso assigned it to Apple Computer, Inc. in 2005.[15]
The earliest recorded use in commerce of an "iPod" trademark was in 1991 by Chrysalis Corp. of Sturgis, Michigan, styled "iPOD".[16]
The third-generation iPod had a weak bass response, as shown in audio tests.[24][25] The combination of the undersized DC-blocking capacitors and the typical low-impedance of most consumer headphones form a high-pass filter, which attenuates the low-frequency bass output. Similar capacitors were used in the fourth-generation iPods.[26] The problem is reduced when using high-impedance headphones and is completely masked when driving high-impedance (line level) loads, such as an external headphone amplifier. The first-generation iPod Shuffle uses a dual-transistor output stage,[24] rather than a single capacitor-coupled output, and does not exhibit reduced bass response for any load.
For all iPods released in 2006 and earlier, some equalizer (EQ) sound settings would distort the bass sound far too easily, even on undemanding songs.[27][28] This would happen for EQ settings like R&B, Rock, Acoustic, and Bass Booster, because the equalizer amplified the digital audio level beyond the software's limit, causing distortion (clipping) on bass instruments.
From the fifth-generation iPod on, Apple introduced a user-configurable volume limit in response to concerns about hearing loss.[29] Users report that in the sixth-generation iPod, the maximum volume output level is limited to 100 dB in EU markets. Apple previously had to remove iPods from shelves in France for exceeding this legal limit.[30]
Originally, a FireWire connection to the host computer was used to update songs or recharge the battery. The battery could also be charged with a power adapter that was included with the first four generations.
The third generation began including a 30-pin dock connector, allowing for FireWire or USB connectivity. This provided better compatibility with non-Apple machines, as most of them did not have FireWire ports at the time. Eventually Apple began shipping iPods with USB cables instead of FireWire, although the latter was available separately. As of the first-generation iPod Nano and the fifth-generation iPod Classic, Apple discontinued using FireWire for data transfer (while still allowing for use of FireWire to charge the device) in an attempt to reduce cost and form factor. As of the second-generation iPod Touch and the fourth-generation iPod Nano, FireWire charging ability has been removed. The second-, third-, and fourth-generation iPod Shuffle uses a single 3.5mm minijack phone connector which acts as both a headphone jack and a data port for the dock.
The dock connector also allowed the iPod to connect to accessories, which often supplement the iPod's music, video, and photo playback. Apple sells a few accessories, such as the now-discontinued iPod Hi-Fi, but most are manufactured by third parties such as Belkin and Griffin. Some peripherals use their own interface, while others use the iPod's own screen. Because the dock connector is a proprietary interface, the implementation of the interface requires paying royalties to Apple.[31]
Apple introduced a new 8-pin dock connector, named Lightning, on September 12, 2012 with their announcement of the iPhone 5, the fifth generation iPod Touch, and the seventh generation iPod Nano, which all feature it. The new connector replaces the older 30-pin dock connector used by older iPods, iPhones, and iPads. Apple Lightning cables have pins on both sides of the plug so it can be inserted with either side facing up.[32]
Many accessories have been made for the iPod line. A large number are made by third party companies, although many, such as the iPod Hi-Fi, are made by Apple. Some accessories add extra features that other music players have, such as sound recorders, FM radio tuners, wired remote controls, and audio/visual cables for TV connections. Other accessories offer unique features like the Nike+iPod pedometer and the iPod Camera Connector. Other notable accessories include external speakers, wireless remote controls, protective case, screen films, and wireless earphones.[33] Among the first accessory manufacturers were Griffin Technology, Belkin, JBL, Bose, Monster Cable, and SendStation.
BMW released the first iPod automobile interface,[34] allowing drivers of newer BMW vehicles to control an iPod using either the built-in steering wheel controls or the radio head-unit buttons. Apple announced in 2005 that similar systems would be available for other vehicle brands, including Mercedes-Benz,[35] Volvo,[36] Nissan, Toyota,[37] Alfa Romeo, Ferrari,[38] Acura, Audi, Honda,[39] Renault, Infiniti[40] and Volkswagen.[41] Scion offers standard iPod connectivity on all their cars.
Some independent stereo manufacturers including JVC, Pioneer, Kenwood, Alpine, Sony, and Harman Kardon also have iPod-specific integration solutions. Alternative connection methods include adapter kits (that use the cassette deck or the CD changer port), audio input jacks, and FM transmitters such as the iTripalthough personal FM transmitters are illegal in some countries. Many car manufacturers have added audio input jacks as standard.[42]
Beginning in mid-2007, four major airlines, United, Continental, Delta, and Emirates, reached agreements to install iPod seat connections. The free service will allow passengers to power and charge an iPod, and view video and music libraries on individual seat-back displays.[43] Originally KLM and Air France were reported to be part of the deal with Apple, but they later released statements explaining that they were only contemplating the possibility of incorporating such systems.[44]
The iPod line can play several audio file formats including MP3, AAC/M4A, Protected AAC, AIFF, WAV, Audible audiobook, and Apple Lossless. The iPod photo introduced the ability to display JPEG, BMP, GIF, TIFF, and PNG image file formats. Fifth and sixth generation iPod Classics, as well as third generation iPod Nanos, can additionally play MPEG-4 (H.264/MPEG-4 AVC) and QuickTime video formats, with restrictions on video dimensions, encoding techniques and data-rates.[45] Originally, iPod software only worked with Mac OS; iPod software for Microsoft Windows was launched with the second generation model.[46] Unlike most other media players, Apple does not support Microsoft's WMA audio formatbut a converter for WMA files without Digital Rights Management (DRM) is provided with the Windows version of iTunes. MIDI files also cannot be played, but can be converted to audio files using the "Advanced" menu in iTunes. Alternative open-source audio formats, such as Ogg Vorbis and FLAC, are not supported without installing custom firmware onto an iPod (e.g., Rockbox).
During installation, an iPod is associated with one host computer. Each time an iPod connects to its host computer, iTunes can synchronize entire music libraries or music playlists either automatically or manually. Song ratings can be set on an iPod and synchronized later to the iTunes library, and vice versa. A user can access, play, and add music on a second computer if an iPod is set to manual and not automatic sync, but anything added or edited will be reversed upon connecting and syncing with the main computer and its library. If a user wishes to automatically sync music with another computer, an iPod's library will be entirely wiped and replaced with the other computer's library.
iPods with color displays use anti-aliased graphics and text, with sliding animations. All iPods (except the 3rd-generation iPod Shuffle, the 6th generation iPod Nano, and iPod Touch) have five buttons and the later generations have the buttons integrated into the click wheel  an innovation that gives an uncluttered, minimalist interface. The buttons perform basic functions such as menu, play, pause, next track, and previous track. Other operations, such as scrolling through menu items and controlling the volume, are performed by using the click wheel in a rotational manner. The 3rd-generation iPod Shuffle does not have any controls on the actual player; instead it has a small control on the earphone cable, with volume-up and -down buttons and a single button for play and pause, next track, etc. The iPod Touch has no click-wheel; instead it uses a 3.5" touch screen along with a home button, sleep/wake button and (on the second and third generations of the iPod Touch) volume-up and -down buttons. The user interface for the iPod Touch is identical to that of the iPhone. Differences include a lack of a phone application. Both devices use iOS.
The iTunes Store (introduced April 29, 2003) is an online media store run by Apple and accessed through iTunes. The store became the market leader soon after its launch[47] and Apple announced the sale of videos through the store on October 12, 2005. Full-length movies became available on September 12, 2006.[48]
At the time the store was introduced, purchased audio files used the AAC format with added encryption, based on the FairPlay DRM system. Up to five authorized computers and an unlimited number of iPods could play the files. Burning the files with iTunes as an audio CD, then re-importing would create music files without the DRM. The DRM could also be removed using third-party software. However, in a deal with Apple, EMI began selling DRM-free, higher-quality songs on the iTunes Stores, in a category called "iTunes Plus." While individual songs were made available at a cost of US$1.29, 30 more than the cost of a regular DRM song, entire albums were available for the same price, US$9.99, as DRM encoded albums. On 17October 2007, Apple lowered the cost of individual iTunes Plus songs to US$0.99 per song, the same as DRM encoded tracks. On January 6, 2009, Apple announced that DRM has been removed from 80% of the music catalog, and that it would be removed from all music by April 2009.
iPods cannot play music files from competing music stores that use rival-DRM technologies like Microsoft's protected WMA or RealNetworks' Helix DRM. Example stores include Napster and MSN Music. RealNetworks claims that Apple is creating problems for itself[49] by using FairPlay to lock users into using the iTunes Store. Steve Jobs stated that Apple makes little profit from song sales, although Apple uses the store to promote iPod sales.[50] However, iPods can also play music files from online stores that do not use DRM, such as eMusic or Amie Street.
Universal Music Group decided not to renew their contract with the iTunes Store on July 3, 2007. Universal will now supply iTunes in an 'at will' capacity.[51]
Apple debuted the iTunes Wi-Fi Music Store on September 5, 2007, in its Media Event entitled "The Beat Goes On..." This service allows users to access the Music Store from either an iPhone or an iPod Touch and download songs directly to the device that can be synced to the user's iTunes Library over a WiFi connection, or, in the case of an iPhone, the telephone network.
Video games are playable on various versions of iPods. The original iPod had the game Brick (originally invented by Apple's co-founder Steve Wozniak) included as an easter egg hidden feature; later firmware versions added it as a menu option. Later revisions of the iPod added three more games: Parachute, Solitaire, and Music Quiz.
In September 2006, the iTunes Store began to offer additional games for purchase with the launch of iTunes 7, compatible with the fifth generation iPod with iPod software 1.2 or later. Those games were: Bejeweled, Cubis 2, Mahjong, Mini Golf, Pac-Man, Tetris, Texas Hold 'Em, Vortex, Asphalt 4: Elite Racing and Zuma. Additional games have since been added. These games work on the 6th and 5th generation iPod Classic and the 5th and 4th generation iPod Nano.
With third parties like Namco, Square Enix, Electronic Arts, Sega, and Hudson Soft all making games for the iPod, Apple's MP3 player has taken steps towards entering the video game handheld console market. Even video game magazines like GamePro and EGM have reviewed and rated most of their games as of late.[52]
The games are in the form of .ipg files, which are actually .zip archives in disguise[citation needed]. When unzipped, they reveal executable files along with common audio and image files, leading to the possibility of third party games. Apple has not publicly released a software development kit (SDK) for iPod-specific development.[53] Apps produced with the iPhone SDK are compatible only with the iOS on the iPod Touch and iPhone, which cannot run clickwheel-based games.
All iPods except for the iPod Touch can function in "disk mode" as mass storage devices to store data files[54] but this may not be the default behavior, and in the case of the iPod Touch, requires special software.[citation needed] If an iPod is formatted on a Mac OS X computer, it uses the HFS+ file system format, which allows it to serve as a boot disk for a Mac computer.[55] If it is formatted on Windows, the FAT32 format is used. With the release of the Windows-compatible iPod, the default file system used on the iPod line switched from HFS+ to FAT32, although it can be reformatted to either file system (excluding the iPod Shuffle which is strictly FAT32). Generally, if a new iPod (excluding the iPod Shuffle) is initially plugged into a computer running Windows, it will be formatted with FAT32, and if initially plugged into a Mac running Mac OS X it will be formatted with HFS+.[56]
Unlike many other MP3 players, simply copying audio or video files to the drive with a typical file management application will not allow an iPod to properly access them. The user must use software that has been specifically designed to transfer media files to iPods, so that the files are playable and viewable. Usually iTunes is used to transfer media to an iPod, though several alternative third-party applications are available on a number of different platforms.
iTunes 7 and above can transfer purchased media of the iTunes Store from an iPod to a computer, provided that computer containing the DRM protected media is authorized to play it.
Media files are stored on an iPod in a hidden folder, along with a proprietary database file. The hidden content can be accessed on the host operating system by enabling hidden files to be shown. The media files can then be recovered manually by copying the files or folders off the iPod. Many third-party applications also allow easy copying of media files off of an iPod.
While the suffix "Classic" was not introduced until the sixth generation, it has been applied here retroactively to all generic iPods for clarity.
In 2005, Apple faced two lawsuits claiming patent infringement by the iPod line and its associated technologies:[60] Advanced Audio Devices claimed the iPod line breached its patent on a "music jukebox",[61] while a Hong Kong-based IP portfolio company called Pat-rights filed a suit claiming that Apple's FairPlay technology breached a patent[62] issued to inventor Ho Keung Tse. The latter case also includes the online music stores of Sony, RealNetworks, Napster, and Musicmatch as defendants.[63]
Apple's application to the United States Patent and Trademark Office for a patent on "rotational user inputs",[64] as used on the iPod interface, received a third "non-final rejection" (NFR) in August 2005. Also in August 2005, Creative Technology, one of Apple's main rivals in the MP3 player market, announced that it held a patent[65] on part of the music selection interface used by the iPod line, which Creative Technology dubbed the "Zen Patent", granted on August 9, 2005.[66] On May 15, 2006, Creative filed another suit against Apple with the United States District Court for the Northern District of California. Creative also asked the United States International Trade Commission to investigate whether Apple was breaching U.S. trade laws by importing iPods into the United States.[67]
On August 24, 2006, Apple and Creative announced a broad settlement to end their legal disputes. Apple will pay Creative US$100million for a paid-up license, to use Creative's awarded patent in all Apple products. As part of the agreement, Apple will recoup part of its payment, if Creative is successful in licensing the patent. Creative then announced its intention to produce iPod accessories by joining the Made for iPod program.[68]
Since October 2004, the iPod line has dominated digital music player sales in the United States, with over 90% of the market for hard drive-based players and over 70% of the market for all types of players.[69] During the year from January 2004 to January 2005, the high rate of sales caused its U.S. market share to increase from 31% to 65% and in July 2005, this market share was measured at 74%. In January 2007 the iPod market share reached 72.7% according to Bloomberg Online.
On January 8, 2004, Hewlett-Packard (HP) announced that they would sell HP-branded iPods under a license agreement from Apple. Several new retail channels were usedincluding Wal-Martand these iPods eventually made up 5% of all iPod sales. In July 2005, HP stopped selling iPods due to unfavorable terms and conditions imposed by Apple.[70]
In January 2007, Apple reported record quarterly revenue of US$7.1billion, of which 48% was made from iPod sales.[not in citation given][71]
On April 9, 2007, it was announced that Apple had sold its one-hundred millionth iPod, making it the biggest selling digital music player of all time. In April 2007, Apple reported second quarter revenue of US$5.2billion, of which 32% was made from iPod sales.[72] Apple and several industry analysts suggest that iPod users are likely to purchase other Apple products such as Mac computers.[73]
On October 22, 2007, Apple reported quarterly revenue of US$6.22billion, of which 30.69% came from Apple notebook sales, 19.22% from desktop sales and 26% from iPod sales. Apple's 2007 year revenue increased to US$24.01billion with US$3.5billion in profits. Apple ended the fiscal year 2007 with US$15.4billion in cash and no debt.[74]
On January 22, 2008, Apple reported the best quarter revenue and earnings in Apple's history so far. Apple posted record revenue of US$9.6billion and record net quarterly profit of US$1.58billion. 42% of Apple's revenue for the First fiscal quarter of 2008 came from iPod sales, followed by 21% from notebook sales and 16% from desktop sales.[75]
On October 21, 2008, Apple reported that only 14.21% of total revenue for fiscal quarter 4 of year 2008 came from iPods.[76] At the September 9, 2009 keynote presentation at the Apple Event, Phil Schiller announced total cumulative sales of iPods exceeded 220 million.[77]
As of October 2011[update], Apple reported that total number of iPods sold worldwide was 300 million.[78]
iPods have won several awards ranging from engineering excellence,[79] to most innovative audio product,[80] to fourth best computer product of 2006.[81] iPods often receive favorable reviews; scoring on looks, clean design, and ease of use. PC World says that iPod line has "altered the landscape for portable audio players".[80] Several industries are modifying their products to work better with both the iPod line and the AAC audio format. Examples include CD copy-protection schemes,[82] and mobile phones, such as phones from Sony Ericsson and Nokia, which play AAC files rather than WMA.
Besides earning a reputation as a respected entertainment device, the iPod has also been accepted as a business device. Government departments, major institutions and international organisations have turned to the iPod line as a delivery mechanism for business communication and training, such as the Royal and Western Infirmaries in Glasgow, Scotland, where iPods are used to train new staff.[83]
iPods have also gained popularity for use in education. Apple offers more information on educational uses for iPods on their website,[84] including a collection of lesson plans. There has also been academic research done in this area in nursing education[85] and more general K-16 education.[86] Duke University provided iPods to all incoming freshmen in the fall of 2004, and the iPod program continues today with modifications.[87] Entertainment Weekly put it on its end-of-the-decade, "best-of" list, saying, "Yes, children, there really was a time when we roamed the earth without thousands of our favorite jams tucked comfortably into our hip pockets. Weird."[88]
The advertised battery life on most models is different from the real-world achievable life. For example, the fifth generation 30GB iPod is advertised as having up to 14 hours of music playback. An MP3.com report stated that this was virtually unachievable under real-life usage conditions, with a writer for MP3.com getting on average less than 8 hours from an iPod.[89] In 2003, class action lawsuits were brought against Apple complaining that the battery charges lasted for shorter lengths of time than stated and that the battery degraded over time.[90] The lawsuits were settled by offering individuals either US$50 store credit or a free battery replacement.[91]
iPod batteries are not designed to be removed or replaced by the user, although some users have been able to open the case themselves, usually following instructions from third-party vendors of iPod replacement batteries. Compounding the problem, Apple initially would not replace worn-out batteries. The official policy was that the customer should buy a refurbished replacement iPod, at a cost almost equivalent to a brand new one. All lithium-ion batteries lose capacity during their lifetime even when not in use[92] (guidelines are available for prolonging life-span) and this situation led to a market for third-party battery replacement kits.
Apple announced a battery replacement program on November 14, 2003, a week before[93] a high publicity stunt and website by the Neistat Brothers.[94] The initial cost was US$99,[95] and it was lowered to US$59 in 2005. One week later, Apple offered an extended iPod warranty for US$59.[96] For the iPod Nano, soldering tools are needed because the battery is soldered onto the main board. Fifth generation iPods have their battery attached to the backplate with adhesive.[97][98]
The first generation iPod Nano may overheat and pose a health and safety risk. Affected iPod Nanos were sold between September 2005 and December 2006. This is due to a flawed battery used by Apple from a single battery manufacturer.[99] Apple recommended that owners of affected iPod Nanos stop using them. Under an Apple product replacement program, affected Nanos were replaced with current generation Nanos free of charge.
iPods have been criticized for alleged short life-span and fragile hard drives. A 2005 survey conducted on the MacInTouch website found that the iPod line had an average failure rate of 13.7% (although they note that comments from respondents indicate that "the true iPod failure rate may be lower than it appears"). It concluded that some models were more durable than others.[100] In particular, failure rates for iPods employing hard drives was usually above 20% while those with flash memory had a failure rate below 10%. In late 2005, many users complained that the surface of the first generation iPod Nano can become scratched easily, rendering the screen unusable.[101][102] A class action lawsuit was also filed.[103] Apple initially considered the issue a minor defect, but later began shipping these iPods with protective sleeves.
On June 11, 2006, the British tabloid The Mail on Sunday reported that iPods are mainly manufactured by workers who earn no more than US$50 per month and work 15-hour shifts.[104] Apple investigated the case with independent auditors and found that, while some of the plant's labour practices met Apple's Code of Conduct, others did not: employees worked over 60 hours a week for 35% of the time, and worked more than six consecutive days for 25% of the time.[105]
Foxconn, Apple's manufacturer, initially denied the abuses,[106] but when an auditing team from Apple found that workers had been working longer hours than were allowed under Chinese law, they promised to prevent workers working more hours than the code allowed. Apple hired a workplace standards auditing company, Verit, and joined the Electronic Industry Code of Conduct Implementation Group to oversee the measures. On December 31, 2006, workers at the Foxconn factory in Longhua, Shenzhen formed a union affiliated with the All-China Federation of Trade Unions,[107] the Chinese government-approved union umbrella organization.[108][109]
In 2010, a number of workers committed suicide at a Foxconn operations in China. Apple, HP, and others stated that they were investigating the situation. Foxconn guards have been videotaped beating employees. Another employee killed himself in 2009 when an Apple prototype went missing, and claimed in messages to friends, that he had been beaten and interrogated.[5][6]
As of 2006, the iPod was produced by about 14,000 workers in the U.S. and 27,000 overseas. Further, the salaries attributed to this product were overwhelmingly distributed to highly skilled U.S. professionals, as opposed to lower skilled U.S. retail employees or overseas manufacturing labor. One interpretation of this result is that U.S. innovation can create more jobs overseas than domestically.[110]
  
1 History
2 Hardware

2.1 Audio
2.2 Connectivity
2.3 Accessories


2.1 Audio
2.2 Connectivity
2.3 Accessories
3 Software

3.1 Interface
3.2 iTunes Store
3.3 Games
3.4 File storage and transfer


3.1 Interface
3.2 iTunes Store
3.3 Games
3.4 File storage and transfer
4 Models
5 Patent disputes
6 Sales
7 Industry impact
8 Criticism

8.1 Battery problems
8.2 Reliability and durability
8.3 Allegations of worker exploitation


8.1 Battery problems
8.2 Reliability and durability
8.3 Allegations of worker exploitation
9 Timeline of iPod models
10 See also
11 References
12 External links
2.1 Audio
2.2 Connectivity
2.3 Accessories
3.1 Interface
3.2 iTunes Store
3.3 Games
3.4 File storage and transfer
8.1 Battery problems
8.2 Reliability and durability
8.3 Allegations of worker exploitation
Book: Apple Inc.
Comparison of portable media players
Comparison of iPod managers
iPhone
Apple iPodOfficial website
iPod troubleshooting basics and service FAQFrom the official website
Apple's 21st century WalkmanBrent Schlender, Fortune, November 12, 2001
iPod NationSteven Levy, Newsweek, July 26, 2004
The Perfect ThingSteven Levy, Wired, November 2006
.
#(`*IPad*`)#.
1st & 2nd Generation
Bluetooth 2.1 + EDR
3rd Generation
Bluetooth 4.0 technology
1st generation:
The iPad (/apd/ EYE-pad) is a line of tablet computers designed and marketed by Apple Inc. The iPad runs Apple's iOS operating system. The first iPad was released on April 3, 2010; the most recent iPads, the fourth-generation iPad and iPad Mini, were released on November 2, 2012. The user interface is built around the device's multi-touch screen, including a virtual keyboard rather than a physical one. The iPad has Wi-Fi and cellular connectivity (2G, 3G and 4G (third and fourth generations and iPad Mini only)).
An iPad can shoot video, take photos, play music, send and receive email, and browse the web. Other functionsgames, reference, GPS navigation, social networking, etc.can be enabled by downloading apps; as of 2012[update], the App Store offered more than 700,000 apps by Apple and third parties.[13]
There are five variants of the iPad: the original, the iPad 2, the third generation, the fourth generation, and the iPad mini.
Apple co-founder Steve Jobs said in a 1983 speech[14] that his company's
Apple's first tablet computer was the Newton MessagePad 100,[15][16] introduced in 1993, which led to the creation of the ARM6 processor core with Acorn Computers. Apple also developed a prototype PowerBook Duo-based tablet, the PenLite, but decided not to sell it in order to avoid hurting MessagePad sales.[17] Apple released several more Newton-based PDAs; the final one, the MessagePad 2100, was discontinued in 1998.
Apple re-entered the mobile-computing markets in 2007 with the iPhone. Smaller than the iPad, but featuring a camera and mobile phone, it pioneered the multitouch finger-sensitive touchscreen interface of Apple's iOS mobile operating system. By late 2009, the iPad's release had been rumored for several years. Such speculation mostly talked about "Apple's tablet"; specific names included iTablet and iSlate.[18] The actual name is reportedly a homage to the Star Trek PADD, a fictional device very similar in appearance to the iPad,[19] as well as being a variation of the word "iPod". The iPad was announced on January 27, 2010, by Jobs at an Apple press conference at the Yerba Buena Center for the Arts in San Francisco.[20][21]
Jobs later said that Apple began developing the iPad before the iPhone,[22][23] but temporarily shelved the effort upon realizing that its ideas would work just as well in a mobile phone.[24] The iPad's internal codename was K48, which was revealed in the court case surrounding leaking of iPad information before launch.[25]
Apple began taking pre-orders for the first-generation iPad from American customers on March 12, 2010.[3] The only major change to the device between its announcement and being available to pre-order was the change of the behavior of the side switch from sound muting to that of a screen rotation lock.[26] The Wi-Fi version of the iPad went on sale in the United States on April 3, 2010.[3][27] The Wi-Fi+3G version was released on April 30.[3][4][4] 3G service in the United States is provided by AT&T and was initially sold with two prepaid contract-free data plan options: one for unlimited data and the other for 250MB per month at half the price.[28][29] On June 2, 2010, AT&T announced that effective June 7 the unlimited plan would be replaced for new customers with a 2GB plan at slightly lower cost; existing customers would have the option to keep the unlimited plan.[30] The plans are activated on the iPad itself and can be canceled at any time.[31]
The iPad was initially only available online at the Apple Store as well as the company's retail locations, but has since become available for purchase through retailers including Amazon, Walmart, and network operators. The iPad was launched in countries including Australia, Canada, France, Germany, Japan and the United Kingdom on May 28.[32][33] Online pre-orders in those countries began on May 10.[4] Apple released the iPad in Hong Kong, Ireland, Mexico, New Zealand and Singapore on July 23, 2010.[34][35][36] Israel briefly prohibited importation of the iPad because of concerns that its Wi-Fi might interfere with other devices.[37] On September 17, 2010, the iPad was officially launched in China.[38]
300,000 iPads were sold on their first day of availability.[39] By May 3, 2010, Apple had sold a million iPads,[40] this was in half the time it took Apple to sell the same number of original iPhones.[41] After passing the one million mark they continued selling rapidly reaching 3 million sales after 80 days.[42] During the October 18, 2010, Financial Conference Call, Steve Jobs announced that Apple had sold more iPads than Macs for the fiscal quarter.[43] In total, Apple sold more than 15 million first-generation iPads prior to the launch of the iPad 2.[44]  selling more than all other tablet PCs combined since the iPad's release.[45] and reaching 75% of tablet PC sales at the end of 2010.[46]
Jobs unveiled the iPad 2 at a March 2, 2011, press conference.[47][48] About 33% thinner than its predecessor and 15% lighter, the iPad 2 has a better processor, a dual core Apple A5 that Apple says is twice as fast as its predecessor for CPU operations and up to nine times as fast for GPU operations. The iPad 2 includes front and back cameras that support the FaceTime videophone application, as well as a three-axis gyroscope. It retains the original's 10-hour battery life and has a similar pricing scheme.
The successor to the iPad 2 was unveiled on March 7, 2012 by Apple CEO Tim Cook at the Yerba Buena Center for the Arts.[49][50] The new iPad sports the new dual core A5X processor with quad-core graphics, and a Retina Display with a resolution of 2,048 by 1,536 pixels[51] this is over 50 percent more pixels than a standard 1,920 by 1,080 high definition TV screen. As with previous iPads, there are two models, in this case a Wi-Fi only model and a Wi-Fi + Cellular model.[51]
On October 23, 2012, Apple announced the fourth generation of the iPad, expected to start shipping on November 2, 2012. The new hardware includes an A6X processor, HD FaceTime camera, improved LTE compatibility, and the all-digital Lightning connector. It will be available in the same storage increments and pricing structure as the third generation.[52][53] Following the announcement of the fourth-generation iPad, the previous generation was discontinued.
On October 23, 2012, Apple announced the iPad Mini. With a screen measuring 7.9 inches, it is aimed at the emerging sector of smaller tablets such as the Kindle Fire. The hardware of the new iPad Mini is similar to the iPad 2, with a 1024 by 768 pixel resolution screen, and with a dual core A5 processor, but 53% lighter and 7.2mm thick. It is scheduled to be released on November 2, in 16GB, 32GB, and 64GB capacities. There are both WiFi and 4G versions, starting at $329 and $459 respectively.[53]
The iPad's (first two generations) touchscreen display is a 1,024 by 768 pixel, 7.755.82 in (197148mm) liquid crystal display (diagonal 9.7in (246.4mm)), with fingerprint- and scratch-resistant glass. Steve Jobs said a 7-inch screen would be "too small to express the software" and that 10inches was the minimum for a tablet screen.[54] Like the iPhone, the iPad is designed to be controlled by bare fingers; normal, non-conductive gloves and styli do not work,[55] although there are special gloves and capacitive styli designed for this use.[56][57]
The display responds to other sensors: an ambient light sensor to adjust screen brightness and a 3-axis accelerometer to sense iPad orientation and switch between portrait and landscape modes. Unlike the iPhone and iPod Touch's built-in applications, which work in three orientations (portrait, landscape-left and landscape-right), the iPad's built-in applications support screen rotation in all four orientations, including upside-down. Consequently, the device has no intrinsic "native" orientation; only the relative position of the home button changes.[58]
There are four physical switches on the iPad, including a home button near the display that returns the user to the main menu, and three plastic physical switches on the sides: wake/sleep and volume up/down, plus a software-controlled switch whose function has changed with software updates. Originally the switch locked the screen to its current orientation, but the iOS 4.2 changed it to a mute switch, with rotation lock now available in an onscreen menu.[59] In the iOS 4.3 update, released with the iPad 2, a setting was added to allow the user to specify whether the side switch was used for rotation lock or mute.[6]
The original iPad had no camera; the iPad 2 has a front VGA camera and a rear-facing 720p camera, both capable of still images (but these are only taken at a low quality 0.3 megapixels) and 30fps video. The rear-facing camera has a 5 digital zoom for still images only. Both shoot photo and video in a 4:3 fullscreen aspect ratio, unlike the iPhone 4, which shoots in a 16:9 widescreen aspect ratio. Unlike the iPhone, the iPad does not support tap to focus, but does allow you to tap to set auto exposure.[60] The cameras allow FaceTime video messaging with iPhone 4, iPod Touch 4, and Snow Leopard, Lion, and Mountain Lion Macs.[61]
The iPad has two internal speakers reproducing left and right channel audio located on the bottom-right of the unit. In the original iPad, the speakers push sound through two small sealed channels leading to the three audio ports carved into the device,[11] while the iPad 2 has its speakers behind a single grill.[9] A volume switch is on the right side of the unit. A 3.5-mm TRRS connector audio-out jack on the top-left corner of the device provides stereo sound for headphones with or without microphones and/or volume controls. The iPad also contains a microphone that can be used for voice recording.
The built-in Bluetooth 2.1 + EDR interface allows wireless headphones and keyboards to be used with the iPad.[62] However iOS does not currently support file transfer via Bluetooth.[63] iPad also features 1024768 VGA video output for limited applications,[64] screen capture,[65] connecting an external display or television through an accessory adapter.
The iPad uses an internal rechargeable lithium-ion polymer (LiPo) battery. The batteries are made in Taiwan by Simplo Technology (60%) and Dynapack International Technology.[66] The iPad is designed to be charged with a high current of 2 amperes using the included 10 W USB power adapter and USB cord with a USB connector at one end and a 30-pin dock connector at the other end. While it can be charged by a standard USB port from a computer, these are limited to 500 milliamperes (0.5 amps). As a result, if the iPad is running while powered by a normal USB computer port, it may charge very slowly, or not at all. High-power USB ports found in newer Apple computers and accessories provide full charging capabilities.[67]
Apple claims that the battery for both generations of iPad can provide up to 10 hours of video, 140 hours of audio playback, or one month on standby. Like any rechargeable battery technology, the iPad's battery loses capacity over time, but is not designed to be user-replaceable. In a program similar to the battery-replacement program for the iPod and the original iPhone, Apple will replace an iPad that does not hold an electrical charge with a refurbished iPad for a fee of US$99 plus $6.95 shipping.[68][69] As a different unit is supplied, user data is not preserved. The refurbished unit will have a new case.[70] The warranty on the refurbished unit may vary between jurisdictions.
Independent companies also provide a battery replacement service, returning the original unit with new battery but original case. Alternatively it is possible for a technically competent user to buy and install a new battery, which may invalidate any remaining warranty on the iPad. The task does not require soldering, but is technically challenging.[71]
The iPad was released with three capacity options for storage: 16, 32, or 64GB of internal flash memory. All data is stored on the internal flash memory, with no option to expand storage. Apple sells a "camera connection kit" with an SD card reader, but it can only be used to transfer photos and videos.[72]
The side of the Wi-Fi + 3G model has a micro-SIM slot (not mini-SIM). The 3G iPad can be used with any compatible GSM carrier, unlike the iPhone, which is usually sold 'locked' to specific carriers.[73] In the U.S., data network access via T-Mobile's network is limited to slower EDGE cellular speeds because T-Mobile's 3G Network uses different frequencies.[74][75] The iPad 2 introduced a third tier of models with CDMA support for Verizon Wireless in the United States, available separately from the AT&T capable version.[76]
Apple offers several iPad accessories,[77] most of which are adapters for the proprietary 30-pin dock connector, the iPad's only port besides the headphone jack.[6] A dock holds the iPad upright at an angle, and has a dock connector and audio line out port. Each generation of iPad requires a corresponding dock. A dock that included a physical keyboard was only supported for the original iPad,[78] but all generations are compatible with Bluetooth keyboards that also work with Macs and PCs. The iPad can be charged by a standalone power adapter ("wall charger") also used for iPods and iPhones, and a 10 W charger is included with the iPad.
Apple sells a camera connection kit that consists of two separate adapters for the dock connector, one to USB Type A, the other an SD card reader. Adapter can be used to transfer photos and videos and to plug USB audio card or MIDI keyboard.[79] A third party sells an adapter that includes USB, SD, and microSD on a single unit.[80] An adapter to VGA connectors allows the iPad to work with external monitors and projectors. Another adapter mirrors the screen onto HDMI compatible devices in 1080p and works with all apps and rotations. Unlike other adapters, it allows the iPad to charge through another dock connector.[81] While the HDMI adapter was released with and advertised for the iPad 2, it also works with the first-generation iPad, the iPhone 4, and the fourth generation iPod Touch.[82]
Smart Covers are screen protectors that magnetically attach and align to the face of the iPad 2. The cover has three folds which allow it to convert into a stand, which is also held together by magnets.[83] While original iPad owners could purchase a black case that included a similarly folding cover, the Smart Cover is meant to be more minimal, easily detachable, and protects only the screen. Smart Covers have a microfiber bottom that cleans the front of the iPad, which wakes up when the cover is removed. There are five different colors of both polyurethane and leather, with leather being more expensive. Smart Covers are not compatible with the original iPad.[84] In June 2012, Apple started selling the Smart Case - a case with the combined function of a smart cover and a back protection case which is compatible with the second and third generation iPads.[85]
Like the iPhone, with which it shares a development environment[86] the iPad only runs its own software, software downloaded from Apple's App Store, and software written by developers who have paid for a developer's license on registered devices.[87] The iPad runs almost all third-party iPhone applications, displaying them at iPhone size or enlarging them to fill the iPad's screen.[88] Developers may also create or modify apps to take advantage of the iPad's features.[89] Application developers use iOS SDK for developing applications for iPad.[90] The iPad originally shipped with a customized iPad-only version of iPhone OS, dubbed v3.2. On September 1, it was announced the iPad would get iOS 4.2 by November 2010;[91] to fulfill this Apple released iOS 4.2.1 to the public on November 22.[92]
The interface is based around the home screen, a graphical list of available applications. The home screen can be accessed at any time by a hardware button below the screen, closing an open application in the process.[93]
Users can also add and delete icons from the dock, which is the same on every home screen. Each home screen holds up to sixteen icons, and the dock holds up to four icons. Users can delete Web Clips and third-party applications at any time, and may select only certain applications for transfer from iTunes. Apple's default programs, however, may not be removed.
Almost all input is given through the touch screen, which understands complex gestures using multi-touch. The iPad's interaction techniques enable the user to move the content up or down by a touch-drag motion of the finger. For example, zooming in and out of web pages and photos is done by placing two fingers on the screen and spreading them farther apart or bringing them closer together, a gesture known as "pinching".
Scrolling through a long list or menu is achieved by sliding a finger over the display from bottom to top, or vice versa to go back. In either case, the list moves as if it is pasted on the outer surface of a wheel, slowly decelerating as if affected by friction. In this way, the interface simulates the physics of a real object.
Other user-centered interactive effects include horizontally sliding sub-selection, the vertically sliding keyboard and bookmarks menu, and widgets that turn around to allow settings to be configured on the other side. Menu bars are found at the top and bottom of the screen when necessary. Their options vary by program, but always follow a consistent style motif. In menu hierarchies, a "back" button in the top-left corner of the screen displays the name of the parent folder.
The layout of the music library is similar to that of an iPod or current Symbian S60 phones. The iPad can sort its media library by songs, artists, albums, videos, playlists, genres, composers, podcasts, audiobooks, and compilations. Options are always presented alphabetically, except in playlists, which retain their order from iTunes. The iPhone uses a large font that allows users plenty of room to touch their selection.
Users can rotate their device horizontally to landscape mode to access Cover Flow. Like on iTunes, this feature shows the different album covers in a scroll-through photo library. Scrolling is achieved by swiping a finger across the screen. Alternatively, headset controls can be used to pause, play, skip, and repeat tracks.
The iPad supports gapless playback.[94] Like the fifth-generation iPods introduced in 2005, the iPad can play digital video, allowing users to watch TV shows and movies in widescreen. Double-tapping switches between widescreen and fullscreen video playback.
The iPad allows users to purchase and download songs from the iTunes Store directly to their iPad. It includes software that allows the user to upload, view, and email photos taken with the camera. The user zooms in and out of photos by sliding two fingers further apart or closer together, much like Safari. The Camera application also lets users view the camera roll, the pictures that have been taken with the iPad's camera. Those pictures are also available in the Photos application, along with any transferred from iPhoto or Aperture on a Mac, or Photoshop on a Windows PC.
The iPad can use Wi-Fi network trilateration from Skyhook Wireless to provide location information to applications such as Google Maps. The 3G model supports A-GPS to allow its position to be calculated with GPS or relative to nearby cellphone towers; it also has a black strip on the back to aid 3G reception.[95] The iPad has a headphone jack and a proprietary Apple dock connector, but no Ethernet or USB port.[6] However, the Apple Camera Connection Kit accessory provides two dock connector adapters for importing photos and videos via USB and SD memory cards.
The iPad comes with several applications, including Safari, Mail, Photos, Video, iPod, iTunes, App Store, iBooks, Maps, Notes, Calendar, and Contacts.[96] Several are improved versions of applications developed for the iPhone or Mac.
The iPad syncs with iTunes on a Mac or Windows PC.[20] Apple ported its iWork suite from the Mac to the iPad, and sells pared down versions of Pages, Numbers, and Keynote apps in the App Store.[97] Although the iPad is not designed to replace a mobile phone, a user can use a wired headset or the built-in speaker and microphone and place telephone calls over Wi-Fi or 3G using a VoIP application.[98] As of June 2012, there were about 225,000 iPad specific apps on the App Store.[99]
In December 2010, Reuters reported that iPhone and iPad users have lodged a lawsuit against Apple alleging that some applications were passing their information to third party advertisers without consent.[100]
The iPad has an optional iBooks application that can be downloaded from the App Store, which displays books and other ePub-format content downloaded from the iBookstore.[101] For the iPad launch on April 3, 2010, the iBookstore is available only in the United States.[3][20][96] Several major book publishers including Penguin Books, HarperCollins, Simon & Schuster and Macmillan have committed to publishing books for the iPad.[102] Despite being a direct competitor to both the Amazon Kindle and Barnes & Noble Nook,[103] both Amazon.com and Barnes & Noble have made Kindle & Nook apps available for the iPad.[104][105]
In February 2010, Cond Nast said it would sell iPad subscriptions for several of its magazines by June.[106]
In April 2010, the New York Times announced that it would begin publishing daily on the iPad.[107] The "Top News" section is available free of charge, and the remainder on payment of a subscription.[108] Major news organizations, including the Wall Street Journal, the BBC, and Reuters have released iPad applications. NewsCorp created an iPad-only publication, The Daily, in February 2011.[109]
The iPad is assembled by Foxconn, which also manufactures Apple's iPod, iPhone and Mac Mini, in its largest plant in Shenzhen, China.[124] In April 2011, Foxconn announced that it would be moving production of the iPad and other Apple products to Brazil where it could begin production before the end of 2011.[125]
iSuppli estimated that each first-generation iPad 16 GB Wi-Fi version costs US$259.60 to manufacture, a total that excludes research, development, licensing, royalty and patent costs.[126] Apple does not disclose the makers of iPad components, but teardown reports and analysis from industry insiders indicate that various parts and their suppliers include:
The iPad does not employ Digital Rights Management but the OS prevents users from copying or transferring certain content outside of Apple's platform without authorization, such as TV shows, movies, and apps. Also, the iPad's development model requires anyone creating an app for the iPad to sign a non-disclosure agreement and pay for a developer subscription. Critics argue Apple's centralized app approval process and control of the platform itself could stifle software innovation. Of particular concern to digital rights advocates is Apple's ability to remotely disable or delete apps on any iPad at any time.[136][137][138]
Digital rights advocates, including the Free Software Foundation, Electronic Frontier Foundation, and computer engineer and activist Brewster Kahle, have not criticized the iPad for its digital rights restrictions. In April 2010, Paul Sweeting, an analyst with GigaOM, was quoted by National Public Radio as saying, "With the iPad, you have the anti-Internet in your hands. ... It offers [the major media companies] the opportunity to essentially re-create the old business model, wherein they are pushing content to you on their terms rather than you going out and finding content, or a search engine discovering content for you." But Sweeting also thought that the limitations imposed by Apple impart the feeling of a safe neighborhood, saying, "Apple is offering you a gated community where there's a guard at the gate, and there's probably maid service, too." Laura Sydell, the article's author, concludes, "As more consumers have fears about security on the Internet, viruses and malware, they may be happy to opt for Apple's gated community."[139]
Like other iOS devices, the iPad can be "jailbroken", allowing applications and programs that are not authorized by Apple to run on the device.[140][141] Once it is jailbroken, users are able to download many applications previously unavailable through the App Store via unofficial installers such as Cydia, as well as illegally pirated applications.[141] Apple claims jailbreaking voids the factory warranty on the device in the United States even though jailbreaking is legal.[141][142] The iPad, released in April 2010, was first jailbroken in May 2010 with the Spirit jailbreak for iOS version 3.1.2.[143] The iPad can be jailbroken on iOS versions 4.3 through 4.3.3 with the web-based tool JailbreakMe 3.0 (released in July 2011),[144] and on iOS versions including 5.0 and 5.0.1 using redsn0w[145] Absinthe 2.0 was released on May 25, 2012 as the first jailbreak method for all iOS 5.1.1 devices except the 32nm version of the iPad 2.[146]
Apple's App Store, which provides iPhone and iPad applications, imposes censorship of content, which has become an issue for book publishers and magazines seeking to use the platform. The Guardian newspaper described the role of Apple as analogous to that of British magazine distributor WH Smith, which for many years imposed content restrictions.[147]
Due to the exclusion of pornography from the App Store, YouPorn and others changed their video format from Flash to H.264 and HTML5 specifically for the iPad.[148] In an e-mail exchange[149] with Ryan Tate from Valleywag, Steve Jobs claimed that the iPad offers "freedom from porn", leading to many upset replies including Adbustings in Berlin by artist Johannes P. Osterhoff[150] and in San Francisco during WWDC10.[151]
On May 28, 2010, the iPad was released in Australia, Canada, and Japan, as well as in several larger European countries. Media reaction to the launch was mixed. The media noted the positive response from fans of the device, with thousands of people queued on the first day of sale in a number of these countries.[152][153]
Media reaction to the iPad announcement was mixed. Walt Mossberg wrote, "It's about the software, stupid", meaning hardware features and build are less important to the iPad's success than software and user interface, his first impressions of which were largely positive. Mossberg also called the price "modest" for a device of its capabilities, and praised the ten-hour battery life.[154] Others, including PC Advisor and the Sydney Morning Herald, wrote that the iPad would also compete with proliferating netbooks, most of which use Microsoft Windows.[155][156] The base model's $499 price was lower than pre-release estimates by the tech press, Wall Street analysts, and Apple's competitors, all of whom were expecting a much higher entry price point.[157][158][159]
CNET also criticized the iPad for its apparent lack of wireless sync which other portable devices such as Microsoft's Zune have had for a number of years. The built-in iTunes app is able to download from the Internet as well.[160]
Reviews of the iPad have been generally favorable. Walt Mossberg of The Wall Street Journal called it a "pretty close" laptop killer.[161] David Pogue of The New York Times wrote a "dual" review, one part for technology-minded people, and the other part for non-technology-minded people. In the former section, he notes that a laptop offers more features for a cheaper price than the iPad. In his review for the latter audience, however, he claims that if his readers like the concept of the device and can understand what its intended uses are, then they will enjoy using the device.[162] PC Magazine's Tim Gideon wrote, "you have yourself a winner" that "will undoubtedly be a driving force in shaping the emerging tablet landscape."[163] Michael Arrington of TechCrunch said, "the iPad beats even my most optimistic expectations. This is a new category of device. But it also will replace laptops for many people."[164] PC World criticized the iPad's file sharing and printing abilities,[165] and ArsTechnica said sharing files with a computer is "one of our least favorite parts of the iPad experience."[166]
The media also praised the quantity of applications, as well as the bookstore and other media applications.[167][168] In contrast they criticized the iPad for being a closed system and mentioned that the iPad faces competition from Android-based tablets.[152] However, the Android tablet OS, known as "Honeycomb", is not open source and has fewer apps available for it than for the iPad.[169] The Independent criticized the iPad for not being as readable in bright light as paper but praised it for being able to store large quantities of books.[167] After its UK release, The Daily Telegraph said the iPad's lack of Adobe Flash support was "annoying."[170]
The iPad was selected by Time Magazine as one of the 50 Best Inventions of the Year 2010,[171] while Popular Science chose it as the top gadget[172] behind the overall "Best of What's New 2010" winner Groasis Waterboxx.[173]
While the iPad is mostly used by consumers it also has been taken up by business users.[174] Within 90 days of its release, the iPad managed to penetrate 50% of Fortune 100 companies.[175] Some companies are adopting iPads in their business offices by distributing or making available iPads to employees. Examples of uses in the workplace include attorneys responding to clients, medical professionals accessing health records during patient exams, and managers approving employee requests.[176][177][178]
A survey by Frost & Sullivan shows that iPad usage in office workplaces is linked to the goals of increased employee productivity, reduced paperwork, and increased revenue. The research firm estimates that "The mobile-office application market in North America may reach $6.85billion in 2015, up from an estimated $1.76billion [in 2010]."[179]
Since March 2011, the US Federal Aviation Administration has approved the iPad for in-cockpit use to cut down on the paper consumption in several airlines.[180] In 2011, Alaska Airlines became the first airline to replace pilots' paper manuals with iPads, weighing 0.68kg compared to 11kg for the printed flight manuals. It hopes to have fewer back and muscle injuries.[181] More than a dozen airlines have followed suit, including United, which has distributed 11,000 iPads to cockpits.[182] Also, many airlines now offer their inflight magazine as a downloadable application for the iPad.[183][184][185]
The iPad has several uses in the classroom,[186] and has been praised as a valuable tool for homeschooling.[187][188] Soon after the iPad was released, it was reported that 81% of the top book apps were for children.[189] The iPad has also been called a revolutionary tool to help children with autism learn how to communicate and socialize more easily.[190]
In the healthcare field, iPads and iPhones have been used to help hospitals manage their supply chain. For example, Novation, a healthcare contracting services company, developed VHA PriceLynx (based on the mobile application platform of business intelligence software vendor MicroStrategy), a business intelligence app to help health care organizations manage its purchasing procedures more efficiently and save money for hospitals. Guillermo Ramas of Novation states, "Doctors won't walk around a hospital with a laptop. With an iPad it's perfect to walk around the hospital with as long as they have the information they need."[191]
During the 2010 Major League Baseball free agent season, the agent for the player Carl Crawford was sending iPads to prospective teams interested in Mr. Crawford. These iPads were pre-loaded with video clips highlighting his player, and how it would benefit their team to have him.[192]
In the United States fans attending Super Bowl XLV, the first Super Bowl since the iPad was released, could use an official National Football League (NFL) app to navigate Cowboys Stadium.[193] In 2011, the Tampa Bay Buccaneers became the first NFL club to discontinue the use of paper copies of playbooks, and instead distributed all players their playbook and videos in electronic format via an iPad 2.[194]
The iPad is able to support many music creation applications in addition to the iTunes music playback software. These include sound samplers, guitar and voice effects processors, sequencers for synthesized sounds and sampled loops, virtual synthesizers and drum machines, theremin-style and other touch responsive instruments, drum pads and many more. Gorillaz's 2010 album, The Fall, was created almost exclusively using the iPad by Damon Albarn while on tour with the band.[195]
The iPad has also greatly increased social television use. Viewers can use the iPad as a convenient second networked computer (or "second screen") for communicating with other viewers or with the television provider. Viewers can use a web browser or specialised applications to discuss a program with other viewers, while it is being broadcast, while content providers may use the second screen to interact with viewers in real time. The latter facility allows content providers to conduct (e.g.) real-time polls or to collect comments about the program, that can be displayed as text on the main television screen. Viewer interaction via a second screen is becoming increasingly popular.[196]
v
t
e
1 History
2 Hardware

2.1 Screen and input
2.2 Audio and output
2.3 Battery
2.4 Storage and SIM
2.5 Accessories


2.1 Screen and input
2.2 Audio and output
2.3 Battery
2.4 Storage and SIM
2.5 Accessories
3 Software

3.1 Interface
3.2 Multimedia
3.3 Internet connectivity
3.4 Applications
3.5 iBooks


3.1 Interface
3.2 Multimedia
3.3 Internet connectivity
3.4 Applications
3.5 iBooks
4 Model comparison
5 Restrictions

5.1 Digital rights management
5.2 Jailbreaking
5.3 Censorship


5.1 Digital rights management
5.2 Jailbreaking
5.3 Censorship
6 Reception

6.1 Reaction to the announcement
6.2 Reviews
6.3 Recognition


6.1 Reaction to the announcement
6.2 Reviews
6.3 Recognition
7 Usage

7.1 Business
7.2 Education and healthcare
7.3 Consumer usage


7.1 Business
7.2 Education and healthcare
7.3 Consumer usage
8 Timeline
9 See also
10 References
11 External links
2.1 Screen and input
2.2 Audio and output
2.3 Battery
2.4 Storage and SIM
2.5 Accessories
3.1 Interface
3.2 Multimedia
3.3 Internet connectivity
3.4 Applications
3.5 iBooks
5.1 Digital rights management
5.2 Jailbreaking
5.3 Censorship
6.1 Reaction to the announcement
6.2 Reviews
6.3 Recognition
7.1 Business
7.2 Education and healthcare
7.3 Consumer usage
Apple A4, A5 and A5X SoC: Samsung[6][127][128][129]
NAND flash RAM chips: Toshiba and Samsung (64 GB model)[130][131]
Touch-screen chips: Broadcom[130]
IPS Display (1st & 2nd Gen): LG Display
IPS Display (3rd Gen): Samsung[132]
Touch panels: Wintek (after TPK Touch Solutions was unable to fulfill its orders, delaying the iPad's release from late March to early April)[133]
Case: Catcher Technologies[134]
Batteries: 60% are made in Taiwan by Simplo Technology, 40% by Dynapack International[66][134]
Accelerometer: STMicroelectronics[135]
Book: Apple Inc.
Pen computing for a broad history of gesture-based user interfaces
Apple media events for Apple media events in general.
Comparison of:

iOS devices
E-book readers
Portable media players
Tablet computers


iOS devices
E-book readers
Portable media players
Tablet computers
iOS devices
E-book readers
Portable media players
Tablet computers
Official website
iPad technical specifications
Nielsen's iPad usability research findings
.
#(`*Microsoft Windows*`)#.
Windows 8
NT 6.2 (Build 9200) (October26,2012; 50 days ago(2012-10-26))
Microsoft Windows is a series of graphical interface operating systems developed, marketed, and sold by Microsoft.
Microsoft introduced an operating environment named Windows on November 20, 1985 as an add-on to MS-DOS in response to the growing interest in graphical user interfaces (GUIs).[2] Microsoft Windows came to dominate the world's personal computer market with over 90% market share, overtaking Mac OS, which had been introduced in 1984.
The most recent client version of Windows is Windows 8; the most recent mobile client version is Windows Phone 8; the most recent server version is Windows Server 2012.
The term Windows collectively describes any or all of several generations of Microsoft operating system products. These products are generally categorized as follows:
The history of Windows dates back to September 1981, when Chase Bishop, a computer scientist, designed the first model of an electronic device and project "Interface Manager" was started. It was announced in November 1983 (after the Apple Lisa, but before the Macintosh) under the name "Windows", but Windows 1.0 was not released until November 1985.[3] The shell of Windows 1.0 was a program known as the MS-DOS Executive. Other supplied programs were Calculator, Calendar, Cardfile, Clipboard viewer, Clock, Control Panel, Notepad, Paint, Reversi, Terminal, and Write. Windows 1.0 did not allow overlapping windows. Instead all windows were tiled. Only dialog boxes could appear over other windows.
Windows 2.0 was released in October 1987 and featured several improvements to the user interface and memory management.[3] Windows 2.0 allowed application windows to overlap each other and also introduced more sophisticated keyboard shortcuts. It could also make use of expanded memory.
Windows 2.1 was released in two different versions: Windows/386 employed the 386 virtual 8086 mode to multitask several DOS programs, and the paged memory model to emulate expanded memory using available extended memory. Windows/286 (which, despite its name, would run on the 8086) still ran in real mode, but could make use of the high memory area.
In addition to full Windows-packages, there were runtime only versions that shipped with early Windows software from third parties and made it possible to run their Windows software under MS-DOS and without the full Windows feature set.
The early versions of Windows were often thought of as simply graphical user interfaces, mostly because they ran on top of MS-DOS and used it for file system services.[4] However, even the earliest 16-bit Windows versions already assumed many typical operating system functions; notably, having their own executable file format and providing their own device drivers (timer, graphics, printer, mouse, keyboard and sound) for applications. Unlike MS-DOS, Windows allowed users to execute multiple graphical applications at the same time, through cooperative multitasking. Windows implemented an elaborate, segment-based, software virtual memory scheme, which allowed it to run applications larger than available memory: code segments and resources were swapped in and thrown away when memory became scarce, and data segments moved in memory when a given application had relinquished processor control.
Windows 3.0 (1990) and Windows 3.1 (1992) improved the design, mostly because of virtual memory and loadable virtual device drivers (VxDs) that allowed them to share arbitrary devices between multitasked DOS windows.[citation needed] Also, Windows applications could now run in protected mode (when Windows was running in Standard or 386 Enhanced Mode), which gave them access to several megabytes of memory and removed the obligation to participate in the software virtual memory scheme. They still ran inside the same address space, where the segmented memory provided a degree of protection, and multi-tasked cooperatively. For Windows 3.0, Microsoft also rewrote critical operations from C into assembly.
Windows 95 was released in August 1995, featuring a new object oriented user interface, support for long file names of up to 255 characters, and the ability to automatically detect and configure installed hardware (plug and play). It could natively run 32-bit applications, and featured several technological improvements that increased its stability over Windows 3.1. There were several OEM Service Releases (OSR) of Windows 95, each of which was roughly equivalent to a service pack.
Microsoft's next release was Windows 98 in June 1998. Microsoft released a second version of Windows 98 in May 1999, named Windows 98 Second Edition (often shortened to Windows 98 SE).
In February 2000, Windows 2000 (in the NT family) was released, followed by Windows ME in September 2000 (Me standing for Millennium Edition). Windows ME updated the core from Windows 98, but adopted some aspects of Windows 2000 and removed the "boot in DOS mode" option. It also added a new feature called System Restore, allowing the user to set the computer's settings back to an earlier date.
Windows ME is often confused with Windows 2000 (because of its name), and has been said to be one of the worst operating systems Microsoft ever released.[5]
The NT family of Windows systems was fashioned and marketed for higher reliability business use. The first release was NT 3.1 (1993), numbered "3.1" to match the consumer Windows version, which was followed by NT 3.5 (1994), NT 3.51 (1995), NT 4.0 (1996), and Windows 2000, which is the last NT-based Windows release that does not include Microsoft Product Activation. Windows NT 4.0 was the first in this line to implement the "Windows 95" user interface (and the first to include Windows 95's built-in 32-bit runtimes).
Microsoft then moved to combine their consumer and business operating systems with Windows XP that was released on October 25, 2001. It came both in home and professional versions (and later niche market versions for tablet PCs and media centers); they also diverged release schedules for server operating systems. Windows Server 2003, released a year and a half after Windows XP, brought Windows Server up to date with Windows XP. After a lengthy development process, Windows Vista was released on November 30, 2006 for volume licensing and January 30, 2007 for consumers. Its server counterpart, Windows Server 2008 was released in early 2008. On July 22, 2009, Windows 7 and Windows Server 2008 R2 were released as RTM (release to manufacturing) while the former was released to the public 3 months later on October 22, 2009.
Windows NT included support for several different platforms before the x86-based personal computer became dominant in the professional world. Versions of NT from 3.1 to 4.0 variously supported PowerPC, DEC Alpha and MIPS R4000, some of which were 64-bit processors, although the operating system treated them as 32-bit processors.
With the introduction of the Intel Itanium architecture (also known as IA-64), Microsoft released new versions of Windows to support it. Itanium versions of Windows XP and Windows Server 2003 were released at the same time as their mainstream x86 (32-bit) counterparts. On April 25, 2005, Microsoft released Windows XP Professional x64 Edition and Windows Server 2003 x64 Editions to support the x86-64 (or x64 in Microsoft terminology) architecture. Microsoft dropped support for the Itanium version of Windows XP in 2005. Windows Vista was the first end-user version of Windows that Microsoft released simultaneously in x86 and x64 editions. Windows Vista does not support the Itanium architecture. The modern 64-bit Windows family comprises AMD64/Intel64 versions of Windows 7 and Windows Server 2008, in both Itanium and x64 editions. Windows Server 2008 R2 drops the 32-bit version, although Windows 7 does not.
Windows CE (officially known as Windows Embedded Compact), is an edition of Windows that runs on minimalistic computers, like satellite navigation systems and some mobile phones. Windows Embedded Compact is based on its own dedicated kernel, dubbed Windows CE kernel. Microsoft licenses Windows CE to OEMs and device makers. The OEMs and device makers can modify and create their own user interfaces and experiences, while Windows CE provides the technical foundation to do so.
Windows CE was used in the Dreamcast along with Sega's own proprietary OS for the console. Windows CE was the core from which Windows Mobile was derived. It's successor, Windows Phone 7, was based on components from both Windows CE 6.0 R3 and Windows CE 7.0. Windows Phone 8 however, is based on the same NT-kernel as Windows 8.
Windows Embedded Compact is not to be confused with Windows XP Embedded or Windows NT 4.0 Embedded, modular editions of Windows based on Windows NT kernel.
Windows 8, the successor to Windows 7, was released to the market on 26 October 2012. Windows 8 has been designed to be used on both tablets and the conventional PC. The Microsoft Surface tablet was released alongside Windows 8, as a competitor to the Apple iPad and Samsung Galaxy Tab. It has been announced by Microsoft that Windows Surface will be available in two editions - one for the typical end user, and a Professional edition aimed at designers and other work-based users. Windows 8 was released to manufacturing on 1 August 2012, with a build of 6.2.9200.
The first version of Microsoft Windows, version 1.0, released in November 1985, lacked a degree of functionality, achieved little popularity and was to compete with Apple's own operating system. Windows 1.0 is not a complete operating system; rather, it extends MS-DOS. Microsoft Windows version 2.0 was released in November 1987 and was slightly more popular than its predecessor. Windows 2.03 (release date January 1988) had changed the OS from tiled windows to overlapping windows. The result of this change led to Apple Computer filing a suit against Microsoft alleging infringement on Apple's copyrights.[6][7]
Microsoft Windows version 3.0, released in 1990, was the first Microsoft Windows version to achieve broad commercial success, selling 2 million copies in the first six months.[8][9] It featured improvements to the user interface and to multitasking capabilities. It received a facelift in Windows 3.1, made generally available on March 1, 1992. In August 1993, a special version with integrated peer-to-peer networking was released with version number 3.11. It was sold in parallel with the basic version as Windows for Workgroups. Windows 3.1 support ended on December 31, 2001.[10]
In July 1993, Microsoft released Windows NT based on a new kernel. Windows NT 3.1 was the first release of Windows NT. NT was considered to be the professional OS and was the first Windows version to utilize preemptive multitasking.[citation needed] Windows NT would later be retooled to also function as a home operating system, with Windows XP.
On August 24, 1995, Microsoft released Windows 95, a new, and major, consumer version that made further changes to the user interface, and also used preemptive multitasking. Windows 95 was designed to replace not only Windows 3.1, but also Windows for Workgroups, and MS-DOS. It was also the first Windows operating system to include object oriented document management and use Plug and Play capabilities. The changes Windows 95 brought to the desktop were revolutionary, as opposed to evolutionary, such as those in Windows 98 and Windows ME. Mainstream support for Windows 95 ended on December 31, 2000 and extended support for Windows 95 ended on December 31, 2001.[11]
Next in the consumer line was Microsoft Windows 98 released on June 25, 1998. It was followed with the release of Windows 98 Second Edition (Windows 98 SE) in 1999. Mainstream support for Windows 98 ended on June 30, 2002 and extended support for Windows 98 ended on July 11, 2006.[12]
As part of its "professional" line, Microsoft released Windows 2000 in February 2000. During 2004 part of the Source Code for Windows 2000 was leaked onto the Internet. This was bad for Microsoft as the same kernel used in Windows 2000 was used in Windows XP. The consumer version following Windows 98 was Windows ME (Windows Millennium Edition). Released in September 2000, Windows ME implemented a number of new technologies for Microsoft: most notably publicized was "Universal Plug and Play". Windows ME was heavily criticized due to slowness, freezes and hardware problems.
In October 2001, Microsoft released Windows XP, a version built on the Windows NT kernel that also retained the consumer-oriented usability of Windows 95 and its successors. This new version was widely praised in computer magazines.[13] It shipped in two distinct editions, "Home" and "Professional", the former lacking many of the superior security and networking features of the Professional edition. Additionally, the first "Media Center" edition was released in 2002,[14] with an emphasis on support for DVD and TV functionality including program recording and a remote control. Mainstream support for Windows XP ended on April 14, 2009. Extended support will continue until April 8, 2014.[15]
In April 2003, Windows Server 2003 was introduced, replacing the Windows 2000 line of server products with a number of new features and a strong focus on security; this was followed in December 2005 by Windows Server 2003 R2.
On January 30, 2007, Microsoft released Windows Vista. It contains a number of new features, from a redesigned shell and user interface to significant technical changes, with a particular focus on security features. It is available in a number of different editions, and has been subject to some criticism.
On October 22, 2009, Microsoft released Windows 7. Unlike its predecessor, Windows Vista, which introduced a large number of new features, Windows 7 was intended to be a more focused, incremental upgrade to the Windows line, with the goal of being compatible with applications and hardware which Windows Vista was not at the time.[16] Windows 7 has multi-touch support, a redesigned Windows shell with a new taskbar, referred to as the Superbar, a home networking system called HomeGroup,[17] and performance improvements.
On February 29, 2012, Microsoft released Windows 8 Consumer Preview, the beta version of Windows 8, build 8250. For the first time since Windows 95, the Start button is no longer available on the taskbar. It has been replaced with the Start screen and can be triggered by clicking the bottom-left corner of the screen and by clicking Start in the Charm or by depressing the Windows Key on the keyboard. Windows president Steven Sinofsky said more than 100,000 changes had been made since the developer version went public. In the first day of its release, Windows 8 Consumer Preview was downloaded over one million times. Microsoft released the Windows 8 Release Preview, Build 8400 on June 1, 2012. Like the Developer Preview, the Consumer Preview and the Release Preview are both set to expire on January 15, 2013. On October 26, 2012, Microsoft released Windows 8.

Consumer versions of Windows were originally designed for ease-of-use on a single-user PC without a network connection, and did not have security features built in from the outset.[26] However, Windows NT and its successors are designed for security (including on a network) and multi-user PCs, but were not initially designed with Internet security in mind as much, since, when it was first developed in the early 1990s, Internet use was less prevalent.[27]
These design issues combined with programming errors (e.g. buffer overflows) and the popularity of Windows means that it is a frequent target of computer worm and virus writers. In June 2005, Bruce Schneier's Counterpane Internet Security reported that it had seen over 1,000 new viruses and worms in the previous six months.[28] In 2005, Kaspersky Lab found around 11,000 malicious programsviruses, Trojans, back-doors, and exploits written for Windows.[29]
Microsoft releases security patches through its Windows Update service approximately once a month (usually the second Tuesday of the month), although critical updates are made available at shorter intervals when necessary.[30] In versions of Windows after and including Windows 2000 SP3 and Windows XP, updates can be automatically downloaded and installed if the user selects to do so. As a result, Service Pack 2 for Windows XP, as well as Service Pack 1 for Windows Server 2003, were installed by users more quickly than it otherwise might have been.[31]
While the Windows 9x series offered the option of having profiles for multiple users, they had no concept of access privileges, and did not allow concurrent access; and so were not true multi-user operating systems. In addition, they implemented only partial memory protection. They were accordingly widely criticised for lack of security.
The Windows NT series of operating systems, by contrast, are true multi-user, and implement absolute memory protection. However, a lot of the advantages of being a true multi-user operating system were nullified by the fact that, prior to Windows Vista, the first user account created during the setup process was an administrator account, which was also the default for new accounts. Though Windows XP did have limited accounts, the majority of home users did not change to an account type with fewer rights partially due to the number of programs which unnecessarily required administrator rights and so most home users ran as administrator all the time.
Windows Vista changes this[32] by introducing a privilege elevation system called User Account Control. When logging in as a standard user, a logon session is created and a token containing only the most basic privileges is assigned. In this way, the new logon session is incapable of making changes that would affect the entire system. When logging in as a user in the Administrators group, two separate tokens are assigned. The first token contains all privileges typically awarded to an administrator, and the second is a restricted token similar to what a standard user would receive. User applications, including the Windows Shell, are then started with the restricted token, resulting in a reduced privilege environment even under an Administrator account. When an application requests higher privileges or "Run as administrator" is clicked, UAC will prompt for confirmation and, if consent is given (including administrator credentials if the account requesting the elevation is not a member of the administrators group), start the process using the unrestricted token.[33]
All Windows versions from Windows NT 3 have been based on a file system permission system referred to as AGLP (Accounts, Global, Local, Permissions) AGDLP which in essence where file permissions are applied to the file/folder in the form of a 'local group' which then has other 'global groups' as members. These global groups then hold other groups or users depending on different Windows versions used. This system varies from other vendor products such as Linux and NetWare due to the 'static' allocation of permission being applied directory to the file or folder. However using this process of AGLP/AGDLP/AGUDLP allows a small number of static permissions to be applied and allows for easy changes to the account groups without reapplying the file permissions on the files and folders.
On January 6, 2005, Microsoft released a Beta version of Microsoft AntiSpyware, based upon the previously released Giant AntiSpyware. On February 14, 2006, Microsoft AntiSpyware became Windows Defender with the release of Beta 2. Windows Defender is a freeware program designed to protect against spyware and other unwanted software. Windows XP and Windows Server 2003 users who have genuine copies of Microsoft Windows can freely download the program from Microsoft's web site, and Windows Defender ships as part of Windows Vista and 7.[34] In Windows 8, Windows Defender and Microsoft Security Essentials have been combined into a single program, named Windows Defender. It is based on Microsoft Security Essentials borrowing its features and user interface. Although it is enabled by default, it can be turned off to use another anti-virus solution.[35]
In an article based on a report by Symantec,[36] internetnews.com has described Microsoft Windows as having the "fewest number of patches and the shortest average patch development time of the five operating systems it monitored in the last six months of 2006."[37]
A study conducted by Kevin Mitnick and marketing communications firm Avantgarde in 2004 found that an unprotected and unpatched Windows XP system with Service Pack 1 lasted only 4 minutes on the Internet before it was compromised, and an unprotected and also unpatched Windows Server 2003 system was compromised after being connected to the internet for 8 hours.[38] This study does not apply to Windows XP systems running the Service Pack 2 update (released in late 2004), which vastly improved the security of Windows XP.[citation needed] The computer that was running Windows XP Service Pack 2 was not compromised. The AOL National Cyber Security Alliance Online Safety Study of October 2004 determined that 80% of Windows users were infected by at least one spyware/adware product.[citation needed] Much documentation is available describing how to increase the security of Microsoft Windows products. Typical suggestions include deploying Microsoft Windows behind a hardware or software firewall, running anti-virus and anti-spyware software, and installing patches as they become available through Windows Update.[39]
Emulation allows the use of some Windows applications without using Microsoft Windows. These include:
  
1 Versions

1.1 Early versions
1.2 Windows 3.0 and 3.1
1.3 Windows 95, 98, and ME
1.4 Windows NT family
1.5 Windows XP, Vista and 7

1.5.1 64-bit operating systems


1.6 Windows CE
1.7 Windows 8


1.1 Early versions
1.2 Windows 3.0 and 3.1
1.3 Windows 95, 98, and ME
1.4 Windows NT family
1.5 Windows XP, Vista and 7

1.5.1 64-bit operating systems


1.5.1 64-bit operating systems
1.6 Windows CE
1.7 Windows 8
2 History
3 Timeline of releases
4 Usage share
5 Security

5.1 File permissions
5.2 Windows Defender
5.3 Third-party analysis


5.1 File permissions
5.2 Windows Defender
5.3 Third-party analysis
6 Emulation software
7 See also
8 References
9 External links
1.1 Early versions
1.2 Windows 3.0 and 3.1
1.3 Windows 95, 98, and ME
1.4 Windows NT family
1.5 Windows XP, Vista and 7

1.5.1 64-bit operating systems


1.5.1 64-bit operating systems
1.6 Windows CE
1.7 Windows 8
1.5.1 64-bit operating systems
5.1 File permissions
5.2 Windows Defender
5.3 Third-party analysis
Wine a free and open source software implementation of the Windows API, allowing one to run many Windows applications on x86-based platforms, including Linux and Mac OS X. Wine developers refer to it as a "compatibility layer";[40] and make use of Windows-style APIs to emulate the Windows environment.

CrossOver A Wine package with licensed fonts. Its developers are regular contributors to Wine, and focus on Wine running officially supported applications.
Cedega TransGaming Technologies' proprietary fork of Wine, designed specifically for running games written for Microsoft Windows under Linux. A version of Cedega known as Cider is used by some video game publishers to allow Windows games to run on Mac OS X. Since Wine was licensed under the LGPL, Cedega has been unable to port the improvements made to Wine to their proprietary codebase. Cedega ceased its service in February 2011.
Darwine A bundling of Wine to the PowerPC Macs running OS X by running Wine on top of QEMU. Intel Macs use the same Wine as other *NIX x86 systems.


CrossOver A Wine package with licensed fonts. Its developers are regular contributors to Wine, and focus on Wine running officially supported applications.
Cedega TransGaming Technologies' proprietary fork of Wine, designed specifically for running games written for Microsoft Windows under Linux. A version of Cedega known as Cider is used by some video game publishers to allow Windows games to run on Mac OS X. Since Wine was licensed under the LGPL, Cedega has been unable to port the improvements made to Wine to their proprietary codebase. Cedega ceased its service in February 2011.
Darwine A bundling of Wine to the PowerPC Macs running OS X by running Wine on top of QEMU. Intel Macs use the same Wine as other *NIX x86 systems.
ReactOS An open-source OS that is intended to run the same software as Windows, originally designed to simulate Windows NT 4.0, now aiming at Windows XP and Vista/7 compatibility. It has been in the development stage since 1996.
CrossOver A Wine package with licensed fonts. Its developers are regular contributors to Wine, and focus on Wine running officially supported applications.
Cedega TransGaming Technologies' proprietary fork of Wine, designed specifically for running games written for Microsoft Windows under Linux. A version of Cedega known as Cider is used by some video game publishers to allow Windows games to run on Mac OS X. Since Wine was licensed under the LGPL, Cedega has been unable to port the improvements made to Wine to their proprietary codebase. Cedega ceased its service in February 2011.
Darwine A bundling of Wine to the PowerPC Macs running OS X by running Wine on top of QEMU. Intel Macs use the same Wine as other *NIX x86 systems.
Architecture of the Windows NT operating system line
Criticism of Microsoft Windows
Comparison of operating systems
Comparison of Windows versions
List of Microsoft Windows components
List of operating systems
Wintel
Official website
Microsoft Developer Network
Windows Client Developer Resources
Microsoft Windows History Timeline
Pearson Education, InformIT History of Microsoft Windows
Microsoft Windows 7 for Government
.
#(`*Macintosh*`)#.
The Macintosh (/mknt/ MAK-in-tosh),[1] marketed as Mac, is a line of personal computers (PCs) designed, developed, and marketed by Apple Inc. It is targeted mainly at the home, education, and creative professional markets, and includes the descendants of the original iMac, the entry-level Mac mini desktop model, the Mac Pro tower graphics workstation, and the MacBook Air and MacBook Pro laptops. Its Xserve server was discontinued on January 31, 2011.[2]
Apple Inc.'s then-chairman Steve Jobs introduced the first Macintosh on January 24, 1984. It became the first commercially successful personal computer to feature a mouse and a graphical user interface, rather than a command-line interface.[3] The Apple II saw success through the end of the decade, though popularity dropped in the 1990s as the personal computer market shifted toward the "Wintel" platform: IBM PC compatible machines running MS-DOS and Microsoft Windows.[4] In 1998, Apple consolidated its multiple consumer-level desktop models into the all-in-one iMac, which proved to be a sales success and saw the brand revitalized.
Production of the Mac is based on a vertical integration model. Apple facilitates all aspects of its hardware and creates its own operating system[5] that is pre-installed on all Mac computers, unlike most IBM PC compatibles, where multiple sellers create and integrate hardware intended to run another company's operating software. Apple exclusively produces Mac hardware, choosing internal systems, designs, and prices. Apple uses third party components, however, such as graphics subsystems from nVidia and ATi. Current Mac CPUs use Intel's X86-64 architecture. The earliest models (19841994) used Motorola's 68k, and models from 1994 until 2006 used the AIM alliance's PowerPC. Apple also develops the operating system for the Mac, OS X, currently on version 10.8 "Mountain Lion". The modern Mac, like other personal computers, is capable of running alternative operating systems such as Linux, FreeBSD, and, in the case of Intel-based Macs, Microsoft Windows. However, Apple does not license OS X for use on non-Apple computers.
The Macintosh project began in the late 1970s with Jef Raskin, an Apple employee who envisioned an easy-to-use, low-cost computer for the average consumer. He wanted to name the computer after his favorite type of apple, the McIntosh,[6] but the name had to be changed for legal reasons as it was too close, phonetically, to that of the McIntosh audio equipment manufacturer. Steve Jobs requested a release of the name so that Apple could use it, but was denied, forcing Apple to eventually buy the rights to use the name.[7] Raskin was authorized to start hiring for the project in September 1979,[8] and he began to look for an engineer who could put together a prototype. Bill Atkinson, a member of Apple's Lisa team (which was developing a similar higher-end computer,) introduced him to Burrell Smith, a self-taught engineer who worked as a service technician and had been hired earlier that year. Over the years, Raskin assembled a large development team that designed and built the original Macintosh hardware and the original version of the Mac OS operating system that the computer ran. Besides Raskin, Atkinson and Smith, the team included George Crow,[9] Chris Espinosa, Joanna Hoffman, Bruce Horn, Susan Kare, Andy Hertzfeld, Guy Kawasaki, Daniel Kottke,[10] and Jerry Manock.[11][12]
Smith's first Macintosh board was built to Raskin's design specifications: it had 64kilobytes (kB) of RAM, used the Motorola 6809E microprocessor, and was capable of supporting a 256256-pixel black-and-white bitmap display. Bud Tribble, a member of the Mac team, was interested in running the Lisa's graphical programs on the Macintosh, and asked Smith whether he could incorporate the Lisa's Motorola 68000 microprocessor into the Mac while still keeping the production cost down. By December 1980, Smith had succeeded in designing a board that not only used the 68000, but increased its speed from 5MHz to 8MHz; this board also had the capacity to support a 384256-pixel display. Smith's design used fewer RAM chips than the Lisa, which made production of the board significantly more cost-efficient. The final Mac design was self-contained and had the complete QuickDraw picture language and interpreter in 64 kB of ROM far more than most other computers; it had 128kB of RAM, in the form of sixteen 64kilobit (kb) RAM chips soldered to the logicboard. Though there were no memory slots, its RAM was expandable to 512kB by means of soldering sixteen IC sockets to accept 256kb RAM chips in place of the factory-installed chips. The final product's screen was a 9-inch, 512x342 pixel monochrome display, exceeding the size of the planned screen.[13]
Burrel's innovative design, which combined the low production cost of an Apple II with the computing power of Lisa's CPU, the Motorola 68K, set off shock waves within Apple, capturing the attention of Steve Jobs,[14] co-founder of Apple. Realizing that the Macintosh was more marketable than the Lisa, he began to focus his attention on the project. Raskin left the team in 1981 over a personality conflict with Jobs. Team member Andy Hertzfeld said that the final Macintosh design is closer to Jobs' ideas than Raskin's.[8] After hearing of the pioneering GUI technology being developed at Xerox PARC, Jobs had negotiated a visit to see the Xerox Alto computer and its Smalltalk development tools in exchange for Apple stock options. The Lisa and Macintosh user interfaces were influenced by technology seen at Xerox PARC and were combined with the Macintosh group's own ideas.[15] Jobs also commissioned industrial designer Hartmut Esslinger to work on the Macintosh line, resulting in the "Snow White" design language; although it came too late for the earliest Macs, it was implemented in most other mid- to late-1980s Apple computers.[16] However, Jobs' leadership at the Macintosh project did not last; after an internal power struggle with new CEO John Sculley, Jobs resigned from Apple in 1985.[17] He went on to found NeXT, another computer company targeting the education market,[18] and did not return until 1997, when Apple acquired NeXT.[19] The Macintosh 128K was manufactured at an Apple plant in Fremont, California.[20]
The Macintosh 128K was announced to the press in October 1983, followed by an 18-page brochure included with various magazines in December.[21] The Macintosh was introduced by the now-famous US$1.5million Ridley Scott television commercial, "1984".[22] It most notably aired during the third quarter of Super Bowl XVIII on January 22, 1984, and is now considered a "watershed event"[23] and a "masterpiece."[24] "1984" used an unnamed heroine to represent the coming of the Macintosh (indicated by a Picasso-style picture of the computer on her white tank top) as a means of saving humanity from the "conformity" of IBM's attempts to dominate the computer industry. The ad alludes to George Orwell's novel, Nineteen Eighty-Four, which described a dystopian future ruled by a televised "Big Brother."[25][26]
Two days after "1984" aired, the Macintosh went on sale, and came bundled with two applications designed to show off its interface: MacWrite and MacPaint. It was first demonstrated by Steve Jobs in the first of his famous Mac keynote speeches, and though the Mac garnered an immediate, enthusiastic following, some labeled it a mere "toy."[27] Because the operating system was designed largely around the GUI, existing text-mode and command-driven applications had to be redesigned and the programming code rewritten. This was a time-consuming task that many software developers chose not to undertake, and could be regarded as a reason for an initial lack of software for the new system. In April 1984, Microsoft's MultiPlan migrated over from MS-DOS, with Microsoft Word following in January 1985.[28] In 1985, Lotus Software introduced Lotus Jazz for the Macintosh platform after the success of Lotus 1-2-3 for the IBM PC, although it was largely a flop.[29] Apple introduced the Macintosh Office suite the same year with the "Lemmings" ad. Infamous for insulting its own potential customers, the ad was not successful.[30]
Apple spent upwards of $2.5 million purchasing all 39 advertising pages in a special, post-election issue of Newsweek[31] Apple also ran a "Test Drive a Macintosh" promotion, in which potential buyers with a credit card could take home a Macintosh for 24 hours and return it to a dealer afterwards. While 200,000 people participated, dealers disliked the promotion, the supply of computers was insufficient for demand, and many were returned in such a bad condition that they could no longer be sold. This marketing campaign caused CEO John Sculley to raise the price from US$1,995 to US$2,495 (about $5,200 when adjusted for inflation in 2010).[30][32]
In 1985, the combination of the Mac, Apple's LaserWriter printer, and Mac-specific software like Boston Software's MacPublisher and Aldus PageMaker enabled users to design, preview, and print page layouts complete with text and graphicsan activity to become known as desktop publishing. Initially, desktop publishing was unique to the Macintosh, but eventually became available for other platforms.[33] Later, applications such as Macromedia FreeHand, QuarkXPress, and Adobe's Photoshop and Illustrator strengthened the Mac's position as a graphics computer and helped to expand the emerging desktop publishing market.
The Macintosh's minimal memory became apparent, even compared with other personal computers in 1984, and could not be expanded easily. It also lacked a hard disk drive or the means to easily attach one. Many small companies sprang up to address the memory issue. Suggestions revolved around either upgrading the memory to 512 KB or removing the computer's 16 memory chips and replacing them with larger-capacity chips, a tedious operation that was not always successful. In October 1985, Apple introduced the Macintosh 512K, with quadruple the memory of the original, at a price of US$3,195.[34] It also offered an upgrade for 128k Macs that involved replacing the logicboard. In an attempt to improve connectivity, Apple released the Macintosh Plus on January 10, 1986, for a price of US$2,600. It offered one megabyte of RAM, easily expandable to four megabytes by the use of socketed RAM boards. It also featured a SCSI parallel interface, allowing up to seven peripheralssuch as hard drives and scannersto be attached to the machine. Its floppy drive was increased to an 800kB capacity. The Mac Plus was an immediate success and remained in production, unchanged, until October 15, 1990; on sale for just over four years and ten months, it was the longest-lived Macintosh in Apple's history.[35] In September 1986, Apple introduced the Macintosh Programmer's Workshop, or MPW, an application that allowed software developers to create software for Macintosh on Macintosh, rather than cross compiling from a Lisa. In August 1987, Apple unveiled HyperCard and MultiFinder, which added cooperative multitasking to the Macintosh. Apple began bundling both with every Macintosh.
Updated Motorola CPUs made a faster machine possible, and in 1987 Apple took advantage of the new Motorola technology and introduced the Macintosh II, powered by a 16MHz Motorola 68020 processor.[36] The primary improvement in the Macintosh II was Color QuickDraw in ROM, a color version of the graphics language which was the heart of the machine. Among the many innovations in Color QuickDraw were the ability to handle any display size, any color depth, and multiple monitors. The Macintosh II marked the start of a new direction for the Macintosh, as now for the first time it had an open architecture with several NuBus expansion slots, support for color graphics and external monitors, and a modular design similar to that of the IBM PC. It had an internal hard drive and a power supply with a fan, which was initially fairly loud.[37] One third-party developer sold a device to regulate fan speed based on a heat sensor, but it voided the warranty.[38] Later Macintosh computers had quieter power supplies and hard drives. The Macintosh SE was released at the same time as the Macintosh II, as the first compact Mac with a 20MB internal hard drive and an expansion slot.[39] The SE's expansion slot was located inside the case along with the CRT, potentially exposing an upgrader to high voltage. For this reason, Apple recommended users bring their SE to an authorized Apple dealer to have upgrades performed.[40] The SE also updated Jerry Manock and Terry Oyama's original design and shared the Macintosh II's Snow White design language, as well as the new Apple Desktop Bus (ADB) mouse and keyboard that had first appeared on the Apple IIGS some months earlier.
In 1987, Apple spun off its software business as Claris. It was given the code and rights to several applications that had been written within Apple, most notably MacWrite, MacPaint, and MacProject. In the late 1980s, Claris released a number of revamped software titles; the result was the "Pro" series, including MacDraw Pro, MacWrite Pro, and FileMaker Pro. To provide a complete office suite, Claris purchased the rights to the Informix Wingz spreadsheet program on the Mac, renaming it Claris Resolve, and added the new presentation software Claris Impact. By the early 1990s, Claris applications were shipping with the majority of consumer-level Macintoshes and were extremely popular. In 1991, Claris released ClarisWorks, which soon became their second best-selling application. When Claris was reincorporated back into Apple in 1998, ClarisWorks was renamed AppleWorks beginning with version 5.0.[41]
In 1988, Apple sued Microsoft and Hewlett-Packard on the grounds that they infringed Apple's copyrighted GUI, citing (among other things) the use of rectangular, overlapping, and resizable windows. After four years, the case was decided against Apple, as were later appeals. Apple's actions were criticized by some in the software community, including the Free Software Foundation (FSF), who felt Apple was trying to monopolize on GUIs in general, and boycotted GNU software for the Macintosh platform for seven years.[42][43]
With the new Motorola 68030 processor came the Macintosh IIx in 1988, which had benefited from internal improvements, including an on-board MMU.[44] It was followed in 1989 by the Macintosh IIcx, a more compact version with fewer slots [45] and a version of the Mac SE powered by the 16MHz 68030, the Macintosh SE/30.[46] Later that year, the Macintosh IIci, running at 25MHz, was the first Mac to be "32-bit clean." This allowed it to natively support more than 8MB of RAM,[47] unlike its predecessors, which had "32-bit dirty" ROMs (8 of the 32bits available for addressing were used for OS-level flags). System 7 was the first Macintosh operating system to support 32-bit addressing.[48] The following year, the Macintosh IIfx, starting at US$9,900, was unveiled. Apart from its fast 40MHz 68030 processor, it had significant internal architectural improvements, including faster memory and two Apple II CPUs dedicated to I/O processing.[49]
Microsoft Windows 3.0 was released in May 1990 as a less expensive alternative to the Macintosh platform, which began to approach the Macintosh operating system in both performance and feature set.[citation needed] In response, Apple introduced a range of relatively inexpensive Macs in October 1990. The Macintosh Classic, essentially a less expensive version of the Macintosh SE, was the least expensive Mac offered until early 2001.[50] The 68020-powered Macintosh LC, in its distinctive "pizza box" case, offered color graphics and was accompanied by a new, low-cost 512384 pixel monitor.[51] The Macintosh IIsi was essentially a 20MHz IIci with only one expansion slot.[52] All three machines sold well,[53] although Apple's profit margin on them was considerably lower than that on earlier models.[50]
Apple improved Macintosh computers by introducing models equipped with newly available processors from the 68k lineup. The Macintosh Classic II[54] and Macintosh LC II, which used a 16MHz 68030 CPU,[55] were joined in 1991 by the Macintosh Quadra 700[56] and 900,[57] the first Macs to employ the faster Motorola 68040 processor. In 1994, Apple abandoned Motorola CPUs for the RISC PowerPC architecture developed by the AIM alliance of Apple Computer, IBM, and Motorola.[58] The Power Macintosh line, the first to use the new chips, proved to be highly successful, with over a million PowerPC units sold in nine months.[59]
The Macintosh Portable was replaced in 1991 with the first of the PowerBook line: the PowerBook 100, a miniaturized Portable; the 16MHz 68030 PowerBook 140; and the 25MHz 68030 PowerBook 170.[60] They were the first portable computers with the keyboard behind a palm rest and a built-in pointing device (a trackball) in front of the keyboard.[61] The 1993 PowerBook 165c was Apple's first portable computer to feature a color screen, displaying 256 colors with 640 x 400-pixel resolution.[62] The second generation of PowerBooks, the 68040-equipped 500 series, introduced trackpads, integrated stereo speakers, and built-in Ethernet to the laptop form factor in 1994.[63]
As for Mac OS, System 7 was a 32-bit rewrite from Pascal to C++ that introduced virtual memory and improved the handling of color graphics, as well as memory addressing, networking, and co-operative multitasking. Also during this time, the Macintosh began to shed the "Snow White" design language, along with the expensive consulting fees they were paying to Frogdesign. Apple instead brought the design work in-house by establishing the Apple Industrial Design Group, becoming responsible for crafting a new look for all Apple products.[64]
Despite these technical and commercial successes, Microsoft and Intel began to rapidly lower Apple's market share with the introduction of the Windows 95 operating system and Pentium processors. These significantly enhanced the multimedia capability and performance of IBM PC compatible computers, and brought Windows closer to the Mac GUI. Furthermore, Apple had created too many similar models that confused potential buyers. At one point, its product lineup was subdivided into Classic, LC, II, Quadra, Performa, and Centris models, with essentially the same computer being sold under a number of different names.[65] These models competed against Macintosh clones, hardware manufactured by third-parties that ran Apple's System 7. This succeeded in increasing the Macintosh's market share somewhat, and provided cheaper hardware for consumers, but hurt Apple financially as existing Apple customers began to buy cheaper clones while Apple shouldered the burden of developing the platform.
When Steve Jobs returned to Apple in 1997 following the company's purchase of NeXT, he ordered that the OS that had been previewed as version 7.7 be branded Mac OS 8 (in place of the never-to-appear Copland OS). Since Apple had licensed only System 7 to third-parties, this move effectively ended the clone line. The decision caused significant financial losses for companies like Motorola, who produced the StarMax; Umax, who produced the SuperMac;[66] and Power Computing, who offered several lines Mac clones, including the PowerWave, PowerTower, and PowerTower Pro.[67] These companies had invested substantial resources in creating their own Mac-compatible hardware.[68] Apple bought out Power Computing's license, but allowed Umax to continue selling Mac clones until their license expired, as they had a sizeable presence in the lower-end segment that Apple did not.[citation needed]
In 1998, Apple introduced its new iMac which, like the original 128K Mac, was an all-in-one computer. Its translucent plastic case, originally Bondi blue and later various additional colors, is considered an industrial design landmark of the late 1990s. The iMac did away with most of Apple's standard (and usually proprietary) connections, such as SCSI and ADB, in favor of two USB ports, effectively making it the first Legacy-free PC.[70] It replaced a floppy disk drive with a CD-ROM drive for installing software,[4][71] but was incapable of writing to CDs or other media without external third-party hardware. The iMac proved to be phenomenally successful, with 800,000 units sold in 139 days.[72] It made the company an annual profit of US$309million, Apple's first profitable year since Michael Spindler took over as CEO in 1995.[73] This aesthetic was applied to the Power Macintosh and later the iBook, Apple's first consumer-level laptop computer, filling the missing quadrant of Apple's "four-square product matrix" (desktop and portable products for both consumers and professionals).[74] More than 140,000 pre-orders were placed before it started shipping in September,[75] and by October proved to be a large success.[76]
In early 2001, Apple began shipping computers with CD-RW drives and emphasized the Mac's ability to play DVDs by including DVD-ROM and DVD-RAM drives as standard.[77] Steve Jobs admitted that Apple had been "late to the party" on writable CD technology, but felt that Macs could become a "digital hub" that linked and enabled an "emerging digital lifestyle".[78] Apple would later introduce an update to its iTunes music player software that enabled it to burn CDs, along with a controversial "Rip, Mix, Burn" advertising campaign that some[79] felt encouraged media piracy.[80] This accompanied the release of the iPod, Apple's first successful handheld device. Apple continued to launch products, such as the unsuccessful Power Mac G4 Cube,[81] the education-oriented eMac, and the titanium (and later aluminium) PowerBook G4 laptop for professionals.
The original iMac used a PowerPC G3 processor, but G4 and G5 chips were soon added, both accompanied by complete case redesigns that dropped the array of colors in favor of white plastic. As of 2007, all iMacs use aluminium cases. On January 11, 2005, Apple announced the Mac Mini, priced at US$499, making it the cheapest Mac.[82][83]
Mac OS continued to evolve up to version 9.2.2, including retrofits such as the addition of a nanokernel and support for Multiprocessing Services 2.0 in Mac OS 8.6, though its dated architecture made replacement necessary.[84] Initially developed in the Pascal programming language, it was substantially rewritten in C++ for System 7. From its beginnings on an 8MHz machine with 128 KB of RAM, it had grown to support Apple's latest 1GHz G4-equipped Macs. Since its architecture was laid down, features that were already common on Apple's competition, like preemptive multitasking and protected memory, had become feasible on the kind of hardware Apple manufactured. As such, Apple introduced Mac OS X, a fully overhauled Unix-based successor to Mac OS 9. OS X uses Darwin, XNU, and Mach as foundations, and is based on NeXTSTEP. It was released to the public in September 2000, as the Mac OS X Public Beta, featuring a revamped user interface called "Aqua". At US$29.99, it allowed adventurous Mac users to sample Apple's new operating system and provide feedback for the actual release.[85] The initial version of Mac OS X, 10.0 "Cheetah", was released on March 24, 2001. Older Mac OS applications could still run under early Mac OS X versions, using an environment called "Classic". Subsequent releases of Mac OS X included 10.1 "Puma" (September 25, 2001), 10.2 "Jaguar" (August 24, 2002), 10.3 "Panther" (October 24, 2003) and 10.4 "Tiger" (April 29, 2005).
Apple discontinued the use of PowerPC microprocessors in 2006. At WWDC 2005, Steve Jobs revealed this transition, also noting that Mac OS X was always developed to run on both the Intel and PowerPC architectures.[87] All new Macs now use x86 processors made by Intel, and some were renamed as a result.[88] Intel-based Macs running OS X 10.6 and below (support has been discontinued since 10.7) can run pre-existing software developed for PowerPC using an emulator called Rosetta,[89] although at noticeably slower speeds than native programs. The Classic environment is unavailable on the Intel architecture, though. Intel chips introduced the potential to run the Microsoft Windows operating system natively on Apple hardware, without emulation software such as Virtual PC. In March 2006, a group of hackers announced that they were able to run Windows XP on an Intel-based Mac. The group released their software as open source and has posted it for download on their website.[90] On April 5, 2006, Apple announced the availability of the public beta of Boot Camp, software that allows owners of Intel-based Macs to install Windows XP on their machines; later versions added support for Windows Vista and Windows 7. Classic was discontinued in Mac OS X 10.5, and Boot Camp became a standard feature on Intel-based Macs.[91][92]
Starting in 2006, Apple's industrial design shifted to favor aluminum, which was used in the construction of the first MacBook Pro. Glass was added in 2008 with the introduction of the unibody MacBook Pro. These materials are billed as environmentally friendly.[93] The iMac, MacBook Pro, MacBook Air, and Mac Mini lines currently all use aluminum enclosures, and are now made of a single unibody.[94][95][96] Chief designer Jonathan Ive continues to guide products towards a minimalist and simple feel,[97][98] including eliminating of replaceable batteries in notebooks.[99] Multi-touch gestures from the iPhone's interface have been applied to the Mac line in the form of touch pads on notebooks and the Magic Mouse and Magic Trackpad for desktops.
In recent years, Apple has seen a significant boost in sales of Macs.[100] This has been attributed, in part, to the success of the iPod and the iPhone, a halo effect whereby satisfied iPod or iPhone owners purchase more Apple equipment,[101] as well as the use of Intel microprocessors.[102] From 2001 to 2008, Mac sales increased continuously on an annual basis. Apple reported worldwide sales of 3.36million Macs during the 2009 holiday season.[103] As of Mid-2011, the Macintosh continues to enjoy rapid market share increase in the US, growing from 7.3% of all computer shipments in 2010 to 9.3% in 2011.[104] On February 24, 2011, Apple became the first company to bring to market a computer that utilized Intel's new Thunderbolt (codename Light Peak) I/O interface. Using the same physical interface as a Mini DisplayPort, and backwards compatible with that standard, Thunderbolt boasts two-way transfer speeds of 10Gbit/s.[105]
































































Apple directly sub-contracts hardware production to Asian original equipment manufacturers such as Asus, maintaining a high degree of control over the end product. By contrast, most other companies (including Microsoft) create software that can be run on hardware produced by a variety of third-parties such as Dell, HP/Compaq, and Lenovo. Consequently, the Macintosh buyer has comparably fewer options.
The current Mac product family uses Intel x86-64 processors. Apple introduced an emulator during the transition from PowerPC chips (called Rosetta), much as it did during the transition from Motorola 68000 architecture a decade earlier. The Macintosh is the only mainstream computer platform to have successfully transitioned to a new CPU architecture,[106] and has done so twice. All current Mac models ship with at least 2GB of RAM as standard. Current Mac computers use ATI Radeon or nVidia GeForce graphics cards as well as Intel graphics built into the main CPU. All current Macs (except for the MacBook Air, Mac Mini, and MacBook Pro with Retina Display) ship with an optical media drive that includes a dual-function DVD/CD burner. Apple refers to this as a SuperDrive. Current Macs include two standard data transfer ports: USB and FireWire (except for the MacBook Air, which does not include FireWire). MacBook Pro, iMac, MacBook Air, and Mac Mini computers now also feature the "Thunderbolt" port, which Apple says can transfer data at speeds up to 10 gigabits per second.[107] USB was introduced in the 1998 iMac G3 and is ubiquitous today,[4] while FireWire is mainly reserved for high-performance devices such as hard drives or video cameras. Starting with the then-new iMac G5, released in October 2005, Apple started to include built-in iSight cameras on appropriate models, and a media center interface called Front Row that can be operated by an Apple Remote or keyboard for accessing media stored on the computer. Front Row has been discontinued as of 2011, however, and the Apple Remote is no longer bundled with new Macs.[108][109]
Apple was initially reluctant to embrace mice with multiple buttons and scroll wheels. Macs did not natively support pointing devices that featured multiple buttons, even from third parties, until Mac OS X arrived in 2001.[110] Apple continued to offer only single button mice, in both wired and Bluetooth wireless versions, until August 2005, when it introduced the Mighty Mouse. While it looked like a traditional one-button mouse, it actually had four buttons and a scroll ball, capable of independent x- and y-axis movement.[111] A Bluetooth version followed in July 2006.[112] In October 2009, Apple introduced the Magic Mouse, which uses multi-touch gesture recognition (similar to that of the iPhone) instead of a physical scroll wheel or ball.[113] It is available only in a wireless configuration, but the wired Mighty Mouse (re-branded as "Apple Mouse") is still available as an alternative. Since 2010, Apple has also offered the Magic Trackpad as a means to control Macintosh desktop computers in a way similar to laptops.
The original Macintosh was the first successful personal computer to use a graphical user interface devoid of a command line. It used a desktop metaphor, depicting real-world objects like documents and a trashcan as icons onscreen. The System software was introduced in 1984 with the first Macintosh and renamed Mac OS in 1997. It continued to evolve until version 9.2.2. In 2001, Apple introduced Mac OS X, based on Darwin and NEXTSTEP; its new features included the Dock and the Aqua user interface. During the transition, Apple included an emulator known as Classic, allowing users to run Mac OS 9 applications under Mac OS X 10.4 and earlier on PowerPC machines. The most recent version is Mac OS X v10.7 "Lion." In addition to Lion, all new Macs are bundled with assorted Apple-produced applications, including iLife, the Safari web browser and the iTunes media player. Apple introduced Mac OS X 10.7 in 2010, and it was made available in the summer of 2011. Lion includes many new features, such as Mission Control, the Mac App Store (available to Mac OS X v10.6.6 "Snow Leopard." users by software update), Launchpad, an application viewer and launcher akin to the iOS Home Screen, and Resume, a feature similar to the hibernate function found in Microsoft Windows.
Historically, Mac OS X enjoyed a near-absence of the types of malware and spyware that affect Microsoft Windows users.[114][115][116] Mac OS X has a smaller usage share compared to Microsoft Windows (roughly 5% and 92%, respectively),[117] but it also has secure UNIX roots. Worms, as well as potential vulnerabilities were noted in February 2006, which led some industry analysts and anti-virus companies to issue warnings that Apple's Mac OS X is not immune to malware.[118] Increasing market share coincided with additional reports of a variety of attacks.[119] Apple releases security updates for its software.[120] In early 2011, Mac OS X experienced a large increase in malware attacks,[121] and malware such as Mac Defender, MacProtector, and MacGuard were seen as an increasing problem for Mac users. At first, the malware installer required the user to enter the administrative password, but later versions were able to install without user input[122] Initially, Apple support staff were instructed not to assist in the removal of the malware or admit the existence of the malware issue, but as the malware spread, a support document was issued. Apple announced an OS X update to fix the problem. An estimated 100,000 users were affected.[123][124]
Originally, the hardware architecture was so closely tied to the Mac OS operating system that it was impossible to boot an alternative operating system. The most common workaround, used even by Apple for A/UX, was to boot into Mac OS and then to hand over control to a program that took over the system and acted as a bootloader. This technique was no longer necessary with the introduction of Open Firmware-based PCI Macs, though it was formerly used for convenience on many Old World ROM systems due to bugs in the firmware implementation.[citation needed] Now, Mac hardware boots directly from Open Firmware (most PowerPC-based Macs) or EFI (all Intel-based Macs), and Macs are no longer limited to running just Mac OS X.
Following the release of Intel-based Macs, third-party platform virtualization software such as Parallels Desktop, VMware Fusion, and VirtualBox began to emerge. These programs allow users to run Microsoft Windows or previously Windows-only software on Macs at near native speed. Apple also released Boot Camp and Mac-specific Windows drivers that help users to install Windows XP or Vista and natively dual boot between Mac OS X and Windows. Though not condoned by Apple, it is possible to run the Linux operating system using Boot camp or other virtualization workarounds.[125][126]
Because Mac OS X is a UNIX-like operating system, borrowing heavily from FreeBSD, many applications written for Linux or BSD run on Mac OS X, often using X11. Apple's smaller market share than Microsoft's means that a smaller range of shareware is available, but many popular commercial software applications from large developers, such as Microsoft's Office and Adobe's Photoshop are ported to both Mac OS and Windows. A large amount of open-source software applications, like the Firefox web browser and the OpenOffice.org office suite, are cross-platform, and thereby also run natively on the Mac.
Apple hyped the introduction of the original Mac with their "1984" commercial that aired during that year's Super Bowl.[127] It was supplemented by a number of printed pamphlets and other TV ads demonstrating the new interface and emphasizing the mouse. Many more brochures for new models like the Macintosh Plus and the Performa followed. In the 1990s, Apple started the "What's on your PowerBook?" campaign, with print ads and television commercials featuring celebrities describing how the PowerBook helps them in their businesses and everyday lives. In 1995, Apple responded to the introduction of Windows 95 with several print ads and a television commercial demonstrating its disadvantages and lack of innovation. In 1997, the Think Different campaign introduced Apple's new slogan, and in 2002 the Switch campaign followed. The most recent advertising strategy by Apple is the Get a Mac campaign, with North American, UK, and Japanese variants.[128][129]
Apple introduces new products at "special events" hosted at the Apple Town Hall auditorium, and at keynotes at the Apple Worldwide Developers Conference. Formerly, it also announced new products at trade shows like the Apple Expo and the Macworld Expo. The events typically draw a large gathering of media representatives and spectators, and are preceded by speculation about possible new products. In the past, special events have been used to unveil Apple's desktop and notebook computers, such as the iMac and MacBook, and other consumer electronic devices like the iPod, Apple TV, and iPhone. The keynotes as well as provide updates on sales and market share statistics. Apple has begun to focus its advertising on its retail stores instead of these trade shows; the company's last Macworld keynote was in 2009.[130]
Since the introduction of the Macintosh, Apple has struggled to gain a significant share of the personal computer market. At first, the Macintosh 128K suffered from a dearth of available software compared to IBM's PC, resulting in disappointing sales in 1984 and 1985. It took 74days for 50,000units to sell.[131] Market share is measured by browser hits, sales and installed base. If using the browser metric, Mac market share has increased substantially in 2007.[132] If measuring market share by installed base, there were more than 20million Mac users by 1997, compared to an installed base of around 340million Windows PCs.[133][134] Statistics from late 2003 indicate that Apple had 2.06percent of the desktop share in the United States that had increased to 2.88percent by Q42004.[135] As of October 2006, research firms IDC and Gartner reported that Apple's market share in the U.S. had increased to about 6percent.[136] Figures from December 2006, showing a market share around 6percent (IDC) and 6.1percent (Gartner) are based on a more than 30percent increase in unit sale from 2005 to 2006. The installed base of Mac computers is hard to determine, with numbers ranging from 5% (estimated in 2009)[137] to 16% (estimated in 2005).[138] Mac OS X's share of the OS market increased from 7.31% in December 2007 to 9.63% in December 2008, which is a 32% increase in market share during 2008, compared with a 22% increase during 2007.
By March 2011, OS X market share in North America had increased to slightly over 14%.[139] Whether the size of the Mac's market share and installed base is relevant, and to whom, is a hotly debated issue. Industry pundits have often called attention to the Mac's relatively small market share to predict Apple's impending doom, particularly in the early and mid 1990s when the company's future seemed bleakest. Others argue that market share is the wrong way to judge the Mac's success. Apple has positioned the Mac as a higher-end personal computer, and so it may be misleading to compare it to a budget PC.[140] Because the overall market for personal computers has grown rapidly, the Mac's increasing sales numbers are effectively swamped by the industry's expanding sales volume as a whole. Apple's small market share, then, gives the impression that fewer people are using Macs than did ten years ago, when exactly the opposite is true.[141] Soaring sales of the iPhone and iPad mean that the portion of Apple's profits represented by the Macintosh has declined in 2010, dropping to 24% from 46% two years earlier.[142] Others try to de-emphasize market share, citing that it is rarely brought up in other industries.[143] Regardless of the Mac's market share, Apple has remained profitable since Steve Jobs' return and the company's subsequent reorganization.[144] Notably, a report published in the first quarter of 2008 found that Apple had a 14% market share in the personal computer market in the US, including 66% of all computers over $1,000.[145] Market research indicates that Apple draws its customer base from a higher-income demographic than the mainstream personal computer market.[146]

1 History

1.1 Development and introduction
1.2 Desktop publishing
1.3 Decline
1.4 Revival
1.5 Transition to Intel
1.6 Timeline of Macintosh models


1.1 Development and introduction
1.2 Desktop publishing
1.3 Decline
1.4 Revival
1.5 Transition to Intel
1.6 Timeline of Macintosh models
2 Product line
3 Hardware and software

3.1 Hardware
3.2 Software


3.1 Hardware
3.2 Software
4 Advertising
5 Market share and user demographics
6 See also
7 References
8 External links
1.1 Development and introduction
1.2 Desktop publishing
1.3 Decline
1.4 Revival
1.5 Transition to Intel
1.6 Timeline of Macintosh models
3.1 Hardware
3.2 Software
Book: Apple Inc.
Apple Inc. litigation
Apple community
History of computing hardware (1960s-present)
List of Macintosh models by case type
List of Macintosh models grouped by CPU type
List of Macintosh software
List of Macintosh software published by Microsoft
Macintosh User Groups
Mac gaming
Reality distortion field
Lilith (computer)
Apple & Raskin, Jef (1992). Macintosh Human Interface Guidelines. Addison-Wesley Professional. ISBN0-201-62216-5.
Apple. "Press release Library". http://www.apple.com/pr/library/. Retrieved November 18, 2007.
Deutschman, Alan (2001). The Second Coming of Steve Jobs. Broadway. ISBN0-7679-0433-8.
Hertzfeld, Andy. "folklore.org: Macintosh stories". http://folklore.org/index.py. Retrieved April 24, 2006.
Hertzfeld, Andy (2004). Revolution in the Valley. O'Reilly Books. ISBN0-596-00719-1.
Kahney, Leander (2004). The Cult of Mac. No Starch Press. ISBN1-886411-83-2.
Kawasaki, Guy (1989). The Macintosh Way. Scott Foresman Trade. ISBN0-673-46175-0.
Kelby, Scott (2002). Macintosh... The Naked Truth. New Riders Press. ISBN0-7357-1284-0.
Knight, Dan (2005). "Macintosh History: 1984". http://lowendmac.com/history/1984dk.shtml. Retrieved April 24, 2006.
Levy, Steven (2000). Insanely Great: The Life and Times of Macintosh, the Computer That Changed Everything. Penguin Books. ISBN0-14-029177-6.
Linzmayer, Owen (2004). Apple Confidential 2.0. No Starch Press. ISBN1-59327-010-0.
Page, Ian (2007). "MacTracker Macintosh model database 4.3.1". http://www.mactracker.ca/. Retrieved November 31, 2007.
Sanford, Glen (2006). "Apple History". http://www.apple-history.com/. Retrieved April 24, 2006.
Singh, Amit (2005). "A History of Apple's Operating Systems". http://www.kernelthread.com/mac/oshistory/. Retrieved April 24, 2006.
Official website
Making the Macintosh: Technology and Culture in Silicon Valley
.
#(`*Inception*`)#.
Inception is a 2010 British-American science fiction/action heist film written, co-produced, and directed by Christopher Nolan. The film stars a large ensemble cast that includes Leonardo DiCaprio, Ken Watanabe, Joseph Gordon-Levitt, Marion Cotillard, Ellen Page, Tom Hardy, Dileep Rao, Cillian Murphy, Tom Berenger, and Michael Caine. DiCaprio plays Dom Cobb, a thief who commits corporate espionage by infiltrating the subconscious of his targets. He is offered a chance to regain his old life as payment for a task considered to be impossible: "inception", the implantation of another person's idea into a target's subconscious.[5]
Shortly after finishing Insomnia (2002) and long before Batman Begins (2005),[6] Nolan wrote an 80-page treatment about "dream stealers" and presented the idea to Warner Bros., envisioned as a horror film inspired by lucid dreaming.[7] Feeling he needed to have more experience with large-scale film production, Nolan retired the project and instead worked on Batman Begins (2005), The Prestige (2006), and The Dark Knight (2008).[8] He spent six months polishing the script before Warner Bros. purchased it in February 2009.[9] Inception was filmed in six countries and four continents, beginning in Tokyo on June19,2009, and finishing in Canada on November 22, 2009.[10] Its official budget was US$160million, a cost which was split between Warner Bros and Legendary Pictures.[3] Nolan's reputation and success with The Dark Knight helped secure the film's $100million in advertising expenditure,[3] with most of the publicity involving viral marketing.
Inception premiered in London on July8,2010, and was released in both conventional and IMAX theaters on July 16, 2010.[11][12] A box office success, Inception has grossed over $800million worldwide becoming one of the highest-grossing films of all time.[4] The home video market also had strong results, with $68million in DVD sales. Inception has received wide critical acclaim and numerous critics have praised its originality, cast, score, and visual effects.[13] It won Academy Awards for Best Visual Effects, Best Sound Editing, Best Sound Mixing, and Best Cinematography, and was also nominated for four more: Best Picture, Best Original Screenplay, Best Original Score, and Best Art Direction.
Former dream architect Dominick "Dom" Cobb and business partner Arthur perform corporate espionage using an experimental military-developed machine to infiltrate the subconscious of their targets and extract information while dreaming, their latest target being Japanese businessman Saito. Tiered dream within a dream strategies are used and dreamers awaken by a "kick" or by dying in the dream. If the dreamer is the one who dies, the dream "collapses". Each extractor carries a totem, a small object the behavior of which is only predictable to its owner, used to determine whether a dreamer is in someone else's dream. Cobb's totem is a spinning top that perpetually spins in the dream state. The extraction fails due to Mallorie "Mal" Cobb, Cobb's deceased wife, whose memory projection sabotages the mission. Saito reveals, after Cobb's and Arthur's associate sells them out, that he was actually auditioning the team to perform the difficult act of inception: implanting an idea into a person's subconscious while they sleep.
Saito wishes to break up the energy conglomerate of his ailing competitor Maurice Fischer, by planting the idea in his son and heir Robert Fischer to disintegrate his father's company. Should Cobb succeed, Saito would use his influence to clear a murder charge against Cobb, so he can return to the United States and his children. Cobb accepts the offer and assembles his team: Eames, a conman and identity forger; Yusuf, a chemist who concocts the powerful sedative for a stable dream within a dream strategy; Ariadne, an architecture student tasked with designing the labyrinth of the dream landscapes; and Arthur. Saito accompanies so that he knows whether or not Cobb and his team succeeded.
When the elder Fischer dies in Sydney and his body is flown back to Los Angeles, the team share the flight with Robert Fischer and Cobb sedates him, bringing him into the shared dream. At each stage, the member of the team generating the dream stays behind to initiate the "kick", while the other members sleep within the dream to travel a level deeper. In the first level, Yusuf's rainy downtown dream, the team abducts Fischer. However Fischer's trained subconscious projections attack and severely wound Saito. Eames temporarily takes the appearance of Fischer's godfather, Peter Browning, to suggest Fischer reconsider his father's will. Yusuf drives the team in a van as they are sedated into Arthur's dream, a hotel, where the team recruit Fischer, convincing him his kidnapping was orchestrated by Browning. In the third dream level, a snowy mountain fortress dreamed by Eames, Fischer is told they are in Browning's subconscious, but they are really going deeper into Fischer's. Yusuf, under assault by trained projections, initiates his kick too soon by driving off a bridge, sending Arthur's dream world into zero-gravity and causing an avalanche in Eames' dream. Arthur improvises a new kick using an elevator that will be synchronized with the van hitting the water, while the team in Eames' dream races to finish the job before the new round of kicks.
Due to the effects of heavy sedation and multi-layered dreaming, death during this mission will result in entering Limbo, dream space of unknown content where the dreamer could be trapped. Elapsed time in each dream level is roughly twenty times greater than in the level above it; in Limbo, the deepest level of all, 24 hours of outer-world time would be experienced as about half a century. Cobb reveals to Ariadne that he spent "fifty years" with Mal in Limbo constructing a world from their shared memories whilst seemingly growing old together. After returning to the waking world, Mal remained convinced she was still dreaming and committed suicide, trying to persuade Cobb to do so by retroactively incriminating him in her death. He fled the U.S. and left his children behind, ostensibly in the care of his father-in-law.
Saito succumbs to his wounds, and Cobb's projection of Mal sabotages the plan by killing Fischer, sending them both into Limbo.[14] Cobb and Ariadne enter Limbo to find Fischer and Saito, while Eames remains on his dream level to set up a kick by rigging the fortress with explosives. Cobb confronts his projection of Mal, who tries convincing him to stay in Limbo. Cobb refuses and confesses that he was responsible for Mal's suicide: having convinced her to leave Limbo by using inception to plant the idea in her mind that the world they had been living in for fifty years was not real, and hence the need to kill themselves in order to return to the real world, once back in the real world she continued to believe dying would wake her. Mal attacks Cobb but Ariadne shoots her. Through his confession, Cobb attains catharsis and chooses to remain in Limbo to search for Saito. Ariadne pushes Fischer off a balcony, bringing him back up to the mountain fortress, where he enters a safe room to discover and accept the planted idea: that his father wishes him to be his "own man", and that splitting up the conglomerate might not be a radical notion.
All of the team members except Cobb and Saito ride the synchronized kicks back to reality: Ariadne jumps off a balcony in Limbo, Eames detonates the explosives in the fortress, Arthur blasts an elevator containing the team's sleeping bodies up an elevator shaft, and the van in Yusuf's dream hits the water. Cobb eventually finds an aged Saito and the two remember their arrangement, presumably killing themselves and awakening to outer-world reality on the airplane. Saito honors the arrangement and Cobb passes through U.S. customs once the plane lands in Los Angeles. Before reuniting with his children, Cobb tests reality with his spinning top, but he turns away to greet them before observing the results. As he plays with his children, the camera pans to the spinning top, at which point the film ends.
Initially, Nolan wrote an 80-page treatment about dream-stealers.[6] Originally, Nolan had envisioned Inception as a horror film,[6] but eventually wrote it as a heist film even though he found that "traditionally [they] are very deliberately superficial in emotional terms."[32] Upon revisiting his script, he decided that basing it in that genre did not work because the story "relies so heavily on the idea of the interior state, the idea of dream and memory. I realized I needed to raise the emotional stakes."[32] Nolan worked on the script for nine to ten years.[15] When he first started thinking about making the film, Nolan was influenced by "that era of movies where you had The Matrix (1999), you had Dark City (1998), you had The Thirteenth Floor (1999) and, to a certain extent, you had Memento (2000), too. They were based in the principles that the world around you might not be real."[32]
Nolan first pitched the film to Warner Bros. in 2001, but then felt that he needed more experience making large-scale films, and embarked on Batman Begins and The Dark Knight.[8] He soon realized that a film like Inception needed a large budget because "as soon as you're talking about dreams, the potential of the human mind is infinite. And so the scale of the film has to feel infinite. It has to feel like you could go anywhere by the end of the film. And it has to work on a massive scale."[8] After making The Dark Knight, Nolan decided to make Inception and spent six months completing the script.[8] Nolan states that the key to completing the script was wondering what would happen if several people shared the same dream. "Once you remove the privacy, you've created an infinite number of alternative universes in which people can meaningfully interact, with validity, with weight, with dramatic consequences."[33]
Leonardo DiCaprio was the first actor to be cast in the film.[15] Nolan had been trying to work with the actor for years and met him several times, but was unable to convince him to appear in any of his films until Inception. DiCaprio finally agreed because he was "intrigued by this concept  this dream-heist notion and how this character's going to unlock his dreamworld and ultimately affect his real life."[34] He read the script and found it to be "very well written, comprehensive but you really had to have Chris in person, to try to articulate some of the things that have been swirling around his head for the last eight years."[8] DiCaprio and Nolan spent months talking about the screenplay. Nolan took a long time re-writing the script in order "to make sure that the emotional journey of his character was the driving force of the movie."[15] On February 11, 2009, it was announced that Warner Bros. purchased Inception, a spec script written by Nolan.[9]
Principal photography began in Tokyo on June19,2009, with the scene where Saito first hires Cobb during a helicopter flight over the city.[6][35]
The production moved to the United Kingdom and shot in a converted airship hangar in Cardington, Bedfordshire, north of London.[36] There, the hotel bar set which tilted 30 degrees was built.[37] A hotel corridor was also constructed by Guy Hendrix Dyas, the production designer, Chris Corbould, the special effects supervisor, and Wally Pfister, the director of photography, it rotated a full 360degrees to create the effect of alternate directions of gravity for scenes set during the second level of dreaming; where dream-sector physics become chaotic. The idea was inspired by a technique used in Stanley Kubrick's 2001: A Space Odyssey (1968). Nolan said, "I was interested in taking those ideas, techniques, and philosophies and applying them to an action scenario".[38] The filmmakers originally planned to make the hallway 40ft (12m) long but as the action sequence became more elaborate, the hallway's length grew to 100ft (30m). The corridor was suspended along eight large concentric rings that were spaced equidistantly outside its walls and powered by two massive electric motors.[36] Joseph Gordon-Levitt, who plays Arthur, spent several weeks learning to fight in a corridor that spun like "a giant hamster wheel".[32] Nolan said of the device, "It was like some incredible torture device; we thrashed Joseph for weeks, but in the end we looked at the footage, and it looks unlike anything any of us has seen before. The rhythm of it is unique, and when you watch it, even if you know how it was done, it confuses your perceptions. It's unsettling in a wonderful way".[32] Gordon-Levitt remembered, "it was six-day weeks of just, like, coming home at night battered ... The light fixtures on the ceiling are coming around on the floor, and you have to choose the right time to cross through them, and if you don't, you're going to fall."[39] On July15,2009, filming took place at University College London library, for the sequences occurring inside a Paris college of architecture in the story.[6]
Filming moved to France where they shot Cobb entering the college of architecture (the place used for the entrance was the Muse Galliera) and the pivotal scenes between Ariadne and Cobb, in a bistro (a fictional one set up at the corner of Rue Csar Franck and Rue Bouchut) and then on the Bir-Hakeim bridge.[40] For the explosion that takes place during the bistro scene, the local authorities would not allow the actual use of explosives. High-pressure nitrogen was used to create the effect of a series of explosions. Pfister used six high-speed cameras to capture the sequence from different angles and make sure that they got the shot. The visual effects department then enhanced the sequence, adding more destruction and flying debris. For the "Paris folding" sequence and when Ariadne "creates" the bridges, green screen and CGI were used on location.[40]
Tangiers, Morocco, doubled as Mombasa, where Cobb hires Eames and Yusuf. A foot chase was shot in the streets and alleyways of the historic medina quarter.[41] To capture this sequence, Pfister employed a mix of hand-held camera and steadicam work.[42] Tangiers was also used to film an important riot scene during the initial foray into Saito's mind.
Filming moved to the Los Angeles area, where some sets were built on a Warner Brothers sound stage, including the interior rooms of Saito's Japanese castle (the exterior was done on a small set built in Malibu beach). The dining room was inspired by the Nijo Castle built around 1603. These sets were inspired by a mix of Japanese architecture and Western influences.[42] The production also staged a multi-vehicle car chase on the streets of downtown Los Angeles, which involved a freight train crashing down the middle of a street.[43] To do this, the filmmakers configured a train engine on the chassis of a tractor trailer. The replica was made from fiberglass molds taken from authentic train parts and then matched in terms of color and design.[44] Also, the car chase was supposed to be set in the midst of a downpour but the L.A. weather stayed typically sunny. The filmmakers were forced to set up elaborate effects (e.g., rooftop water cannons) to give the audience the impression that the weather was overcast and soggy. L.A. was also the site of the climactic scene where a Ford Econoline van flies off the Schuyler Heim Bridge in slow motion.[45] This sequence was filmed on and off for months with the van being shot out of a cannon, according to actor Dileep Rao. Capturing the actors suspended within the van in slow motion took a whole day to film. Once the van landed in the water, the challenge for the actors was not to panic. "And when they ask you to act, it's a bit of an ask," explained Cillian Murphy.[45] The actors had to be underwater for four to five minutes while drawing air from scuba tanks; underwater buddy breathing is shown in this sequence.[45] Cobb's house was in Pasadena. The hotel lobby was filmed at the CAA building in Century City. Limbo was made on location in Los Angeles and Morocco with the beach scene filmed at Palos Verdes beach with CGI buildings. N Hope St. in Los Angeles was the primary filming location for Limbo, with green screen and CGI being used to create the dream landscape.
The final phase of principal photography took place in Alberta in late November2009. The location manager discovered a temporarily closed ski resort, Fortress Mountain.[46] An elaborate set was assembled near the top station of the Canadian chairlift, taking three months to build.[47] The production had to wait for a huge snowstorm, which eventually arrived.[6] The ski-chase sequence was inspired by Nolan's favorite James Bond film, On Her Majesty's Secret Service (1969): "What I liked about it that we've tried to emulate in this film is there's a tremendous balance in that movie of action and scale and romanticism and tragedy and emotion."[48]
The film was shot primarily in the anamorphic format on 35 mm film, with key sequences filmed on 65 mm, and aerial sequences in VistaVision. Nolan did not shoot any footage with IMAX cameras as he had with The Dark Knight. "We didn't feel that we were going to be able to shoot in IMAX because of the size of the cameras because this film given that it deals with a potentially surreal area, the nature of dreams and so forth, I wanted it to be as realistic as possible. Not be bound by the scale of those IMAX cameras, even though I love the format dearly".[15] Nolan also chose not to shoot any of the film in 3D as he prefers shooting on film[15] using prime lenses, which is not possible with 3D cameras.[49] Nolan has also criticised the dim image that 3D projection produces, and disputes that traditional film does not allow realistic depth perception, saying "I think it's a misnomer to call it 3D versus 2D. The whole point of cinematic imagery is it's three dimensional... You know 95% of our depth cues come from occlusion, resolution, color and so forth, so the idea of calling a 2D movie a '2D movie' is a little misleading."[50] Nolan did test converting Inception into 3D in post-production but decided that, while it was possible, he lacked the time to complete the conversion to a standard he was happy with.[6][50] In February 2011 Jonathan Liebesman suggested that Warner Bros were attempting a 3D conversion for Blu-ray release.[51]
Wally Pfister gave each location and dream level a distinctive look: the mountain fortress appears sterile and cool, the hotel hallways have warm hues, and the scenes in the van are more neutral.[52] This was done to aid the audience's recognition of the narrative's location during the heavily crosscut portion of the film.[52]
Nolan has said that the film "deals with levels of reality, and perceptions of reality which is something I'm very interested in. It's an action film set in a contemporary world, but with a slight science-fiction bent to it," while also describing it as "very much an ensemble film structured somewhat as a heist movie. It's an action adventure that spans the globe".[53]
For dream sequences in Inception, Nolan used little computer-generated imagery, preferring practical effects whenever possible. Nolan said, "It's always very important to me to do as much as possible in-camera, and then, if necessary, computer graphics are very useful to build on or enhance what you have achieved physically."[54] To this end, visual effects supervisor Paul Franklin built a miniature of the mountain fortress set and then blew it up for the film. For the fight scene that takes place in zero gravity, he used CG-based effects to "subtly bend elements like physics, space and time."[55]
The most challenging effect was the "limbo" city level at the end of the film because it continually developed during production. Franklin had artists build concepts while Nolan gave his ideal vision: "Something glacial, with clear modernist architecture, but with chunks of it breaking off into the sea like icebergs".[55] Franklin and his team ended up with "something that looked like an iceberg version of Gotham City with water running through it."[55] They created a basic model of a glacier and then designers created a program that added elements like roads, intersections and ravines until they had a complex, yet organic-looking, cityscape. For the Paris-folding sequence, Franklin had artists producing concept sketches and then they created rough computer animations to give them an idea of what the sequence looked like while in motion. Later during principal photography, Nolan was able to direct Leonardo DiCaprio and Ellen Page based on this rough computer animation Franklin had created. Inception had close to 500 visual effects shots (in comparison, Batman Begins had approximately 620) which is considered minor in comparison to contemporary visual effects epics that can have around 1,500 or 2,000 special effects images.[55]
The score for Inception was written by Hans Zimmer,[1] who described his work as "a very electronic,[56] dense score",[57] filled with "nostalgia and sadness" to match Cobb's feelings throughout the film.[58] The music was written simultaneously to filming,[57] and features a guitar sound reminiscent of Ennio Morricone, played by Johnny Marr, former guitarist of The Smiths. dith Piaf's "Non, je ne regrette rien" appears recurringly throughout the film, and Zimmer reworked pieces of the song into cues of the score.[58] A soundtrack album was released on July11,2010 by Reprise Records.[59] The majority of the score was also included in high resolution 5.1 surround sound on the 2nd disc of the 2 disc Blu-ray release [60] Hans Zimmer's music was nominated for an Academy Award in the Best Original Score category in 2011, losing to Trent Reznor and Atticus Ross of The Social Network.[61]
In Inception, Nolan wanted to explore "the idea of people sharing a dream space...That gives you the ability to access somebody's unconscious mind. What would that be used and abused for?"[15] The majority of the film's plot takes place in these interconnected dream worlds. This structure creates a framework where actions in the real or dream worlds ripple across others. The dream is always in a state of production, and shifts across the levels as the characters navigate it.[62] By contrast, the world of The Matrix (1999) is an authoritarian, computer-controlled one, alluding to theories of social control developed by Michel Foucault and Jean Baudrillard. Nolan's world has more in common with the works of Gilles Deleuze and Flix Guattari.[62]
David Denby in The New Yorker compared Nolan's cinematic treatment of dreams to Luis Buuel's in Belle de Jour (1967) and The Discreet Charm of the Bourgeoisie (1972).[63] He criticised Nolan's "literal-minded" action level sequencing compared to Buuel, who "silently pushed us into reveries and left us alone to enjoy our wonderment, but Nolan is working on so many levels of representation at once that he has to lay in pages of dialogue just to explain what's going on." The latter captures "the peculiar malign intensity of actual dreams."[63]
Deirdre Barrett, a dream researcher at Harvard University, said that Nolan did not get every detail accurate regarding dreams, but their illogical, rambling, disjointed plots would not make for a great thriller anyway. However, "he did get many aspects right," she said, citing the scene in which a sleeping Cobb is shoved into a full bath, and in the dream world water gushes into the windows of the building, waking him up. "That's very much how real stimuli get incorporated, and you very often wake up right after that intrusion".[64]
Nolan himself said, "I tried to work that idea of manipulation and management of a conscious dream being a skill that these people have. Really the script is based on those common, very basic experiences and concepts, and where can those take you? And the only outlandish idea that the film presents, really, is the existence of a technology that allows you to enter and share the same dream as someone else."[32]
Others have argued that the film is itself a metaphor for film-making, and that the filmgoing experience itself, images flashing before one's eyes in a darkened room, is akin to a dream. Writing in Wired, Jonah Lehrer supported this interpretation and presented neurological evidence that brain activity is strikingly similar during film-watching and sleeping. In both, the visual cortex is highly active and the prefrontal cortex, which deals with logic, deliberate analysis, and self-awareness, is quiet.[65] Paul argued that the experience of going to a picturehouse is itself an exercise in shared dreaming, particularly when viewing Inception: the film's sharp cutting between scenes forces the viewer to create larger narrative arcs to stitch the pieces together. This demand of production parallel to consumption of the images, on the part of the audience is analogous to dreaming itself. As in the film's story, in a cinema one enters into the space of another's dream, in this case Nolan's, as with any work of art, one's reading of it is ultimately influenced by one's own subjective desires and subconscious.[62] At Bir-Hakeim bridge in Paris, Ariadne creates an illusion of infinity by adding facing mirrors underneath its struts, Stephanie Dreyfus in la Croix asked "Is this not a strong, beautiful metaphor for the cinema and its power of illusion?"[66]
Nolan combined elements from several different film genres into the film, notably science fiction, heist film, and film noir. Marion Cotillard plays "Mal" Cobb, Dom Cobb's projection of his guilt over his deceased wife's suicide. As the film's main antagonist, she is a frequent, malevolent presence in his dreams. Dom is unable to control these projections of her, challenging his abilities as an extractor.[16] Nolan described Mal as "the essence of the femme fatale",[67] the key noir reference in the film. As a "classic femme fatale" her relationship with Cobb is in his mind, a manifestation of Cobb's own neurosis and fear of how little he knows about the woman he loves.[68] DiCaprio praised Cotillard's performance saying that "she can be strong and vulnerable and hopeful and heartbreaking all in the same moment, which was perfect for all the contradictions of her character".[69]
Nolan began with the structure of a heist movie, since exposition is an essential element of that genre, though adapted it to have a greater emotional narrative suited to the world of dreams and subconscious.[68] Or, as Denby surmised, "the outer shell of the story is an elaborate caper".[63] Kirstin Thompson argued that exposition was a major formal device in the film. While a traditional heist movie has a heavy dose of exposition at the beginning as the team assembles and the leader explains the plan, in Inception this becomes nearly continuous as the group progresses through the various levels of dreaming.[70] Three-quarters of the film, until the van begins to fall from the bridge, are devoted to explaining its plot. In this way, exposition takes precedence over characterisation. Their relationships are created by their respective skills and roles. Ariadne, like her ancient namesake, creates the maze and guides the others through it, but also helps Cobb navigate his own subconscious, and as the sole student of dream sharing, helps the audience understand the concept of the plot.[71]
Nolan drew inspiration from the works of Jorge Luis Borges,[6][72] the anime film Paprika (2006) by the late Satoshi Kon as an influence on the character "Ariadne", and Blade Runner (1982) by Ridley Scott.[73]
The film cuts to the closing credits from a shot of the top beginning to wobble (but not falling), inviting speculation about whether the final sequence was reality or another dream. Nolan confirmed that the ambiguity was deliberate, saying "I've been asked the question more times than I've ever been asked any other question about any other film I've made... What's funny to me is that people really do expect me to answer it."[74] The film's script concludes with "Behind him, on the table, the spinning top is STILL SPINNING. And we  FADE OUT"[75] However, Christopher Nolan also said, "I put that cut there at the end, imposing an ambiguity from outside the film. That always felt the right ending to me  it always felt like the appropriate 'kick' to me The real point of the scene  and this is what I tell people  is that Cobb isn't looking at the top. He's looking at his kids. He's left it behind. That's the emotional significance of the thing."[76]
In September 2010, Michael Caine, explained his interpretation of the ending, "If I'm there it's real, because I'm never in the dream. I'm the guy who invented the dream."[77] Nolan himself noted that "I choose to believe that Cobb gets back to his kids, because I have young kids. People who have kids definitely read it differently than those who don't".[68] He indicated that the top was not the most crucial element of the ending, saying "I've read plenty of very off-the-wall interpretations... The most important emotional thing about the top spinning at the end is that Cobb is not looking at it. He doesn't care."[68]
Warner Bros. spent $100million marketing the film. Unlike most tent-pole films, which are adaptations or sequels, Inception was a totally original property, but Sue Kroll, president of Warner's worldwide marketing, said the company believed it could gain awareness due to the strength of "Christopher Nolan as a brand". Kroll declared that "We don't have the brand equity that usually drives a big summer opening, but we have a great cast and a fresh idea from a filmmaker with a track record of making incredible movies. If you can't make those elements work, it's a sad day."[78] The studio also tried to maintain a campaign of secrecy  as reported by the Senior VP of Interactive Marketing, Michael Tritter, "You have this movie which is going to have a pretty big built in fanbase... but you also have a movie that you are trying to keep very secret. Chris [Nolan] really likes people to see his movies in a theater and not see it all beforehand so everything that you do to market that  at least early on  is with an eye to feeding the interest to fans."[79]
A viral marketing campaign was employed for the film. After the revelation of the first teaser trailer, in August2009, the film's official website featured only an animation of Cobb's spinning top. In December, the top toppled over and the website opened the online game Mind Crime, which upon completion revealed Inception's poster.[80] The rest of the campaign unrolled after WonderCon in April2010, where Warner gave away promotional T-shirts featuring the PASIV briefcase used to create the dream space, and had a QR code linking to an online manual of the device.[81] Mind Crime also received a stage2 with more resources, including a hidden trailer for the movie.[82] More pieces of viral marketing began to surface before Inception's release, such as a manual filled with bizarre images and text sent to Wired magazine,[83] and the online publication of posters, ads, phone applications, and strange websites all related to the film.[84][85] Warner also released an online prequel comic, Inception: The Cobol Job.[86]
The official trailer released on May10,2010 through Mind Game was extremely well received.[82] It featured an original piece of music, "Mind Heist", by recording artist Zack Hemsey,[87] rather than music from the score.[88] The trailer quickly went viral with numerous mashups copying its style, both by amateurs on sites like YouTube[89] and by professionals on sites such as CollegeHumor.[90][91] On June7,2010, a behind-the-scenes featurette on the film was released in HD on Yahoo! Movies.[92]
Inception was released on DVD and Blu-ray on December3,2010, in France,[93] and the week after in the UK and USA (December7,2010).[94][95] Warner Bros. also made available in the United States a limited Blu-ray edition packaged in a metal replica of the PASIV briefcase, which included extras such as a metal replica of the spinning top totem. With a production run of less than 2000, it sold out in one weekend.[96]
In a November2010 interview, Nolan expressed his intention to develop a video game set in the Inception world, working with a team of collaborators. He described it as "a longer-term proposition", referring to the medium of video games as "something I've wanted to explore".[97]
Inception was released in both conventional and IMAX theaters on July16,2010.[11][99] The film had its world premiere at Leicester Square in London, United Kingdom on July8,2010.[100] In the United States and Canada, Inception was released theatrically in 3,792 conventional theaters and 195 IMAX theaters.[11] The film grossed $21.8million during its opening day on July16,2010, with midnight screenings in 1,500 locations.[101] Overall the film made $62.7million and debuted at No.1 on its opening weekend.[102] Inception's opening weekend gross made it the second-highest-grossing debut for a science-fiction film that was not a sequel, remake or adaptation, behind Avatar's $77million opening weekend gross in 2009.[102] The film held the top spot of the box office rankings in its second and third weekends, with drops of just 32% ($42.7million) and 36% ($27.5million) respectively,[103][104] before dropping to second place in its fourth week, behind The Other Guys.[105]
Inception grossed US$292million in the United States and Canada, US$56 in the United Kingdom, Ireland and Malta and US$475million in other countries for a total of $823million.[4] Its five highest-grossing markets after the USA and Canada (US$292) were China (US$68million), the United Kingdom, Ireland and Malta (US$56million), France and the Maghreb region (US$43million), Japan (US$40million) and South Korea (US$38million).[106] It was the sixth-highest grossing film of 2010 in North America,[107] and the fourth-highest internationally, behind Toy Story 3, Alice in Wonderland and Harry Potter and the Deathly Hallows - Part 1.[108] The film currently stands as the 32nd highest-grossing of all time.[109] Inception is the third most lucrative production in Christopher Nolan's career  behind The Dark Knight and The Dark Knight Rises -[110] and the second most for Leonardo DiCaprio  behind Titanic.[111]
 The film received very positive reviews. Rotten Tomatoes gives the film a score of 86% based on reviews from 284 critics, with an average score of 8/10. The website reported the critical consensus as "smart, innovative, and thrilling," and lauded the film as "that rare summer blockbuster that succeeds viscerally as well as intellectually."[112] Metacritic, another review aggregator, assigned the film a weighted average score of 74 (out of 100) based on 42 reviews from mainstream critics, considered to be "generally favorable reviews."[113] In polls conducted by CinemaScore during the opening weekend cinemagoers gave Inception an average score of "B+".[114]
While some critics have tended to view the film as perfectly straightforward, and even criticize its overarching themes as "the stuff of torpid platitudes," online discussion has been much more positive.[115] Heated debate has centered on the ambiguity of the ending, with many critics like Devin Faraci making the case that the film is self-referential and tongue-in-cheek, both a film about film-making and a dream about dreams.[116] Other critics read Inception as Christian allegory and focus on the film's use of religious and water symbolism.[117] Yet other critics, such as Kirsten Thompson, see less value in the ambiguous ending of the film and more in its structure and novel method of storytelling, highlighting Inception as a new form of narrative that revels in "continuous exposition".[118]
Whatever its meaning, the film has had excellent reviews in general. Perhaps playing off the film's game imagery, Rolling Stone magazine's Peter Travers called Inception a "wildly ingenious chess game," and concluded "the result is a knockout."[119] In Variety, Justin Chang praised the film as "a conceptual tour de force" and wrote, "applying a vivid sense of procedural detail to a fiendishly intricate yarn set in the labyrinth of the unconscious mind, the writer-director has devised a heist thriller for surrealists, a Jungian's Rififi, that challenges viewers to sift through multiple layers of (un)reality."[120] Jim Vejvoda of IGN rated the film as perfect, deeming it "a singular accomplishment from a filmmaker who has only gotten better with each film."[121] Relevant Magazine's David Roark called it Nolan's greatest accomplishment, saying, "Visually, intellectually and emotionally, Inception is a masterpiece."[122]
Empire magazine rated it five stars in the August2010 issue and wrote, "it feels like Stanley Kubrick adapting the work of the great sci-fi author William Gibson ... Nolan delivers another true original: welcome to an undiscovered country."[123] Entertainment Weekly gave the film a B+ rating and Lisa Schwarzbaum wrote, "It's a rolling explosion of images as hypnotizing and sharply angled as any in a drawing by M.C. Escher or a state-of-the-biz videogame; the backwards splicing of Nolan's own Memento looks rudimentary by comparison."[124] The New York Post gave the film a four star rating and Lou Lumenick wrote, "DiCaprio, who has never been better as the tortured hero, draws you in with a love story that will appeal even to non-sci-fi fans."[125] Roger Ebert of the Chicago Sun-Times awarded the film a full four stars and said that Inception "is all about process, about fighting our way through enveloping sheets of reality and dream, reality within dreams, dreams without reality. It's a breathtaking juggling act."[126] Richard Roeper, also of the Sun-Times, gave Inception a perfect score of "A+" and called it "one of the best movies of the [21st] century."[127]
BBC Radio 5 Live's Mark Kermode named Inception as the best film of 2010, stating "Inception is proof that people are not stupid, that cinema is not trash, and that it is possible for blockbusters and art to be the same thing."[128]
In his review for the Chicago Tribune, Michael Phillips gave the film 3 stars out of 4 and wrote, "I found myself wishing Inception were weirder, further out ... the film is Nolan's labyrinth all the way, and it's gratifying to experience a summer movie with large visual ambitions and with nothing more or less on its mind than (as Shakespeare said) a dream that hath no bottom."[129] Time magazine's Richard Corliss wrote the film's "noble intent is to implant one man's vision in the mind of a vast audience ... The idea of moviegoing as communal dreaming is a century old. With Inception, viewers have a chance to see that notion get a state-of-the-art update."[130] Los Angeles Times' Kenneth Turan felt that Nolan was able to blend "the best of traditional and modern filmmaking. If you're searching for smart and nervy popular entertainment, this is what it looks like."[131] USA Today rated the film three-and-a-half stars out of four and Claudia Puig felt that Nolan "regards his viewers as possibly smarter than they areor at least as capable of rising to his inventive level. That's a tall order. But it's refreshing to find a director who makes us stretch, even occasionally struggle, to keep up."[132]
Not all reviewers gave the film positive reviews. New York magazine's David Edelstein claimed in his review to "have no idea what so many people are raving about. It's as if someone went into their heads while they were sleeping and planted the idea that Inception is a visionary masterpiece andhold on ... Whoa! I think I get it. The movie is a metaphor for the power of delusional hypea metaphor for itself."[133] Rex Reed of The New York Observer explained the film's development as "pretty much what we've come to expect from summer movies in general and Christopher Nolan movies in particular ... [it] doesn't seem like much of an accomplishment to me."[134] A. O. Scott of The New York Times commented "there is a lot to see in Inception, there is nothing that counts as genuine vision. Mr. Nolan's idea of the mind is too literal, too logical, and too rule-bound to allow the full measure of madness."[135] David Denby, writing in The New Yorker, considered the film not nearly as much fun as Nolan imagined it to be, concluding "Inception is a stunning-looking film that gets lost in fabulous intricacies, a movie devoted to its own workings and to little else."[63]
Several sources have noted many plot similarities between the film and the 2004 Uncle Scrooge comic, The Dream of a Lifetime.[136][137][138]
Inception appeared on over 273 critics' lists of the top ten films of 2010, being picked as No.1 on 55 of those lists. It was the second most mentioned film behind The Social Network and one of the most critically acclaimed films of 2010, alongside the former, The King's Speech and Black Swan.[139]
The film won many awards in technical categories, such as Academy Awards for Best Cinematography, Best Sound Editing, Best Sound Mixing, and Best Visual Effects,[61] and the British Academy Film Awards for Best Production Design, Best Special Visual Effects and Best Sound.[140] In most of its artistic nominations, such as Film, Director and Screenplay at the Oscars, BAFTAs and Golden Globes, the film was defeated by The Social Network and The King's Speech.[61][140][141] However, the film did win the two highest honors for a science fiction or fantasy film: the 2011 Bradbury Award for best dramatic production[142] and the 2011 Hugo Award for best dramatic presentation, long form.[143]

July 8, 2010(2010-07-08) (London premiere)
July 16, 2010(2010-07-16) (United States)
1 Plot
2 Cast
3 Production

3.1 Development
3.2 Locations and sets 
3.3 Cinematography
3.4 Visual effects
3.5 Music


3.1 Development
3.2 Locations and sets 
3.3 Cinematography
3.4 Visual effects
3.5 Music
4 Themes

4.1 Reality and dreams
4.2 Dreams and cinema


4.1 Reality and dreams
4.2 Dreams and cinema
5 Cinematic technique

5.1 Genre
5.2 Ending


5.1 Genre
5.2 Ending
6 Release

6.1 Marketing
6.2 Home media
6.3 Putative video game


6.1 Marketing
6.2 Home media
6.3 Putative video game
7 Reception

7.1 Box office earnings
7.2 Critical reception
7.3 Accolades


7.1 Box office earnings
7.2 Critical reception
7.3 Accolades
8 See also
9 References
10 Further reading
11 External links
3.1 Development
3.2 Locations and sets 
3.3 Cinematography
3.4 Visual effects
3.5 Music
4.1 Reality and dreams
4.2 Dreams and cinema
5.1 Genre
5.2 Ending
6.1 Marketing
6.2 Home media
6.3 Putative video game
7.1 Box office earnings
7.2 Critical reception
7.3 Accolades
Leonardo DiCaprio as Dominic "Dom" Cobb, a professional thief who specializes in conning secrets from his victims by infiltrating their dreams. DiCaprio was the first actor to be cast in the film.[15] Nolan had been trying to work with the actor for years and met him several times, but was unable to convince him to appear in any of his films until Inception.[16] According to Hollywood Reporter, both Brad Pitt and Will Smith were offered the role.[17]
Joseph Gordon-Levitt as Arthur, Cobb's partner who manages and researches the missions. Gordon-Levitt compared Arthur to the producer of Cobb's art, "the one saying, 'Okay, you have your vision; now I'm going to figure out how to make all the nuts and bolts work so you can do your thing'".[18] The actor did all of his stunts but one scene and said the preparation "was a challenge and it would have to be for it to look real".[19] James Franco was in talks with Christopher Nolan to play Arthur, but was ultimately unavailable due to scheduling conflicts.
Ellen Page as Ariadne, a graduate student of architecture who is recruited to construct the various dreamscapes, which are described as mazes. The name Ariadne alludes to a princess of Greek myth, daughter of King Minos, who aided the hero Theseus by giving him a sword and a ball of string to help him navigate the labyrinth which was the prison of the Minotaur. Nolan said that Page was chosen for being a "perfect combination of freshness and savvy and maturity beyond her years".[20] Page said her character acts as a proxy to the audience, as "she's just learning about these ideas and, in essence, assists the audience in learning about dream sharing".[21] Evan Rachel Wood was Christopher Nolan's first choice to play Ariadne, but she turned it down. Before Ellen Page was offered and accepted the role, Nolan considered casting Emily Blunt, Rachel McAdams, Emma Roberts, Jessy Schram, and Carey Mulligan.
Tom Hardy as Eames, a sharp-tongued associate of Cobb's. He is referred to as a fence but his speciality is forgery, more accurately identity theft. Eames uses his ability to impersonate others inside the dream world in order to manipulate Fischer. Hardy described his character as "an old, Graham Greene-type diplomat; sort of faded, shabby, grandeur  the old Shakespeare lovey mixed with somebody from Her Majesty's Special Forces", who wears "campy, old money" costumes.[22]
Ken Watanabe as Mr. Saito, a Japanese businessman who employs Cobb for the team's mission. Nolan wrote the role with Watanabe in mind, as he wanted to work with him again after Batman Begins.[23] Inception is Watanabe's first work in a contemporary setting where his primary language is English. Watanabe tried to emphasize a different characteristic of Saito in every dream level  "First chapter in my castle, I pick up some hidden feelings of the cycle. It's magical, powerful and then the first dream. And back to the second chapter, in the old hotel, I pick up [being] sharp and more calm and smart and it's a little bit [of a] different process to make up the character of any movie".[24]
Dileep Rao as Yusuf, the team's chemist. Rao describes Yusuf as "an avant-garde pharmacologist, who is a resource for people, like Cobb, who want to do this work unsupervised, unregistered and unapproved of by anyone". Co-producer Jordan Goldberg said the role of the chemist was "particularly tough because you don't want him to seem like some kind of drug dealer", and that Rao was cast for being "funny, interesting and obviously smart".[25]
Cillian Murphy as Robert Michael Fischer, the heir to a business empire and the team's target.[23] Murphy said Fischer was portrayed as "a petulant child who's in need of a lot of attention from his father, he has everything he could ever want materially, but he's deeply lacking emotionally". The actor also researched the sons of Rupert Murdoch, "to add to that the idea of living in the shadow of someone so immensely powerful".[26]
Tom Berenger as Peter Browning, Robert Fischer's godfather and fellow executive at the Fischers' company.[1] Berenger said Browning acts as a "surrogate father" to Robert, who calls the character "Uncle Peter", and emphasized that "Browning has been with [Robert] his whole life and has probably spent more quality time with him than his own father".[25]
Marion Cotillard as Mallorie "Mal" Cobb, Dom's deceased wife. She is a manifestation of Dom's guilt about the real Mal's suicide. He is unable to control these projections of her, challenging his abilities as an extractor.[16] Nolan described Mal as "the essence of the femme fatale," and DiCaprio praised Cotillard's performance saying that "she can be strong and vulnerable and hopeful and heartbreaking all in the same moment, which was perfect for all the contradictions of her character".[27]
Michael Caine as Prof. Stephen Miles, Cobb's mentor and father-in-law,[25] and Ariadne's college professor who recommends her to the team.[28]
Pete Postlethwaite as Maurice, the dying founder of a business empire. The film became one of Postlethwaite's final film roles before his death in early 2011.
Lukas Haas as Nash, an architect in Cobb's employment who betrays the team and is later replaced by Ariadne.[29]
Talulah Riley as a woman whom Eames disguises himself as in a dream. Riley liked the role, despite it being minimal  "I get to wear a nice dress, pick up men in bars, and shove them in elevators. It was good to do something adultish. Usually I play 15-year-old English schoolgirls."[30]
Miranda Nolan plays a minor role as an air hostess. Miranda is a first cousin to the film's director Christopher Nolan.[31]
Lucid Dreaming
Suggestion
Johnson, David Kyle (Editor); Irwin, William (Series Editor) (2011), Inception and Philosophy: Because It's Never Just a Dream, John Wiley & Sons, ISBN1-118-07263-4
Nolan, Christopher (Author); Nolan, Jonathan (Preface) (2010), Inception: The Shooting Script, Insight Editions, ISBN1-60887-015-4
Crawford, Kevin Ray (Author) (2012), The Rhetorics of the Time-Image: Deleuzian Metadiscourse on the Role of Nooshock Temporality (viz. "Inception") in Christopher Nolan's Cinema of the Brain, ProQuest LLC
Official website
Inception at the Internet Movie Database
Inception at AllRovi
.
#(`*Modern Family*`)#.
Modern Family is an American comedy which debuted on ABC on September 23, 2009. Presented in mockumentary style, the fictional characters frequently talk directly into the camera. It tells of Jay Pritchett, his second wife and step son, and his two children and their families. Christopher Lloyd and Steven Levitan conceived the series while sharing stories of their own "modern families."
The series premiered to critical acclaim and was watched by 12.6 million viewers.[1][2] Early on, it was named as a key contender for the 62nd Primetime Emmy Awards.[3] Soon after, on October 8, 2009, the series was picked up for a full season.[4][5] The series has received positive reviews from critics and received several award nominations. The series has won the Emmy Award for Outstanding Comedy Series in each of the past three years and the Emmy Award for Outstanding Supporting Actor in a Comedy Series three times so far as well, twice for Eric Stonestreet and once for Ty Burrell, as well as the Outstanding Supporting Actress in a Comedy Series twice for Julie Bowen.[6][7] It also won the Golden Globe Award for Best Television Series Musical or Comedy.[8]
The syndication rights to the show have also been sold to USA Network and 10 Fox affiliates for a fall 2013 premiere.[9][10][11] The success of the show has also led it to being the sixteenth-highest revenue-generating show for 2010, earning $1.6 million an episode.[12] Brian Lowry, of Variety, sums up the show in regards to the airing of the pilot episode: "Flitting among three storylines, it's smart, nimble and best of all, funny, while actually making a point about the evolving nature of what constitutes 'family'".[13]
As Lloyd and Levitan retold stories about their families, it occurred to them that that could be the basis for a show. They started working on the idea of a family being observed in a mockumentary style show. They later decided it could be a show about three families and their experiences,[14] and originally called My American Family.[15] The creators pitched it to three of the four major networks. (They did not pitch it to Fox because of issues Lloyd had with the network over Back to You.)[16] CBS, not ready to use the single-camera style of filming nor ready to make another large commitment, rejected the series (Welcome to The Captain and Worst Week were single-camera sitcoms that recently aired on CBS but both lasted one season). NBC, already having two shows with a mockumentary format  The Office and Parks and Recreation  decided against accepting the show before the success of the other two series decreased. ABC accepted the series and picked it up for a full season.[16]
Modern Family has some similarities to the French TV show Fais pas ci, fais pas a, which ABC bought adaptation rights to in 2008.[17] However, the proposed adaptation was a separate project, titled Don't Do This, Don't Do That, which does not appear to have gone past the pilot script stage.[18]
The series quickly became a priority for the American Broadcasting Company (ABC) after the pilot episode tested high with focus groups, resulting in the network ordering 16 episodes and adding it to the 20092010 fall lineup days ahead of ABC's official schedule announcement.[19][20][21] The series was given a full season pickup on October 8, 2009.[4][5] On January 12, 2010, ABC Entertainment President Stephen McPherson announced that Modern Family had been renewed for a second season.[22] A third season was ordered by ABC on January 10, 2011.[23] The series was also picked up for syndication by USA during the first season for 1.5 million dollars and to 10 Fox affiliates during the second season.[9][10][11][24] The series airs in the United Kingdom and Republic of Ireland on Sky1.[25]
Principal photography takes place in Los Angeles.[13] Creators Christopher Lloyd and Steven Levitan, whose credits both include Frasier, Wings, and Just Shoot Me,[15] are executive producers of the series, serving as showrunner and head writer under their Lloyd-Levitan Productions label in affiliation with Twentieth Century Fox Television.[13] The other producers on the writing team are Paul Corrigan, Sameer Gardezi, Joe Lawson, Dan O'Shannon, Brad Walsh, Caroline Williams, Bill Wrubel, Danny Zuker, and Jeff Morton.[13][26] The first team of directors included Jason Winer, Michael Spiller, Randall Einhorn, and Chris Koch. Winer has directed nineteen episodes of the series, making him the most prolific director of the series.[13][27]
As a result of the show's success, the cast attempted in the summer of 2012 to renegotiate their existing contracts to obtain higher per-episode fees. However, talks broke down to the point where the first table read for the fourth season had to be postponed. Five of the cast members (Burrell, Bowen, Ferguson, Stonestreet and Vergara) retained the Quinn Emanuel law firm and sued 20th Century Fox Television in Los Angeles County Superior Court on July 24, 2012. While not part of the lawsuit, Ed O'Neill (who had been earning more per episode than the other five) joined his fellow castmates in seeking raises for each to about $200,000 per episode. Their complaint invoked the "seven-year rule" in California Labor Code Section 2855 (the De Havilland Law), and requested a declaration that their contracts were void because as drafted, the contracts were in violation of that rule.[28] Although the lawsuit is still pending, the performers returned for their first table read on July 26, 2012, with the understanding that the producers would resume contract negotiations in good faith.[29]
Modern Family employs an ensemble cast. The show revolves around three families that are interrelated through Jay Prichett and his children, Claire and Mitchell. Jay Prichett (Ed O'Neill), the patriarch, is married to a much younger woman, Gloria (Sofa Vergara), a passionate[30] mother, who raises her son, Manny (Rico Rodriguez). Claire (Julie Bowen) is a homemaker mom married to Phil (Ty Burrell), a real estate agent and self-professed "cool dad". They have three children: Haley (Sarah Hyland) the stereotypical teenager,[31] Alex (Ariel Winter), the smart middle child[32] and Luke (Nolan Gould), the offbeat only son.[33] Mitchell (Jesse Tyler Ferguson), a lawyer, and his partner Cameron (Eric Stonestreet) have adopted a Vietnamese baby, Lily (Aubrey Anderson-Emmons). In the first season, the adult cast was paid from a range of $30,000 per episode to about $90,000.[34]
The series has also had several recurring characters. Reid Ewing appeared in several episodes as Haley's boyfriend, Dylan.[35] Fred Willard has guest starred as Phil's father, Frank Dunphy, and later went on to be nominated at the 62nd Primetime Emmy Awards for Outstanding Guest Actor in a Comedy Series, but lost to Neil Patrick Harris's performance on Glee.[36] Shelley Long has appeared in both seasons as Claire and Mitchell's biological mother and Jay's ex-wife, DeDe Pritchett.[37][38] Nathan Lane appeared twice during the second season as Cameron and Mitchell's flamboyant friend, Pepper Saltzman.[39][40][41]
Appearing in a few episodes is Jay and Gloria's dog Stella. Stella was played by Brigitte, but is now being played by Beatrice.[42]
The characters in green have regular roles on the show. Dotted lines indicate a parental relationship through adoption or marriage, and dashed lines indicate a divorce between characters.
The series premiered Wednesday, September 23, 2009 in the 9:00pm ET timeslot. Soon after, the series was picked up for a full season of 24 episodes on October 8, 2009.[4][5] On January 12, 2010, Modern Family was renewed for a second season by ABC.[43] The second season premiered September 22, 2010, airing in the same timeslot as the previous season.[44] Midway through the second season, the network renewed the series for a third season on January 10, 2011.[23] The third season premiered on September 21, 2011 with two back-to-back episodes,[45] beginning with a one-hour special. On May 10, 2012, Modern Family was renewed for a fourth season by the channel,[46][47] premiering on September 26, 2012.
The first season was met with universal critical acclaim. It received a Metacritic score of 86 out of 100.[48] Entertainment Weekly gave it an A-, calling it "immediately recognizable as the best new sitcom of the fall".[49] In Time's review, the show was named "the funniest new family comedy of the year".[50] It has also been compared to the 1970s series Soap, in regards to the multiple family aspect. Some have made comparisons to The Office and Parks and Recreation, due to their mockumentary formats.[51] BuddyTV named the show the second best show in 2009, saying, "Every actor is fantastic, every family is interesting, and unlike many shows, there isn't a weak link".[52] Robert Canning of IGN gave the season a 8.9 saying it was "Great" and called it "Simply put, Modern Family was one of the best new comedies of the season." He also praised the ensemble cast and the characters calling them lovable.[53] According to Metacritic, the first season was the best reviewed new broadcast television series.[54]
Modern Family's outstanding cast continues to impress, and even wobbly episodes reliably supply sharp observations and goofball charm.
The second season received positive reviews much like the first season, despite being given criticism for a sophomore slump. Robert Bianco of USA Today gave the new season four stars out of four, saying, "Not since Frasier has a sitcom offered such an ideal blend of heart and smarts, or proven itself so effortlessly adept at so many comic variations, from subtle wordplay to big-laugh slapstick to everything in between."[56] Robert Bianco in a later review stated "as good as it was in its first year, is even better in its second" positively comparing the characters to the characters from The Mary Tyler Moore Show, The Cosby Show and Friends.[57] During the second season, Adweek named the show one of the 100 Most Influential TV Shows (98th chronologically).[58][59] Despite this, the season received criticism from some critics for a sophomore slump.[60][61] Eric Stonestreet, who received praise during the first season,[62] was criticized during the second season for being too over the top.[63][64] Alan Sepinwall called Cameron Tucker a "whiny, overly-sensitive diva".[63] On the other hand, Ty Burrell has received praise for his performance as Phil Dunphy through both seasons.[62][65][66]
The third season received an overall mixed reception. Slant Magazine reviewer Peter Swanson wrote that while the first episode was "the type of wacky-location stunt that's usually reserved for the fifth or sixth season of a dying sitcom", the following episodes "have been better [...] but they're still uneven".[67] He also criticized the writers for relying too much on "stunt episodes and celebrity cameos, like David Cross".[67] He ultimately gave the season 3 out of 4 stars.[67] James Parker of The Atlantic said, at the beginning of the third season that "Modern Family is very, very funny, almost ruthlessly so ... [It's] a bit of a master class in pace and brevity ... The writing is Vorsprung durch Technik: hectically compressed but dramatically elegant, prodigal in its zingers and snorters but austere in its construction." He found it an exception to his dislike for sitcoms that eschew a laugh track.[68]
In The New York Times, Bruce Feiler called attention to how the show depicts the increasing way communications technology shapes the way people perceive others, even family members. "[It] is surely the first family comedy that incorporates its own hashtag of simultaneous self-analysis directly into the storyline," he writes. "Mark Zuckerberg may be a greater influence on Modern Family than Norman Lear."[69]
The show's writers and actors agree. "We used to talk about how cellphones killed the sitcom because no one ever goes to anyone's house anymore" for routine information, Abraham Higginbotham told Feiler. "We embrace technology so it's part of the story." Ty Burrell draws on Fran Lebowitz's observation that there is no institution other than media. "I had this little flash of Philand methat we are parsing our personality together externally from how people perceive us."[69]
James Parker said that "The American family circa 2011 is, after all, an acutely self-conscious and self-interrogating unit: How does one 'parent'? Who does what, which 'role'? Is Dad sufficiently dad-like and Mom enough of a mom?" he writes. "Modern Family taps right into all this, the cameras that lurch through its three households producing the sensation of a wild and shaky experiment, recorded for purposes educational or scientific."[68]
Modern Family drew criticism from the LGBT community for its portrayal of Cameron and Mitchell as not being physically affectionate with each other. The criticism spawned a Facebook campaign to demand Mitchell and Cameron be allowed to kiss. In response to the controversy, producers released a statement that a season two episode would address Mitchell's discomfort with public displays of affection. Executive producer Levitan has said that it was unfortunate that the issue had arisen, since the show's writers had always planned on such a scene "as part of the natural development of the show."[70] The episode "The Kiss" eventually aired with the kiss scene in the background which drew praise from multiple critics.[71][72]
During the third season, New York Times columnist Frank Bruni argued that gay criticism of Cameron and Mitchell actually showed the progress gays have made toward social acceptance. "A decade ago," he writes, "[gays] would have balkedand balked loudlyat how frequently Cameron in particular tips into limp-wristed, high-voiced caricature." But now, "most gay people trust that the television audience knows we're a diverse tribe, not easily pigeonholed ... Modern Family endows us with a sort of comic banality. It's an odd kind of progress. But it's progress nonetheless."[73]
Another notable criticism of Modern Family from various online news sources is that the show reinforces gender roles and sexist stereotypes. Michelle Haimoff of the Christian Science Monitor criticized the show for only casting the women as stay-at-home moms while the husbands on the show have very successful careers: "There is a difference between quirky, flawed characters and ones who are incapable of professional success. And when the latter is reliably female, it makes for sexist television. It also makes for unrealistic television."[74] Other authors reinforce this criticism by pointing out that stay-at-home mothers are no longer the norm in today's society.[75] According to the Department of Labor, 68.9% of married moms are working or looking for work. Thus, it's no surprise that the lack of representation for working moms sparked conversation on Twitter: "Late Night with Jimmy Fallon writer Ali Waller asked her Twitter followers, If Modern Family is so modern then why dont any of the women have jobs?[76]
According to a CNET staffer, commenting on an episode: "The wife and daughter are unable to learn how to use the remote and must be taught by the father, while the son is 'good with electronics,' even though he is thought of as the stupidest member of the family."[77] In the episode "Game Changer," one of the wives on the show, Gloria, hides her skill at chess so that her husband will not be upset at losing: "On its own, this moment is at best a sappy quip about compromise in an often heavy-handed series, and at worst, it's a moment in a show with 9.3 million viewers, on a network owned by Disney, which explicitly validates girls and women subduing their intellect."[78]
By contrast, Hanna Rosin has praised the show as one of several that have begun subverting the longtime American sitcom stereotype of bumbling fathers incapable of managing anything at home despite being the only one employed. "Phil Dunphy should be the house boob, with his embarrassing philosophy of 'peerenting,'" she wrote in Slate. "But Dunphy is not just comic relief; he's a successful salesman and the center of joy and fun in his household, especially compared to his uptight wife. No one really disdains him, and the domestic space belongs as much to him as to her."[79]
In 2010, Modern Family was nominated with five Television Critics Association Awards. The show gained nominations for best new series, best comedy series and best program of the year, while Ty Burrell and Eric Stonestreet were nominated individually.[80] Like Friends, to reinforce the idea of an ensemble cast, the cast all submitted themselves in the Supporting Actor and Actress categories instead of Lead Actor and Actress for the 62nd Primetime Emmy Awards.[81] On August 29, 2010, Modern Family won Outstanding Comedy Series, Outstanding Writing for a Comedy Series (for the pilot episode), and Outstanding Supporting Actor in a Comedy Series (Eric Stonestreet). The show also later received a GLSEN Respect Award for its portrayal of "positive images and storylines that reflect a diverse America, including the depiction of a family headed by a gay couple."[82] In 2010, the cast won a Screen Actors Guild Award for Outstanding Performance by an Ensemble in a Comedy, beating the previous year's winner, Glee.[83] On July 14, 2011, the series received 17 Emmy nominations, the third most nominations for the year after Mad Men and Boardwalk Empire.[84] The awards the series was nominated for include Outstanding Comedy Series, Outstanding Supporting Actor for a Comedy Series and Outstanding Supporting Actress in a Comedy Series.[84] The series has also been put on multiple critics' lists. In 2010, the series was listed on BuddyTV's Top Ten Best Comedy Shows of 20092010,[85] 2nd on Time's Top Ten Best shows of 2009,[86] 2nd on BuddyTV's Top Ten Best Shows of 2009,[87] Jason Hughes Best TV of 2009,[88] 10th on BuddyTV's Top 10 Returning Shows We're Most Excited to Come Back,[89] and on TV Guide's Our Favorite Families[90] Modern Family was awarded a Peabody Award in 2009.[91] In 2012, the show won the Golden Globe Award for Best Television Series  Musical or Comedy[8] and was nominated for a British Academy Television Award. In 2012, the show received a total of fourteen Primetime Emmy Award nominations, including Outstanding Comedy Series, and nominations for all six of its adult actors in the Supporting Actor and Actress Comedy categories.
Both Ann Romney (wife of US presidential candidate Mitt Romney), in an interview with The Insider,[92] and First Lady Michelle Obama, in an interview with Kal Penn at the 2012 Democratic National Convention,[93] have cited Modern Family as their favorite TV show.
Since its premiere, the series has remained popular. In its first season, the show became the sixth highest-rated scripted show in America among adults between the ages of 18 and 49, and the third-highest rated new show.[94] Aided by winning the Primetime Emmy Award for Outstanding Comedy Series, the show's second season became the highest rated show on Wednesday on premiere week[95] and also rose 34% from the previous season among adults between the ages of 18 and 49.[96] The show frequently ranked as television's top scripted series in adults 1849 as well.[97][98][99] The success of the show has been positively compared to The Cosby Show.[100] During the 20102011 season, Modern Family was the highest rated scripted show in the 1849 demographic, and the third highest rated overall sitcom behind CBS's The Big Bang Theory and Two and a Half Men.[101][102] The season also ranked first among DVR viewers.[103] The third season premiere became ABC's top-rated season premiere in six years.[104] The series success in ratings has also led to the series being credited for reviving sitcoms.[105]
1 Production

1.1 Conception
1.2 Pickup
1.3 Filming
1.4 Litigation


1.1 Conception
1.2 Pickup
1.3 Filming
1.4 Litigation
2 Cast and characters

2.1 Family tree


2.1 Family tree
3 Episodes
4 Reception

4.1 Critical reception

4.1.1 Analysis and commentary
4.1.2 Criticism and controversy


4.2 Accolades
4.3 Ratings


4.1 Critical reception

4.1.1 Analysis and commentary
4.1.2 Criticism and controversy


4.1.1 Analysis and commentary
4.1.2 Criticism and controversy
4.2 Accolades
4.3 Ratings
5 References
6 External links
1.1 Conception
1.2 Pickup
1.3 Filming
1.4 Litigation
2.1 Family tree
4.1 Critical reception

4.1.1 Analysis and commentary
4.1.2 Criticism and controversy


4.1.1 Analysis and commentary
4.1.2 Criticism and controversy
4.2 Accolades
4.3 Ratings
4.1.1 Analysis and commentary
4.1.2 Criticism and controversy
Official website
Modern Family at the Internet Movie Database
Modern Family at TV.com
Modern Family at TVGuide.com
.
#(`*Shark Tank (TV series)*`)#.
Shark Tank is an American reality TV show which premiered on ABC television in August 2009 and in May 2012 was renewed for a fourth season.[1]
The series premiered on Japanese TV in 2001, and is known elsewhere around the world as Dragons' Den. Sony Pictures Television, which owned the U.S. rights to the show, shopped it around for five years before it was selected as the next project by Mark Burnett, who was familiar with the series from his visits to England, where his relatives spoke highly of it to him.[2][3] The series features a panel of entrepreneurs and business executives called "Sharks" who consider offers from other entrepreneurs seeking investments for their business or product.[3]
The show includes a panel of potential investors. Two of the panel members, Robert Herjavec and Kevin O'Leary, have also appeared on the Canadian show, Dragons' Den. Other past and present members of the panel include: Barbara Corcoran, Daymond John, Kevin Harrington, Dallas Mavericks owner Mark Cuban, comedian Jeff Foxworthy and Lori Greiner.
Shark Tank Season 1 premiered on Sunday, August 9, 2009 and aired fourteen episodes through January 2010. In August 2010 it was renewed for a second season. Season 2 premiered with a sneak peek episode on Sunday, March 20, 2011 before resuming its regular Friday night time slot on March 25, 2011. Season 2 had nine episodes, five of them featuring new panel members. Comedian Jeff Foxworthy[4] and Mark Cuban replaced panel member Kevin Harrington for three episodes.[5]
Shark Tank's third season premiered Friday, January 20, 2012.[6][7] Kevin Harrington was replaced by Mark Cuban, and the "Queen of QVC" Lori Greiner filled in for Barbara Corcoran on four episodes. On February 27, 2012, ABC ordered two additional episodes for Season 3 using un-aired footage which brings the season's episode total to 15.[8] On May 10, 2012, Shark Tank was renewed for a fourth season, consisting of 24 episodes.[9] This is the first time the series has received a full season order.[1] Lori Greiner has confirmed that she will be a regular on Season Four,[10] which began filming on June 30, 2012.[11]
In July 2012 the show was nominated for a Primetime Emmy Award in the Outstanding Reality Program category.[12]
By 2012 the show was averaging 7 million viewers an episode, making it the most watched program on Friday nights in the 18-49-year old demographic. As a result, ABC added three more episodes to the original season order of 22. TV Guide reported that the show's panel members had invested $12.4 million in the business opportunities presented to them during that season. Those whose business ideas were not selected for investment also benefited from the publicity generated by the show. Those who appear on the show are required to give 5 percent of the company's equity to Mark Burnett Productions and 36,076 people applied to present their business ideas in the 2012 season.[2]
The show was nominated for the 2012 Primetime Emmy Award for Outstanding Reality Program.[citation needed]
Season 1
Season 2
Season 3
Season 4
1 Description
2 History
3 Appearances
4 Ratings
5 See also
6 References
7 External links
Kevin O'Leary, Barbara Corcoran, Daymond John, Robert Herjavec and Kevin Harrington appeared in all 14 episodes.
Kevin O'Leary, Barbara Corcoran, Daymond John, Robert Herjavec appeared in all 9 episodes. Mark Cuban appeared in three, Kevin Harrington in four and Jeff Foxworthy in two.
Kevin O'Leary, Daymond John, Robert Herjavec, Mark Cuban appeared in all 15 episodes. Barbara Corcoran appeared in 11 episodes and Lori Greiner in 4 episodes.
Kevin O'Leary, Barbara Corcoran, Daymond John, Robert Herjavec, Mark Cuban and Lori Greiner.
List of Shark Tank episodes
Official website
Shark Tank at the Internet Movie Database
Shark Tank at TV.com
.
#(`*The Middle (TV series)*`)#.
The Middle is an American sitcom television series, airing on ABC since September 30, 2009.[1] The series features Frances "Frankie" Heck (Patricia Heaton), a working-class, Midwestern woman and her husband Mike Heck (Neil Flynn), who reside in the small fictional town of Orson, Indiana. They are the parents of three children, Axl (Charlie McDermott), Sue (Eden Sher), and Brick (Atticus Shaffer).[2] The series was picked up for a full season of 24 episodes after airing just two episodes.[3]
The third season premiered with a one-hour episode on September 21, 2011. On May 10, 2012, ABC renewed the show for a fourth season,[4] which premiered with a one-hour special on September 26, 2012.
It was nominated for one 2012 Emmy Awards, for Outstanding Makeup for a Single-Camera Series (Non-Prosthetic)[5].
The series features a middle-class family of five (the Hecks) living in a mid-sized city in the American midwest state of Indiana. It is narrated by Frankie Heck, the wife and mother, an expressive under-performing salesperson at a used-car dealership and later a dental student. Her stoic husband Mike manages a local quarry and serves as a stabilizing influence in the family. They have three children: Axl (the oldest), an under-motivated and cynical teenager, who does well in sports but not in academics; Sue (the middle child), an enthusiastic young teen, but chronically unsuccessful and socially awkward; and Brick (the youngest), an intelligent but introverted compulsive reader with behavioral quirks and socialization problems.
See page List of The Middle episodes.
The series was originally developed in the 200607 development cycle and was to star Ricki Lake as Frankie.[2] Atticus Shaffer was the only actor to retain his role when the show was re-developed.[7]
The series was created by Eileen Heisler and DeAnn Heline and the pilot was directed by Julie Anne Robinson. On October 8, 2009, the show was picked up for a full season.[3] On January 12, 2010, ABC entertainment President Steve McPherson announced that he was renewing The Middle for a second season.[8][9] It has finished airing its third season and has been renewed for a fourth season, along with other Wednesday comedies like Modern Family and Suburgatory.
The show has received positive reviews from critics. It holds a score of 71 on the review aggregator website Metacritic as of April 1, 2011.[10] Critics praise the show's realistic look at the middle class, its good timing, writing, and acting, with Robert Bianco of USA Today saying, "...This series seems to more assuredly offer a first-class version of what so many viewers say they want: a humorous, heartfelt, realistic look at middle-class, middle-America family life."[11] The praise has been, so far, consistent, with Entertainment Weekly's Ken Tucker saying that, in season two, The Middle continues to be "...a rock-solid show, the saga of a family struggling to keep their heads above the choppy economic waters..."[12] In the 20092010 season, The Middle ranked number six on Metacritic's "Best Reviewed New Network Show" list.[13] Airing behind the quickly cancelled Hank during its first season, ratings were not impressive at first. At the beginning of the 2010/2011 season, ABC moved the show to 8:00pm and ratings increased substantially, with the show usually ranking second in its time slot to CBS's Survivor.
In 2011, the series received a Gracie Award for Outstanding Comedy.[14] The 1st Critics' Choice Television Awards nominated the series for Best Comedy Series, Patricia Heaton for Best Actress in a Comedy Series and Eden Sher for Best Supporting Actress in a Comedy Series.
The season 3 episode "Halloween II" was the most watched episode so far, viewed by 10.16 million viewers.[27]
On March 6, 2012, it was announced that ABC Family obtained the rights to The Middle, and will begin airing the series in the fall of 2013.[28] Hallmark Channel also acquired The Middle for syndication to begin airing in March 2014.[29]
1 Premise
2 Cast

2.1 Main cast
2.2 Recurring characters


2.1 Main cast
2.2 Recurring characters
3 Development and production
4 Reception

4.1 Critical reception
4.2 Awards and nominations
4.3 Ratings
4.4 Syndication


4.1 Critical reception
4.2 Awards and nominations
4.3 Ratings
4.4 Syndication
5 DVD distribution
6 International broadcasting
7 References
8 External links
2.1 Main cast
2.2 Recurring characters
4.1 Critical reception
4.2 Awards and nominations
4.3 Ratings
4.4 Syndication
Patricia Heaton as Frances "Frankie" Heck, (ne Spence), a wife and mother of three. Frankie (born 11/14/1966) [6] is the central character in the show and provides the voice-over narration. She is a devoted wife and mother and sees family as the most important thing in her life. Her motto, "You do for family," guides her daily routine, despite the frustrations she encounters withish. her three kids and her older, ailing relatives who depend on her for their needs. She worked as a saleswoman at Ehlert Motors, a job she took after losing her position at a dentist's office that closed down, only to lose her job at Ehlert Motors early in Season 4. As of the end of season one, we know that Frankie attended college but did not finish. She is currently in dental school. She has a mother, Pat (Marsha Mason), and a sister named Janet (Molly Shannon). Frankie's voice is heard in the background narrating at some point in every episode. Her father, Tag, is played by Jerry Van Dyke.
Neil Flynn as Michael "Mike" Heck (born 1968), Frankie's husband, known for his straightforward manner and emotional stability. Mike works at the quarry as the manager, and despite his no-nonsense approach to work and family, he is a devoted and understanding husband and father who always seems to come through for Frankie and the kids. From what has been said in the show so far, we know Mike pursued Frankie when they were younger and that at first she did not like him and even lied about what her name was to avoid him. Despite a short period where they broke up while dating, they have been together since their first date. It has been said on several occasions that Mike's favorite film is Reservoir Dogs (a peculiar fact considering his thoughtful and laid-back personality), and he only asked Frankie out on a second date because she lied and said she liked the film. Mike's father, "Big Mike" (John Cullum), and Mike's brother, Rusty (Norm Macdonald), both live in Orson. In the middle of season 2, the two move in together, as "Big Mike" recovers from a broken hip and Rusty needs a roof over his head after his house burns down. Mike has said that Axl is his favorite out of his children.
Charlie McDermott as Axl Redford Heck (Axl was born in 1994), athletic teenage son of Frankie and Mike, who can't be bothered to put on more than Boxer shorts most of the time at home. He is the stereotypical elder brother: sarcastic, lazy, and selfish to his siblings. However, he has shown flashes of kindness to his family occasionally and is good at heart, which he shows when he deliberately loses a basketball game in an attempt to win back his ex-girlfriend, Morgan. He is a trial for Frankie, who wishes he would be more open with her, as he was apparently a very loving "momma's boy" when he was a child, something Frankie seems to want to re-capture. It is revealed in Season 3 that Axl is two years older than Sue as he starts his junior year at high school while Sue starts her first. Mike has said that Axl is his favorite child.
Eden Sher as Sue Sue Heck (Sue was born February 29, 1996), socially awkward early-teen middle daughter of Frankie and Mike whose genuine nature, bubbly personality, and persistent optimism tend to make her an object of ridicule, especially by her elder brother. Despite her best efforts to get noticed, she is often unrecognized by her teachers and snubbed by her peers. For example, in season one she is not pictured in the year book after taking her picture three times and then Sue's favorite teacher having no memory of her at all. Her first and middle names are "Sue", as it was mistakenly written down twice on her birth certificate. Like many others her age, she wears braces on her teeth. (According to Mike, Frankie was too soft trying to get Sue to get rid of her pacifier and she had it until she was six.) As of season 3, she is a freshman in high school. Her braces were briefly removed in season 3, but it was determined they over-corrected her condition, and she now has to wear braces plus head gear. She is known for trying out for different things but never actually gets in. She dates Matt, a wrestler, for a while and then Matt moves away. Many of her plot lines involve her being left out of groups, even the school yearbook. At the end of the episode, she is placed in the yearbook (in the section reserved for a student who died, with the caption reading: In Loving Memory of Sue Heck, Gone Too Soon.)
Atticus Shaffer as Brick Heck (Brick was born in late 2002), youngest child and an unusual boy who loves to read and has a habit of repeating words from his previous sentence to himself in a whisper. In one episode,[specify] it is revealed that he possesses an eidetic memory. He is smarter than most teenagers, being able to read Mice and Men, a book Axl was struggling with at the same time. Also, beginning in Season 4, Brick also developed a habit to occasionally say "whoop". He is exceptionally intelligent but easily distracted, leading him to procrastinate on or just forget his homework assignments and projects. Brick is a known gephyrophobiac and also has a fascination with fonts. Due to his awkwardness, he struggles to make friends, however in Season 2 he befriends an equally awkward boy named Arlo, whom Brick himself struggles to deal with. There has been a lot of mention of occupational therapy, but he hasn't actually seen one yet. In Season 1, he is in the second grade, Season 2 in the third grade, Season 3 in the fourth grade and now is in the fifth grade in Season 4.
Bob (Chris Kattan) is Frankie's former coworker at Ehlert Motors, and Mike dislikes him. At one time, he and Mike work the night shift at Little Betty Snacks, a parody of Little Debbie Snacks. Having few close friends or family members, outside of his mother, Bob tends to treat the Hecks as his surrogate family.
Aunt Edie Spence (ne Freehold) and Aunt Ginny Freehold (Jeanette Miller and Frances Bay) are Frankie's elderly aunts who are heavy smokers and drinkers. Aunt Edie used to work in Mike's quarry as the bookkeeper, often showing signs of memory loss, while Aunt Ginny uses a wheelchair and rarely talks. They have a sickly Basset Hound named Doris. On many occasions,Frankie has been stuck taking care of Doris unexpectedly. One time when Frankie was watching Doris, she had puppies. In the episode, "The Map," which premiered in early 2012, Aunt Ginny passes away, as did the actress Frances Bay portraying her. Brick's elderly friend, Grandma Dot, then moves in with Edie to take care of her.
Brad Bottig (Brock Ciarlelli) is Sue's flamboyant ex-boyfriend. They dated during Thanksgiving but shortly broke up when Sue found out Brad is a smoker. Frankie and Mike both suspect that Brad is gay, but Sue is oblivious to this. Brad and Sue were good friends as of Season 4.
Carly (Blaine Saunders) is Sue's best friend. She starts off in Season 1 as a geeky character, wearing glasses and braces. Her braces were removed mid-Season 1, and she became more popular as a result. She begins wearing higher clothes that highlight her breasts and butt. Sue gets jealous and begins to wear make up, try beer, and get sexual with both males and females. From Season 2 onwards, Carly has since reverted back to a more geeky look, most likely due to feeling more comfortable with it.
Nancy Donahue (Jen Ray) is the Hecks' neighbor and president of the booster club. The Donahues are usually portrayed as the idyllic family Frankie yearns for herself.
Sean Donahue (Beau Wirick) is one of Axl's best friends and teammate on the football team. He is helpful, polite, and friendly, which makes him outwardly the opposite of Axl. He always supports everything Axl does. In season 2, Sue develops a crush on Sean, but eventually ends the one- way relationship due to think her ex- boyfriend Brad was "heartbroken".
Paula Norwood (Julie Brown) is another of the Hecks' neighbors. She serves as somewhat of a counterpoint to the idyllic Nancy Donahue in that her life and family is depicted as being similar to Frankie's, having a lot of the same issues.
Darrin (John Gammon) Axl's other best friend and teammate. Darrin comes across as the most passive and idiotic of Axl's friends  often landing himself, Sean and Axl in problematic situations. In season 3, Darrin takes Sue to Axl's prom because of her "sunny disposition". Darrin is known as not clever and humorous. His last name has never been revealed.
Ms. Rinsky (Doris Roberts) Brick's third grade teacher, who seems to be the bane of Frankie's life, in a reference to the previous relationship between Heaton and Roberts' characters on Everybody Loves Raymond. Frankie describes her as an "alcoholic Nazi" following a drunken letter of complaint. Despite his mother's opinion, Brick finds Ms. Rinsky a good teacher, because she designates reading time during tests.
Jack Meenahan (Thomas F. Duffy) is the Heck family's next-door neighbor. His lost his home in Season 3.
Reverend Timothy "Tim-Tom" Thomas (Paul Hipp) is a youth minister who Sue deeply admires. He surprised Sue by remembering her name after only having met her once, and always seems to appear just as Sue needs help the most. He is known for frequently playing his acoustic guitar and making almost anything into a song. He played Axl's electric guitar on one occasion when encouraging him to do the right thing.
Derrick Glossner (David Chandler VII), Wade Glossner (Parker Bolek) and the littlest Glossner (Gibson Bobby Sjobeck) make up the dreaded Glossner boys. They are the neighborhood bullies and sons of Rita Glossner (Brooke Shields) that terrorize the Heck kids. The third Glossner boy's first name has never been revealed.
Zack (Andrew J. Fishman) is Brick's friend from the socially challenged class. He has a tendency to behave like a cat.
Pete (Peter Breitmayer) is Ehlert Motors' best salesman and star employee, and he is not afraid to tell everyone about it. Pete often brags about his success at Frankie's and Bob's expense, putting them down about how little they contribute to the business whilst attempting to impress Mr. Ehlert.
Morgan (Alexa Vega) is Axl's ex-girlfriend, whom Frankie took an instant liking to due to her straight A's and talent in multiple areas. However, after she breaks Axl's heart time and time again, Frankie eventually explodes with anger and throws a beanbag at Morgan during a birthday party.
Don Ehlert (Brian Doyle-Murray), a sexist and racist owner of the car dealership where Frankie and Bob work. Ehlert is a crass, self-absorbed, rather sleazy man. Frankie and Bob both constantly worry he is going to fire them both. Ehlert's wife left him when he chose work over her, and Frankie was first sympathetic but later turned angry. He often drinks while working.
Matt (Moiss Arias), a boy on the wrestling team at Sue and Axl's school who likes Sue; he becomes Sue's first proper boyfriend but eventually ends the relationship after moving away and falling for another girl.
Ashley Wyman (Katlin Mastandrea), a creepy, weird (hence her nickname is "Weird Ashley") classmate who has twice been Axl's accidental date to prom. She is also on Sue's wrestlerette team. She has a keen interest in wizards, although she has never heard of Harry Potter.
Ruth (Grace Bannon) is a member of Sue's wrestlerette team. She is depicted as an extreme religious conservative.
Courtney and Debbie (Brittany Ross and Natalie Lander), cheerleaders in Axl and Sue's high school who date Axl as one. They stopped dating Axl when he kissed Courtney inappropriately, which caused both girls to slap him.
Mrs. Tompkins (Krista Braun), Brick's social skills mentor and Guidance Counselor.
24 episodes
3-disc set
1.78:1 aspect ratio
Languages:

English (Dolby Digital 5.1, with subtitles)
Portugus (Dolby Digital 5.1, with subtitles)
Franais subtitles
Espaol subtitles
Chinese subtitles
Thai subtitles


English (Dolby Digital 5.1, with subtitles)
Portugus (Dolby Digital 5.1, with subtitles)
Franais subtitles
Espaol subtitles
Chinese subtitles
Thai subtitles
English (Dolby Digital 5.1, with subtitles)
Portugus (Dolby Digital 5.1, with subtitles)
Franais subtitles
Espaol subtitles
Chinese subtitles
Thai subtitles
Raising a Sitcom Family
Sue's Best Shots
Unaired scenes
Gag reel
24 episodes
3-disc set
1.78:1 aspect ratio
Languages:

English (Dolby Digital 5.1, with subtitles)
Portugus subtitles
Franais subtitles
Espaol subtitles
Chinese subtitles


English (Dolby Digital 5.1, with subtitles)
Portugus subtitles
Franais subtitles
Espaol subtitles
Chinese subtitles
English (Dolby Digital 5.1, with subtitles)
Portugus subtitles
Franais subtitles
Espaol subtitles
Chinese subtitles
Unaired scenes
Gag reel
Official website
The Middle at the Internet Movie Database
The Middle at TV.com
.
#(`*Top Chef*`)#.
Top Chef is an American reality competition show that airs on the cable television network Bravo, in which chefs compete against each other in culinary challenges. They are judged by a panel of professional chefs and other notables from the food and wine industry with one or more contestants eliminated in each episode. The show is produced by Magical Elves Productions, the same company that created Project Runway.
The show has produced three spin-offs: Top Chef: Masters, featuring established, award-winning chefs, Top Chef: Just Desserts, featuring pastry chefs, and Top Chef Canada. One more spin-off is planned: Top Chef Junior, featuring contestants in their early teens.[5]
Generally, each episode of Top Chef, other than the finale has had two challenges.
The first is called the Quickfire challenge. Each chef must cook a dish that meets certain requirements (for example, using specific ingredients or inspiring a certain taste) or participate in a culinary-related challenge (for example, a mise en place relay or a taste test). They are usually given an hour or less to complete these tasks.[6] A guest judge selects one or more chefs as the best in the challenge. Early in the season the winning chef(s) are granted immunity from the episode's Elimination Challenge. As the number of contestants dwindle immunity is withdrawn, and instead the winner receives an advantage (such as being team leader for a team challenge or getting first pick of ingredients) or a prize. To emphasise the culture and environment of the sixth season's Las Vegas setting, the show introduced "high-stakes" Quickfires, which featured an extravagant prize (usually a large cash prize); high-stakes Quickfires continued onward in further seasons. Occasionally, a Quickfire will also include the poorest performer 's elimination from the competition, and sometimes contestants have been dismissed for violations such as tasting a sauce with a finger.[7]
The other is called an Elimination Challenge, in which the chefs prepare one or more dishes to meet the challenge requirements, which are usually more complex and require longer time to execute than a Quickfire challenge. Elimination Challenges may be individual challenges or may require chefs to work in a team; some may require a chef or chefs to produce several courses. Teams may be selected by the remaining contestants, but more often are selected by a random process such as by "drawing knives" from a butcher's block, with the team identification revealed on the blade of the knife. The chefs may have from a few hours to a few days to complete an Elimination Challenge, which typically includes preparation and planning time. Ingredients for Elimination Challenges generally allow chefs to access both what staples are available in the "Top Chef" pantry and what the chefs purchase at a grocery store, within a specified budget and shopping time limit. However, certain challenges may provide specific ingredients or limit the type or number of ingredients that can be used, while others require non-traditional methods of obtaining ingredients (such as asking people door-to-door or fishing) or preparation methods (such as tailgate cooking). After shopping, the contestants will cook for up to four judges, usually including at least one guest judge. In most cases, the contestants cook for a group of guests (for example, the cowboys in Colorado) as well.
After the Elimination Challenge ends, the chefs report to Judges' Table, where the four judges will consider the guests' comments (if available) and deliberate on their choices for the best and worst dishes. The top individuals or teams are called in, and may be asked questions about their dishes or preparation before they are notified of their placement. One or more chefs are named the winner of the challenge and may be awarded an additional prize by the guest judge. The same procedure is repeated with the poorest performing chefs or team, after which similar discussion takes place. From this group, one chef is chosen for elimination, with the host asking the chef to "pack [their] knives and go." This is usually followed by a knife packing sequence for the eliminated chef, and with a voice over of their final thoughts about their performance, at the close of the episode. According to the credits, some elimination decisions are made in consultation with the show's producers.
The prize money awarded to the Top Chef was $100,000 for the first 5 seasons, and then increased to $125,000 from season 6 on.[8] Season 8 prize money was for $200,000.
Midway through each season, the contestants participate in a "Restaurant Wars" (or similarly named) Elimination Challenge. They are split into two teams, created by the winner of the previous Quickfire Challenge, or by "drawing knives". In these teams, the chefs must transform an empty space into a restaurant within a set time limit and budget, selecting and creating the name, theme, decor, and menu. According to Dale Levitski, a season three finalist, this is one of the contestants' most anticipated challenges. Season four had not only "Restaurant Wars", but "Wedding Wars" as well.
In the final challenge, the two (or three) remaining chefs prepare a multiple course dinner with the help of other chefs. These other chefs could be previously eliminated chefs or celebrity chefs (for example, Rocco DiSpirito). The winning chef is selected based on several factors, including food quality, their ability to lead their assistant chefs, and general performance during the show. There is no Quickfire Challenge in this episode.
On June 6, 2007, as part of the buildup for season 3, Bravo aired a special charity episode called "4 Star All Star", which awarded $20,000 to the charity of the winning team's choice. The show brought together season 1 contestants Stephen, Harold, Dave, and Tiffani, and pitted them against season 2 contestants Ilan, Elia, Marcel, and Sam. The show format was kept the same, with a Quickfire Challenge to start, followed by an Elimination Challenge. The show was filmed in Miami Beach at the kitchen and hotel used for season 3. Padma Lakshmi hosted.
The December 24, 2007 special brought selected chefs from all three seasons back with a prize of $20,000 for the winner. Padma Lakshmi presided as host, with regulars Ted Allen, Tom Colicchio, Gail Simmons among the judges. The following Chefs participated: Tiffani Faison and Stephen (season 1); Josie, Marcel Vigneron, and Betty (season 2); CJ, Tre, and Sandee (season 3). It was shot in Chicago, Illinois.
On November 4, 2009, Fabio Viviani from season five has invited contestants from his season and those in the past for what is supposed to be a convivial gathering of chefs cooking, eating and talking. Chefs who attended were: Harold and Tiffani from season one; Ilan and Marcel from season two; Casey, Dale,and Hung from season three; Richard and Lisa from season four and Carla, Fabio and Stefan from season five.
Season 5 was released on DVD in Region 1 by A&E Home Video on October 20, 2009.[11] A season 6 DVD set has been announced for release August 24, 2010.[citation needed] Seasons 4-9 are sold exclusively at Target in 4 disc collections, with the exception of season 8, which is 5 discs, and season 9, which is 6 discs.
Top Chef Masters features established, award-winning chefsa contrast from Top Chef, which features chefs that who have been called relative "culinary amateurs".[12] During its first two seasons, food journalist Kelly Choi hosted the show, and restaurant critic Gael Greene, culinary expert and Saveur Editor-in-Chief James Oseland, and food critic Jay Rayner served as judges.[13][14]
The show debuted in 2009, with contestants including Rick Bayless, John Besh, Michael Chiarello, Wylie Dufresne, and Hubert Keller.[15] As of 2012, four seasons have been produced and aired.
Top Chef: Just Desserts is a spin-off featuring pastry chefs. The show is hosted by Top Chef judge Gail Simmons; the head judge is Johnny Iuzzini, the award-winning pastry chef at Jean-Georges.[16] Hubert Keller, owner of world-renowned restaurant Fleur de Lys and Top Chef Masters finalist,[17] and Dannielle Kyrillos, "an entertaining expert and Editor-at-Large of DailyCandy"[18] are regularly featured judges.
The show debuted on Bravo on September 15, 2010, after the seventh season finale of Top Chef.
Top Chef: Healthy Showdown was a special webisode series in 2011 sponsored by Healthy Choice. It featured former Top Chef contestants Sara Nguyen (Top Chef: Miami), Ryan Scott (Top Chef: Chicago), Casey Thompson (Top Chef: Miami, Top Chef: All-Stars), and Tre Wilcox (Top Chef: Miami, Top Chef: All-Stars) competing in a series Quickfire Challenges to win $25,000 and inspire a Top Chef line of Healthy Choice entres.[19] It was hosted by Curtis Stone, and Ryan Scott won the competition.
Life After Top Chef is a spin-off featuring former Top Chef contestants and focuses on various aspects of their lives, from managing and opening a restaurant to dealing with family dynamics and personal issues.
It premiered on October 3, 2012[20] and features Richard Blais, Jen Carroll, Spike Mendelsohn, and Fabio Viviani.[21]
Top Chef Junior is a tentatively-titled upcoming American reality competition show that will air on the cable television network Bravo. It is expected that teenage chefs will compete against each other in weekly challenges. Following the Top Chef format, they will be judged by a panel of professional chefs and other notables from the culinary world, and one or more contestants will be eliminated each week. The show is produced by Magical Elves Productions, the same company that created and produces Top Chef.[22][23][24]
Bravo has ordered eight episodes; currently no air date has been set.[25]
On April 21, 2010 Food Network Canada announced that they had come to an agreement with NBC Universal to produce a Canadian version of the program, called Top Chef Canada.[26] It first aired April 11, 2011.[26][27][28] The sixteen contestants compete for a grand prize of CDN$100,000 and a GE Monogram kitchen valued at $30,000.[28][29]
Thea Andrews hosted Top Chef Canada's first season.[30] Lisa Ray is the host for the second season. The head judge (the position held by Tom Colicchio on the original version) is Mark McEwan, chef and owner of several restaurants in Toronto and host of the Food Network Canada program The Heat with Mark McEwan.[31] The "resident judge", as described by the network, is Shereen Arazm, a Toronto native and owner of several Los Angeles-area restaurants.[32]
Rotana Masriya
Top Chef University is a comprehensive online culinary school involving 12 courses and over 220 in-depth video-lessons. The program at www.topchefuniversity.com takes participants through a structured program of the basics (knife skills, kitchen set-up, ingredients) through to advanced culinary techniques (sous vide, molecular gastronomy). Instructors at Top Chef University are some of the show's most successful and popular former "cheftestants". Enrollment costs US$199.95 for an annual membership and US$24.95 for a monthly membership.
Top Chef: The Game is a computer game released by Brighter Minds for the PC. It challenges players to create the best dish from items in a virtual pantry. Games magazine gave the game an unfavorable review, calling it a "quick cash-in... for an undiscriminating audience."[39]
In efforts to make certain dishes available to viewers who watch Top Chef but do not have time to try preparing those dishes themselves, Schwan's Home Service started offering Top Chefbranded frozen meals in late 2009.[40]
In June 2009, news broke that Top Chef is working with Food & Wine magazine on a Top Chef print magazine.[41]
On March 20, 2008, Chronicle Books released Top Chef: The Cookbook, with a foreword by Tom Colicchio.[42][43]
On September 30, 2009, Chronicle Books released Top Chef: The Quickfire Cookbook, with a foreword by Padma Lakshmi.[44]
On July 14, 2010, Chronicle Books released How to Cook Like a Top Chef, with a foreword by Rick Bayless.[44]
Top Chef was nominated at the 59th Primetime Emmy Awards for Outstanding Cinematography for Reality Programming and Outstanding Reality-Competition Program for its second season.[45] Top Chef won the award for Outstanding Editing in a Reality Series at the 60th Primetime Emmy Awards. Top Chef won the award for Outstanding Reality-Competition Program at the 62nd Primetime Emmy Awards,[6] defeating The Amazing Race which had won the award every year since the category's inception in 2003.
Time magazine's James Poniewozik named it one of the Top 10 Returning Series of 2007, ranking it at #10.[46]
Ratings of Top Chef premieres:
1 Show format

1.1 Basic format
1.2 Special formats
1.3 Judges


1.1 Basic format
1.2 Special formats
1.3 Judges
2 Seasons
3 Special episodes

3.1 4 Star All Star (Season One vs. Season Two)
3.2 Top Chef Holiday Special
3.3 Top Chef Reunion Dinner


3.1 4 Star All Star (Season One vs. Season Two)
3.2 Top Chef Holiday Special
3.3 Top Chef Reunion Dinner
4 DVD release
5 Spin-offs

5.1 Top Chef Masters
5.2 Top Chef: Just Desserts
5.3 Top Chef: Healthy Showdown
5.4 Life After Top Chef
5.5 Top Chef Junior


5.1 Top Chef Masters
5.2 Top Chef: Just Desserts
5.3 Top Chef: Healthy Showdown
5.4 Life After Top Chef
5.5 Top Chef Junior
6 International adaptations

6.1 Top Chef Canada
6.2 Other international adaptations


6.1 Top Chef Canada
6.2 Other international adaptations
7 Other media

7.1 Top Chef University
7.2 Top Chef: The Game
7.3 Top Chef TV Dinners
7.4 Top Chef: The Magazine
7.5 Top Chef: The Cookbook (series)

7.5.1 Top Chef: The Cookbook
7.5.2 Top Chef: The Quickfire Cookbook
7.5.3 How to Cook Like a Top Chef




7.1 Top Chef University
7.2 Top Chef: The Game
7.3 Top Chef TV Dinners
7.4 Top Chef: The Magazine
7.5 Top Chef: The Cookbook (series)

7.5.1 Top Chef: The Cookbook
7.5.2 Top Chef: The Quickfire Cookbook
7.5.3 How to Cook Like a Top Chef


7.5.1 Top Chef: The Cookbook
7.5.2 Top Chef: The Quickfire Cookbook
7.5.3 How to Cook Like a Top Chef
8 Reception and awards

8.1 Awards
8.2 Ratings


8.1 Awards
8.2 Ratings
9 References
10 External links
1.1 Basic format
1.2 Special formats
1.3 Judges
3.1 4 Star All Star (Season One vs. Season Two)
3.2 Top Chef Holiday Special
3.3 Top Chef Reunion Dinner
5.1 Top Chef Masters
5.2 Top Chef: Just Desserts
5.3 Top Chef: Healthy Showdown
5.4 Life After Top Chef
5.5 Top Chef Junior
6.1 Top Chef Canada
6.2 Other international adaptations
7.1 Top Chef University
7.2 Top Chef: The Game
7.3 Top Chef TV Dinners
7.4 Top Chef: The Magazine
7.5 Top Chef: The Cookbook (series)

7.5.1 Top Chef: The Cookbook
7.5.2 Top Chef: The Quickfire Cookbook
7.5.3 How to Cook Like a Top Chef


7.5.1 Top Chef: The Cookbook
7.5.2 Top Chef: The Quickfire Cookbook
7.5.3 How to Cook Like a Top Chef
7.5.1 Top Chef: The Cookbook
7.5.2 Top Chef: The Quickfire Cookbook
7.5.3 How to Cook Like a Top Chef
8.1 Awards
8.2 Ratings
Quickfire Challenge: Create a two-egg breakfast dish in 10 minutes, using only one hand. Prize: An extra $100 for the overall winner's team to spend on groceries for the Elimination Challenge.

SEASON 1 TEAM WINNER: Stephen.
SEASON 2 TEAM WINNER: Marcel.
OVERALL WINNER: Season 1


SEASON 1 TEAM WINNER: Stephen.
SEASON 2 TEAM WINNER: Marcel.
OVERALL WINNER: Season 1
Elimination Challenge: Prepare a four-course meal for the season 3 contestants consisting of scallops, lobster, duck, and Kobe beef. Teams were given 30 minutes of shopping time, with season 1 able to spend $300 and season 2 able to spend $200. Teams were given two hours of cooking time. Courses were served head to head, with the team that wins the most courses winning the challenge.

SCALLOPS: Dave made cold smoked scallops & triple olive tapenade with lemon vinaigrette. Elia made a scallop duo with citrus marmalade and endive salad. Elia won.
LOBSTER: Stephen made lobster poached in beurre blanc with cauliflower crme anglaise. Marcel made a lobster duo with vanilla gele and lobster foam. Stephen won.
DUCK: Harold made spicy duck meatballs with minted gnocchi. Ilan made almond stuffed duck breast with wild rice, white asparagus & raw egg yolk. Harold won.
KOBE BEEF: Tiffani made Kobe beef two ways served with creamy polenta. Sam made seared Kobe beef with mushroom confit & onion relish. Tiffani won.
OVERALL WINNER: Season 1, with three wins.


SCALLOPS: Dave made cold smoked scallops & triple olive tapenade with lemon vinaigrette. Elia made a scallop duo with citrus marmalade and endive salad. Elia won.
LOBSTER: Stephen made lobster poached in beurre blanc with cauliflower crme anglaise. Marcel made a lobster duo with vanilla gele and lobster foam. Stephen won.
DUCK: Harold made spicy duck meatballs with minted gnocchi. Ilan made almond stuffed duck breast with wild rice, white asparagus & raw egg yolk. Harold won.
KOBE BEEF: Tiffani made Kobe beef two ways served with creamy polenta. Sam made seared Kobe beef with mushroom confit & onion relish. Tiffani won.
OVERALL WINNER: Season 1, with three wins.
Judges: Tom Colicchio, Gail Simmons, and Ted Allen.
Stephen, as winner of the season 1 team in the Quickfire, became team captain for the Elimination and got to choose his team's charity. He chose Susan G. Komen for the Cure.
Marcel, as winner of the season 2 team in the Quickfire, became team captain for his team. He chose Share Our Strength for his charity.
Prior to the Quickfire Challenge, season 1 contestant Lee Anne Wong was introduced as having been hired as a food consultant for Top Chef. This also explained Lee Anne's absence from the competition, as she was the fourth-place contestant in the first season of the show.
SEASON 1 TEAM WINNER: Stephen.
SEASON 2 TEAM WINNER: Marcel.
OVERALL WINNER: Season 1
SCALLOPS: Dave made cold smoked scallops & triple olive tapenade with lemon vinaigrette. Elia made a scallop duo with citrus marmalade and endive salad. Elia won.
LOBSTER: Stephen made lobster poached in beurre blanc with cauliflower crme anglaise. Marcel made a lobster duo with vanilla gele and lobster foam. Stephen won.
DUCK: Harold made spicy duck meatballs with minted gnocchi. Ilan made almond stuffed duck breast with wild rice, white asparagus & raw egg yolk. Harold won.
KOBE BEEF: Tiffani made Kobe beef two ways served with creamy polenta. Sam made seared Kobe beef with mushroom confit & onion relish. Tiffani won.
OVERALL WINNER: Season 1, with three wins.
Quickfire Challenge: Chefs drew knives to determine the order in which they would choose giftsingredientsfrom under the tree. Upon their turn, the chef could either choose to 'steal' an already opened gift of another chef, or take a new gift from under the tree.

Winner: CJ (Roasted walnuts sauted with apples & shrimp)
Guest Judge: Eric Ripert
Prize: One free pass to the next round during the Elimination Challenge. (The pass could not be used to advance to the final round.)


Winner: CJ (Roasted walnuts sauted with apples & shrimp)
Guest Judge: Eric Ripert
Prize: One free pass to the next round during the Elimination Challenge. (The pass could not be used to advance to the final round.)
Elimination Challenge: Create a three-course meal for a party of nine. After each course, two chefs will be eliminated.

Course 1: Stephen and Sandee eliminated.
Course 2: Josie and Betty eliminated.
Course 3: Marcel and CJ eliminated.


Course 1: Stephen and Sandee eliminated.
Course 2: Josie and Betty eliminated.
Course 3: Marcel and CJ eliminated.
Surprise Challenge: After the three courses, Tiffani and Tre were given a surprise challenge to create an additional dish, utilizing anything left in the kitchen, in 30 minutes.

WINNER: Tiffani
Winning Menu:

First Course: Bacon-Wrapped Apples with Brussels Sprout Salad & Apple Chicken Jus
Second Course: Roasted Duck Breast on Spaetzle with Wine & Orange Jus
Third Course: Salted Butterscotch Pudding with Whipped Cream
Surprise Course: Braised Veal on Puff Pastry with Truffle Butter.




WINNER: Tiffani
Winning Menu:

First Course: Bacon-Wrapped Apples with Brussels Sprout Salad & Apple Chicken Jus
Second Course: Roasted Duck Breast on Spaetzle with Wine & Orange Jus
Third Course: Salted Butterscotch Pudding with Whipped Cream
Surprise Course: Braised Veal on Puff Pastry with Truffle Butter.


First Course: Bacon-Wrapped Apples with Brussels Sprout Salad & Apple Chicken Jus
Second Course: Roasted Duck Breast on Spaetzle with Wine & Orange Jus
Third Course: Salted Butterscotch Pudding with Whipped Cream
Surprise Course: Braised Veal on Puff Pastry with Truffle Butter.
Guest Judges: Eric Ripert, Elizabeth Falkner, Norman van Aken, Alan Wong, Alfred Portale
Winner: CJ (Roasted walnuts sauted with apples & shrimp)
Guest Judge: Eric Ripert
Prize: One free pass to the next round during the Elimination Challenge. (The pass could not be used to advance to the final round.)
Course 1: Stephen and Sandee eliminated.
Course 2: Josie and Betty eliminated.
Course 3: Marcel and CJ eliminated.
WINNER: Tiffani
Winning Menu:

First Course: Bacon-Wrapped Apples with Brussels Sprout Salad & Apple Chicken Jus
Second Course: Roasted Duck Breast on Spaetzle with Wine & Orange Jus
Third Course: Salted Butterscotch Pudding with Whipped Cream
Surprise Course: Braised Veal on Puff Pastry with Truffle Butter.


First Course: Bacon-Wrapped Apples with Brussels Sprout Salad & Apple Chicken Jus
Second Course: Roasted Duck Breast on Spaetzle with Wine & Orange Jus
Third Course: Salted Butterscotch Pudding with Whipped Cream
Surprise Course: Braised Veal on Puff Pastry with Truffle Butter.
First Course: Bacon-Wrapped Apples with Brussels Sprout Salad & Apple Chicken Jus
Second Course: Roasted Duck Breast on Spaetzle with Wine & Orange Jus
Third Course: Salted Butterscotch Pudding with Whipped Cream
Surprise Course: Braised Veal on Puff Pastry with Truffle Butter.
Siham Tueini
Joe Barza
Changing judge
Joumana Mrad
Joe Barza
Changing judge
Mark McEwan
Shereen Arazm
Pia Kmppi
Hans Vlimki
Cyril Lignac
Ghislaine Arabian
Thierry Marx
Jean-Franois Pige
Christian constant
Elias Mamalakis
Christopher Peska
Trastelis Apostle
Herve Pronzato
Robert Kranenborg
Julius Jaspers
Changing judge
Jos Cordeiro
Susana Felicidade
Ricardo Costa
Joseph Hadad
Tudor Constantinescu
Nicolai Tand
Perkins, Arthur (2011). Chef Wars: Top Chef, Top Chef Masters, Top Chef-Just Desserts. Charleston, South Carolina: Author House. pp.various. ISBN97814515221818.
Official website
Top Chef at the Internet Movie Database
Top Chef at TV.com
.
#(`*Lost (TV series)*`)#.
Lost is an American television series that was originally aired on the American Broadcasting Company (ABC) from September 22, 2004 to May 23, 2010, consisting of six seasons. Lost is a drama series containing elements of science fiction and the supernatural that follows the survivors of the crash of a commercial passenger jet flying between Sydney and Los Angeles, on a mysterious tropical island somewhere in the South Pacific Ocean. The story is told in a heavily serialized manner. Episodes typically feature a primary storyline on the island, as well as a secondary storyline from another point in a character's life.
Lost was created by Jeffrey Lieber, J. J. Abrams and Damon Lindelof who share story-writing credits for the pilot episode, which Abrams directed. Throughout the show's run, Lindelof and Carlton Cuse served as showrunners and head writers, working together with a large number of other executive producers and writers. Due to its large ensemble cast and the cost of filming primarily on location in Oahu, Hawaii, the series was one of the most expensive on television.[1] The fictional universe and mythology of Lost is expanded upon by a number of related media, most importantly a series of short mini-episodes called Missing Pieces, and a 12-minute epilogue titled "The New Man in Charge."
A critically acclaimed and popular success, Lost was consistently ranked by critics on their lists of top ten series of all time.[2] The first season garnered an average of 15.69 million viewers per episode on ABC.[3] During its sixth and final season, the show averaged over 11 million US viewers per episode. Lost was the recipient of hundreds of award nominations throughout its run, and won numerous industry awards, including the Emmy Award for Outstanding Drama Series in 2005,[4] Best American Import at the British Academy Television Awards in 2005, the Golden Globe Award for Best Drama in 2006 and a Screen Actors Guild Award for Outstanding Ensemble in a Drama Series.
The first season begins with a plane crash that strands the surviving passengers of Oceanic Airlines Flight 815 on what seems to be a deserted tropical island. Their survival is threatened throughout the season by a number of mysterious entities, including polar bears, an unseen creature that roams the jungle (the "Smoke Monster"), and the island's malevolent, and largely unseen, inhabitants known as "the Others". They encounter a French woman named Danielle Rousseau who was shipwrecked on the island 16 years prior to their crash. They also find a mysterious metal hatch buried in the ground. While two characters try to force their way into the hatch, four other survivors attempt to leave the island on a raft that they have constructed. Meanwhile, flashbacks centered on individual survivors detail their lives prior to the plane crash.
The second season follows the growing conflict between the survivors and the Others, and continues the theme of the clash between faith and science, while resolving old mysteries and posing new ones. A power struggle between Jack and John over control of the guns and medicine in the hatch develops, resolved in "The Long Con" by the machinations of Sawyer when he gains control of them. New characters are introduced, including the tail-section survivors (the "Tailies") and other island inhabitants. The hatch is revealed to be a research station built by the Dharma Initiative, a scientific research project that was conducting experiments on the island decades earlier. A man named Desmond Hume has been living in the hatch for three years, pushing a button every 108 minutes to prevent a catastrophic event from occurring. As the truth about the mysterious Others begins to unfold, one of the crash survivors betrays the other castaways, and the cause of the plane crash is revealed.
In the third season, the crash survivors learn more about the Others and their long history on the mysterious island. Desmond and one of the Others join the survivors, while one of their number in turn defects to the Others. A war between the Others and the survivors comes to a head, and the survivors make contact with a rescue team aboard the freighter Kahana.
Season four focuses on the survivors dealing with the arrival of people from the freighter, who have been sent to the island not as part of a rescue operation, but for far more nefarious purposes. The survivors begin planning a way to get off the island before the freighter crew can carry out their plan. Flashforwards reveal the identities and future actions of the so-called "Oceanic Six," a group of survivors who have escaped the island and attempted to resume their normal lives. In an attempt to "move the island" to safety, one of the Others uses an ancient device on the island that not only moves the island physically but also moves it to another point in time, while simultaneously teleporting that Other to a desert in Tunisia.
The fifth season follows two timelines. The first takes place on the island where the survivors who were left behind erratically jump forward and backward through time until they are finally stranded with the Dharma Initiative in 1974. The second continues the original timeline, which takes place on the mainland after the Oceanic Six escape, and then follows their return to the island on Ajira Airways flight 316 in 2007 (three years after they escaped). Some passengers on the Ajira flight land in 1977 and some remain in 2007. The ones who land in 1977 reunite with the other survivors who have lived for three years with the Dharma Initiative. They then attempt to change past events in order to prevent their Oceanic plane from crashing in the future.
In the sixth and final season, the main storyline follows the survivors, reunited in the present day. Following the demise of Jacob, the island's protector, the survivors are up against the Man in Black, known previously as the Smoke Monster. A "flash-sideways" narrative also follows the lives of the main characters in a setting where Oceanic 815 never crashed. In the final episodes, a flashback to the distant past shows the origins of the island's power and of the conflict between Jacob and the Man in Black, who are revealed to be twin brothers. One survivor becomes the successor to Jacob as caretaker of the island, and kills the Man in Black in a final showdown, with the island at stake. A small handful of survivors escape on the Ajira plane. It is implied that a few survivors return home later, while others remain living happily on the island. The series finale reveals that the flash-sideways timeline is actually a form of limbo, where some of the survivors and other characters from the island are reunited after having died because their time on the island had been the most important part of their existence. In the end the survivors are all reunited in a church where they "move on" together.
Episodes of Lost include a number of mysterious elements ascribed to science fiction or supernatural phenomena. The creators of the series refer to these elements as composing the mythology of the series, and they formed the basis of fan speculation.[5] The show's mythological elements include a "Monster" that roams the island, a mysterious group of inhabitants the survivors called "The Others," a scientific organization called the Dharma Initiative that placed several research stations on the island, a sequence of numbers that frequently appears in the lives of the characters in the past, present and future, and personal connections (synchronicity) between the characters they are often unaware of.
At the heart of the series is a complex and cryptic storyline, which spawned numerous questions and discussions among viewers.[6] Encouraged by Lost's writers and stars, who often interacted with fans online, viewers and TV critics alike took to widespread theorizing in an attempt to unravel the mysteries. Theories mainly concerned the nature of the island, the origins of the "Monster" and the "Others," the meaning of the numbers, and the reasons for both the crash and the survival of some passengers.[6] Several of the more common fan theories were discussed and rejected by the show's creators, the most common being that the survivors of Oceanic flight 815 are dead and in purgatory. Lindelof rejected speculation that spaceships or aliens influenced the events on the island, or that everything seen was a fictional reality taking place in someone's mind. Carlton Cuse dismissed the theory that the island was a reality TV show and the castaways unwitting housemates[7] and Lindelof many times refuted the theory that the "Monster" was a nanobot cloud similar to the one featured in Michael Crichton's novel Prey (which happened to share the protagonist's name, Jack).[8]
There are several recurring elements and motifs on Lost, which generally have no direct effect on the story itself, but expand the show's literary and philosophical subtext. These elements include frequent appearances of black and white, which reflect the dualism within characters and situations; as well as rebellion in almost all characters, especially Kate;[9] dysfunctional family situations (especially ones that revolve around the fathers of many characters), as portrayed in the lives of nearly all the main characters;[10] apocalyptic references, including Desmond's pushing the button to forestall the end of the world; coincidence versus fate, revealed most apparently through the juxtaposition of the characters Locke and Mr. Eko; conflict between science and faith, embodied by the leadership tug-of-war between Jack and Locke;[11] and references to numerous works of literature, including mentions and discussions of particular novels.[12] There are also many allusions in characters' names to famous historical thinkers and writers, such as Ben Linus (after chemist Linus Pauling), John Locke (after the philosopher) and his alias Jeremy Bentham (after the philosopher), Danielle Rousseau (after philosopher Jean-Jacques Rousseau), Desmond David Hume (after philosopher David Hume), Juliet's ex-husband (after philosopher Edmund Burke), Mikhail Bakunin (after the anarchist philosopher), Daniel Faraday (after physicist Michael Faraday), Eloise Hawking (after physicist Stephen Hawking), George Minkowski (after mathematician Hermann Minkowski), Richard Alpert (the birth name of spiritual teacher Ram Dass), Boone Carlyle (after Daniel Boone, American pioneer), Charlotte Staples Lewis (after author Clive Staples Lewis C. S. Lewis).[13]
Of the 324 people on board Oceanic Flight 815,[14] there are 70 initial survivors (as well as one dog) spread across the three sections of the plane crash.[15][16][17] Although a large cast made Lost more expensive to produce, the writers benefited from added flexibility in story decisions.[18] According to series executive producer Bryan Burk, "You can have more interactions between characters and create more diverse characters, more back stories, more love triangles."[18]
Lost was planned as a multi-cultural show with an international cast. The initial season had 14 regular speaking roles that received star billing. Matthew Fox played the protagonist, a troubled surgeon named Jack Shephard. Evangeline Lilly portrayed fugitive and love interest Kate Austen. Jorge Garcia played Hugo "Hurley" Reyes, an unlucky lottery winner. Josh Holloway played a con man, James "Sawyer" Ford. Ian Somerhalder played Boone Carlyle, chief operating officer of his mother's wedding business. Maggie Grace played his stepsister Shannon Rutherford, a former dance teacher. Harold Perrineau portrayed construction worker and aspiring artist Michael Dawson, while Malcolm David Kelley played his young son, Walt Lloyd. Terry O'Quinn played the mysterious John Locke. Naveen Andrews portrayed former Iraqi Republican Guard Sayid Jarrah. Emilie de Ravin played a young Australian mother-to-be, Claire Littleton. Yunjin Kim played Sun-Hwa Kwon, the daughter of a powerful Korean businessman and mobster, with Daniel Dae Kim as her husband and father's enforcer Jin-Soo Kwon. Dominic Monaghan played English ex-rock star drug addict Charlie Pace.
During the first two seasons, some characters were written out, while new characters with new stories were added.[19][20] Boone Carlyle was written out near the end of season one,[21] and Kelley became a guest star making occasional appearances throughout season two after Walt is captured by the Others in the season one finale. Shannon's departure eight episodes into season two made way for newcomers Mr. Eko, a Nigerian fake Catholic priest and former criminal played by Adewale Akinnuoye-Agbaje; Ana Lucia Cortez, an airport security guard and former police officer played by Michelle Rodriguez; and Libby Smith, a purported clinical psychologist portrayed by Cynthia Watros. Ana Lucia and Libby were written out of the series toward the end of season two after being shot by Michael, who then left the island along with his son.[22]
In season three, two actors were promoted from recurring to starring roles: Henry Ian Cusick as former Scottish soldier Desmond Hume, and Michael Emerson as the manipulative leader of the Others, Ben Linus. In addition, three new actors joined the regular cast: Elizabeth Mitchell, as fertility doctor and Other Juliet Burke, and Kiele Sanchez and Rodrigo Santoro as background survivor couple Nikki Fernandez and Paulo. Several characters died throughout the season; Eko was written out early on when Akinnuoye-Agbaje did not wish to continue on the show,[23][24] Nikki and Paulo were buried alive mid-season after poor fan response,[25] and Charlie was written out in the third season finale.
In season four, Harold Perrineau rejoined the main cast to reprise the role of Michael, now suicidal and on a desperate redemptive journey to atone for his previous crimes.[26] Along with Perrineau, additional new actors Jeremy Davies as Daniel Faraday, a nervous physicist who takes a scientific interest in the island; Ken Leung as Miles Straume, a sarcastic supposed ghost whisperer, and Rebecca Mader as Charlotte Staples Lewis, a hard-headed and determined anthropologist and successful academic joined the cast.[27] Michael was written out in the fourth season finale.[28] Claire, who mysteriously disappears with her dead biological father near the end of the season, did not return as a series regular for the fifth season, but returned for the sixth and final season.[29]
In season five, no new characters joined the main cast, however several characters exited the show: Charlotte was written out early in the season in episode five, with Daniel being written out later in the antepenultimate episode. Season six saw several cast changes; Juliet was written out in the season premiere while three previous recurring characters were upgraded to starring status.[30] These included Nestor Carbonell as mysterious, age-less Other Richard Alpert, Jeff Fahey as pilot Frank Lapidus[31] and Zuleikha Robinson as Ajira Airways Flight 316 survivor Ilana Verdansky. Additionally, former cast members Ian Somerhalder, Dominic Monaghan, Rebecca Mader, Jeremy Davies, Elizabeth Mitchell, Maggie Grace,[32] Michelle Rodriguez,[33] Harold Perrineau and Cynthia Watros[34] made return appearances.
Numerous supporting characters have been given expansive and recurring appearances in the progressive storyline. Danielle Rousseau (Mira Furlan), a French member of an earlier scientific expedition to the island first encountered as a voice recording in the pilot episode, appears throughout the series; she is searching for her daughter, who later turns up in the form of Alex Rousseau (Tania Raymonde). Cindy (Kimberley Joseph), an Oceanic flight attendant who first appeared in the pilot, survived the crash and subsequently became one of the Others. In the second season, married couple Rose Henderson (L. Scott Caldwell) and Bernard Nadler (Sam Anderson), separated on opposite sides of the island (she with the main characters, he with the tail section survivors) were featured in a flashback episode after being reunited. Corporate magnate Charles Widmore (Alan Dale) has connections to both Ben and Desmond. Desmond is in love with Widmore's daughter Penelope "Penny" Widmore (Sonya Walger). The introduction of the Others featured Tom aka Mr. Friendly (M. C. Gainey) and Ethan Rom (William Mapother) all of whom have been shown in both flashbacks and the ongoing story. Jack's father Christian Shephard (John Terry) has appeared in multiple flashbacks of various characters. In the third season, Naomi Dorrit (Marsha Thomason), parachutes onto the island, the team leader of a group hired by Widmore to find Benjamin Linus. One member of her team includes the ruthless mercenary Martin Keamy (Kevin Durand). In the finale episode "The End," recurring guest stars Sam Anderson, L. Scott Caldwell, Francois Chau, Fionnula Flanagan, Sonya Walger, and John Terry were credited under the "starring" rubric alongside the principal cast. The mysterious, black, smoke cloud-like entity known as "the Monster" appeared in human form during season five and six as a middle-aged man dressed in black robes known as "The Man in Black" played by Titus Welliver, and in season six, it appears in the form of John Locke played by O'Quinn in a dual role. His rival, Jacob, was played by Mark Pellegrino.
Lost was produced by ABC Studios, Bad Robot Productions and Grass Skirt Productions. Throughout its run, the executive producers of the series were Damon Lindelof, J.J. Abrams, Bryan Burk, Carlton Cuse, Jack Bender, Jeff Pinkner, Edward Kitsis, Adam Horowitz, Jean Higgins and Elizabeth Sarnoff, with Lindelof and Cuse serving as showrunners.
The series was conceived by Lloyd Braun, head of ABC at the time, while he was on vacation in Hawaii during 2003.[35] Braun ordered an initial script from Spelling Television based on his concept of a cross between the novel Lord of the Flies, the movie Cast Away, the television series Gilligan's Island, and the popular reality show Survivor, which began script development for Lost. ABC had also premiered a short-lived series about plane crash survivors in 1969 called The New People with the opening episode by Rod Serling. Gadi Pollack notes that some of "the influences of Lost came from...the game Myst."[36] Though never officially mentioned some striking similarities with the 1976 series The Lost Islands can be found.
Jeffrey Lieber was hired and wrote Nowhere, based on his pitch to write the pilot. Unhappy with the result and a subsequent rewrite, Braun contacted J. J. Abrams in January 2004, who had a deal with Touchstone Television (now ABC Studios), and was also the creator of the TV series Alias, to write a new pilot script. Lieber would later receive a story credit for the Lost pilot, and subsequently shared the "created by" credit with Abrams and Lindelof, after a request for arbitration at the Writer's Guild of America.[37]
Although initially hesitant, Abrams warmed to the idea on the condition that the series would have a supernatural angle to it, and collaborated with Damon Lindelof to create the series' style and characters.[38] Together, Abrams and Lindelof also created a series "bible," and conceived and detailed the major mythological ideas and plot points for an ideal four to five season run for the show.[39][40] Following the commercial success of the show, the network ABC requested to the pair that the length of the series be extended to include further seasons.[41] Abrams withdrew from production of Lost partway through the first season to direct Mission: Impossible III,[42] leaving Lindelof and new executive producer Carlton Cuse to develop much of the overall mythology of the series themselves.[43] The development of the show was constrained by tight deadlines, as it had been commissioned late in the 2004 season's development cycle. Despite the short schedule, the creative team remained flexible enough to modify or create characters to fit actors they wished to cast.[44]
Lost's two-part pilot episode was the most expensive in the network's history, reportedly costing between US$10 and $14million,[45] compared to the average cost of an hour-long pilot in 2005 of $4million.[46] The series debuted on September 22, 2004, becoming one of the biggest critical and commercial successes of the 2004 television season. Along with fellow new series Desperate Housewives and Grey's Anatomy, Lost helped to reverse the flagging fortunes of ABC.[47] Yet, before it had even been aired, Lloyd Braun was fired by executives at ABC's parent company, Disney, partly because of low ratings at the network and also because he had greenlighted such an expensive and risky project.[38] The world premiere of the pilot episode was on July 24, 2004 at Comic-Con International in San Diego.[48]
Many of the first season roles were a result of the executive producers' liking of various actors. The main character Jack was originally going to die in the pilot, and the role was planned for Michael Keaton. However, ABC executives were adamant that Jack live.[49] Before it was decided that Jack would live, Kate was to emerge as the leader of the survivors; she was originally conceived as a middle-aged businesswoman whose husband had apparently died in the crash, a role later fulfilled by the recurring character Rose. Dominic Monaghan auditioned for the role of Sawyer, who at the time was supposed to be a slick suit-wearing city con man. The producers enjoyed Monaghan's performance and changed the character of Charlie, originally an over-the-hill former rock star, to fit him. Jorge Garcia also auditioned for Sawyer, and the part of Hurley was written for him. When Josh Holloway auditioned for Sawyer, the producers liked the edge he brought to the character (he reportedly kicked a chair when he forgot his lines and got angry in the audition) and his southern accent, so they changed Sawyer to fit Holloway's acting. Yunjin Kim auditioned for Kate, but the producers wrote the character of Sun for her and the character of Jin, portrayed by Daniel Dae Kim, to be her husband. Sayid, played by Naveen Andrews, was also not in the original script. Locke and Michael were written with their actors in mind. Emilie de Ravin, who plays Claire, was originally cast in what was supposed to be a recurring role.[49] In the second season, Michael Emerson was contracted to play Ben ("Henry Gale") for three episodes. His role was extended to eight episodes because of his acting skills, and eventually for the whole of season three and later seasons.[50]
Lost was filmed on Panavision 35 mm cameras almost entirely on the Hawaiian island of Oahu due to the wide range of diverse filming locations available in close range. The original island scenes for the pilot were filmed at Mokul'ia Beach, near the northwest tip of the island. Later beach scenes take place in secluded spots of the famous North Shore. Cave scenes in the first season were filmed on a sound stage built at a Xerox parts warehouse, which had been empty since an employee mass shooting took place there in 1999.[51] The sound-stage and production offices have since moved to the Hawaii Film Office-operated Hawaii Film Studio,[52] where the sets depicting Season 2's "Swan Station" and Season 3's "Hydra Station" interiors were built.[53]
Various urban areas in and around Honolulu are used as stand-ins for locations around the world, including California, New York, Iowa, Miami, South Korea, Iraq, Nigeria, United Kingdom, Paris, Thailand, Berlin, Maldives and Australia. For example, scenes set in a Sydney Airport were filmed at the Hawaii Convention Center, while a World War II-era bunker was used as both an Iraqi Republican Guard installation and a Dharma Initiative research station. Scenes set in Germany during the winter were filmed at the Bernice P. Bishop Museum, with crushed ice scattered everywhere to create snow and Russian storeshop and automobile signs on the street. Several scenes in the Season 3 finale, "Through the Looking Glass," were shot in Los Angeles, including a hospital set borrowed from Grey's Anatomy. Two scenes during season four were filmed in London because Alan Dale who portrays Widmore was at the time performing in the musical Spamalot and was unable to travel to Hawaii.[54] Extensive archives of filming locations are tracked at a repository at the Lost Virtual Tour.[55]
During its six years of broadcasting, Lost developed an extensive collection of promotional tools ranging from the traditional promotions of the TV show made by the channel, to the creation of alternate reality games such as the Lost Experience.[56] Lost showed innovation in the use of new advertising strategies in the sector and the transformation of the conventional devices used previously. With this aim, the creators of the promotional campaign of Lost not only understood the importance of the implications of the fandom of the show in advertising actions, but also converted the internet in the strategic center from which each and every one of the advertising devices start.[56]
Lost features an orchestral score performed by the Hollywood Studio Symphony Orchestra and composed by Michael Giacchino, incorporating many recurring themes for subjects such as events, locations and characters. Giacchino achieved some of the sounds for the score using unusual instruments, such as striking suspended pieces of the plane's fuselage.[57] On March 21, 2006, the record label Varse Sarabande released the original television soundtrack for Lost's first season.[58] The soundtrack included select full-length versions of the most popular themes of the season and the main title, which was composed by series creator J. J. Abrams.[58] Varse Sarabande released a soundtrack featuring music from season 2 of Lost on October 3, 2006.[59] The soundtrack for season 3 was released on May 6, 2008, the soundtrack for season 4 was released on May 11, 2009, the soundtrack for season 5 was released on May 11, 2010 and the soundtrack for the final season was released on September 14, 2010. A final soundtrack, featuring music from series finale, was released on October 11, 2010.
The series uses pop culture songs sparingly, and used a mainly orchestral score (consisting usually of divided Strings, Percussion, Harp and 3 Trombones.) When it features pop songs, they often originate from a diegetic source. Examples include the various songs played on Hurley's portable CD player throughout the first season (until its batteries died in the episode "...In Translation," which featured Damien Rice's "Delicate), or the use of the record player in the second season, which included Cass Elliot's "Make Your Own Kind of Music," and Petula Clark's "Downtown" in the second and third season premieres respectively. Two episodes show Charlie on a street corner playing guitar and singing the Oasis song "Wonderwall." In the third season's finale, Jack drives down the street listening to Nirvana's "Scentless Apprentice," right before he arrives to the Hoffs/Drawlar Funeral Parlor, and in the parallel scene in the fourth season's finale he arrives listening to "Gouge Away" by Pixies. The third season also used Three Dog Night's "Shambala" on two occasions in the van. The only two pop songs that have ever been used without an on-screen source (i.e., non-diegetic) are Ann-Margret's "Slowly," in the episode "I Do" and "I Shall Not Walk Alone," written by Ben Harper and covered by The Blind Boys of Alabama in the episode "Confidence Man." Alternate music is used in several international broadcasts. For instance, in the Japanese broadcast of Lost, the theme song used varies by season; season one uses "Here I Am" by Chemistry, season two uses "Losin'" by Yuna Ito, and season three uses "Lonely Girl" by Crystal Kay.
Lost originally aired on the American Broadcasting Company (ABC) from September 22, 2004 to May 23, 2010. The pilot episode had 18.6million viewers, easily winning its 9/8 central timeslot, and giving ABC its strongest ratings since 2000 when Who Wants to Be a Millionaire was initially airedbeaten only the following month by the premiere of Desperate Housewives. According to Variety, "ABC sure could use a breakout drama success, as it hasn't had a real hit since The Practice. Lost represents the network's best start for a drama with 18- to 49-year-olds since Once and Again in 1999, and in total viewers since Murder One in 1995."[78]
For its first season, Lost averaged 16million viewers, ranking 14th in viewership among prime-time shows, and 15th among the eighteen to forty-nine-year-old demographic.[79] Its second season fared equally well: again, Lost ranked 14th in viewership, with an average of 15.5million viewers. However, it improved its rating with 18- to 49-year-olds, ranking eighth.[80] The second season premiere was even more viewed than the first, pulling in over 23million viewers and setting a series record.[81] The third season premiere brought in 18.8million viewers. The seventh episode of the season, back from a three-month hiatus, saw a drop to 14.5million. Over the course of the spring season, ratings would plunge to as low as 11million viewers before recovering to near 14million for the season finale. The ratings drop was partially explained when Nielsen released DVR ratings, showing Lost as the most recorded series on television. However, despite overall ratings losses, Lost still won its hour in the crucial 1849 demographic and put out the highest 1849 numbers in the 10:00p.m. time slot ahead of any show on any network that season. The fourth season premiere saw an increase from the previous episode to 16.1million viewers,[82] though by the eighth episode, viewers had decreased to a series low of 11.461million.[82] A survey of 20 countries by Informa Telecoms and Media in 2006 concluded that Lost was the second most popular TV show in those countries, after CSI: Miami.[83] The sixth-season premiere was the first to climb in the ratings year-over-year since the second season, drawing 12.1million viewers.[84] Lost was declared the highest rated show for the first ten years of IMDb.com Pro (20022012).[85]
Capping its successful first season, Lost won the Emmy Award for Outstanding Drama Series[86] and J. J. Abrams was awarded an Emmy in September 2005 for his work as the director of "Pilot". Terry O'Quinn and Naveen Andrews were nominated in the Outstanding Supporting Actor in a Drama Series category.[4] Lost swept the guild awards in 2005, winning the Writers Guild of America Awards 2005 for Outstanding Achievement in Writing for a Dramatic Television Series,[87] the 2005 Producers Guild Award for Best Production,[88] the 2005 Director's Guild Award for Best Direction of a Dramatic Television Program,[89] and the Screen Actors Guild Awards 2005 for Best Ensemble Cast.
It was nominated for a Golden Globe Award for Best Television Drama Series three times (20052007), and it won the award in 2006.[90] In 2006, Matthew Fox and Naveen Andrews received Golden Globe nominations for Best Lead Actor in a Drama Series[90] and Best Supporting Actor[90] respectively, and in 2007, Evangeline Lilly received a nomination for Best Actress in a Television Drama Series.[90] Lost was nominated for the 2005 British Academy of Film and Television Award for Best International.[91] In 2006, Jorge Garcia and Michelle Rodriguez took home ALMA Awards for Best Supporting Actor and Actress, respectively, in a Television Series.[92] It won the Saturn Award for Best Television Series in both 2005 and 2006.[93] In, 2005 Terry O'Quinn won a Saturn Award for Best Supporting Actor in a television series, and in 2006,[93] Matthew Fox won for Best Lead Actor.[93] Lost won consecutive Television Critics Association Awards for Outstanding Achievement in Drama, for both its first and second seasons.[94] Consecutively as well, it won in 2005 and 2006 the Visual Effects Society Award for Outstanding Supporting Visual Effects in a Broadcast Program.[95][96] Malcolm David Kelley won a Young Artist Award for his performance as Walt in 2006.[97]
In 2005, Lost was voted Entertainment Weekly's Entertainer of the Year. The show won a 2005 Prism Award for Charlie's drug storyline in the episodes "Pilot," "House of the Rising Sun," and "The Moth".[98] In 2007 Lost was listed as one of Time magazine's "100 Best TV Shows of All-TIME."[99] The series was nominated for but did not win a Writer's Guild Award and Producer's Guild Award again in 2007.[100] In June 2007, Lost beat out over 20 nominated television shows from countries all over the globe to win the Best Drama award at the Monte Carlo Television Festival. In September 2007 both Michael Emerson and Terry O'Quinn were nominated for an Emmy Award for Outstanding Supporting Actor in a Drama Series, the award going to O'Quinn.[101] Lost was again nominated for Outstanding Drama Series at the 60th Primetime Emmy Awards in 2008. The show also garnered seven other Emmy nominations, including Outstanding Supporting Actor in a Drama Series for Michael Emerson.[4] In 2009, Lost was again nominated for Outstanding Drama Series, as well Outstanding Supporting Actor in a Drama Series for Michael Emerson at the 61st Primetime Emmy Awards, of which the latter won.[4]
In 2010, the sixth and final season was nominated for twelve Emmy Awards at the 62nd Primetime Emmy Awards including Outstanding Drama Series, Outstanding Writing for a Drama Series for Carlton Cuse and Damon Lindelof for the show's series finale, "The End," Outstanding Lead Actor in a Drama Series for Matthew Fox, Outstanding Supporting Actor in a Drama Series for Michael Emerson and Terry O'Quinn and Outstanding Guest Actress in a Drama Series for Elizabeth Mitchell, it won only one Emmy (Outstanding Single-Camera Picture Editing) out of its twelve nominations for a series total of 11 wins and 55 nominations in its six year run.[102]
Lost has been described by many critics as one of the greatest television series of all time.[99][103][104] Bill Carter, television reporter of The New York Times, defined Lost as "the show with perhaps the most compelling continuing story line in television history."[105] Entertainment Weekly put the show on its end-of-the-decade, "best-of" list, saying, "Plane crash. Smoke monster. Polar bear. Crazy French lady. The Others. The hatch. The Dharma Initiative. Time-travel flashes. Name another network drama that can so wondrously turn a? into a!"[106] In 2012, Entertainment Weekly also listed the show at #10 in the "25 Best Cult TV Shows from the Past 25 Years," with a hot-and-cold description:
The first season received critical acclaim. USA Today said it was a "totally original, fabulously enjoyable lost-at-sea series, Lost had taken "an outlandish Saturday-serial setup and imbued it with real characters and honest emotions, without sacrificing any of the old-fashioned fun."[108] The Los Angeles Times praised the production values and said it "knows the buttons it wants to push (fear of flying, fear of abandonment, fear of the unknown) and pushes them, repeatedly, like a kid playing a video game."[109] IGN noted that the first season "succeeded first and foremost in character development."[110] Lost season one was ranked number one in the "Best of 2005 TV Coverage: Critic Top Ten Lists" by Matthew Gilbert of The Boston Globe, Tom Gliatto of People Weekly, Charlie McCollum of the San Jose Mercury News and Robert Bianco of USA Today.[111]
The second season received favorable reviews, but it was noted that the season "stumbled with some storylines going nowhere and some characters underutilized." IGN also noted the addition of Desmond Hume as a standout new character.[112] The San Francisco Chronicle called Season 2 an "extended, mostly unsatisfying foray into deeper mythology with very little payoff."[113] After winning "Best Drama Series" for season one, Lost was snubbed by the Emmy Awards in Season 2. Nearing the end of the second season, USA Today listed the most popular fan theories during Season 2  the island as a psychological experiment, that the hatch had electromagnetic properties, string theory of time, and that everyone on the island had developed a "collective consciousness" that allowed them to appear in each other's past. One fan interview by USA Today said that "Real suspense comes from answers, not questions. Suspense comes not from wondering what's going on but from wondering what happens next. If you withhold answers, it becomes impossible to satisfy."[114]
The first block of episodes of the third season was criticized for raising too many mysteries[115] and not providing enough answers.[116] Complaints were also made about the limited screen-time for many of the main characters in the first block.[117] Locke, played by Terry O'Quinn, who had tied for the highest second season episode count, appeared in only 13 of 22 episodes in the third seasononly two more than guest star M.C. Gainey, who played Tom. Reaction to two new characters, Nikki and Paulo, was generally negative, with Lindelof even acknowledging that the couple was "universally despised" by fans.[118] The decision to split the season and the American timeslot switch after the hiatus were also criticized.[119][120] Cuse acknowledged that, "No one was happy with the six-episode run."[121] The second block of episodes was critically acclaimed however,[122] with the crew dealing with problems from the first block.[123] More answers were written into the show,[124] and Nikki and Paulo were killed off.[125] It was also announced that the series would end three seasons after the third season,[126] which Cuse hoped would tell the audience that the writers knew where the story was going.[127]
The fourth season opened to critical acclaim not seen since the first season. Metacritic gave season four a weighted average of 87 based on the impressions of a select twelve critical reviews,[128] earning the second highest Metascore in the 20072008 television season after the fifth and final season of HBO's The Wire.[129] For the first time since season one, Lost received an Emmy nomination for 'Outstanding Drama Series'. Tim Goodman of the San Francisco Chronicle said that the Season 4 episodes were "roller coasters of fast action and revelation" and that series was "back on track".[113] In a survey conducted by TVWeek of professional critics, Lost was voted the best show on television in the first half of 2008 "by a wide margin," apparently "crack[ing] the top five on nearly every critic's submission" and receiving "nothing but praise."[2] The New York Times said the show reveled in critiques of capitalism, using the fictional Mittelos Bioscience and the "malevolent British industrialist" character of Charles Widmore as examples. The critic also said that the show "in the dark business of exploring just how futile the modern search for peace, knowledge, recovery or profit really is." The critic did go on to say that series was not as "philosophically refined" as The Sopranos or The Wire but that it "has maximized the potential of narrative uncertainty and made it a beguiling constant".[130]
The fifth season once again received mostly positive critical reception. Season 5 was given a weighted average of 78 out of 100 by Metacritic. Variety said that "The ABC series remains one of primetime's most uncompromising efforts, and this year's latest wrinkle on flashbacks, flash-forwards and island-disappearing flashes of light does nothing to alter that perception."[131] Alan Sepinwall of The Newark Star Ledger said that Season May 5, finally be "a day of reckoning between those viewers who embrace the show's science-fiction trappings, and those who prefer not to think about them." Sepinwall also related that "I loved every minute. But I'm also a geek who read Ray Bradbury and Isaac Asimov growing up."[132] Heather Havrilesky of Salon.com criticized the use of time travel saying that "when a narrator brings magic or time travel or an act of God into the picture, then uses it without restraint, the story loses its anchor to real life." The critic also asked "Why does it matter what Locke and Richard Alpert and Daniel Faraday or anyone else does, when they all seem as clueless and unfettered from reality as we are as viewers? How can these characters have any concrete agenda or strategic approach or philosophical perspective on anything when the rug is pulled out from under them by another Act of God every few seconds?"[133] The New York Times also commented that "what has been most dispiriting about the current season is the show's willingness to abandon many of the larger and more compelling themes that grounded the elaborate plot: the struggles between faith and reason; the indictments of extreme capitalism, the futility of recovery. All that remains is the reductively limned battle between fate and free will largely playing out, now, in Jack Shephard's belief that returning to the island is his Destiny."[134] The A.V. Club said of the fifth season finale, "Me? I found the ending frustrating, but in a good way. This finale was entertaining as all get-out to me, and despite the occasional groaner moment, I think this may be Lost's most purposeful, surprising finale."[135]
Season Six opened to much hype and curiosity. The A.V. Club asked, "I'm guessing that one of the biggest fears of Lost fans as we ride out this sixth and final seasonbumps and allis that we're going to come to the end and find a big nothing in return for all we've invested in these characters. We don't just need answers, we need justifications. Why has whatever happened, happened? Who has called this particular meeting to order, and does it really matter who showed up?"[136] The episodes "Dr. Linus", "Ab Aeterno", "Happily Ever After", and "The Candidate" opened to highly positive critical reception while the third-to-last episode "Across the Sea" was the episode with the most negative reception.[137] The time spent at the Other's temple was criticized.[138] E! Online described the show as "lightning in a bottle" and picked it as "Top TV Drama of 2010."[139]
The series finale opened to highly polarized critical and fan reception. According to the web site Metacritic, "The End" received "generally favorable reviews" with a Metascore  a weighted average based on the impressions of 31 critical reviews  of 74 out of 100.[137] IGN reviewer Chris Carbot gave the finale a 10/10, tying it with the initial review of "Pilot, Part 1", "Through the Looking Glass", "The Constant" and "There's No Place Like Home, Parts 2 & 3" as the best reviewed episode of Lost. He described it as "one of the most enthralling, entertaining and satisfying conclusions I could have hoped for." Carbot also noted that the discussions about the episode may never end, saying "Lost may be gone, but it will hardly be forgotten."[140] Eric Deggans of the St. Petersburg Times also gave the finale a perfect score, stating "Sunday's show was an emotional, funny, expertly measured reminder of what Lost has really centered on since its first moments on the prime time TV landscape: faith, hope, romance and the power of redemption through belief in the best of what moves mankind."[141] Robert Bianco of USA Today rated the episode perfect as well, deeming the finale "can stand with the best any series has produced".[142] Hal Boedeker of Orlando Sentinel cited the finale being "a stunner."[143]
British newspapers The Guardian and The Daily Telegraph both reported that "The End" had received negative reviews and disappointed its viewers. Alan Sepinwall of Star-Ledger was less enthusiastic of the finale, stating "I'm still wrestling with my feelings about 'The End'... I thought most of it worked like gangbusters. ... But as someone who did spend at least part of the last six years dwelling on the questions that were unanswered  be they little things like the outrigger shootout or why The Others left Dharma in charge of the Swan station after the purge, or bigger ones like Walt  I can't say I found 'The End' wholly satisfying, either as closure for this season or the series. ... There are narrative dead ends in every season of 'Lost,' but it felt like season six had more than usual."[144] Mike Hale of The New York Times gave "The End" a mixed review, as the episode showed that the series was "shaky on the big picture  on organizing the welter of mythic-religious-philosophical material it insisted on incorporating into its plot  but highly skilled at the small one, the moment to moment business of telling an exciting story. Rendered insignificant ... were the particulars of what they had done on the island."[145] David Zurawik of the Baltimore Sun gave the episode a highly negative review, writing "If this is supposed to be such a smart and wise show, unlike anything else on network TV (blah, blah, blah), why such a wimpy, phony, quasi-religious, white-light, huggy-bear ending. ... Once Jack stepped into the church it looked like he was walking into a Hollywood wrap party without food or music  just a bunch of actors grinning idiotically for 10 minutes and hugging one another."[146]
As a mainstream cult television show, Lost has generated a dedicated and thriving international fan community. Lost fans, sometimes dubbed Lostaways[147] or Losties,[148] have gathered at Comic-Con International and conventions organized by ABC,[148][149] but have also been active in developing a large number of fan websites, including Lostpedia, and forums dedicated to the program and its related incarnations. Because of the show's elaborate mythology, its fansites have focused on speculation and theorizing about the island's mysteries, as well as on more typical fan activities such as producing fan fiction and videos, compiling episode transcripts, shipping characters, and collecting memorabilia.[150][151][152]
Anticipating fan interest and trying to keep its audience engrossed, ABC embarked on various cross-media endeavors, often using new media. Fans of Lost have been able to explore ABC-produced tie-in websites, tie-in novels, an official forum sponsored by the creative team behind Lost ("The Fuselage"), "mobisodes," podcasts by the producers, an official magazine, and an alternate reality game (ARG) "The Lost Experience."[153] An official fanclub was launched in the summer of 2005 through Creation Entertainment.[148]
Due to the show's popularity, references to it and elements from its story have appeared in parody and popular culture usage. These include appearances on television, such as on the series Will & Grace, Curb Your Enthusiasm, 30 Rock,[154] Scrubs, The Office, Family Guy, American Dad!, The Simpsons and The Venture Bros..[155] Lost is also featured as an Easter egg in several video games, including Dead Island, Half-Life 2: Episode Two, Fallout 3, Uncharted 2: Among Thieves, World of Warcraft, Just Cause 2, Batman: Arkham City, and Singularity.[156] Similarly, several songs have been published whose themes and titles were derived from the series, such as Moneen ("Don't Ever Tell Locke What He Can't Do"), Veil of Maya ("Namaste"), Cosmo Jarvis ("Lost"), Senses Fail ("Lost and Found" and "All the Best Cowboys Have Daddy Issues"), Gatsbys American Dream ("You All Everybody" and "Station 5: The Pearl"), and Punchline ("Roller Coaster Smoke"). Weezer named their eighth studio album Hurley after the character, with a photo of actor Jorge Garcia on the cover.[157]
After the episode "Numbers" aired on March 2, 2005, numerous people used the eponymous figures (4, 8, 15, 16, 23 and 42) as lottery entries. According to the Pittsburgh Tribune-Review, within three days, the numbers were tried over 500 times by local players.[158] By October 2005, thousands had tried them for the multi-state Powerball lottery.[159] A study of the Quebec Lottery showed that the sequence was the third most popular choice of numbers for lottery players, behind only the arithmetic sequences 123456 and 71421283542.[160] The issue came to attention after a Mega Millions drawing for a near-record US$380,000,000 jackpot on January 4, 2011 drew a series of numbers in which the three lowest numbers (4815) and the mega ball (42) matched four of the six numbers. The No. 42 is also the "Mega Number" in Hurley's "Mega Lotto" ticket. The players who played the combination won $150 each (or $118 in California).[161]
In addition to traditional terrestrial and satellite broadcasting, Lost is available from various online services, including Amazon Instant Video,[162] Hulu,[163] and Netflix.[164] It was one of the first series issued through Apple's iTunes Store beginning in October 2005.[165] On August 29, 2007, Lost became one of the first TV programs available for download in the UK iTunes Store.[166]
In April 2006, Disney announced that Lost would be available for free online in streaming format, with advertising, on ABC's website, as part of a two-month experiment of future distribution strategies. The trial, which ran from May to June 2006, caused a stir among network affiliates who were afraid of being cut out of advertising revenue. The streaming of Lost episodes direct from ABC's website was only available to viewers in the United States due to international licensing agreements.[167][168] In 2009, Lost was named the most watched show on the Internet based on viewers of episodes on ABC's website. The Nielsen Company reported that 1.425million unique viewers have watched at least one episode on ABC's website.[169]
The first season of Lost was released under the title Lost: The Complete First Season as a widescreen seven-disc Region 1 DVD box set on September 6, 2005, two weeks before the premiere of the second season. It was distributed by Buena Vista Home Entertainment. In addition to all the episodes that had been aired, it included several DVD extras such as episode commentaries, behind-the-scenes footage and making-of features as well as deleted scenes, deleted flashback scenarios and a blooper reel. The same set was released on November 30, 2005 in Region 4,[170] The season was first released split into two parts: the first twelve episodes of season 1 were available as a wide screen four-disc Region 2 DVD box set on October 31, 2005, while the remaining thirteen episodes of season 1 were released on January 16, 2006.[171] The DVD features available on the Region 1 release were likewise split over the two box sets. The first two seasons were released separately on Blu-ray Disc on June 16, 2009.[172]
The second season was released under the title Lost: The Complete Second Season  The Extended Experience as a wide screen seven-disc Region 1 DVD box set on September 5, 2006. The sets include several DVD extras, including behind the scenes footage, deleted scenes and a "Lost Connections" chart, which shows how all of the characters on the island are inter-connected.[173] Again, the season was initially delivered in two sets for Region 2: the first twelve episodes were released as a widescreen four-disc DVD box set on July 17, 2006, while the remaining episodes of season 2 were released as a four-disc DVD box set on October 2, 2006.[174] The set was released in Region 4 on October 4, 2006.
The third season was released under the title Lost: The Complete Third Season  The Unexplored Experience on DVD and Blu-ray in Region 1 on December 11, 2007.[175] As with seasons 1 and 2, the third season release includes audio commentaries with the cast and crew, bonus featurettes, deleted scenes, and bloopers. The third season was released in Region 2 solely on DVD on October 22, 2007, though this time only as a complete set, unlike previous seasons.[176]
The fourth season was released as Lost: The Complete Fourth Season  The Expanded Experience in Region 1 on December 9, 2008 on both DVD and Blu-ray Disc.[177] It was released on DVD in Region 2 on October 20, 2008.[178] The set includes audio commentaries, deleted scenes, bloopers and bonus featurettes.
The first three seasons of Lost have sold successfully on DVD. The Season 1 boxset entered the DVD sales chart at number two in September 2005,[179] and the Season 2 boxset entered the DVD sales chart at the number one position in its first week of release in September 2006, believed to be the second TV-DVD ever to enter the chart at the top spot.[180] The Season 3 boxset sold over 1,000,000 copies in three weeks.[181]
Both the Season 6 boxset and the complete series collection contained a 12minute epilogue-like bonus feature called "The New Man in Charge."[182][183] The Season 6 DVD set entered the DVD sales chart at the number one position in its first week of release in September 2010 boasting strong sales in the DVD and Blu-ray format for the regular season set as well as for the series box set.[184]
The characters and setting of Lost have appeared in several official tie-ins outside of the television broadcast, including in print, on the Internet, and in short videos for mobile phones. Three novelizations have been released by Hyperion Books, a publisher owned by Disney, ABC's parent company. They are Endangered Species (ISBN 0-7868-9090-8) and Secret Identity (ISBN 0-7868-9091-6) both by Cathy Hapka and Signs of Life (ISBN 0-7868-9092-4) by Frank Thompson. Additionally, Hyperion published a metafictional book titled Bad Twin (ISBN 1-4013-0276-9), written by Laurence Shames,[185] and credited to fictitious author "Gary Troup," who ABC's marketing department claimed was a passenger on Oceanic Flight 815.
Several unofficial books relating to the show have also been published. Finding Lost: The Unofficial Guide (ISBN 1-55022-743-2) by Nikki Stafford and published by ECW Press is a book detailing the show for fans and those new to the show. What Can Be Found in Lost? (ISBN 0-7369-2121-4) by John Ankerberg and Dillon Burrough, published by Harvest House is the first book dedicated to an investigation of the spiritual themes of the series from a Christian perspective. Living Lost: Why We're All Stuck on the Island (ISBN 1-891053-02-7) by J. Wood,[186] published by the Garett County Press, is the first work of cultural criticism based on the series. The book explores the show's strange engagement with the contemporary experiences of war, (mis)information, and terrorism, and argues that the audience functions as a character in the narrative. The author also writes a blog column[187] during the second part of the third season for Powell's Books. Each post discusses the previous episode's literary, historical, philosophical and narrative connections.
The show's networks and producers have made extensive use of the Internet in expanding the background of the story. For example, during the first season, a fictional diary by an unseen survivor called "Janelle Granger" was presented on the ABC web site for the series. Likewise, a tie-in website about the fictional Oceanic Airlines appeared during the first season, which included several Easter eggs and clues about the show. Another tie-in website was launched after the airing of "Orientation" about the Hanso Foundation. In the UK, the interactive back-stories of several characters were included in "Lost Untold," a section of Channel 4's Lost website. Similarly, beginning in November 2005, ABC produced an official podcast, hosted by series writers and executive producers Damon Lindelof and Carlton Cuse. The podcast typically features a discussion about the weekly episode, interviews with cast members and questions from viewers.[188] Sky1 also hosted a podcast presented by Iain Lee on their website, which analyzed each episode after it aired in the United Kingdom.[189]
The foray into the online realm culminated in the Lost Experience, an Internet-based alternate reality game produced by Channel 7 (Australia), ABC (America) and Channel Four (UK), which began in early May 2006. The game presents a five-phase parallel storyline, primarily involving the Hanso Foundation.[190]
Short mini-episodes ("mobisodes") called the Lost Video Diaries were originally scheduled for viewing by Verizon Wireless subscribers via its V-Cast system, but were delayed by contract disputes.[191][192] The mobisodes were renamed Lost: Missing Pieces and aired from November 7, 2007 to January 28, 2008.
In addition to tie-in novels, several other products based on the series, such as toys and games, have been licensed for release. A video game, Lost: Via Domus, was released to average reviews, developed by Ubisoft, for game consoles and home computers,[193] while Gameloft developed a Lost game for mobile phones and iPods.[194] Cardinal Games released a Lost board game on August 7, 2006.[195] TDC Games created a series of four 1000-piece jigsaw puzzles ("The Hatch," "The Numbers," "The Others," and "Before the Crash"), which, when put together, reveal embedded clues to the overall mythology of Lost. Inkworks has published three sets of Lost trading cards, Season One, Season Two, and Revelations.[196] In May 2006, McFarlane Toys announced recurring lines of character action figures[197] and released the first series in November 2006, with the second series being released July 2007. Furthermore, ABC sold a myriad of Lost merchandise in their online store, including clothing, jewelry and other collectibles.[198] In November 2010, more than five months after the final episode aired, DK Publishing released a 400-page reference titled The Lost Encyclopedia, written by Tara Bennett and Paul Terry. The book compiled information from the TV show producers "writers bible," listing nearly every character, chronological event, location, and plot detail of the series, filling in the gaps for die hard fans.[199]
 Quotations related to Lost at Wikiquote  Media related to Lost at Wikimedia Commons
     
1 Synopsis

1.1 Overview
1.2 Mythology and interpretations
1.3 Recurring elements


1.1 Overview
1.2 Mythology and interpretations
1.3 Recurring elements
2 Cast and characters
3 Production

3.1 Conception
3.2 Casting
3.3 Filming
3.4 Promotion
3.5 Music


3.1 Conception
3.2 Casting
3.3 Filming
3.4 Promotion
3.5 Music
4 Impact and reception

4.1 Ratings
4.2 Awards
4.3 Critical reception
4.4 Fandom and popular culture


4.1 Ratings
4.2 Awards
4.3 Critical reception
4.4 Fandom and popular culture
5 Distribution

5.1 Online
5.2 Home video releases


5.1 Online
5.2 Home video releases
6 Other media

6.1 Licensed merchandise


6.1 Licensed merchandise
7 References
8 External links
1.1 Overview
1.2 Mythology and interpretations
1.3 Recurring elements
3.1 Conception
3.2 Casting
3.3 Filming
3.4 Promotion
3.5 Music
4.1 Ratings
4.2 Awards
4.3 Critical reception
4.4 Fandom and popular culture
5.1 Online
5.2 Home video releases
6.1 Licensed merchandise
Official website
Lost at the Internet Movie Database
Lost at TV.com
Ajira Airways fictional airline featured in season five promotional material
Lost University fictional university.
.
#(`*Toy Story*`)#.
Toy Story is an American computer animated family comedy film produced by Pixar Animation Studios and directed by John Lasseter. Released in 1995 by Walt Disney Pictures, Toy Story was the first feature length computer animated film and the first film produced by Pixar. Toy Story follows a group of anthropomorphic toys who pretend to be lifeless whenever humans are present, and focuses on Woody, a pullstring cowboy doll (Tom Hanks), and Buzz Lightyear, an astronaut action figure (Tim Allen). Woody feels profoundly threatened and jealous when Buzz supplants him as the top toy in the room. The film was written by John Lasseter, Andrew Stanton, Joel Cohen, Alec Sokolow, and Joss Whedon, and featured music by Randy Newman. Its executive producer was Steve Jobs with Edwin Catmull.
Pixar, who had been producing short animated films to promote their computers, was approached by Disney to produce a computer animated feature after the success of the short Tin Toy (1988), which is told from the perspective of a toy. Lasseter, Stanton, and Pete Docter wrote early story treatments which were thrown out by Disney, who pushed for a more edgy film. After disastrous story reels, production was halted and the script was re-written, better reflecting the tone and theme Pixar desired: that "toys deeply want children to play with them, and that this desire drives their hopes, fears, and actions."[2] The studio, then consisting of a relatively small number of employees, produced the film under minor financial constraints.[3][4]
The top-grossing film on its opening weekend,[5] Toy Story went on to earn over $361million worldwide.[1] Reviews were highly positive, praising both the technical innovation of the animation and the wit and sophistication of the screenplay,[6][7] and it is now widely considered by many critics to be one of the best animated films.[8][9][10][11][12][13][14] In addition to home media releases and theatrical re-releases, Toy Story-inspired material has run the gamut from toys, video games, theme park attractions, spin-offs, merchandise, and two sequelsToy Story 2 (1999) and Toy Story 3 (2010)both of which received massive commercial success and critical acclaim. Toy Story was selected into the National Film Registry as being "culturally, historically, or aesthetically significant" in 2005, its first year of eligibility.[15]
Woody is a pull-string cowboy doll and leader of a group of toys that belong to a boy named Andy Davis, which act lifeless when humans are present. With his family moving homes one week before his birthday, the toys stage a reconnaissance mission to discover Andy's new presents. Andy receives a space ranger Buzz Lightyear action figure, whose impressive features see him replacing Woody as Andy's favorite toy. Woody is resentful, especially as Buzz also gets attention from the other toys. However Buzz believes himself to be a real space ranger on a mission to return to his home planet, as Woody fails to convince him he is a toy.
Andy prepares for a family outing at the space themed Pizza Planet restaurant with Buzz. Woody attempts to be picked by misplacing Buzz. He intends to trap Buzz in a gap behind Andy's desk, but the plan goes disastrously wrong when he accidentally knocks Buzz out the window, resulting in him being accused of murdering Buzz out of jealousy. With Buzz missing, Andy takes Woody to Pizza Planet, but Buzz climbs into the car and confronts Woody when they stop at a gas station. The two fight and fall out of the car, which drives off and leaves them behind. Woody spots a truck bound for Pizza Planet and plans to rendezvous with Andy there, convincing Buzz to come with him by telling him it will take him to his home planet. Once at Pizza Planet, Buzz makes his way into a claw game machine shaped like a spaceship, thinking it to be the ship Woody promised him. Inside, he finds squeaky aliens who revere the claw arm as their master. When Woody clambers into the machine to rescue Buzz, the aliens force the two towards the claw and they are captured by Andys neighbor Sid Phillips, who finds amusement in torturing and destroying toys.
At Sid's house, the two attempt to escape before Andy's moving day, encountering Sids nightmarish toy creations and his vicious dog, Scud. Buzz sees a commercial for Buzz Lightyear action figures and realizes that he really is a toy. Attempting to fly to test this, Buzz falls and loses one of his arms, going into depression and unable to cooperate with Woody. Woody waves Buzzs arm from a window to seek help from the toys in Andys room, but they are horrified thinking Woody attacked him, while Woody realizes Sid's toys are friendly when they reconnect Buzz's arm. Sid prepares to destroy Buzz by strapping him to a rocket, but is delayed that evening by a thunderstorm. Woody convinces Buzz that life is worth living because of the joy he can bring to Andy, which helps Buzz regain his spirit. Cooperating with Sid's toys, Woody rescues Buzz and scares Sid away by coming to life in front of him, warning him to never torture toys again. Woody and Buzz then wave goodbye to the mutant toys and return home through a fence, but miss Andys car as it drives away to his new house.
Down the road, they climb onto the moving truck containing Andys other toys, but Scud chases them and Buzz tackles the dog to save Woody. Woody attempts to rescue Buzz with Andy's RC car but the other toys, who think Woody now got rid of RC, toss Woody off onto the road. Spotting Woody driving RC back with Buzz alive, the other toys realize their mistake and try to help. When RC's batteries become depleted, Woody ignites the rocket on Buzz's back and manages to throw RC into the moving truck before they soar into the air. Buzz opens his wings to cut himself free before the rocket explodes, gliding with Woody to land safely into a box in Andys car. Andy looks into it and is elated to have found his two missing toys.
On Christmas Day at their new house, Buzz and Woody stage another reconnaissance mission to prepare for the new toy arrivals, one of which is a Mrs. Potato Head, much to the delight of Mr. Potato Head. As Woody jokingly asks what might be worse than Buzz, the two share a worried smile as they discover Andy's new gift is a puppy.
Director John Lasseter's first experience with computer animation was during his work as an animator at Disney, when two of his friends showed him the lightcycle scene from Tron. It was an eye-opening experience which awakened Lasseter to the possibilities offered by the new medium of computer-generated animation.[16] Lasseter tried to pitch the idea of a fully computer animated film to Disney, but the idea was rejected and Lasseter was fired. He then went on to work at Lucasfilm and later as a founding member of Pixar, which was purchased by entrepreneur and Apple Computer founder Steve Jobs in 1986.[17] At Pixar, Lasseter created short, computer-animated films to show off the Pixar Image Computer's capabilities, and Tin Toy (1988) a short told from the perspective of a toy, referencing Lasseter's love of classic toys would go on to claim the 1988 Academy Award for animated short films, the first computer- generated film to do so.[18] Tin Toy gained Disney's attention, and the new team at DisneyCEO Michael Eisner and chairman Jeffrey Katzenberg in the film division began a quest to get Lasseter to come back.[18] Lasseter, grateful for Jobss faith in him, felt compelled to stay with Pixar, telling co-founder Ed Catmull, "I can go to Disney and be a director, or I can stay here and make history."[18] Katzenberg realized he could not lure Lasseter back to Disney and therefore set plans into motion to ink a production deal with Pixar to produce a film.[18]
Both sides were willing. Catmull and fellow Pixar co-founder Alvy Ray Smith had long wanted to produce a computer-animated feature.[19] In addition, The Walt Disney Company had licensed Pixar's Computer Animation Production System (CAPS), and that made it the largest customer for Pixars computers.[20] Jobs made it apparent to Katzenberg that although Disney was happy with Pixar, it was not the other way around: "We want to do a film with you," said Jobs. "That would make us happy."[20] At this same time, Peter Schneider, president of Walt Disney Feature Animation, was potentially interested in making a feature film with Pixar.[19] When Catmull, Smith and head of animation Ralph Guggenheim met with Schneider in the summer of 1990, they found the atmosphere to be puzzling and contentious. They later learned that Katzenberg intended that if Disney were to make a film with Pixar, it would be outside Schneider's purview, which aggravated Schneider.[21] After that first meeting, the Pixar contingent went home with low expectations and were surprised when Katzenberg called for another conference. Catmull, Smith and Guggenheim were joined by Bill Reeves (head of animation research and development), Jobs, and Lasseter. They brought with them an idea for a half-hour television special called A Tin Toy Christmas. They reasoned that a television program would be a sensible way to gain experience before tackling a feature film.[22]
They met with Katzenberg at a conference table in the Team Disney building at the company's headquarters in Burbank.[22] Catmull and Smith considered it would be difficult to keep Katzenberg interested in working with the company over time. They considered it even more difficult to sell Lasseter and the junior animators on the idea of working with Disney, who had a bad reputation for how they treated their animators, and Katzenberg, who had built a reputation as a micromanaging tyrant.[22] Katzenberg asserted this himself in the meeting: "Everybody thinks Im a tyrant. I am a tyrant. But Im usually right."[20] He threw out the idea of a half-hour special and eyed Lasseter as the key talent in the room: "John, since you won't come work for me, I'm going to make it work this way."[20][22] He invited the six visitors to mingle with the animators"ask them anything at all"and the men did so, finding they all backed up Katzenberg's statements. Lasseter felt he would be able to work with Disney and the two companies began negotiations.[23] Pixar at this time was on the verge of bankruptcy and needed a deal with Disney.[20] Katzenberg insisted that Disney be given the rights to Pixars proprietary technology for making 3-D animation, but Jobs refused.[23] In another case, Jobs demanded Pixar would have part ownership of the film and its characters, sharing control of both video rights and sequels, but Katzenberg refused.[20] Disney and Pixar reached accord on contract terms in an agreement dated May 3, 1991, and signed on in early July.[24] Eventually the deal specified that Disney would own the picture and its characters outright, have creative control, and pay Pixar about 12.5% of the ticket revenues.[25][26] It had the option (but not the obligation) to do Pixars next two films and the right to make (with or without Pixar) sequels using the characters in the film. Disney could also kill the film at any time with only a small penalty. These early negotiations would become a point of contention between Jobs and Eisner for many years.[20]
An agreement to produce a feature film based on Tin Toy with a working title of Toy Story was finalized and production began soon thereafter.[27]
The original treatment for Toy Story, drafted by Lasseter, Andrew Stanton, and Pete Docter, had little in common with the eventual finished film.[2] It paired Tinny, the one-man band from Tin Toy with a ventriloquist's dummy and sent them on a sprawling odyssey. The core idea of Toy Story was present from the treatment onward, however: that "toys deeply want children to play with them, and that this desire drives their hopes, fears, and actions."[2] Katzenberg felt the original treatment was problematic and told Lasseter to reshape Toy Story as more of an odd-couple buddy picture, and suggested they watch some classic buddy movies, such as The Defiant Ones and 48 Hrs., in which two characters with different attitudes are thrown together and have to bond.[28][29] Lasseter, Stanton, and Docter emerged in early September 1991 was the second treatment, and although the lead characters were still Tinny and the dummy, the outline of the final film was beginning to take shape.[28]
The script went through many changes before the final version. Lasseter decided Tinny was "too antiquated", and the character was changed to a military action figure, and then given a space theme. Tinny's name changed to Lunar Larry, then Tempus from Morph, and eventually Buzz Lightyear (after astronaut Buzz Aldrin).[30] Lightyear's design was modeled on the suits worn by Apollo astronauts as well as G.I. Joe action figures.[31][32] Woody the second character, was inspired by a Casper the Friendly Ghost doll that Lasseter had when he was a child. Originally Woody was a ventriloquist's dummy with a pull-string (hence the name Woody). However, character designer, Bud Luckey suggested that Woody could be changed to a cowboy ventriloquist dummy, John Lasseter liked the contrast between the Western genre and the Sci-Fi genre and the character immediately changed. Eventually all the ventriloquist dummy aspects of the character were deleted, because the dummy was designed to look "sneaky and mean."[33] However they kept the name Woody to pay homage to the Western Actor Woody Strode.[30] The story department drew inspiration from films such as Midnight Run and The Odd Couple,[34] and Lasseter screened Hayao Miyazaki's Castle in the Sky (1986) for further influence.[35]
Toy Story's script was strongly influenced by the ideas of screenwriter Robert McKee. The members of Pixar's story teamLasseter, Stanton, Docter and Joe Ranftwere aware that most of them were beginners at writing for feature films. None of them had any feature story or writing credits to their name besides Ranft, who had taught a story class at CalArts and did some storyboard work prior.[33] Seeking insight, Lasseter and Docter attended a three-day seminar in Los Angeles given by McKee. His principles, grounded in Aristotle's Poetics, dictated that a character emerges most realistically and compellingly from the choices that the protagonist makes in reaction to his problems.[35] Disney also appointed Joel Cohen, Alec Sokolow and, later, Joss Whedon to help develop the script. Whedon found that the script wasn't working but had a great structure, and added the character of Rex and sought a pivotal role for Barbie.[36] The story team continued to touch up the script as production was underway. Among the late additions was the encounter between Buzz and the alien squeak toys at Pizza Planet, which emerged from a brainstorming session with a dozen directors, story artists, and animators from Disney.[37]
Katzenberg gave approval for the script on January 19, 1993, at which point voice casting could begin.[38] Lasseter always wanted Tom Hanks to play the character of Woody. Lasseter claimed Hanks "... has the ability to take emotions and make them appealing. Even if the character, like the one in A League of Their Own, is down-and-out and despicable."[38] Billy Crystal was approached to play Buzz, but turned down the role, which he later regretted, although he would voice Mike Wazowski in Pixar's later success, Monsters, Inc..[39][40] Lasseter took the role to Tim Allen, who was appearing in Disney's Home Improvement, and he accepted.[41]
To gauge how an actor's voice would fit with a character, Lasseter borrowed a common Disney technique: animate a vocal monologue from a well-established actor to meld the actor's voice with the appearance or actions of the animated character.[36] This early test footage, using Hanks' voice from Turner & Hooch, convinced Hanks to sign on to the film.[38][42] Toy Story was both Hanks and Allen's first animated film role.[43]
Every couple of weeks, Lasseter and his team would put together their latest set of storyboards or footage to show Disney. In early screen tests, Pixar impressed Disney with the technical innovation but convincing Disney of the plot was more difficult. At each presentation by Pixar, Katzenberg would tear much of it up, giving out detailed comments and notes. Katzenbergs big push was to add more edginess to the two main characters.[29] Disney wanted the film to appeal to both children and adults, and asked for adult references to be added to the film.[38] After many rounds of notes from Katzenberg and other Disney execs, the general consensus was that Woody had been stripped of almost all charm.[29][41] Tom Hanks, while recording the dialogue for the story reels, exclaimed at one point that the character was a jerk.[29] Lasseter and his Pixar team had the first half of the movie ready to screen, so they brought it down to Burbank to show to Katzenberg and other Disney executives on November 19, 1993, a day they later dubbed "Black Friday."[4][38] The results were disastrous, and Schneider, who was never particularly enamored of Katzenbergs idea of having outsiders make animation for Disney, declared it a mess and ordered that production be stopped immediately.[44] Katzenberg asked colleague Tom Schumacher why the reels were bad. Schumacher replied bluntly: "Because its not their movie anymore."[4]
Lasseter was embarrassed with what was on the screen, later recalling, "It was a story filled with the most unhappy, mean characters that Ive ever seen." He asked Disney for the chance to retreat back to Pixar and rework the script in two weeks, and Katzenberg was supportive.[4] Lasseter, Stanton, Docter and Ranft delivered the news of the production shutdown to the production crew, many of whom had left other jobs to work on the project. In the meantime, the crew would shift to television commercials while the head writers worked out a new script. Although Lasseter kept morale high by remaining outwardly buoyant, the production shutdown was "a very scary time," recalled story department manager BZ Petroff.[45] Schneider had initially wanted to shutdown production altogether and fire all recently hired animators.[46] Katzenberg put the film under the wing of Disney Feature Animation. The Pixar team was pleased that the move would give them an open door to counsel from Disney's animation veterans. Schenider, however, continued to take a dim view of the project and would later go over Katzenberg's head to urge Eisner to cancel it.[28] Stanton retreated into a small, dark windowless office, emerging periodically with new script pages. He and the other story artists would then draw the shots on storyboards. Whedon came back to Pixar for part of the shutdown to help with revising, and the script was revised in two weeks as promised.[45] When Katzenberg and Schneider halted production on Toy Story, Steve Jobs kept the work going with his own personal funding. Jobs did not insert himself much into the creative process, respecting the artists at Pixar and instead managing the relationship with Disney.[4]
The Pixar team came back with a new script three months later, with the character of Woody morphed from being a tyrannical boss of Andys other toys to being their wise leader. It also included a more adult-oriented staff meeting amongst the toys rather than a juvenile group discussion that had existed in earlier drafts. Buzz Lightyear's character was also changed slightly "to make it more clear to the audience that he really doesn't realize he's a toy."[46] Katzenberg and Schneider approved the new approach, and by February 1994 the film was back in production.[4] The voice actors returned in March 1994 to record their new lines.[38] When production was greenlit, the crew quickly grew from its original size of 24 to 110, including 27 animators, 22 technical directors, and 61 other artists and engineers.[3][47] In comparison, The Lion King, released in 1994, required a budget of $45million and a staff of 800.[3] In the early budgeting process, Jobs was eager to produce the film as efficiently as possible, impressing Katzenberg with his focus on cost-cutting. Despite this, the $17 million production budget was proving inadequate, especially given the major revision that was necessary after Katzenberg had pushed them to make Woody too edgy. Jobs demanded more funds in order to complete the film right, and insisted that Disney was liable for the cost overruns. Katzenberg was not willing, and Ed Catmull, described as "more diplomatic than Jobs," was able to reach a compromise new budget.[4]
"We couldn't have made this movie in traditional animation. This is a story that can only really be told with three-dimensional toy characters. ... Some of the shots in this film are so beautiful."
Toy Story was the first fully computer animated feature film. Recruiting animators for Toy Story was brisk; the magnet for talent was not the pay, generally mediocre, but rather the allure of taking part in the first computer-animated feature.[47] Lasseter spoke on the challenges of the computer animation in the film: "We had to make things look more organic. Every leaf and blade of grass had to be created. We had to give the world a sense of history. So the doors are banged up, the floors have scuffs."[38] The film began with animated storyboards to guide the animators in developing the characters. 27 animators worked on the film, using 400 computer models to animate the characters. Each character was either created out of clay or was first modeled off of a computer-drawn diagram before reaching the computer animated design.[49] Once the animators had a model, articulation and motion controls were coded, allowing each character to move in a variety of ways, such as talking, walking, or jumping.[49] Of all of the characters, Woody was the most complex as he required 723 motion controls, including 212 for his face and 58 for his mouth.[38][50] The first piece of animation, a 30-second test, was delivered to Disney in June 1992 when the company requested a sample of what the film would look like. Lasseter wanted to impress Disney with a number of things in the test piece that could not be done in traditional, hand-drawn animation, such as Woody's plaid shirt or venetian blind shadows falling across the room.[33]
Every shot in the film passed through the hands of eight different teams. The art department gave a shot its color scheme and general lighting.[51] The layout department, under Craig Good, then placed the models in the shot, framed the shot by setting the location of the virtual camera, and programmed any camera moves. To make the medium feel as familiar as possible, they sought to stay within the limits of what might be done in a live-action film with real cameras, dollies, tripods and cranes.[51] From layout, a shot went to the animation department, headed by directing animators Rich Quade and Ash Brannon. Lasseter opted against Disney's approach of assigning an animator to work on a character throughout a film, but made certain exceptions in scenes where he felt acting was particularly critical.[51] The animators used the Menv program to set the character into a desired pose. Once a sequence of hand-built poses, or "keyframes", was created, the software would build the poses from the frames in-between.[52] The animators studied videotapes of the actors for inspiration, and Lasseter rejected automatic lip-syncing.[52] To sync the characters mouths and facial expressions to the actors' voices, animators spent a week per 8 seconds of animation.[49]
After this the animators would compile the scenes, and develop a new storyboard with the computer animated characters. Animators then added shading, lighting, visual effects, and finally used 300 computer processors to render the film to its final design.[49][50] The shading team, under Tom Porter, used RenderMan's shader language to create shader programs for each of a model's surfaces. A few surfaces in Toy Story came from real objects: a shader for the curtain fabric in Andy's room used a scan of actual cloth.[53] After animation and shading, the final lighting of the shot was orchestrated by the lighting team, under Galyn Susman and Sharon Calahan. The completed shot then went into rendering on a "render farm" of 117 Sun Microsystems computers that ran 24 hours a day.[37] Finished animation emerged in a steady drip of around three minutes a week.[54] Each frame took from 45 minutes up to 30 hours to render, depending on its complexity. In total, the film required 800,000 machine hours and 114,240 frames of animation.[38][49][55] There is over 77 minutes of animation spread across 1,561 shots.[51] A camera team, aided by David DiFrancesco, recorded the frames onto film stock. Toy Story was rendered at a mere 1,536 by 922 pixels, with each pixel corresponding to roughly a quarter inch of screen area on a typical cinema screen.[37] During post-production, the film was sent to Skywalker Sound where sound effects were mixed with the music score.[50]
Disney was concerned with Lasseter's position on the use of music. Unlike other Disney films of the time, Lasseter did not want the film to be a musical, saying it was a buddy film featuring "real toys." Joss Whedon agreed saying, "It would have been a really bad musical, because it's a buddy movie. It's about people who won't admit what they want, much less sing about it. ... Buddy movies are about sublimating, punching an arm, 'I hate you.' It's not about open emotion."[38] However, Disney favored the musical format, claiming "Musicals are our orientation. Characters breaking into song is a great shorthand. It takes some of the onus off what they're asking for."[38] Disney and Pixar reached a compromise: the characters in Toy Story would not break into song, but the film would use songs over the action, as in The Graduate, to convey and amplify the emotions that Buzz and Woody were feeling.[36] Disney tapped Randy Newman to compose the film. The edited Toy Story was due to Randy Newman and Gary Rydstrom in late September 1995 for their final work on the score and sound design, respectively.[56]
Lasseter claimed "His songs are touching, witty, and satirical, and he would deliver the emotional underpinning for every scene."[38] Newman developed the film's signature song "You've Got a Friend in Me" in one day[38] although the tune is closely based on his own song, "I Love to See You Smile" from the soundtrack to the 1989 film, Parenthood.
It was difficult for crew members to perceive the film's quality during much of the production process, when the finished footage was in scattered pieces and lacked elements like music and sound design.[54] Some animators felt the film would be a significant disappointment commercially, but felt animators and animation fans would find it interesting.[54] According to Lee Unkrich, one of the original editors of Toy Story, a scene was cut out of the original final edit. The scene features Sid, after Pizza Planet, torturing Buzz and Woody violently. Unkrich decided to cut right into the scene where Sid is interrogating the toys because the creators of the movie thought the audience would be loving Buzz and Woody at that point.[57] Another scene, where Woody was trying to get Buzz's attention when he was stuck in the box crate, was shortened because the creators felt it would lose the energy of the movie.[57] Peter Schneider had grown buoyant about the film as it neared completion, and announced a United States release date of November, coinciding with Thanksgiving weekend and the start of the winter holiday season.[58]
Sources indicate that executive producer Steve Jobs lacked confidence in the film during its production, and he had been talking to various companies, ranging from Hallmark to Microsoft, about selling Pixar.[4][58] However, as the film progressed, Jobs became ever more excited about it, feeling that he might be on the verge of transforming the movie industry.[4] As scenes from the movie were finished, he watched them repeatedly and had friends come by his home to share his new passion. Jobs decided that the release of Toy Story that November would be the occasion to take Pixar public.[4] A test audience near Anaheim in late July 1995 indicated the need for last-minute tweaks, which added further pressure to the already frenetic final weeks. Response cards from the audience were encouraging, but were not top of the scale, adding further question as to how audiences would respond.[56] The film ended with a shot of Andy's house and the sound of a new puppy. Michael Eisner, who attended the screening, told Lasseter afterward that the film needed to end with a shot of Woody and Buzz together, reacting to the news of the puppy.[56]
The soundtrack for Toy Story was produced by Walt Disney Records and was released on November 22, 1995, the week of the film's release. Scored and written by Randy Newman, the soundtrack has received praise for its "sprightly, stirring score".[60] Despite the album's critical success, the soundtrack only peaked at number 94 on the Billboard 200 album chart.[61] A cassette and CD single release of "You've Got a Friend in Me" was released on April 12, 1996, in order to promote the soundtrack's release.[59] The soundtrack was remastered in 2006 and although it is no longer available physically, the album is available for purchase digitally in retailers such as iTunes.[62]
All songs are written and composed by Randy Newman.
There were two premieres of Toy Story in November 1995. Disney organized one at El Capitan in Los Angeles, and built a fun house next door featuring the characters. Jobs did not attend and instead rented the Regency, a similar theater in San Francisco, and held his own premiere the next night. Instead of Tom Hanks and Steve Martin, the guests were Silicon Valley celebrities, such as Larry Ellison and Andy Grove. The dueling premieres highlighted a festering issue between the companies: whether Toy Story was a Disney or a Pixar movie.[63] "The audience appeared to be captivated by the film," wrote David Price in his 2008 book The Pixar Touch. "Adult-voiced sobs could be heard during the quiet moments after Buzz Lightyear fell and lay broken on the stairway landing."[64] Toy Story opened on 2,281 screens in in the United States on November 22, 1995 (before later expanding to 2,574 screens).[64] It was paired alongside a rerelease of a Roger Rabbit short called Rollercoaster Rabbit, while select prints contained The Adventures of Andr and Wally B..
The film was also shown at the Berlin Film Festival out of competition from February 15 to 26, 1996.[65] Elsewhere, the film opened in March 1996.[58]
Marketing for the film included $20million spent by Disney for advertising as well as advertisers such as Burger King, Pepsico, Coca-Cola, and Payless ShoeSource paying $125million in tied promotions for the film.[66] A marketing consultant reflected on the promotion: "This will be a killer deal. How can a kid, sitting through a one-and-a-half-hour movie with an army of recognizable toy characters, not want to own one?"[67] Despite this, the consumer products arm of Disney was slow to see the potential of Toy Story early on.[58] When the Thanksgiving release date was announced in January 1995, many toy companies were accustomed to having eighteen months to two years of runway time, and passed on the project. In February 1995, Disney took the idea to Toy Fair, a toy industry trade show in New York. There, a Toronto-based company with a factory based in China, Thinkaway Toys, became interested. Although Thinkaway was a small player in the industry, mainly producing toy banks in the form of film characters, it was able to scoop up the worldwide master license for Toy Story toys simply because no one else wanted it.[68] Buena Vista Home Video put a trailer for the film on seven million copies of the VHS re-release of Cinderella; the Disney Channel ran a television special on the making of Toy Story; Walt Disney World in Orlando held a daily Toy Story parade at Disney-MGM Studios.[56]
It was screenwriter Joss Whedon's idea to incorporate Barbie as a character who would rescue Woody and Buzz in the film's final act.[69] The idea was dropped after Mattel objected and refused to license the toy. Producer Ralph Guggenheim claimed that Mattel did not allow the use of the toy as "They [Mattel] philosophically felt girls who play with Barbie dolls are projecting their personalities onto the doll. If you give the doll a voice and animate it, you're creating a persona for it that might not be every little girl's dream and desire."[38] Hasbro likewise refused to license G.I. Joe (mainly because Sid was going to blow one up), but they did license Mr. Potato Head.[38] The only toy in the movie that was not currently in production was Slinky Dog, which was discontinued since the 1970s. When designs for Slinky were sent to Betty James (Richard James's Wife) she said that Pixar had improved the toy and that it was "cuter" than the original.[70]
On October 2, 2009, the film was re-released in Disney Digital 3-D.[71] The film was also released with Toy Story 2 as a double feature for a two-week run[72] which was extended due to its success.[73][74] In addition, the film's second sequel, Toy Story 3, was also released in the 3-D format.[71] Lasseter commented on the new 3-D re-release:
"The Toy Story films and characters will always hold a very special place in our hearts and we're so excited to be bringing this landmark film back for audiences to enjoy in a whole new way thanks to the latest in 3-D technology. With Toy Story 3 shaping up to be another great adventure for Buzz, Woody and the gang from Andy's room, we thought it would be great to let audiences experience the first two films all over again and in a brand new way."[75]
Translating the film into 3-D involved revisiting the original computer data and virtually placing a second camera into each scene, creating left-eye and right-eye views needed to achieve the perception of depth.[76] Unique to computer animation, Lasseter referred to this process as "digital archaeology."[76] The process took four months, as well as an additional six months for the two films to add the 3-D. The lead stereographer Bob Whitehill oversaw this process and sought to achieve an effect that affected the emotional storytelling of the film:
"When I would look at the films as a whole, I would search for story reasons to use 3-D in different ways. In 'Toy Story, for instance, when the toys were alone in their world, I wanted it to feel consistent to a safer world. And when they went out to the human world, that's when I really blew out the 3-D to make it feel dangerous and deep and overwhelming."[76]
Unlike other countries, the United Kingdom received the films in 3-D as separate releases. Toy Story was released on October 2, 2009. Toy Story 2 was instead released January 22, 2010.[77] The re-release performed well at the box office, opening with $12,500,000 in its opening weekend, placing at the third position after Zombieland and Cloudy with a Chance of Meatballs.[78] The double feature grossed $30,714,027 in its five-week release.[78]
"Yes, we worry about what the critics say. Yes, we worry about what the opening box office is going to be. Yes, we worry about what the final box office is going to be. But really, the whole point why we do what we do is to entertain our audiences. The greatest joy I get as a filmmaker is to slip into an audience for one of our movies anonymously, and watch people watch our film. Because people are 100 percent honest when they're watching a movie. And to see the joy on people's faces, to see people really get into our films...to me is the greatest reward I could possibly get."
Ever since its original 1995 release, Toy Story received universal acclaim from critics; Review aggregate Rotten Tomatoes (which gave the movie an "Extremely Fresh" rating) reports that 100% of critics have given the film a positive review based on 74 reviews, with an average score of 9/10. The critical consensus is: As entertaining as it is innovative, Toy Story kicked off Pixar's unprecedented run of quality pictures, reinvigorating animated film in the process. The film is Certified Fresh.[7] At the website Metacritic, which utilizes a normalized rating system, the film earned a "universal acclaim" level rating of 92/100 based on 16reviews by mainstream critics.[6] Reviewers hailed the film for its computer animation, voice cast, and ability to appeal to numerous age groups.
Leonard Klady of Variety commended the animation's "... razzle-dazzle technique and unusual look. The camera loops and zooms in a dizzying fashion that fairly takes one's breath away."[80] Roger Ebert of the Chicago Sun-Times compared the film's innovative animation to Disney's Who Framed Roger Rabbit, saying "Both movies take apart the universe of cinematic visuals, and put it back together again, allowing us to see in a new way."[81] Due to the film's animation, Richard Corliss of TIME claimed that it was "... the year's most inventive comedy."[82]
The voice cast was also praised by various critics. Susan Wloszczyna of USA Today approved of the selection of Hanks and Allen for the lead roles.[83] Kenneth Turan of the Los Angeles Times stated that "Starting with Tom Hanks, who brings an invaluable heft and believability to Woody, Toy Story is one of the best voiced animated features in memory, with all the actors ... making their presences strongly felt."[84] Several critics also recognized the film's ability to appeal to various age groups, specifically children and adults.[81][85] Owen Gleiberman of Entertainment Weekly wrote: "It has the purity, the ecstatic freedom of imagination, that's the hallmark of the greatest children's films. It also has the kind of spring-loaded allusive prankishness that, at times, will tickle adults even more than it does kids."[86]
In 1995, Toy Story was named eighth in TIME's list of the best ten films of 1995.[87] In 2011, TIME named it one of "The 25 All-TIME Best Animated Films".[88] It also ranks at number 99 in Empire magazines list of the 500 Greatest Films of All Time, and as the highest ranked animated movie.[89]
In 2003, the Online Film Critics Society ranked the film as the greatest animated film of all time.[90] In 2007, the Visual Effects Society named the film 22nd in its list of the "Top 50 Most Influential Visual Effects Films of All Time".[91] In 2005 the film was selected for preservation in the United States National Film Registry, one of five films to be selected in its first year of eligibility.[92] The film is ranked ninety-ninth on the AFI's list of the hundred greatest American films of all time.[93][94][95] It was one of only two animated films on the list, the other being Snow White and the Seven Dwarfs. It was also sixth best in the animation genre on AFI's 10 Top 10.
Director Terry Gilliam would praise the film as "a work of genius. It got people to understand what toys are about. They're true to their own character. And that's just brilliant. It's got a shot that's always stuck with me, when Buzz Lightyear discovers he's a toy. He's sitting on this landing at the top of the staircase and the camera pulls back and he's this tiny little figure. He was this guy with a massive ego two seconds before... and it's stunning. I'd put that as one of my top ten films, period."[96]
Prior to the film's release, executive producer and Apple Computer founder Steve Jobs stated "If Toy Story is a modest hitsay $75million at the box officewe'll [Pixar and Disney] both break even. If it gets $100million, we'll both make money. But if it's a real blockbuster and earns $200million or so at the box office, we'll make good money, and Disney will make a lot of money." Upon its release on November 22, 1995, Toy Story managed to gross more than $350million worldwide.[55] Disney chairman Michael Eisner stated "I don't think either side thought Toy Story would turn out as well as it has. The technology is brilliant, the casting is inspired, and I think the story will touch a nerve. Believe me, when we first agreed to work together, we never thought their first movie would be our 1995 holiday feature, or that they could go public on the strength of it."[55] Toy Story's first five days of domestic release (on Thanksgiving weekend), earned the film $39,071,176.[97] The film placed first in the weekend's box office with $29,140,617.[1] The film maintained its number one position at the domestic box office for the following two weekends. Toy Story was the highest grossing domestic film in 1995, beating Batman Forever and Apollo 13 (also starring Tom Hanks).[98] At the time of its release, it was the third highest grossing animated film after The Lion King (1994) and Aladdin (1992).[26] When not considering inflation, Toy Story is 96th on the list of the highest grossing domestic films of all time.[99] The film had gross receipts of $191,796,233 in the U.S. and Canada and $170,162,503 in international markets for a total of $361,958,736 worldwide.[1] At the time of its release, the film ranked 17th highest grossing film (unadjusted) in domestic money, and worldwide it was the 21st highest grossing film.
The film won and was nominated for various other awards including a Kids' Choice Award, MTV Movie Award, and a BAFTA Award, among others. John Lasseter received an Academy Special Achievement Award in 1996 "for the development and inspired application of techniques that have made possible the first feature-length computer-animated film."[100] The film was nominated for three Academy Awards, two to Randy Newman for Best MusicOriginal Song, for "You've Got a Friend in Me", and Best MusicOriginal Musical or Comedy Score.[101] It was also nominated for Best WritingScreenplay Written for the Screen for the work by Joel Cohen, Pete Docter, John Lasseter, Joe Ranft, Alec Sokolow, Andrew Stanton, and Joss Whedon making Toy Story the first animated film to be nominated for a writing award.[101]
Toy Story won eight Annie Awards, including "Best Animated Feature". Animator Pete Docter, director John Lasseter, musician Randy Newman, producers Bonnie Arnold and Ralph Guggenheim, production designer Ralph Eggleston, and writers Joel Cohen, Alec Sokolow, Andrew Stanton, and Joss Whedon all won awards for "Best Individual Achievement" in their respective fields for their work on the film. The film also won "Best Individual Achievement" in technical achievement.[102]
Toy Story was nominated for two Golden Globes, one for "Best Motion PictureComedy/Musical", and one for "Best Original SongMotion Picture" for Randy Newman's "You've Got a Friend in Me".[103] At both the Los Angeles Film Critics Association Awards and the Kansas City Film Critics Circle, the film won "Best Animated Film".[104][105] Toy Story is also among the top ten in the BFI list of the 50 films you should see by the age of 14, and the highest placed (at #99) animated film in Empire's list of "500 Greatest Movie of All Time".[106] In 2005 Toy Story, along with Toy Story 2 was voted the 4th greatest cartoon in Channel 4's 100 Greatest Cartoons poll, behind The Simpsons, Tom and Jerry and South Park.
Toy Story was released on VHS and Laserdisc on October 29, 1996, with no bonus material. In the first week of release VHS rentals totaled $5.1million, debuting Toy Story as the number one video for the week.[107] Over 21.5million VHS copies were sold in the first year.[108] Disney released a deluxe edition widescreen LaserDisc 4-disc box set on December 18, 1996. On January 11, 2000, it was released on VHS in the Gold Classic Collection series with the bonus short, Tin Toy, which sold two million copies.[108] Its first DVD release was on October 17, 2000, in a two-pack with Toy Story 2. This release was later available individually on March 20, 2001. Also on October 17, 2000, a 3-disc "Ultimate Toy Box" set was released, featuring Toy Story, Toy Story 2, and a third disc of bonus materials.[108] The DVD two-pack, The Ultimate Toy Box set, the Gold Classic Collection VHS and DVD and the original DVD were put in the Disney Vault. On September 6, 2005, a 2-disc "10th Anniversary Edition" was released featuring much of the bonus material from the "Ultimate Toy Box", including a retrospective special with John Lasseter, a home theater mix, as well as a new picture.[109] This DVD went back in the Disney Vault on January 31, 2009, along with Toy Story 2. The 10th Anniversary release was the last version of Toy Story to be released before taken out of the Disney Vault lineup, along with Toy Story 2. Also on September 6, 2005, a bare-bones UMD of Toy Story was released for the Sony PlayStation Portable.
The film was available on Blu-ray for the first time in a Special Edition Combo Pack which included two discs, one Blu-ray copy of the movie, and another DVD copy of the movie. This combo-edition was released on March 23, 2010, along with its sequel.[110] There was a DVD-only re-release on May 11, 2010.[111] Another "Ultimate Toy Box," packaging the Combo Pack with those of both sequels, became available on November 2, 2010. On November 1, 2011, along with the DVD and Blu-ray release of Cars 2, Toy Story and the other two films were released on each Blu-ray/Blu-ray 3D/DVD/Digital Copy combo pack (4 discs each for the first two films, and 5 for the third film). They were also be released on Blu-ray 3D in a complete trilogy box set.
Toy Story had a large impact on the film industry with its innovative computer animation. After the film's debut, various industries were interested in the technology used for the film. Graphics chip makers desired to compute imagery similar to the film's animation for personal computers; game developers wanted to learn how to replicate the animation for video games; and robotics researchers were interested in building artificial intelligence into their machines that compared to the lifelike characters in the film.[112] Various authors have also compared the film to an interpretation of Don Quixote as well as humanism.[113][114] In addition, Toy Story left an impact with its catchphrase "To Infinity and Beyond", sequels, and software, among others.
Buzz Lightyear's classic line "To Infinity and Beyond" has seen usage not only on T-shirts, but among philosophers and mathematical theorists as well.[115][116][117] Lucia Hall of The Humanist linked the film's plot to an interpretation of humanism. She compared the phrase to "All this and heaven, too", indicating one who is happy with a life on Earth as well as having an afterlife.[114] In 2008, during STS-124 astronauts took an action figure of Buzz Lightyear into space on the Discovery Space Shuttle as part of an educational experience for students while stressing the catchphrase. The action figure was used for experiments in zero-g.[118] It was reported in 2008 that a father and son had continually repeated the phrase to help them keep track of each other while treading water for 15 hours in the Atlantic Ocean.[119] The phrase occurs in the lyrics of Beyonce's 2008 song "Single Ladies (Put a Ring on It)", during the bridge.
Toy Story has spawned two sequels: Toy Story 2 (1999) and Toy Story 3 (2010). Initially, the first sequel to Toy Story was going to be a direct-to-video release, with development beginning in 1996.[120] However, after the cast from Toy Story returned and the story was considered to be better than that of a direct-to-video release, it was announced in 1998 that the sequel would see a theatrical release.[121] The sequel saw the return of the majority of the voice cast from Toy Story, and the film focuses on rescuing Woody after he is stolen at a yard sale. The film was equally well received by critics, earning a rare 100% approval rating at Rotten Tomatoes, based on 125 reviews.[122] At Metacritic, the film earned a favorable rating of 88/100 based on 34 reviews.[123] The film's widest release was 3,257 theaters and it grossed $485,015,179 worldwide, becoming the second-most successful animated film after The Lion King at the time of its release.[124][125]
Toy Story 3 centers on the toys being accidentally donated to a day-care center when their owner Andy is preparing to go to college.[126][127] Again the majority of the cast from the prior two films returned. It was the first film in the franchise to be released in 3-D for its first run, though the first two films, which were originally released in 2-D, were re-released in 3-D in 2009 as a double feature.[126] Like its predecessors, Toy Story 3 received enormous critical acclaim, earning a 99% approval rating from Rotten Tomatoes.[128] It also grossed more than $1 billion worldwide, making it the highest-grossing animated film to date.[129]
In November 1996, the Disney on Ice: Toy Story ice show opened which featured the voices of the cast as well as the music by Randy Newman.[130] In April 2008, the Disney Wonder cruise ship launched Toy Story: The Musical shows on its cruises.[131]
Toy Story also led to a spin-off direct-to-video animated film, Buzz Lightyear of Star Command: The Adventure Begins, as well as the animated television series Buzz Lightyear of Star Command.[132] The film and series followed Buzz Lightyear and his friends at Star Command as they uphold justice across the galaxy. Although the film was criticized for not using the same animation as in Toy Story and Toy Story 2, it sold three million VHS and DVDs in its first week of release.[133][134] The series ran for 65 episodes.
There were also short films before Cars 2 titled Hawaiian Vacation, centering around Barbie and Ken on vacation in Bonnie's room, and the The Muppets entitled Small Fry, centering on Buzz being left in a fast-food restaurant.
Disney's Animated Storybook: Toy Story and Disney's Activity Center: Toy Story were released for Windows and Mac.[135] Disney's Animated Storybook: Toy Story was the best selling software title of 1996, selling over 500,000 copies.[136] Two console video games were released for the film: the Toy Story video game, for the Sega Genesis, Super Nintendo Entertainment System, Game Boy, and PC as well as Toy Story Racer, for the PlayStation (which contains elements from Toy Story 2).[137] Pixar created original animations for all of the games, including fully animated sequences for the PC titles.
Toy Story had a large promotion prior to its release, leading to numerous tie-ins with the film including images on food packaging.[67] A variety of merchandise was released during the film's theatrical run and its initial VHS release including toys, clothing, and shoes, among other things.[138] When an action figure for Buzz Lightyear and Sheriff Woody was created it was initially ignored by retailers. However, after over 250,000 figures were sold for each character prior to the film's release, demand continued to expand, eventually reaching over 25million units sold by 2007.[79]
Toy Story and its sequels have inspired multiple attractions at the theme parks of Walt Disney World and Disneyland:
 
November 22, 1995(1995-11-22)
1 Plot
2 Cast

2.1 Cast notes


2.1 Cast notes
3 Production

3.1 Development
3.2 Writing
3.3 Casting
3.4 Production shutdown
3.5 Animation
3.6 Music
3.7 Editing and pre-release


3.1 Development
3.2 Writing
3.3 Casting
3.4 Production shutdown
3.5 Animation
3.6 Music
3.7 Editing and pre-release
4 Soundtrack
5 Release

5.1 Marketing
5.2 3-D re-release
5.3 Reception
5.4 Box office performance
5.5 Accolades
5.6 Home media


5.1 Marketing
5.2 3-D re-release
5.3 Reception
5.4 Box office performance
5.5 Accolades
5.6 Home media
6 Impact and legacy

6.1 "To Infinity and Beyond"
6.2 Sequels, shows, and spin-offs
6.3 Software and merchandise
6.4 Theme park attractions


6.1 "To Infinity and Beyond"
6.2 Sequels, shows, and spin-offs
6.3 Software and merchandise
6.4 Theme park attractions
7 Notes
8 References
9 External links
2.1 Cast notes
3.1 Development
3.2 Writing
3.3 Casting
3.4 Production shutdown
3.5 Animation
3.6 Music
3.7 Editing and pre-release
5.1 Marketing
5.2 3-D re-release
5.3 Reception
5.4 Box office performance
5.5 Accolades
5.6 Home media
6.1 "To Infinity and Beyond"
6.2 Sequels, shows, and spin-offs
6.3 Software and merchandise
6.4 Theme park attractions
Tom Hanks as Woody, a cowboy pull string doll
Tim Allen as Buzz Lightyear, a Space Ranger doll
Don Rickles as Mr. Potato Head, a potato shaped doll with put together pieces on his body
Jim Varney as Slinky Dog, a slink toy
Wallace Shawn as Rex, a cowardly green Tyrannosaurus Rex
John Ratzenberger as Hamm, a piggy bank
Annie Potts as Bo Peep, a shepherdess and Woody's love interest
John Morris as Andy Davis, the young boy who owns all the toys
Erik von Detten as Sid Phillips, Andy's former next door neighbor, who destroys toys for his own amusement
Laurie Metcalf as Andy's Mom
R. Lee Ermey as Sarge, a green plastic figure soldier
Sarah Freeman as Hannah Phillips, Sid's sister
Penn Jillette as TV Announcer
Jack Angel as Shark/Rocky Gibraltar
Greg Berg as Minesweeper Soldier
Debi Derryberry as Squeeze Toy Aliens/Pizza Planet Intercom
Mickie McGowan as Sid's Mom
Ryan O'Donohue as kid in Buzz Lightyear commercial
Jeff Pidgeon as Squeeze Toy Aliens/Mr. Spell/Robot
Phil Proctor as Pizza Planet guard/bowling announcer
Joe Ranft as Lenny
Andrew Stanton as Buzz Lightyear commercial chorus
Non-speaking characters include Scud, Barrel of Monkeys, Etch A Sketch, Snake, Clown, and Buster.
Buzz Lightyear's Space Ranger Spin at the Magic Kingdom casts theme park guests as cadets in Buzz's Space Ranger Corps. Guests ride through various scenes featuring Emperor Zurg's henchmen, firing "laser canons" at their Z symbols, scoring points for each hit.[139]
Buzz Lightyear's Astro Blasters at Disneyland, is very similar to Space Ranger Spin, except that the laser canons are hand-held rather than mounted to the ride vehicle.[140]
Buzz Lightyear's Astroblasters at DisneyQuest in Walt Disney World, despite the nearly identical name to the Disneyland attraction, is a bumper car style attraction in which guests compete against each other not only by ramming their ride vehicles into each other, but also by firing "asteroids" (playground balls) at each other.[141]
Toy Story Mania at both Disney's Hollywood Studios in Walt Disney World and Disney's California Adventure Park in Disneyland features a series of interactive carnival-type games hosted by the Toy Story characters. Guests ride in vehicles while wearing 3D glasses, and using a pull-string canon to launch virtual rings, darts, baseballs, etc. Disney announced an update to the attraction to add characters from Toy Story 3 several months before the film's release date.[142][143]
World of Color at Disney California Adventure is a large night time water and light show. Some of the scenes projected on the water screens feature animation from the Toy Story films.[144]
Toy Story Playland at Disneyland Paris and Hong Kong Disneyland, opening in August 2010 and 2011 respectively. The area is designed to create the illusion of "shrinking the guest" down to the size of a toy, and to play in Andy's backyard in several themed rides.[145]
Toy Story Character Greetings are located at almost all Disney Parks. Three of the main characters, Buzz Lightyear, Woody and Jesse are normally the characters you would meet. Sometimes you can even meet Bullseye, the Green Army Men and Mr. Potato Head.
Price, David (2008). The Pixar Touch. New York: Alfred A. Knopf. ISBN0-307-26575-7.
Official Pixar website
Official Disney website
Toy Story at the Internet Movie Database
Toy Story at the Big Cartoon DataBase
Toy Story at AllRovi
Toy Story at Rotten Tomatoes
Toy Story at Metacritic
Toy Story at Box Office Mojo
.
#(`*Toy Story 2*`)#.
Toy Story 2 is a 1999 computer-animated comedy film produced by Pixar Animation Studios and directed by John Lasseter. Co-directed by Lee Unkrich and Ash Brannon, the film is the sequel to Toy Story. In the film, Woody is stolen by a toy collector, prompting Buzz Lightyear and his friends vowing to rescue him. However, Woody finds the idea of immortality in a museum tempting. The film returns many of the original characters and voices from Toy Story and introduces several new characters, including Jessie, Barbie, and Mrs. Potato Head. It is the last Toy Story film in which Jim Varney provides Slinky Dog's voice before he died in 2000.
Disney initially envisioned the film as a direct-to-video sequel and Toy Story 2 began production in a building separated from Pixar and was much smaller scale, with most of the main Pixar staff working on A Bug's Life (1998). When story reels proved promising, Disney upgraded the film to theatrical release, but Pixar was unhappy with the quality of the film. Lasseter and the story team re-developed the entire plot in one weekend. Although most Pixar features take years to develop, the established release date could not be moved and the production schedule for Toy Story 2 was compressed into nine months.[2][3]
Despite production struggles, Toy Story 2 opened in November 1999 to wildly successful box office numbers, eventually grossing over $485 million, and highly positive critical reviews. Toy Story 2 has been considered by critics and audiences alike to be one of few sequels that outshine the original,[4] and it continues to be featured frequently on lists of the greatest animated films ever. The film has seen multiple home media releases and a 3-D re-release (2009). The success of the film led to the production of Toy Story 3 in 2010, which was also highly successful.
Three years after the events in Toy Story, Woody prepares to go to cowboy camp with Andy but his arm is accidentally ripped so Andy leaves him behind while his mom puts him on the shelf. After Woody has a nightmare about Andy throwing him in a trash full of arms, he discovers that a penguin toy named Wheezy has been on the shelf for months because of a broken squeaker. When Woody saves Wheezy from a yard sale, he is stolen by a toy collector who Buzz Lightyear and the other toys recognize as Al McWhiggin, the greedy, avaricious owner of a shop named Al's Toy Barn, from a commercial. Buzz, Hamm, Mr. Potato Head, Slinky Dog, and Rex set out to rescue Woody.
In Al's apartment, Woody discovers that he is a valuable collectible based on an old TV show called Woody's Roundup, and is set to be sold to a toy museum in Tokyo, Japan. The other toys from the show - Jessie the yodeling cowgirl, Woody's horse Bullseye, and Stinky Pete the Prospector, are excited about the trip but Woody intends to go home because he is Andy's toy. Jessie, who is afraid of the dark, is upset with him as the museum will only want the whole gang; without him, they will go back into storage. That night, when Woody's whole arm comes off, his attempt to retrieve it and escape is foiled when the TV comes on. Woody, seeing the remote in front of Jessie, accuses her of sabotaging his escape. The following morning, Woody's arm is reconnected and he decides to stay when Jessie reveals that she was once the beloved toy of a child named Emily who eventually outgrew and gave her away and Prospector warns him that the same fate awaits him when Andy grows up.
Meanwhile, Buzz and the other toys reach Al's Toy Barn. While searching the store for Woody, Buzz is captured and imprisoned in a box by a newer Buzz Lightyear action figure after a fight between them and the New Buzz's utter delusion. The new Buzz joins the other toys, oblivious to the fact he is an imposter, as they make their way to Al's apartment. Buzz escapes and pursues them, thinking they have also been captured by Al. When he gets out of Al's Toy Barn, he unknowingly and accidentally releases an action figure of his archenemy Emperor Zurg who follows him. Buzz rejoins the others as they find Woody, who initially refuses to return because he does not want to abandon the rest of the Roundup Gang. After Buzz reminds Woody of "a toy's true purpose", and he is moved by seeing himself sing "You've Got a Friend in Me", he changes his mind again and asks the Roundup toys to come with him. However, Prospector prevents their escape and reveals that he wants to go to Tokyo because he spent his life on a dime store shelf and was never sold. To ensure this, he made sure Woody would not go home, and was also responsible for sabotaging his escape the previous night. Al arrives and takes Woody and the Roundup toys with him, forcing both Buzz Lightyears and Andy's toys to follow him. They follow Al to an elevator where they encounter Zurg who fights the new Buzz but is knocked off the elevator by Rex. When they reach the ground floor, the new Buzz stays to play with Zurg once he discovers that Zurg is his father (echoing The Empire Strikes Back) while Buzz and the other toys continue their pursuit of Al.
Accompanied by three toy Aliens, they use a Pizza Planet delivery truck to follow Al to Tri-County International Airport where they enter the check-in area, the baggage processing area to find Woody and the Roundup toys. During a fight with Woody, Prospector rips his arm and tries to mutilate him, but is captured and stuffed into a little girl's backpack by Buzz and the other toys. While Woody and Bullseye are saved, Jessie ends up on the plane for Tokyo. Assisted by Buzz and Bullseye, Woody boards the plane and convinces Jessie to come with them to Andy's house, telling her that he has a little sister. However, the plane starts up before they can escape but they leave through an emergency hatch just as the plane gets onto the runway. Woody lassoes his string over a nut on the plane's wheels, and swings with Jessie between the plane wheels before landing on Bullseye just as the plane takes off. Buoyed up by living "Woody's Finest Hour," the toys go home.
Andy returns home, repairs Woody's arm, and accepts Jessie and Bullseye as his new toys. The toys also learn from a commercial that Al's business has suffered due to his failure to sell the Roundup toys. As Jessie and Bullseye delight in having a new owner, Woody tells Buzz that he is not worried about Andy outgrowing him, because when he does, they will always have each other for company "for infinity and beyond."
Bullseye, Barrel of Monkeys, and Buster, Andy's pet dog are all voiced by Frank Welker.
Talk of a sequel to Toy Story began around a month after the film's opening, in December 1995.[5] A few days after its release, Lasseter was traveling with his family and found a young boy clutching a Woody doll at an airport. Lasseter described the boy's excitement to show it to his father as touching him deeply. Lasseter then realized that his character no longer belonged to him only, it belonged to others as well. The memory was a defining factor in the production of Toy Story 2, with Lasseter moved to create a great film for that child and for everyone who loved the characters.[6] Ed Catmull, Lasseter, and Ralph Guggenheim visited Joe Roth, successor to recently-ousted Jeffery Katzenberg as chairman of Walt Disney Studios, shortly afterward. Roth was pleased and embraced the idea.[5] Disney had recently begun making direct-to-video sequels to its successful features, and Roth wanted to handle the Toy Story sequel this way, as well. Prior releases, such as 1994's Aladdin sequel, The Return of Jafar, had returned an estimated hundred million dollars in profits.[7]
Initially, everything regarding the sequel was uncertain at first: whether stars Tom Hanks and Tim Allen would be available and affordable, what the story premise would be, and even whether the film would be computer-animated at Pixar or traditionally at Disney.[7] Lasseter regarded the project as a chance to groom new directing talent, but top choices were already immersed in other projects (Andrew Stanton in A Bug's Life and Pete Docter in early development work for a film about monsters). Instead, Lasseter turned to Ash Brannon, a young directing animator on Toy Story whose work he admired. Brannon, a CalArts graduate, joined the Toy Story team in 1993.[7] Walt Disney Studios and Pixar Animation Studios officially announced the sequel in a press release on March 12, 1997.[8]
Lasseter's intention with a sequel was to respect the original film and create that world again.[6] The story originated with Lasseter pondering what a toy would find upsetting. Lasseter wondered how a toy would feel if they were not played with by a child or, worse, a child growing out of a toy.[7] Brannon suggested the idea of a yard sale where the collector recognizes Woody as a rare artifact.[9] The concept of Woody as a collectible set came from the draft story of A Tin Toy Christmas, an original half-hour special pitched by Pixar to Disney in 1990. The obsessive toy collector known as Al McWhiggin, had appeared in a draft of Toy Story but was later expunged, was inserted into the film.[7] Lasseter claimed that Al was inspired by himself.
[10]
Secondary characters in Woody's set emerged from viewings of 1950s cowboy shows for children, such as Howdy Doody and Hopalong Cassidy.[9] The development of Jessie was kindled by Lasseter's wife, Nancy, who pressed him to include a strong female character in the sequel, one with more substance than Bo Peep.[9]
The scope for the original Toy Story was very basic and only consisted over two residential homes, whereas Toy Story 2 has been described by Unkrich as "all over the map."[6]
To make the project ready for theaters, Lasseter would need to add twelve minutes or so of material and strengthen what was already there. The extra material would be a challenge, since it could not be mere padding; it would have to feel as if it had always been there, an organic part of the film.[2]
With the scheduled delivery date less than a year away, Lasseter called Stanton, Docter, Joe Ranft, and some Disney story people to his house for a weekend. There, he hosted a "story summit," as he called it - a crash exercise that would yield a finished story in just two days. Back at the office that Monday, Lasseter assembled the company in a screening room and pitched the revised version of Toy Story 2 from beginning to end.[2]
Story elements were recycled from the original drafts of Toy Story. The original opening sequence of the original film featured a Buzz Lightyear cartoon playing on television, which evolved into the Buzz Lightyear video game that would open Toy Story 2.[11] A deleted scene from Toy Story, featuring Woody having a nightmare involving him being thrown into a trash can, was incorporated in a milder form for showing Woody's fear of losing Andy. The idea of a squeak-toy penguin with a broken squeaker also resurfaced from an early version of Toy Story.[11]
As the story approached the production stage in early 1997, it was unclear whether Pixar would produce the film, as the entire team of 300 was busy working on A Bug's Life for a 1998 release. The Interactive Products Group, with a staff of 95, had its own animators, art department, and engineers. Under intense time pressure, they had put out two successful CD-ROM titles the previous year: The Toy Story Animated StoryBook and The Toy Story Activity Center.[9] Between the two products, the group had created as much original animation as there was in Toy Story itself. Steve Jobs made the decision to shut down the computer games operation and the staff became the initial core of the Toy Story 2 production team.[8]
Before the switch from direct-to-video to feature film, the Toy Story 2 crew had been on its own, placed in a new building that was well-separated from the rest of the company by railroad tracks. "We were just the small film and we were off playing in our sandbox," co-producer Karen Jackson said.[2] Lasseter looked closely at every shot that had already been animated and called for tweaks throughout. The film reused digital elements from Toy Story but, true to the company's "prevailing culture of perfectionism, [] it reused less of Toy Story than might be expected."[12] Character models received major upgrades internally and shaders went through revisions to bring about subtle improvements. The team did, however, freely borrow models from other productions, such as Geri from Pixar's 1997 short Geri's Game, who became the Cleaner in Toy Story 2.[12] Supervising animator Glenn McQueen inspired the animators to do spectacular work in the short amount of time given, assigning different shots to suit each animators' strengths.[13]
Whilst producing the original Toy Story, the crew was very careful in creating new locations due to technology at that time. By production on Toy Story 2, technology had advanced farther to allow more complicated camera shots than were possible in the first film.[6] In making the sequel, the team at Pixar didn't want to stray too far from the look of the original film, but the company had developed a lot of new software since the first feature had been completed.[13] To achieve the dust visible after Woody is placed on top of a shelf, the crew was faced with the challenge of animating dust, an incredibly difficult task. After much experimentation, a tiny particle of dust was animated and the computer distributed that image throughout the entire shelf. Over two million dust particles are in place on the shelf in the completed film.[14]
Production problems were evident from the beginning. Disney soon became unhappy with the pace of the work on the film and demanded in June 1997 that Guggenheim be replaced as producer, and Pixar complied. As a result, Karen Jackson and Helene Plotkin, associate producers, moved up to the role of co-producers.[16] Lasseter would remain fully preoccupied with A Bug's Life until it wrapped in the fall. Once available, he took over directing duties and added Lee Unkrich as co-director. Unkrich, also fresh from supervising editor duties on A Bug's Life, would focus on layout and cinematography while Brannon would be credited as co-director.[17]
In November 1997, Disney executives Roth and Peter Schneider viewed story reels for the film, with some finished animation, in a screening room at Pixar. They were impressed with the quality of work and became interested in releasing Toy Story 2 in theaters.[16] In addition to the unexpected artistic caliber, there were other reasons that made the case for a theatrical release more compelling. The economics of a direct-to-video Pixar release weren't working as well as hoped thanks to higher salaries of the crew. After negotiations, Jobs and Roth agreed that the split of costs and profits for Toy Story 2 would follow the model of a newly-created five-film deal - but Toy Story 2 would not count as one of the five films. Disney had bargained in the contract for five original features, not sequels, thus assuring five sets of new characters for its theme parks and merchandise. Jobs gathered the crew and announced the change in plans for the film on February 5, 1998.[17]
However, many of the creative staff at Pixar were not happy with how the sequel was turning out. John Lasseter, upon returning from European promotion of A Bug's Life, watched the development reels and agreed that it wasn't working. Pixar met with Disney, telling them that the film would have to be redone. Disney, however, disagreed, and noted that Pixar did not have enough time to remake the film before its established release date. Pixar decided that they simply could not allow the film to be released in its existing state, and asked Lasseter to take over the production. Lasseter agreed, and recruited the creative team behind the first film to redevelop the story. However inorder to meet Disney's deadline, Pixar had to complete the entire film in nine months.[3] Unkrich, concerned with the dwindling amount of time left, asked Jobs whether the release date could be pushed back. Jobs explained that there was no choice, presumably in reference to the film's licensees and marketing partners, who were getting toys and promotions ready.[2] Brannon focused on development, story and animation, Lasseter was in charge of art, modeling and lighting, and Unkrich oversaw editorial and layout. Since they met daily to discuss their progress with each other (they wanted to make sure they were all going in the same direction), the boundaries of their responsibilities overlapped.[13]
As common with Pixar features, the production became difficult as delivery dates loomed and hours inevitably became longer. Still, Toy Story 2, with its highly compressed production schedule, was especially trying.[15] While hard work and long hours were common to the team by that point (especially so to Lasseter), running flat-out on Toy Story 2 for month after month began to take a toll. The overwork spun out into carpal tunnel syndrome for some animators,[15] and repetitive strain injuries for others.[18] Pixar did not encourage long hours, and, in fact, set limits on how many hours employees could work by approving or disapproving overtime. An employee's self-imposed compulsion to excel, however, often trumped any other constraints, and was especially common to younger employees.[15] In one instance, an animator had forgotten to drop his child off at day care one morning and, in a mental haze, forgotten the baby in the backseat of his car in the parking lot. "Although quick action by rescue workers headed off the worst, the incident became a horrible indicator that some on the crew were working too hard," wrote David Price in his 2008 book The Pixar Touch.[19]
Toy Story 2: An Original Walt Disney Records Soundtrack is the original score soundtrack album to Toy Story 2. Although currently out of print in the U.S., the CD is available in the U.S. as an import and all but one song is available digitally.[20]
All songs are written and composed by Randy Newman.
Randy Newman wrote two new songs for Toy Story 2 as well as the complete original score:
The film carried over one song from Toy Story, "You're Got a Friend in Me," sung at different points during the film by Tom Hanks and Robert Goulet.[15]
Pixar showed the completed film at CalArts on November 12, 1999, in recognition of the school's ties with Lasseter and more than forty other alumni who worked on the film; the students were captivated.[19] The film held its official premiere the next day at the El Capitan Theatre in Los Angeles - the same venue as Toy Story's - and released across the United States on November 24.
The film was no less successful than its predecessor in a commercial perspective; it became the highest-grossing animated film of 1999, earning $245 million domestically and $485 million worldwide - beating both of Pixar's previous releases by a significant margin. It was the second highest-grossing animated film of all-time for a time, behind Disney's The Lion King (1994).[4] Toy Story 2 opened over the Thanksgiving Day weekend at No.1 to a three-day tally of $57,388,839 from 3,236 theaters averaging $17,734 per theater over three days, making $80,102,784 since its Wednesday launch, and staying at No.1 for the next two weekends. It eventually made $245,852,179 domestically and $239,163,000 overseas for a total worldwide gross of $485,015,179, becoming the third highest grossing film of 1999, and far surpassing the original.
Toy Story 2: Buzz Lightyear to the Rescue, a video game for the PC, PlayStation, Nintendo 64 and Dreamcast, was released. The game featured original cast voices and clips from the film as introductions to levels. Once earned, these clips could be viewed at the player's discretion. Another game was released for the Game Boy Color.
Toy Story 2 was released on VHS and DVD and as a DVD two-pack with Toy Story on October 17, 2000. That same day an "Ultimate Toy Box" set was released containing both films and a third disc of bonus materials. The standard VHS and DVD and the DVD two-pack and "Ultimate Toy Box" sets returned to the vault on May 1, 2003. On December 26, 2005, it was again re-released as a "2-Disc Special Edition" alongside the 10th Anniversary Edition of the first film, which came out on September 6. Both editions returned to the vault on January 31, 2009.
The film was available on Blu-ray Disc for the first time in a Special Edition Combo Pack that was released on March 23, 2010, along with the original film.[21] There was a DVD-only re-release on May 11, 2010.[22]
On November 1, 2011, along with the DVD and Blu-ray release of Cars 2, Toy Story 2 and the other two films were released on each Blu-ray/Blu-ray 3D/DVD/Digital Copy combo pack (4 discs each for the first two films, and 5 for the third film). They will also be released on Blu-ray 3D in a complete trilogy box set.
In 2009, Toy Story 2, alongside its predecessor, was converted to 3D for a two-week limited theatrical re-issue.[23] The film was released with Toy Story as a double feature for a two-week run[24] which was extended due to its success.[25][26] In addition, the film's sequel, Toy Story 3, was also released in the 3-D format.[23] Lasseter commented on the new 3-D re-release: "The Toy Story films and characters will always hold a very special place in our hearts and we're so excited to be bringing this landmark film back for audiences to enjoy in a whole new way thanks to the latest in 3-D technology. With Toy Story 3 shaping up to be another great adventure for Buzz, Woody and the gang from Andy's room, we thought it would be great to let audiences experience the first two films all over again and in a brand new way."[27]
Translating the films into 3-D involved revisiting the original computer data and virtually placing a second camera into each scene, creating left-eye and right-eye views needed to achieve the perception of depth. Unique to computer animation, Lasseter referred to this process as "digital archaeology." The lead stereographer Bob Whitehill oversaw this process and sought to achieve an effect that impacted the emotional storytelling of the film. It took four months to resurrect the old data and get it in working order. Then, adding 3-D to each of the films took six months per film.[28]
Unlike other countries, the UK and Argentina received the films in 3-D as separate releases. Toy Story 2 was instead released January 22, 2010, in the UK, and February 18, 2010, in Argentina.[29] The double feature was opened in 1,745 theaters on October 2, 2009, and made $12,491,789 in its opening weekend, coming in third place at the box office. The feature(s) closed on November 5, 2009, with a worldwide gross of $32,284,600.[30]
On June 12, 2010, in its broadcast on Disney Channel, the film received 7.479million viewers, making the number one show or film of the week.[31]
Toy Story 2 received very positive reviews by critics. Reviewers found the film to be a rare avid, a sequel that managed to equal or even outshine the original.[4] "Toy Story 2 does what few sequels ever do," The Hollywood Reporter proclaimed. "Instead of essentially remaking an earlier film and deeming it a sequel, the creative team, led by director John Lasseter, delves deeper into their characters while retaining the fun spirit of the original film."[4]
Review aggregate website Rotten Tomatoes reports that 100% of critics have given the film a positive review based on 161 reviews, with an average score of 8.6/10. The film is currently No.1 on Rotten Tomatoes' list of best rated films.[32] Rotten Tomatoes summarizes the critical consensus thus: "Toy Story 2 employs inventive storytelling, gorgeous animation, and a top notch voice cast to deliver another rich moviegoing experience for all ages, one that's arguably even better than its predecessor."[33] Toy Story, Toy Story 2 and Toy Story 3 are all Pixar's highest-rated films to date. It currently holds a 100% approval from critics, and 92% from the community, while the original holds a 96% community rating[34] and the best rated animated film.[35] The film also holds an 88 out of 100 on Metacritic.[36] It joins the rare number of sequels judged to be "as good as or better than the original."[citation needed] Roger Ebert gave the film three-and-a half stars out of four and said in his print review "I forgot something about toys a long time ago, and Toy Story 2 reminded me."[37] Kenneth Turan of the Los Angeles Times said "Toy Story 2 may not have the most original title, but everything else about it is, well, mint in the box."[38] Entertainment Weekly said "It's a great, IQ-flattering entertainment both wonderful and wise."[39]
Unkrich regarded the film with pride while remembering the difficulty of meeting its due date. "Even though Toy Story 2 really killed us in a lot of ways - it was really, really hard - I probably look back on that film the most fondly in terms of how we all came together and did this impossible thing."[19]
The list of nominations include: An Oscar for Best Music; Original Song for Randy Newman's "When She Loved Me", A Saturn Award for Best Fantasy Film and Randy Newman for Best Music. The film was also in the running for two Annie's: One was Outstanding Achievement for character animation; Doug Sweetland. The other Annie nomination was for Outstanding Individual Achievement for Production Design in an animated Feature Production; William Cone and Jim Pearson. Ruth Lambert was nominated for an Artios award for Best Casting for Animated Voiceover- Feature Film. On top of all the others Randy Newman was also nominated for a golden globe in the category Best Original Song- Motion Picture for his song "When She loved Me". Yet another nomination Randy Newman received was Best Score Soundtrack for a Motion Picture, Television or Other Visual Media. Tim Allen and Tom Hanks both were nominated for Blimp Awards in the category of Favorite Voice from an Animated Movie. The film was nominated for the same award under the category of favorite film. It was nominated for two Sierra Awards: one for Best Animated Film, and the other for Best Song "When She Loved Me". Hanks and Allen were both nominated for another award, this time an MTV Movie Award for Best On-Screen Duo. There were a lot of nominations for a Golden Reel Award, Best Sound Editing- Animated Feature: Michael Silvers (supervising sound editor), Mary Helen Leasman (supervising foley editor), Michael Silvers (supervising adr editor), Shannon Mills (sound editor), Teresa Eckton (sound editor), Susan Sanford (foley editor), Bruce Lacey (foley editor) and Jonathan Null (adr editor). It was thought Bruno Coon (supervising music editor) and Lisa Jaime (music editor) would also get a Golden Reel but for Best Sound Editing- Music-Animation. The Online Film Critics Society, appointed Toy Story 2 for 2 of their awards. One was Best Film, the other was John Lasseter and Pete Doctor for best original screenplay. Nickelodeon's Teen Choice Awards suggested Joan Cusack to get their award for Film- Choice Hissy Fit, she did not win.
The American Society of Composers, Artists and Publishers gave the films first award to Randy Newman for Top Box Office Films. Seven Annies were won, but none of them were previous nominations. The first went to Pixar for Outstanding Achievement in an Animated Theatrical Feature. Outstanding Individual Achievement for Directing in an Animated Feature Production was given to John Lasseter, Lee Unkrich and Ash Brannon. Randy Newman won an annie for Outstanding Individual Achievement for Music in an Animated Feature Production. Joan Cusack won Outstanding Individual Achievement for Voice Acting by a Female Performer in an animated feature Production. Tim Allen got the same award for males. The final Annie was received by John Lasseter, Pete Docter, Ash Brannon, Andrew Stanton, Rita Hsiao, Doug Chamberlin and Chris Webb for Outstanding Individual Achievement for Writing in an Animated Feature Production. The film also won many awards by itself. One of them is the Blockbuster Entertainment Award for Favorite Family Film on the internet. The Critics Choice Award for Best Animated Film, the Bogey Award and a Golden Globe for Best Motion Picture-Musical Comedy were also won. Along with his other awards, Randy Newman and his song "When She Loved Me" won a Grammy for Best Song Written for a Motion Picture, Television or Other Visual Media. The Kansas City Film Critics Circle award Woody and the gang for Best Animated Film. A Satellite Award was given for Outstanding Youth DVD, and a Golden Satellite Award for Best Motion Picture, Animated or Mixed Media, and one for Best Original Song "When She Loved Me". And a Young Artist Award for Best Family Feature Film - Animated.
One Pixar tradition is to create trailers for their films that do not contain footage from the released film.[citation needed] In one trailer for Toy Story 2 (released theatrically with A Bug's Life, Doug's 1st Movie, and Tarzan), the aliens watch the metal claw they worship coming down. The claw first brings down the words Toy Story, and the aliens react with their trademark "Oooooh." The claw next brings down the number '2'; in reaction, the aliens turn to face the camera and parody themselves with a "Twoooo." Then Woody appears, saying "Hey howdy hey, folks! It's good to be back." He is swiftly disappointed when Buzz shows up as well, and expresses his annoyance that the Space Ranger is also in the sequel. Buzz retorts, "Excuse me, Pullstring Boy, what would Toy Story 2 be without Buzz Lightyear?" "A good movie," counters Woody.
Another trailer (released theatrically with Muppets from Space and The Adventures of Elmo in Grouchland) shows shots from the movie that are featured.
The initial theatrical and video releases of this film include Luxo Jr., Pixar's first short film released in 1986, starring Pixar's titular mascot. Before Luxo Jr., a message states: "In 1986 Pixar Animation Studios produced their first film. This is why we have a hopping lamp in our logo".
November 24, 1999(1999-11-24)
1 Plot
2 Voice cast

2.1 Cast notes


2.1 Cast notes
3 Production

3.1 Development
3.2 Story
3.3 Animation
3.4 Controversy and troubled production
3.5 Music


3.1 Development
3.2 Story
3.3 Animation
3.4 Controversy and troubled production
3.5 Music
4 Release

4.1 Box office
4.2 Video games
4.3 Home media
4.4 Re-releases
4.5 Television broadcasts


4.1 Box office
4.2 Video games
4.3 Home media
4.4 Re-releases
4.5 Television broadcasts
5 Critical response

5.1 American Film Institute


5.1 American Film Institute
6 Legacy
7 Awards

7.1 Nominations
7.2 Won


7.1 Nominations
7.2 Won
8 Pixar themes

8.1 Trailers
8.2 Attached short film


8.1 Trailers
8.2 Attached short film
9 References
10 External links
2.1 Cast notes
3.1 Development
3.2 Story
3.3 Animation
3.4 Controversy and troubled production
3.5 Music
4.1 Box office
4.2 Video games
4.3 Home media
4.4 Re-releases
4.5 Television broadcasts
5.1 American Film Institute
7.1 Nominations
7.2 Won
8.1 Trailers
8.2 Attached short film
Tom Hanks as Woody
Tim Allen as Buzz Lightyear/Utility Belt Buzz
Joan Cusack as Jessie
Kelsey Grammer as Stinky Pete the Prospector
Don Rickles as Mr. Potato Head
Jim Varney as Slinky Dog
Wallace Shawn as Rex
John Ratzenberger as Hamm
Annie Potts as Bo Peep
Estelle Harris as Mrs. Potato Head
Wayne Knight as Al McWhiggin
John Morris as Andy
Laurie Metcalf as Andy's Mom
R. Lee Ermey as Sarge
Jodi Benson as Barbie
Jonathan Harris as Geri the Cleaner
Joe Ranft as Wheezy
Andrew Stanton as Evil Emperor Zurg
Jeff Pidgeon as Squeeze Toy Aliens
"When She Loved Me"  performed by Sarah McLachlan: Used for the flashback montage in which Jessie experiences being loved, forgotten, and ultimately abandoned by her owner, Emily. This song was nominated at the Oscars in 2000 for Best Song, though the award went to Phil Collins for "You'll Be in My Heart" from another Disney animated film Tarzan.
"Woody's Roundup"  performed by Riders in the Sky: Theme song for the "Woody's Roundup" TV show. Also end-credit music.
AFI's 100 Years...100 Heroes and Villains:

Buzz Lightyear  Nominated Hero


Buzz Lightyear  Nominated Hero
AFI's 100 Years...100 Songs:

"When She Loved Me"  Nominated


"When She Loved Me"  Nominated
AFI's 10 Top 10  Nominated Animated Film
Buzz Lightyear  Nominated Hero
"When She Loved Me"  Nominated
Price, David (2008). The Pixar Touch. New York: Alfred A. Knopf. ISBN0-307-26575-7.
Official website
Toy Story 2 at the Internet Movie Database
Toy Story 2 at the Big Cartoon DataBase
Toy Story 2 at AllRovi
Toy Story 2 at Rotten Tomatoes
Toy Story 2 at Metacritic
Toy Story 2 at Box Office Mojo
.
#(`*Toy Story 3*`)#.
Toy Story 3 is a 2010 American 3D computer animated comedy film, and the third installment in the Toy Story series.[2] It was produced by Pixar Animation Studios and distributed by Walt Disney Pictures. Directed by Lee Unkrich, the film was released worldwide from June through October[3] in Disney Digital 3-D, RealD, and IMAX 3D. Toy Story 3 was also the first film to be released theatrically with Dolby Surround 7.1 sound.
The plot focuses on the toys Woody, Buzz Lightyear, and their friends dealing with an uncertain future as their owner, Andy, prepares to leave for college. Actors Tom Hanks, Tim Allen, Joan Cusack, Don Rickles, Estelle Harris, John Ratzenberger, Wallace Shawn, Jeff Pidgeon, Jodi Benson, R. Lee Ermey, John Morris, and Laurie Metcalf reprised their voice-over roles from the previous films. Jim Varney, who played Slinky Dog in the first two films, and Joe Ranft, who portrayed Lenny and Wheezy, both died before production began on Toy Story 3. The role of Slinky Dog was taken over by Blake Clark (a friend of Varney), while Ranft's characters and various others were written out of the story. New characters include performances by Ned Beatty, Timothy Dalton, Kristen Schaal, Bonnie Hunt, Whoopi Goldberg, Jeff Garlin, Richard Kind, and Michael Keaton.
The feature broke Shrek the Third's record as the biggest opening day North American gross for an animated film unadjusted for inflation,[4] and a big opening with an unadjusted gross of $110,307,189. It is also the highest-grossing opening weekend for a Pixar film,[5] as well as the highest-grossing opening weekend for a film to have opened in the month of June.[6] The film is the highest-grossing film of 2010, both in the United States and Canada, and worldwide. In early August, it became Pixar's highest-grossing film at the North American box office (surpassing Finding Nemo), and the highest-grossing animated film of all time worldwide (surpassing Shrek 2);[7] later that month, Toy Story 3 became the first ever animated film in history to make over $1billion worldwide.[8] It is currently the 9th highest-grossing film of all time.[9]
Toy Story 3 was nominated for five Academy Awards, including Best Picture, Best Adapted Screenplay, and Best Sound Editing.[10] It was the third animated film (after Beauty and the Beast and Up) to be nominated for Academy Award for Best Picture. It won the awards for Best Animated Feature and Best Original Song.
Andy, now nearly 18 years old,[11] is leaving for college, and his toys feel like they have been abandoned as they have not been played with for years. Andy decides to take Woody with him to college and puts Buzz Lightyear and the rest of the toys in a trash bag for storage in the attic. However, the toys are accidentally thrown out when Andy's mother finds the bag and puts it out on the curb, causing the toys to think that they are no longer wanted. They escape and decide to climb in a donation box for Sunnyside Daycare. Woody, the only toy who saw what actually happened, follows the other toys and tries to explain that they were thrown out by mistake, but they refuse to believe him.
Andy's toys are welcomed by the many toys at Sunnyside and given a tour of the seemingly perfect play-setting by Lots-O'-Huggin' Bear (simply known as Lotso), Big Baby and Ken, whom Barbie falls for. All of the toys love their new home, leaving a steadfast Woody alone in an attempt to return to Andy. Woody's escape attempt falls short and he is found outside by Bonnie, an imaginative little girl. She takes him home and plays with him along with her other toys, who are well-treated, happy, and readily welcome Woody. At the daycare, Andy's toys are getting played with very roughly by the rambunctious youngest toddlers.
Buzz goes to ask Lotso to transport him and the other toys to a better room, only to be caught by Lotso's henchmen and restored back to his original space ranger persona. At the same time, Andy's toys realize that Woody was right about Andy when Mrs. Potato Head sees Andy searching for them through her missing eye, which was left behind in Andy's room. Before they could leave, they are imprisoned by Lotso and his gang, including a reset Buzz. Back at Bonnie's, Woody learns from one of the toys, named Chuckles the Clown, that Lotso was once a good toy and had an owner named Daisy who also owned Chuckles the Clown and Big Baby. One day, Daisy left them behind on a picnic. The three eventually find their way back to Daisy's house, only to find that she replaced Lotso with an identical teddy bear. When he found Sunnyside, he and Big Baby took it over and ran it like a prison.
The following morning, Woody returns to Sunnyside through Bonnie's backpack. He sneakily reaches his friends and tells them he is sorry for leaving them. They quickly formulate an escape plan. That night, Woody and Slinky sneak through Sunnyside to the main office, where Chatter informed them that a cymbal-banging monkey monitors the CCTV system to prevent toys escaping. A brief fight ensues, ending with the Monkey wrapped in sticky tape and locked in a filing cabinet by Slinky. Slinky signals to the other toys while Mr. Potato head provides a diversion, they make their escape. In the process, Buzz is accidentally reset into a delusional Spanish mode, in which he becomes very flamboyantly chivalrous and his memory is wiped; despite this, Buzz allies himself with Woody's friends, and immediately falls in love with Jessie. The toys reach a dumpster, but are caught by Lotso and his gang. As a garbage truck approaches, Woody reveals what he heard about Lotso, and Big Baby throws Lotso into the dumpster. Seeking revenge, Lotso pulls Woody in the dumpster just as the truck collects the trash. Woody's friends jump into the back of the truck, trying to rescue him and a falling television hits Buzz when he tries to save Jessie, returning him to his normal self. The toys find themselves at the dump and are pushed onto a conveyor belt leading to a garbage shredder. Woody and Buzz save Lotso just in time as he is about to be shredded and Woody and the other toys end up on another conveyor belt, leading to an incinerator. The toys help Lotso reach an emergency stop button, but he leaves them to their deaths. Thinking that this is the end, the toys join hands and accept their fate but are rescued by the Aliens using a giant claw. Lotso makes his way outside, but a passing garbage truck driver finds him and, recognizing he had the toy as a kid, straps him to the radiator grill of his truck. Meanwhile, Woody and his friends board another trash truck driven by an older Sid Phillips back to Andy's house.
In Andy's room, Woody climbs back into the box with Andy's college supplies while the other toys ready themselves for the attic. Remembering his time with Bonnie and her toys, Woody has an idea and leaves a note for Andy on the toys' box. Andy, thinking the note is from his mother, takes them to Bonnie's house and introduces her to his old toys and Bonnie recognizes Woody, who, to Andy's surprise, is lying at the bottom of the box. Andy is initially reluctant to give him up but eventually does so and spends some time playing with her. After Andy leaves, Woody introduces the gang to Bonnie's toys as the camera pans up to the sky.
During the credits, Woody and the other toys learn through notes passed in Bonnie's backpack that Barbie, Ken and Big Baby have improved the lives of the toys (now including an Emperor Zurg action figure) at Sunnyside. Buzz and Jessie (now a couple) dance a pasodoble to a Spanish version of "You've Got a Friend in Me."
Several other characters (such as Bo Peep, RC, Etch and Wheezy) are only seen in flashbacks. The character of Slinky Dog appeared to be in limbo after the death of his original voice actor Jim Varney on February 10, 2000, three months after Toy Story 2 was released. Varney was replaced by Blake Clark. After Clark was cast to play Slinky Dog, the producers later realized that Clark and Varney had coincidentally been close friends since they appeared in the 1989 film Fast Food, making the transition a lot easier.[14]
According to the terms of Pixar's revised deal with Disney, all characters created by Pixar for their films were owned by Disney. Furthermore, Disney retains the rights to make sequels to any Pixar film, though Pixar retained the right of first refusal to work on these sequels. But in 2004, when the contentious negotiations between the two companies made a split appear likely, Disney Chairman at the time Michael Eisner put in motion plans to produce Toy Story 3 at a new Disney studio, Circle 7 Animation. Tim Allen, the voice of Buzz Lightyear, indicated a willingness to return even if Pixar was not on board.[15]
Jim Herzfeld wrote a script for Circle 7's version of the film. It focused on the other toys shipping a malfunctioning Buzz to Taiwan, where he was built, believing that he will be fixed there. While searching on the Internet, they find out that many more Buzz Lightyear toys are malfunctioning around the world and the company has issued a massive recall. Fearing Buzz's destruction, a group of Andy's toys (Woody, Rex, Slinky, Mr. Potato Head, Hamm, Jessie, and Bullseye) venture to rescue Buzz. At the same time, Buzz meets other toys from around the world that were once loved, but have now been recalled.[15]
In January 2006, Disney bought Pixar in a deal that put Pixar chiefs Edwin Catmull and John Lasseter in charge of all Disney Animation. Shortly thereafter, Circle 7 Animation was shut down and its version of Toy Story 3 was cancelled.[15] The character designs went into the Disney archives.[16] The following month, Disney CEO Robert Iger confirmed that Disney was in the process of transferring the production to Pixar.[17] John Lasseter, Andrew Stanton, Pete Docter, and Lee Unkrich visited the house where they first pitched Toy Story and came up with the story for the film over a weekend. Stanton then wrote a treatment.[18] On February 8, 2007, Catmull announced Toy Story 2's co-director, Lee Unkrich, as the sole director of the film instead of John Lasseter (who was busy directing Cars 2), and Michael Arndt as screenwriter.[19] The release date was moved to 2010.[20] Unkrich said that he felt pressure to avoid creating "the first dud" for Pixar, since as of 2010 all of Pixar's films had been critical and commercial successes.[21]
During the initial development stages of the film, Pixar revisited their work from the original Toy Story and found that although they could open the old computer files for the animated 3D models, error messages prevented them from editing the files. This necessitated recreating the models from scratch.[22] To create the chaotic and complex junkyard scene near the film's end, more than a year and a half was invested on research and development to create the simulation systems required for the sequence.[23]
Instead of sending Tom Hanks, Tim Allen and John Ratzenberger scripts for their consideration in reprising their roles, a complete story reel of the film was shown to the actors in a theater. The reel was made up of moving storyboards with pre-recorded voices, sound effects, and music. At the conclusion of the preview, the actors signed on to the film.[24]
Dolby Laboratories announced that Toy Story 3 would be the first film that will feature theatrical 7.1 surround audio.[25] Thus, even the Blu-ray version will feature original 7.1 audio, unlike other movies which were remixed into 7.1 for Blu-ray.
The film's first teaser trailer was released with Up in Disney Digital 3-D, on May 29, 2009.[26] On October 2, 2009, Toy Story and Toy Story 2 were re-released as a double feature in Disney Digital 3-D.[27] The first full-length trailer was attached as an exclusive sneak peek and a first footage to the Toy Story double feature, on October 12, 2009. A second teaser was released on February 10, 2010, followed by a second full-length trailer on February 11 and appeared in 3D showings of Alice in Wonderland and How to Train Your Dragon. On March 23, 2010, Toy Story was released on Blu-ray/DVD combo pack which included a small feature of "The Story of Toy Story 3." Also, Toy Story 2 was released on that day in the same format which had a small feature on the "Characters of Toy Story 3." On May 11, 2010, both films had a DVD-only re-release which contained the features.
Mattel, Thinkway Toys, and Lego are among those who produced toys to promote the film. Fisher Price, a Mattel Company, has released Toy Story 3 with 21 3D images for viewing with the View-Master viewer.[28][29] Disney Interactive Studios also produced a video game based on the film, Toy Story 3: The Video Game, which was released for Microsoft Windows, Xbox 360, Wii, PlayStation 3, Nintendo DS, and PSP on June 15, 2010.[30] A PlayStation 2 version was released on October 30, 2010 as part of a PS2 Bundle and separately on November 2, 2010 (The same day Toy Story 3 was released on DVD and Blu-ray). It was also the last Disney/Pixar game to be released on PlayStation 2.
Toy Story 3 was featured in Apple's iPhone OS 4 Event on April 8, 2010, with Steve Jobs demonstrating a Toy Story 3 themed iAd written in HTML5.[31]
Pixar designed a commercial for the toy, Lots-O'-Huggin' Bear, and formatted it to look like it came from an old VCR recording. The recording was altered with distorted sound, noise along the bottom of the screen, and flickering video, all designed to make it look like a converted recording from around 1983.[32] A Japanese version of the commercial was also released online, with the name Lots-O'-Huggin Bear being replaced by Little Hug-Hug Bear (Japanese:/Hagu Hagu Beya-Chan).[33]
On Dancing with the Stars' May 11, 2010, episode, the Gipsy Kings performed a Spanish-language version of the song "You've Got a Friend in Me." It also featured a paso doble dance which was choreographed by Cheryl Burke and Tony Dovolani.[34][35] Both the song and dance are featured in the film.
Toy Story 3 was also promoted with airings of the first and second films on several channels in the upcoming weeks of the film's release, including Disney Channel, Disney XD, and ABC Family. Sneak peeks of Toy Story 3 were also revealed, primarily on Disney Channel.
Unlike most recent Oscar campaigns, Toy Story 3's "Not since..." campaign drew a lot of attention during the holiday period, emphasizing on the film's uniqueness and tremendous critical acclaim.[36]
The theatrical release of Toy Story 3 included the short film Day & Night, which focuses on what happens when an animated personification of Day meets his opposite, Night and the resulting growth for both.[37][38] It was also included in the Blu-ray and DVD release of the film.
Toy Story 3 was released in North America on November 2, 2010 in a standard DVD edition, two-disc Blu-ray and in a four-disc Blu-ray/DVD/Digital Copy combo pack. Behind the scenes are featured including a sneak peek teaser for the upcoming Cars 2, the sequel to the 2006 film, Cars.[39] A 10-disc Toy Story trilogy Blu-ray box set also arrived on store shelves on the same day.[40] A 3D version of the Blu-ray was released in North America on November 1, 2011.
On its first week of release (November 27, 2010) it sold 3,859,736 units (equal to $73,096,452) ranking No.1 for the week and immediately becoming the best-selling animated film of 2010 in terms of units sold (surpassing How to Train Your Dragon). As of July 18, 2012, it has sold 10,911,701 units ($185,924,247).[41] It has become the best-selling DVD of 2010 in terms of units sold, but it lacks in terms of sales revenue and therefore ranks second behind Avatar on that list.[42] It also sold about 4.0 million Blu-ray units, ranking as the fourth best-selling film of 2010.[43]
In the UK, it broke the record for the largest first day ever for animated feature both on DVD and Blu-ray in terms of sales revenue. Additionally, on its first day of release on iTunes it immediately became the most downloaded Disney film ever.[44]
Toy Story 3 received very positive reviews from critics. The film review aggregation website Rotten Tomatoes reports that 99% of critics have given the film a positive review based on 255 reviews, with an average score of 8.8/10. The site's consensus is: "Deftly blending comedy, adventure, and honest emotion, Toy Story 3 is a rare second sequel that really works."[45] On the all-time Best of Rotten Tomatoes list, it ranks fourth[46] behind both its predecessors, and was the best-reviewed film of 2010.[47] Another review aggregator, Metacritic, which assigns a normalized rating out of 100 top reviews from mainstream critics, calculated a score of 92 based on 39 reviews.[48] TIME named Toy Story 3 the best movie of 2010,[49] as did Quentin Tarantino.[50] In 2011, TIME named it one of "The 25 All-TIME Best Animated Films."[51]
A. O. Scott of The New York Times stated, "This filmthis whole three-part, 15-year epicabout the adventures of a bunch of silly plastic junk turns out also to be a long, melancholy meditation on loss, impermanence and that noble, stubborn, foolish thing called love."[52] Owen Gleiberman from Entertainment Weekly gave the film an A, saying, "Even with the bar raised high, Toy Story 3 enchanted and moved me so deeply I was flabbergasted that a digitally animated comedy about plastic playthings could have this effect."[53] Gleiberman also wrote in the next issue that he, along with many other grown men, cried at the end of the film.[54] Michael Rechtshaffen of The Hollywood Reporter also gave the film a positive review, saying, "Woody, Buzz and playmates make a thoroughly engaging, emotionally satisfying return."[55] Mark Kermode of the BBC gave the film, and the series, a glowing review, calling it "the best movie trilogy of all time."[56] In USA Today, Claudia Puig gave the film a complete 4 star rating, writing, "This installment, the best of the three, is everything a movie should be: hilarious, touching, exciting and clever."[57] Lou Lumenick of the New York Post wrote, "Toy Story 3 (which is pointlessly being shown in 3-D at most locations) may not be a masterpiece, but it still had me in tears at the end."[58] Michael Phillips of the Chicago Tribune gave the film 3 out of 4 stars, writing that, "Compared with the riches of all kinds in recent Pixar masterworks such as Ratatouille, WALL-E, and Up, Toy Story 3 looks and plays like an exceptionally slick and confident product, as opposed to a magical blend of commerce and popular art."[59] Orlando Sentinel film critic Roger Moore, who gave the film 3 out of 4 stars, wrote, "Dazzling, scary and sentimental, Toy Story 3 is a dark and emotional conclusion to the film series that made Pixar famous."[60]
Toy Story 3 earned $415,004,880 in North America, and $648,167,031 in other countries, totaling $1,063,171,911 worldwide, earning more revenue than the previous two films combined.[1] It is the highest-grossing film in the series,[61] the 9th highest-grossing film,[9] the highest-grossing film of 2010,[62] the third highest-grossing Disney film,[63] the highest-grossing Pixar film,[64] and the highest-grossing animated film of all time.[8] In terms of estimated attendance, though, it still ranks fourth on the list of modern animated films, behind Shrek 2, Finding Nemo, and The Lion King.[8] On its first weekend, Toy Story 3 topped the worldwide box office with $145.3 million ($153.7 million with weekday previews), which stands as the third-largest opening weekend worldwide for an animated feature.[65] On August 27, 2010, its 71st day of release, it surpassed the $1 billion mark, becoming the second Disney film in 2010 (after Alice in Wonderland), the third Disney film overall (the other being Pirates of the Caribbean: Dead Man's Chest), and the only animated film to achieve this.
In North America, Toy Story 3 is the 12th highest-grossing film unadjusted for inflation. Adjusted for ticket price inflation though, it ranks 90th on the all-time chart.[66] The film is also the highest-grossing film of 2010,[67] the highest-grossing Pixar film,[64] the second highest-grossing G-rated film,[68] the 3rd highest-grossing animated film,[69] and the fourth highest-grossing Disney film.[70] It grossed $41,148,961 on its opening day (Friday, June 18, 2010) from 4,028 theaters, setting an opening-day record for an animated film.[71] During its opening weekend, the film grossed $110,307,189, topping the weekend chart and marking the highest-grossing opening weekend for a Pixar film.[72] It averaged $27,385 per venue, marking the second highest for a G-rated film, and the second highest for an animated feature.[73] The film had the second-highest opening weekend for an animated film,[74] and also had the fourth best opening weekend for a 2010 film.[75] It set an opening-weekend record for films opening in June,[6] and for G-rated films.[76] In its first week (Friday-through-Thursday), Toy Story 3 grossed $167.6 million marking the biggest opening week for an animated film and the tenth largest opening week of all time.[77] It also had the largest opening-week and 10-day gross among 2010 films.[78] It topped the box office for two consecutive weekends.[79]
It is the fourteenth highest-grossing film,[80] the third highest-grossing animated film, the third highest-grossing film of 2010,[81] the highest-grossing Pixar film, and the fifth highest-grossing Disney film.[80] It topped the box office outside North America three times, on its first ($35.0 million),[82] second,[83] and sixth weekend (which was its largest).[84]
Its highest-grossing market after North America is Japan ($126.7 million),[85] where it is the highest-grossing U.S. animated feature,[8] followed by the UK & Ireland and Malta (73.8 million - $116.6 million), where it is the fourth highest-grossing film,[86] and Mexico ($59.4 million), where it is the second highest-grossing film.[87] It set opening weekend records for animated films in Ecuador, Colombia, Mexico, China, Argentina,[88] Hong Kong,[89] Spain and the UK.[90] It is currently the highest-grossing animated film of all time in the UK, Ireland and Malta, in Mexico,[87] in Hong Kong,[91] and in Egypt. It is the highest-grossing 2010 film in Argentina,[92] Bolivia,[93] Chile,[94] Colombia,[95] Hong Kong,[96] Mexico,[97] Spain,[98] the UK, & Ireland and Malta.[99]
On January 25, 2011, the Academy of Motion Picture Arts and Sciences announced that Toy Story 3 was not only nominated for Best Animated Feature, but also for Best Picture. This makes Toy Story 3 not only the first only animated sequel in history to be nominated for Best Picture, but also the third animated film to be nominated for Best Picture (following Beauty and the Beast and Up). Toy Story 3 becoming the second Pixar film to be nominated for both awards.[100] Toy Story 3 also became the first ever Pixar film to be nominated for the Academy Award for Best Adapted Screenplay, though six of Pixar's previous films were nominated for the Best Original Screenplay  (Toy Story, Finding Nemo, The Incredibles, Ratatouille, WALL-E, and Up). In 2011, it was nominated for a Kids' Choice Award for favorite animated movie, but lost to Despicable Me.
The film score of Toy Story 3 was composed and conducted by Randy Newman, his sixth for Pixar after Toy Story, A Bug's Life, Toy Story 2, Monsters, Inc. and Cars. Disney did not release the soundtrack album for Toy Story 3 on Compact Disc (CD). It was only available, initially, as a music download in lossy formats such as MP3 and AAC. This was the second instance where Disney did not release the award-winning soundtrack of a Pixar film on CD. The first Pixar film not to have its soundtrack released on CD by Disney was Up. In January 2012, Intrada released the Toy Story 3 soundtrack on Compact Disc.[121]
All songs are written and composed by Randy Newman.
In addition to the tracks included in the soundtrack album, the film also uses "Dream Weaver" by Gary Wright, "Le Freak" by Chic, and Randy Newman's original version of "You've Got a Friend in Me."
Also, tracks "Cowboy!" and "Come to Papa" included material from Newman's rejected score to Air Force One.[122] The song "Losing You" from Newman's own album Harps and Angels was also used in the first trailer for the film.[123]
The Judas Priest song "Electric Eye" was used in the temp score for the opening scene of Toy Story 3.[124] The aliens are playing the tune in their sports car. But the song was ultimately replaced by another piece of music.
In June 2011, Tom Hanks, the voice of Woody in the films, was asked while promoting Larry Crowne whether or not there would be a sequel for his grandchildren to see. "I think there will be, yeah. I think they're working on it now," he said, referring to Pixar.[127] However, no such sequel has been announced.

June 12, 2010(2010-06-12) (Taormina Film Fest)
June 18, 2010(2010-06-18) (North America)
1 Plot
2 Voice cast
3 Production
4 Release

4.1 Marketing

4.1.1 Oscar campaign


4.2 Short film
4.3 Home media


4.1 Marketing

4.1.1 Oscar campaign


4.1.1 Oscar campaign
4.2 Short film
4.3 Home media
5 Reception

5.1 Critical reception
5.2 Box office

5.2.1 Worldwide
5.2.2 North America
5.2.3 Outside North America


5.3 Accolades


5.1 Critical reception
5.2 Box office

5.2.1 Worldwide
5.2.2 North America
5.2.3 Outside North America


5.2.1 Worldwide
5.2.2 North America
5.2.3 Outside North America
5.3 Accolades
6 Music 

6.1 Music awards


6.1 Music awards
7 Possible sequel
8 References
9 External links
4.1 Marketing

4.1.1 Oscar campaign


4.1.1 Oscar campaign
4.2 Short film
4.3 Home media
4.1.1 Oscar campaign
5.1 Critical reception
5.2 Box office

5.2.1 Worldwide
5.2.2 North America
5.2.3 Outside North America


5.2.1 Worldwide
5.2.2 North America
5.2.3 Outside North America
5.3 Accolades
5.2.1 Worldwide
5.2.2 North America
5.2.3 Outside North America
6.1 Music awards
Tom Hanks as Woody
Tim Allen as Buzz Lightyear
Joan Cusack as Jessie
Ned Beatty as Lots-O'-Huggin' Bear
John Morris as Andy
Don Rickles as Mr. Potato Head
Blake Clark as Slinky Dog
Wallace Shawn as Rex
John Ratzenberger as Hamm
Estelle Harris as Mrs. Potato Head
Michael Keaton as Ken
Jodi Benson as Barbie
Emily Hahn as Bonnie
Jeff Pidgeon as Aliens
Timothy Dalton as Mr. Pricklepants
Kristen Schaal as Trixie
Jeff Garlin as Buttercup
Bonnie Hunt as Dolly
Whoopi Goldberg as Stretch
Jack Angel as Chunk
Jan Rabson as Sparks
John Cygan as Twitch
Laurie Metcalf as Andy's Mom
Lori Alan as Bonnie's Mom
R. Lee Ermey as Sarge
Teddy Newton as Chatter Telephone
Richard Kind as Bookworm
Bud Luckey as Chuckles
Javier Fernandez-Pea as Spanish Buzz
Beatrice Miller as Molly
Charlie Bright as Peaty/Young Andy
Amber Kroner as Peatrice
Brianna Maiwand as Peanelope
Erik von Detten as Sid
Jack Willis as Frog
Lee Unkrich as Jack-in-the-box
Bob Peterson as Janitor
Woody Smith as Big Baby[12][13]
Official website
Pixar website
Toy Story 3 at the Internet Movie Database
Toy Story 3 at the Big Cartoon DataBase
Toy Story 3 at Rotten Tomatoes
Toy Story 3 at Metacritic
Toy Story 3 at Box Office Mojo
.
#(`*Finding Nemo*`)#.
Finding Nemo is a 2003 American computer-animated comedy film written and directed by Andrew Stanton, released by Walt Disney Pictures, and the fifth film produced by Pixar Animation Studios. It tells the story of the over-protective clownfish named Marlin (Albert Brooks) who, along with a regal tang named Dory (Ellen DeGeneres), searches for his abducted son Nemo (Alexander Gould) in Sydney Harbour. Along the way, Marlin learns to take risks and let Nemo take care of himself. It is Pixar's first film to be released in cinemas in the northern hemisphere summer. The film was re-released for the first time in 3D on September 14, 2012, and it was released on Blu-ray on December 4, 2012. A sequel is currently in development, set to be released in 2016.
The film received extremely positive reviews and won the Academy Award for Best Animated Feature. It was the second highest-grossing film of the year, earning a total of $921 million worldwide.[1] Finding Nemo is also the best-selling DVD of all time, with over 40 million copies sold as of 2006,[2] and was the highest-grossing G-rated film of all time before Pixar's own Toy Story 3 overtook it. It is also the 22nd highest-grossing film of all time, as well as the 3rd highest-grossing animated film. In 2008, the American Film Institute named it the tenth greatest animated film ever made during their Top 10.[3]
Two clownfish, Marlin and his wife Coral are admiring their new home in the Great Barrier Reef and their clutch of eggs that are due to hatch in a few days. Suddenly, a barracuda attacks them, leaving Marlin unconscious before eating Coral and all but one of their eggs. Marlin names this egg Nemo, a name that Coral liked.
The film then moves on to Nemo's first day of school. Nemo has a tiny right fin, due to a minor injury to his egg from the barracuda attack, which limits his swimming ability. After Marlin embarrasses Nemo during a school field trip, Nemo disobeys his father and sneaks away from the reef towards a boat, resulting in him being captured by scuba divers. As the boat sails away, one of the divers accidentally knocks his diving mask into the water.
While unsuccessfully attempting to save Nemo, Marlin meets Dory, a good-hearted and optimistic Regal blue tang with short-term memory loss. While meeting three sharks on a fish-free diet, Bruce, a great white shark; Anchor, a hammerhead shark; and Chum, a mako shark, Marlin discovers the diver's mask that was dropped from the boat and notices an address written on it. However, when he argues with Dory and accidentally gives her a nosebleed, the scent of blood causes Bruce to lose control of himself and attempt to eat Marlin and Dory. The two escape from Bruce but the mask falls into a trench in the deep sea. During a hazardous struggle with an anglerfish in the trench, Dory realizes she is able to read the address written on the mask, which leads to Sydney, Australia, and manages to remember it. After receiving directions to Sydney from a large school of moonfish, Marlin and Dory accidentally run into a bloom of jellyfish that nearly sting them to death; Marlin falls exhausted after the risky escape and wakes up to see a surf-cultured sea turtle named Crush, who takes Dory and him on the East Australian Current. In the current, Marlin reluctantly shares the details of his journey with a group of young sea turtles; his story spreads rapidly across the ocean through word of mouth and eventually finds Nemo in Sydney.
Meanwhile, Nemo's captor - P. Sherman, a dentist - places him into a fish tank in his office on Sydney Harbour. There, Nemo meets a group of aquarium fish called the "Tank Gang", led by a crafty and ambitious moorish idol named Gill. The "Tank Gang" includes Bloat, a puffer fish; Bubbles, a Yellow Tang; Peach, a starfish; Gurgle, a Royal gramma; Jacques, a pacific cleaner shrimp; and Deb, a Blacktailed Humbug. The fish are frightened to learn that the dentist plans to give Nemo to his niece, Darla. She is infamous for killing a goldfish given to her previously by constantly shaking the bag. In order to avoid this fate, Gill gives Nemo a role in an escape plan, which involves jamming the tank's filter and forcing the dentist to remove the fish from the tank to clean it manually. The fish would be placed in plastic bags, at which point they would roll out the window and into the harbor. After a friendly pelican named Nigel visits with news of Marlin's adventure, Nemo succeeds in jamming the filter, but the plan backfires when the dentist installs a new high-tech filter.
Upon leaving the East Australian Current, Marlin and Dory become lost in the blooms of plankton and krill and are engulfed by a whale. Inside the whale's immense mouth, Marlin desperately tries to escape while Dory communicates with it in whale-speak. In response, the whale carries them to Sydney Harbour and expels them through his blowhole. They are met by Nigel, who recognizes Marlin from the stories he has heard and rescues him and Dory from a flock of hungry seagulls by scooping them into his beak and taking them to the dentist's office. By this time, Darla has arrived and the dentist is prepared to give Nemo to her. Nemo tries to play dead in hopes of saving himself, and, at the same time, Nigel arrives. Marlin sees Nemo and mistakes this act for the actual death of his son. Gill helps Nemo escape into a drain through a sink after a chaotic struggle.
Overcome with despair, Marlin leaves Dory and begins to swim back home. Dory then loses her memory and becomes confused, but meets Nemo, who has reached the ocean through an underwater drainpipe. Dory's memory is restored after she reads the word "Sydney" on a nearby drainpipe and, remembering her journey, she guides Nemo to Marlin. After the two joyfully reunite, Dory is caught in a fishing net with a school of grouper. Nemo bravely enters the net and directs the group to swim downward to break the net, reminiscent of a similar scenario that occurred in the fish tank earlier. The fish, including Dory, succeed in breaking the net and escape. After some days, Nemo leaves for school once more and Marlin who is no longer overprotective or doubtful of his son's safety, proudly watches Nemo swim away into the distance.
Back at the dentist's office, the high-tech filter breaks down and The Tank Gang have escaped into the harbor. However, they realize that they are confined to the bags of water that the dentist put them into when cleaning the tank.
The inspiration for Nemo was made up of multiple experiences. The idea goes back to when director Andrew Stanton was a child, when he loved going to the dentist to see the fish tank, assuming that the fish were from the ocean and wanted to go home.[4] In 1992 shortly after his son was born, he and his family took a trip to Six Flags Discovery Kingdom (which was called Marine World at the time). There he saw the shark tube and various exhibits he felt that the underwater world could be done beautifully in computer animation.[5] Later, in 1997 he took his son for a walk in the park, but found that he was over protecting him constantly and lost an opportunity to have any "father-son experiences" on that day.[4] In an interview with National Geographic magazine, he stated that the idea for the characters of Marlin and Nemo came from a photograph of two clownfish peeking out of an anemone:
"It was so arresting. I had no idea what kind of fish they were, but I couldn't take my eyes off them. And as an entertainer, the fact that they were called clownfishit was perfect. There's almost nothing more appealing than these little fish that want to play peekaboo with you."[6]
Also, clownfish are very colourful, but don't tend to come out of an anemone very often, and for a character who has to go on a dangerous journey, Stanton felt a clownfish was the perfect kind of fish for the character.[4]
Pre-production of the film took place in early 1997. Stanton began writing the screenplay during the post-production of A Bug's Life. As such, it began production with a complete screenplay, something that co-director Lee Unkrich called "very unusual for an animated film."[4] The artists took scuba diving lessons so they could go and study the coral reef. The idea for the initiation sequence came from a story conference between Andrew Stanton and Bob Peterson while driving to record the actors. Ellen DeGeneres was cast after Stanton was watching Ellen with his wife and seeing Ellen "change the subject five times before finishing one sentence" as Stanton recalled.[4] There was a pelican character known as Gerald (who in the final film ends up swallowing and choking on Marlin and Dory) who was originally a friend of Nigel. They were going to play against each other as Nigel being neat fastidious while Gerald being scruffy and sloppy. However the filmmakers could not find an appropriate scene for them that didn't slow the pace of the picture down, so Gerald's character was minimized.[4]
Stanton himself provided the voice of Crush the sea turtle. Stanton originally did the voice for the film's story reel, and assumed they would find an actor later. When Stanton's performance was popular in test screenings, Stanton decided to keep his performance in the film. Stanton recorded all his dialogue while lying on a sofa in co-director Lee Unkrich's office.[4]
Crush's son Squirt was voiced by Nicholas Bird, the young son of fellow Pixar director Brad Bird. According to Stanton, the elder Bird was playing a tape recording of his young son around the Pixar studios one day. Stanton felt the voice was "this generation's Thumper" and immediately cast Nicholas.[4]
Megan Mullally revealed that she was originally doing a voice in the film. According to Mullally, the producers were dissatisfied to learn that the voice of her character Karen Walker on the television show Will & Grace was not her natural speaking voice. The producers hired her anyway, and then strongly encouraged her to use her Karen Walker voice for the role. When Mullally refused, she was dismissed.[7]
The film was dedicated to Glenn McQueen, a Pixar animator who died of melanoma in October 2002.
Finding Nemo shares many plot elements with Pierrot the Clownfish, a children's book published in 2002, but allegedly conceived in 1995. The author, Franck Le Calvez, sued Disney for infringement of his intellectual rights. The judge ruled against him, citing the color differences between Pierrot and Nemo.[8]
To ensure that the movements of the fish in the film were believable the animators essentially took a crash course in fish biology and oceanography. They visited aquariums, went diving in Hawaii and received in-house lectures from an ichthyologist.[9]
Finding Nemo was released on DVD and VHS on November 4, 2003. The film was also released on DVD in a "Gold Edition", which came with a Finding Nemo stuffed toy character. The film had a home video release on both Blu-ray and Blu-ray 3D on December 4, 2012, with both a 3-disc and a 5-disc set.
Finding Nemo currently holds a 99% fresh rating at Rotten Tomatoes with 100% by top critics,[10] and an average of 89% on Metacritic.[11] Roger Ebert gave the film four stars, calling it "one of those rare movies where I wanted to sit in the front row and let the images wash out to the edges of my field of vision."[12] Broadway star Nathan Lane who was the voice of Timon the meerkat in The Lion King, said Finding Nemo was his favorite animated film.[13]
The film's use of clownfish prompted mass purchase of the animal as pets in the United States, even though the movie portrayed the use of fish as pets negatively and suggested that saltwater aquariums are notably tricky and expensive to maintain.[14] The demand for clownfish was supplied by large-scale harvesting of tropical fish in regions like Vanuatu.[15]
At the same time, the film had a quote that "all drains lead back to the ocean" (Nemo escapes from the aquarium by going down a sink drain, ending up in the sea). Since water typically undergoes treatment before leading to the ocean, the JWC Environmental company quipped that a more realistic title for the movie might be Grinding Nemo.[16] However, in Sydney, much of the sewer system does pass directly to outfall pipes deep offshore, without a high level of treatment (although pumping and some filtering occur).[17] Additionally, according to the DVD, there was a cut sequence with Nemo going through a treatment plant's mechanisms before ending up in the ocean pipes. However, in the final product, logos for "Sydney Water Treatment" are featured prominently along the path to the ocean, implying that Nemo did pass through some water treatment.
Tourism in Australia strongly increased during the summer and autumn of 2003, with many tourists wanting to swim off the coast of Eastern Australia to "find Nemo".[citation needed] The Australian Tourism Commission (ATC) launched several marketing campaigns in China and the USA in order to improve tourism in Australia, many of them utilising Finding Nemo clips.[18][19] Queensland also used Finding Nemo to draw tourists to promote its state for vacationers.[20]
On the 3-D re-release, Lisa Schwarzbaum of Entertainment Weekly wrote that its emotional power was deepened by "the dimensionality of the oceanic deep" where "the spatial mysteries of watery currents and floating worlds are exactly where 3-D explorers were born to boldly go."[21]
The 3-D re-release also prompted a retrospective on the film then nine years after its initial release. Stephen Whitty of the Newark Star-Ledger described it as "A genuinely funny and touching film that, in less than a decade, has established itself as a timeless classic,"[22] with Roger Moore of the McClatchy-Tribune News Service calling the movie "the gold standard against which all other modern animated films are measured."[23]
Finding Nemo earned $380,581,893 in North America, and $540,900,000 in other countries, for a worldwide total of $921,481,893.[1] It is the second highest-grossing film of 2003, behind The Lord of the Rings: The Return of the King.[24] In North America, outside North America, and worldwide, it was the highest-grossing Pixar film, up until 2010 when Toy Story 3 surpassed it.[25]
Finding Nemo set an opening-weekend record for an animated feature, making $70,251,710 (first surpassed by Shrek 2). It became the highest-grossing animated film in North America ($339.7 million), outside North America ($528.2 million) and worldwide ($867.9 million), in all three occasions outgrossing The Lion King. In North America, it was surpassed by both Shrek 2 in 2004, and Toy Story 3 in 2010. After the re-release of The Lion King in 2011, it stands as the fourth highest-grossing animated film in these regions. Outside North America, it was surpassed by Ice Age: Dawn of the Dinosaurs, Toy Story 3, and Ice Age: Continental Drift. Worldwide, it now ranks third among animated films.[26][27]
The film had impressive box office runs in many international markets. In Japan, its highest-grossing market after North America, it grossed $102.4 million becoming the highest-grossing Western animated film until it was out-grossed by Toy Story 3 ($126.7 million).[28] Following in biggest grosses are the UK, Ireland and Malta, where it grossed 37.2 million ($67.1 million), France and the Maghreb region ($64.8 million), Germany ($53.9 million), and Spain ($29.5 million).[29]
After the success of the 3D re-release of The Lion King, Disney and Pixar re-released Finding Nemo in 3D on September 14, 2012,[30] with a conversion cost estimated below $5 million.[31] For the opening weekend of its 3D re-release in North America, Finding Nemo grossed $16.7 million, debuting at the No. 2 spot behind Resident Evil: Retribution.[32] From seven foreign markets, it earned a total of $5.1 million.[31]
Finding Nemo won the Academy Award and Saturn Award for Best Animated Film. It also won the award for best Animated Film at the Kansas City Film Critics Circle Awards, the Las Vegas Film Critics Society Awards, the National Board of Review Awards, the Online Film Critics Society Awards, and the Toronto Film Critics Association Awards.[33]
The film received many awards, including:
Finding Nemo was also nominated for:
In June 2008, the American Film Institute revealed its "Ten top Ten", the best ten films in ten "classic" American film genres, after polling over 1,500 people from the creative community. Finding Nemo was acknowledged as the 10th best film in the animation genre.[34][3] It was the most recently released film among all ten lists, and one of only three movies made after the year 2000, the others being The Lord of the Rings: The Fellowship of the Ring and Shrek.
American Film Institute recognition:
The reaction to the film by the general public has led to environmental devastation for the clownfish and has provoked an outcry from several environmental protection agencies, including Marine Aquarium Council, Australia. Apparently, the demand for tropical fish skyrocketed after the film's release. This has caused reef species decimation in Vanuatu and many other reef areas.[35]
Even more bizarre, after seeing the film, some aquarium owners released their pets into the ocean, but the wrong ocean. This has introduced species harmful to the indigenous environment and is harming reefs worldwide as well.[36][37]
Finding Nemo is the original soundtrack album. It was the first Pixar film not to be scored by Randy Newman. The album was nominated for the Academy Award for Original Music Score, losing to The Lord of the Rings: The Return of the King.
All songs are written and composed by Thomas Newman, except track 40 (Charles Trnet, Jack Lawrence and Albert Lasry).
Finding Nemo has inspired numerous attractions and properties at Disney Parks around the world.

The stage musical Tarzan Rocks! occupied the Theater in the Wild at Disney's Animal Kingdom in Orlando, Florida from 1999 to 2006. When, in January 2006, it closed, it was rumored that a musical adaptation of Finding Nemo would replace it.[38] This was confirmed in April 2006, when Disney announced that the adaptation, with new songs written by Tony Award-winning Avenue Q composer Robert Lopez and his wife, Kristen Anderson-Lopez, would "combine puppets, dancers, acrobats and animated backdrops" and open in late 2006.[39] Tony Award-winning director Peter Brosius signed on to direct the show, with Michael Curry, who designed puppets for Disney's successful stage version of The Lion King, serving as leading puppet and production designer.
Anderson-Lopez said that the couple agreed to write the adaptation of "one of their favorite movies of all time" after considering "The idea of people coming in [to see the musical] at 4, 5 or 6 and saying, 'I want to do that'....So we want to take it as seriously as we would a Broadway show."[40] To condense the feature-length film to thirty minutes, she said she and Lopez focused on a single theme from the movie, the idea that "The world's dangerous and beautiful."[40]
The forty-minute show (which is performed five times daily) opened on January 2, 2007. Several musical numbers took direct inspiration from lines in the film, including "(In The) Big Blue World", "Fish Are Friends, Not Food", "Just Keep Swimming", and "Go With the Flow". In January 2007, a New York studio recording of the show was released on iTunes, with Lopez and Anderson-Lopez providing the voices for Marlin and Dory, respectively. Avenue Q star Stephanie D'Abruzzo also appeared on the recording, as Sheldon/Deb.
Nemo was the first non-musical animated film to which Disney added songs in order to produce a stage musical. In 2009, Finding Nemo  The Musical was honored with a Thea award for Best Live Show from the Themed Entertainment Association.
A video game based on the film was released in 2003, for PC, Xbox, PS2, GameCube and GBA.
In 2005, after disagreements between Disney's Michael Eisner and Pixar's Steve Jobs over the distribution of Pixar's films, Disney announced that they would be creating a new animation studio, Circle 7 Animation, to make sequels to the seven Disney-owned Pixar films (which consisted of the films released between 1995 and 2006).[41] The studio had put Toy Story 3 and Monsters, Inc. 2 in development, and had also hired screenwriter Laurie Craig to write a draft for Finding Nemo 2.[42] Circle 7 was subsequently shut down after Robert Iger replaced Eisner as CEO of Disney and arranged the acquisition of Pixar.
In July 2012, it was reported that Andrew Stanton is developing a sequel to Finding Nemo,[43] with Victoria Strouse writing the script and a schedule to be released in 2016.[44] However, the same day the news of a potential sequel broke, director Andrew Stanton posted a message on his personal Twitter calling into question the accuracy of these reports. The message said, "Didn't you all learn from Chicken Little? Everyone calm down. Don't believe everything you read. Nothing to see here now. #skyisnotfalling"[45] According to the report by Hollywood Reporter published in August 2012, Ellen DeGeneres is in negotiations to reprise her role of Dory.[46] In September 2012, it was confirmed by Stanton saying: "What was immediately on the list was writing a second [John] Carter movie. When that went away, everything slid up. I know I'll be accused by more sarcastic people that it's a reaction to Carter not doing well, but only in its timing, but not in its conceit."[47]
May 30, 2003(2003-05-30)
1 Plot
2 Cast
3 Production
4 Home media
5 Reception

5.1 Box office

5.1.1 3D re-release


5.2 Accolades
5.3 Environmental concerns and consequences


5.1 Box office

5.1.1 3D re-release


5.1.1 3D re-release
5.2 Accolades
5.3 Environmental concerns and consequences
6 Music
7 Theme park attractions

7.1 Finding Nemo  The Musical


7.1 Finding Nemo  The Musical
8 Video game
9 Sequel
10 References
11 External links
5.1 Box office

5.1.1 3D re-release


5.1.1 3D re-release
5.2 Accolades
5.3 Environmental concerns and consequences
5.1.1 3D re-release
7.1 Finding Nemo  The Musical
Albert Brooks as Marlin, a clownfish, Nemo's father
Ellen DeGeneres as Dory, a Pacific regal blue tang
Alexander Gould as Nemo, a juvenile clownfish, Marlin's son
Willem Dafoe as Gill, a moorish idol
Brad Garrett as Bloat, a pufferfish
Allison Janney as Peach, a starfish
Austin Pendleton as Gurgle, a royal gramma
Stephen Root as Bubbles, a yellow tang
Vicki Lewis as Deb (and "Flo", Deb's reflection), a four-striped damselfish
Joe Ranft as Jacques, a Pacific cleaner shrimp
Geoffrey Rush as Nigel, an Australian pelican
John Ratzenberger as the school of moonfish
Andrew Stanton as Crush, a green sea turtle.
Nicholas Bird as Squirt, a juvenile sea turtle, Crush's son
Bob Peterson as Mr. Ray, a spotted eagle ray, Nemo's school teacher
Barry Humphries as Bruce, a great white shark
Eric Bana as Anchor, a hammerhead shark, Bruce's sidekick
Bruce Spence as Chum, a mako shark, Bruce's sidekick
Jordy Ranft as Tad, a juvenile yellow longnose butterflyfish
Erica Beck as Pearl, a juvenile flapjack octopus
Erik Per Sullivan as Sheldon, a juvenile seahorse
Bill Hunter as Dr. Philip Sherman, the dentist who captured Nemo on a SCUBA diving trip
LuLu Ebeling as Darla, Dr. Sherman's niece, known as a "fish-killer"
Elizabeth Perkins as Coral, Marlin's wife and Nemo's mother
Rove McManus as a crab
Kids Choice Awards for Favorite Movie and Favorite Voice from an Animated Movie, Ellen DeGeneres.
Saturn Award for Best Supporting Actress, Ellen DeGeneres
Two Chicago Film Critics Association Awards for Best Picture and Best Supporting Actress, Ellen DeGeneres
A Golden Globe Award for Best Motion Picture  Musical or Comedy
Two MTV Movie Awards for Best Movie and Best Comedic Performance, Ellen DeGeneres
AFI's 100 Years...100 Movies (10th Anniversary Edition)  Nominated
AFI's 10 Top 10  #10 Animated film
Disneyland Resort
2007 Finding Nemo Submarine Voyage (Disneyland Park)
2005 Turtle Talk with Crush (Disney California Adventure Park)
Walt Disney World Resort
2004 Turtle Talk with Crush (Epcot)
2007 The Seas with Nemo & Friends (Epcot)
2007 Finding Nemo  The Musical (Disney's Animal Kingdom)
2012 Disney's Art of Animation Resort
Disneyland Resort Paris
2007 Crush's Coaster (Walt Disney Studios Park)
Tokyo Disney Resort
2009 Turtle Talk with Crush (Tokyo DisneySea)
Hong Kong Disneyland Resort
2008 Turtle Talk with Crush (Hong Kong Disneyland)
Official website from Disney
Official website from Pixar
Finding Nemo at the Internet Movie Database
Finding Nemo at AllRovi
Finding Nemo at the Big Cartoon DataBase
Finding Nemo at Rotten Tomatoes
Finding Nemo at Metacritic
Finding Nemo at Box Office Mojo
.
#(`*List of Pixar shorts*`)#.
This is a list of Pixar shorts. Beginning with Pixar's second film A Bug's Life, all subsequent Pixar feature films have been shown along with a theatrically released Pixar-created short. Other Pixar shorts, released only on home media, were created to showcase what Pixar can do (either technologically or cinematically), or were created specifically for a client.
The first shorts were made while Pixar was still a computer hardware company, when John Lasseter was the only professional animator in the company's tiny animation department. Starting with Geri's Game, after Pixar had turned into an animation studio, all the later shorts have been produced with a larger crew and budget.
In 1991, Pixar made four CGI shorts produced for the TV series Sesame Street. The shorts illustrates different weights and directions starring Luxo Jr. and Luxo - Surprise (1992), Light and Heavy (1990), Up and Down (1993), and Front and Back (1994).[1]
Also, beginning with A Bug's Life, Pixar has created extra content for each of their films that is not part of the main story. For their early theatrical releases, this content was in the form of "movie outtakes" and appeared as part of the movie's credits. For each of their films since Monsters, Inc. (Finding Nemo and Toy Story 3 excluded), this content was a short made exclusively for the DVD release of the film.[2] As of 2010, all of the short films except BURN-E and Dug's Special Mission are available to purchase on Apple's iTunes Store.
Notes:
* non-compilation
**Lucasfilm
In 2008, Pixar began making shorts with previously developed characters. These shorts premiered on Toon Disney, Disney Channel or as a theatrical release.
Pixar released Pixar Short Films Collection Volume 1, a collection of their short films, on DVD and Blu-ray on November 6, 2007. The disc is an updated version of the earlier-released VHS tape Tiny Toy Stories, and includes all of Pixar's shorts through 2006's Lifted, including the short The Adventures of Andr and Wally B., which was made by Pixar in 1984 as a Lucasfilm subsidiary before it became its own company. The second volume of Pixar shorts was released on November 13, 2012.[17]
Beginning with Finding Nemo, Pixar has created additional material to serve as instructional material or as background material to illuminate aspects of a film. For straight to DVD/Blu-ray releases or theme park attractions.
Pixar animated a series of clips featuring Luxo and Luxo Jr. for Sesame Street, which were Light & Heavy, and Surprise. Pixar also produced numerous animation tests, commonly confused with theatrical shorts, including The Beach Chair and Flag and Waves. They also produced over 30 commercials after selling their software division to support themselves before Toy Story entered production. Some of their other work includes:
Furthermore, in 1988, Apple Computer's Advanced Technology Group produced "Pencil Test," a computer-animated short to showcase the Apple Macintosh II line. Although Pixar was not officially affiliated with this film, several members of the Pixar staff advised and worked on it, including directors John Lasseter, Andrew Stanton, and producer Galyn Susman.
1 Shorts

1.1 Theatrical shorts
1.2 Home entertainment shorts


1.1 Theatrical shorts
1.2 Home entertainment shorts
2 Short series

2.1 Cars Toons
2.2 Toy Story Toons


2.1 Cars Toons
2.2 Toy Story Toons
3 Compilations
4 Additional work
5 Other work
6 References
7 External links
1.1 Theatrical shorts
1.2 Home entertainment shorts
2.1 Cars Toons
2.2 Toy Story Toons
Short Films at Pixar
.
#(`*Ratatouille*`)#.
Ratatouille (/rttui/ rat--TOO-ee; French:[a.ta.tuj]) is a traditional French Provenal stewed vegetable dish, originating in Nice. The full name of the dish is ratatouille nioise.[1]
The word ratatouille comes from Occitan ratatolha and the recipe comes from Occitan cuisine. The French touiller means to toss food. Ratatouille originated in the area around present day Occitan Provena (French: Provence) and Nia (French: Nice); the Catalan "samfaina" and the Majorcan "tombet" are versions of the same dish.[2]
Ratatouille is usually served as a side dish, but also may be served as a meal on its own (accompanied by pasta, rice or bread). Tomatoes are a key ingredient, with garlic, onions, courgette (zucchini), aubergine, bell peppers, marjoram and basil, or bay leaf and thyme, or a mix of green herbs like herbes de Provence. There is much debate on how to make a traditional ratatouille. One method is to simply saut all of the vegetables together. Some cooks, including Julia Child, insist on a layering approach, where the aubergine and the courgette are sauted separately, while the tomatoes, onion, garlic and bell peppers are made into a sauce. The ratatouille is then layered in a casserole  aubergine, courgette, tomato/pepper mixture  then baked in an oven.[3][4] A third method, favored by Jol Robuchon, is similar to the previous; however, the ingredients are not baked in an oven but rather recombined in a large pot and simmered.[5]
When ratatouille is used as a filling for savory crpes or to fill an omelette, the pieces are sometimes cut smaller than in the illustration. Also, unnecessary moisture is reduced by straining the liquid with a colander into a bowl, reducing it in a hot pan, then adding one or two tablespoons of reduced liquid back into the vegetables.
Filled aubergine dishes exist in Ligurian (Rattatuia), Bulgarian, Dalmatian/Croatian, Greek, Albanian, Maltese, Romanian, Sicilian, Turkish, Persian, and Venetian cuisine, but may include salted sardines or anchovies.[6][7] Pisto manchego and pinakbet are similar dishes in Spanish and Filipino cuisine, respectively.
American chef Thomas Keller popularized a contemporary variation, confit byaldi, for the 2007 animated film Ratatouille.[7] Ratatouille is a dish extremely popular with dieters. This is because not only is it low in fat and calories, but high in nutrients.[citation needed]
.
#(`*The Incredibles*`)#.
The Incredibles is a 2004 American computer-animated action-comedy superhero film written and directed by Brad Bird, released by Walt Disney Pictures, and the sixth film produced by Pixar Animation Studios. The story follows a family of superheroes living a quiet suburban life, forced to hide their powers. When father Bob Parr's yearning for his glory days and desire to help people drags him into battle with an evil villain and his killer robot, the entire Parr family is forced into action to save the world.
Bird, who was Pixar's first outside director, developed the film as an extension of 1960s comic books and spy films from his boyhood and personal family life. He pitched the film to Pixar after the box office disappointment of his first feature, The Iron Giant (1999), and carried over much of its staff to develop The Incredibles. The animation team was tasked with animating an all-human cast, which required creating new technology to animate detailed human anatomy, clothing and realistic skin and hair. Michael Giacchino composed the orchestral score of The Incredibles.
The film premiered on October 27, 2004 at the London Film Festival and had its general release in the United States on November 5, 2004 and performed highly at the box office, grossing $631 million during its original theatrical run. The Incredibles was met with high critical acclaim, garnering high marks from professional critics and audiences, and provoking commentary on its themes. Many critics called it the best film of 2004, receiving the 2004 Annie Award for Best Animated Feature, along with two Academy Awards. It became the first entirely animated film to win the prestigious Hugo Award for Best Dramatic Presentation.
"Supers"humans gifted with superpowerswere once seen as heroes, but collateral damage from their various good deeds led the government to create a "Supers Relocation Program", forcing the Supers to fit in among the civilians and not use their superpowers. Bob and Helen Parr, who are Supers, have married and now have three children, Violet, Dash, and Jack-Jack in the suburbs of Metroville. Violet and Dash have innate superpowers, but the toddler Jack-Jack has yet to show any. Bob, stuck in a white-collar job at an insurance agency, reminisces of his former days as Mr. Incredible, and sneaks out on Wednesday nights with his Super friend, Lucius Best (a.k.a. Frozone) to fight street crime.
One day, Bob loses his temper with his boss who refuses to let Bob help a mugging victim just outside the building, which results in Bob revealing his super strength and losing his job. While trying to figure out what to tell Helen, he finds a message from a woman named "Mirage", who asks for Mr. Incredible's help to stop a rogue robot on a distant island for a lucrative reward. Bob, claiming to Helen that he is going on a business trip, takes up Mirage's offer, and successfully defeats the powerful Omnidroid (v x8). On his return to Metroville, Bob spends his days working out and getting back into shape. He takes his super suit, torn in the battle with the Omnidroid, to Edna Mode, the fashion designer to the Supers, and asks her to repair it. She does so, and also insists on creating a new, better super suit for him. She refuses his request to add a cape, though, highlighting how this accessory has doomed several other Supers before him by getting caught on things.
Mirage soon contacts Bob with another job on the same island. On arriving, he finds the Omnidroid (v x9), rebuilt and reprogrammed to be stronger than before. While trapped by the robot, he meets its creator, the technology-savvy villain Syndrome. Bob recognizes him as a young fan, Buddy Pine, who as a teenager wanted to be Mr. Incredible's sidekick IncrediBoy but was turned down, due to Bob preferring to work alone 15 years ago. Syndrome has vowed revenge for this shunning, and sets the Omnidroid to kill Bob. Bob fakes his death and hides from the robot, discovering the body of a former Super, Gazerbeam. His curiosity piqued, he breaks into Syndrome's base and finds a computer, which outlines Syndrome's obsessive work in tracking down former Supers to lure them into fighting the Omnidroid, and using the results of those fatal battles to improve each incarnation of the machine. Bob is relieved to discover that Helen and his children are not yet identified in Syndrome's database, and learns that a final design of the Omnidroid (v x10) will be launched toward Metroville, seemingly to destroy it.
Meanwhile, Helen has become suspicious of Bob having an affair. After discovering Bob's repaired super-suit, she talks to Edna and learns she created new suits for the entire Parr family, each outfitted with a tracking device. Helen triggers Bob's, identifying the remote island but inadvertently revealing Bob's presence to Syndrome's headquarters and causing him to be captured. Helen borrows a private jet from an old friend and travels to the island. Midway she learns that Violet and Dash have stowed away while leaving Jack-Jack at home with a babysitter. As they near the island, Syndrome shoots down the jet, but Helen and the children safely make it ashore. Though Helen rescues Bob and regroups with Violet and Dash as they outrun Syndrome's guards, they are soon captured by Syndrome, who identifies all of them as a family of Supers. With the Parrs contained, Syndrome explains that he will launch the newly perfected Omnidroid to Metroville, sending the city into chaos, upon which he will appear and, using a control band, "subdue" the robot and become the city's hero. Syndrome launches the Omnidroid on a rocket and follows in his aircraft. After his departure, Violet escapes and helps to free the rest of the family, and with Mirage's help, they board a second rocket bound for the city.
In Metroville, the Omnidroid starts a path of destruction, and Syndrome enacts his plan, stopping the robot resulting in the people's cheers. The Omnidroid observes the remote-control band and fires it off Syndrome's arm, sending the villain scurrying away while the robot continues to wreck the city. The combined abilities of the Parrs and Lucius are able to best and destroy the robot, and the city welcomes them back as heroes. As they are driven back to their home, Helen anxiously calls the babysitter and learns that Syndrome has abducted Jack-Jack. When they arrive at home, Syndrome is taking the toddler to his jet, planning to raise the boy to fight against the Supers in the future. As Bob and Helen launch a rescue attempt, Jack-Jack reveals his powers of transformation and fire-creation, forcing Syndrome to drop him into Helen's waiting arms. Syndrome tries to escape, but due to Bob's intervening, his cape is caught in the suction of his aircraft's engine, which kills him. The ruined plane crashes into the Parrs' home, but Violet is able to protect the family from harm.
Three months later, the Parrs have re-adjusted to normal life, but when a new villain, the Underminer, appears, the Parrs put on their masks, ready to battle the new foe.
The Incredibles as a concept dates back to 1993, when Bird sketched the family during a period in which he tried to break into film.[2][3] Personal issues had percolated into the story as they weighed on him in life.[4] During this time, Bird had inked a production deal with Warner Bros. Animation and was in the process of directing his first feature, The Iron Giant.[5] Bird, who was then in his late thirties, began to wonder, with a measure of fear, about the conflict between career and family responsibilities. Approaching middle age and having high aspirations for his filmmaking, he pondered whether these aspirations were attainable only at the price of his family life.[4] He felt that he would completely fail at one if he focused too much on the other. He stated, "Consciously, this was just a funny movie about superheroes. But I think that what was going on in my life definitely filtered into the movie."[6] To make matters worse, The Iron Giant was released in the fall of 1999 and tanked at the box office due to mismanaged marketing on behalf on Warner Bros.[5] Although it was labeled a masterpiece, Bird was heartsick and gravitated toward his superhero story.[5][4]
He imagined it as an homage to the 1960s comic books and spy films from his boyhood and he initially tried to develop it as a traditionally animated film.[4] When it became clear The Iron Giant would not find the audience he'd hoped for in its theatrical run, he reconciled with old friend John Lasseter at Pixar in March 2000 and pitched his story idea to him.[3] Bird and Lasseter knew each other from their college years at CalArts in the 1970s.[9] Lasseter was sold on the idea and convinced Bird to come to Pixar, where the movie would be done in computer animation. The studio announced a multifilm contract with Bird on May 4, 2000.[4] This broke Pixar's mold of having directors who had all risen through the ranks, and Bird became the first outside director to be hired. In addition, it would be the company's first film in which all characters are human.[9] Bird was a departure from other Pixar directors in many more ways, bringing an auteur approach not found in their earlier productions. Where Pixar films typically had two or three directors and a battalion of screenwriters, The Incredibles was written and directed solely by Brad Bird.[10]
Bird came to Pixar with the lineup of the story's family members worked out: a mom and dad, both suffering through the dad's midlife crisis; a shy teenage girl; a cocky ten-year-old boy; and a baby. Bird had based their powers on family archetypes.[4][8] After several failed attempts to cast Edna Mode, Bird took on her voice role himself. It was an extension of the Pixar custom of tapping in-house staff whose voices came across particularly well on scratch dialogue tracks.[7] During production, Hayao Miyazaki of Studio Ghibli visited Pixar and saw the story reels of The Incredibles. When Bird asked if the reels made any sense or if they were just "American nonsense," Miyazaki replied, through an interpreter, "I think it's a very adventurous thing you are trying to do in an American film."[11]
Upon Pixar's acceptance of the project, Brad Bird was asked to bring in his own team for the production. He brought up a core group of people he worked with on The Iron Giant. Because of this, many 2D artists had to make the shift to 3D, including Bird himself. Bird found working with CG "wonderfully malleable" in a way that traditional animation is not, calling the camera's ability to easily switch angles in a given scene "marvelously adaptable." He found working in computer animation difficult in a different way than working traditionally, finding the software sophisticated and not particularly friendly.[12] Bird wrote the script without knowing the limitations or concerns that went hand-in-hand with the medium of computer animation. As a result, this was to be the most complex film for Pixar yet.[2] The characters in The Incredibles were designed by Tony Fucile and Teddy Newton, whom Bird had brought with him from Warner Bros.[13] Like most computer-animated films, The Incredibles had a year-long period of building the film from the inside out: modeling the exterior and understanding controls that work face and body  the articulation of the character  before animation could even begin.[12] Bird and Fucile tried to emphasize the graphic quality of good 2D animation to the Pixar team, who'd only worked primarily in CG. Bird attempted to incorporate teaching from Disney's Nine Old Men that the crew at Pixar had "never really emphasized."[12]
For the technical crew members, the human characters in The Incredibles posed a difficult set of challenges.[10] Bird's story was filled with elements that were difficult to animate with CGI at the time. Humans are widely considered to be the most difficult thing to execute in animation.[3] Pixar animators filmed themselves walking in order to better grasp proper human motion.[3] Creating an all-human cast required creating new technology to animate detailed human anatomy, clothing and realistic skin and hair. Although the technical team had some experience with hair and cloth in Monsters, Inc. (2001), the amount of hair and cloth required for The Incredibles had never been done by Pixar until this point. Moreover, Bird would tolerate no compromises for the sake of technical simplicity. Where the technical team on Monsters, Inc. had persuaded director Pete Docter to accept pigtails on Boo to make her hair easier to animate, the character of Violet had to have long hair that obscured her face; it was integral to her character.[10] Violet's long hair was extremely difficult to achieve and for the longest time during production, it was not possible. In addition, animators had to adapt to having hair underwater and blowing through the wind.[12] Disney was initially reluctant to make the film because of these issues, feeling a live-action film would be preferable, though Lasseter vetoed this.[14]
The Incredibles not only dealt with the trouble of animating CG humans, but also voluminous amounts of even more complication. The story was bigger than any prior story at the studio, was longer in running time, and had four times the number of locations.[12][15] Supervising technical director Rick Sayre noted that the hardest thing about The Incredibles was that there was "no hardest thing," alluding to the amount of new technical challenges: fire, water, air, smoke, steam, and explosions were all additional to the new difficulty of working with humans.[12] The organization structure of The Incredibles could not be mapped out like previous Pixar features, and it became a running joke to the team.[12] Sayre said the team adopted Alpha Omega," where one team was concerned with building modeling, shading and layout and another that dealt with final camera, lighting and effects. Another team, dubbed the character team, digitally sculpted, rigged and shaded the characters, and a simulation team was responsible for developing simulation technology for hair and clothing.[12] There were 781 visual effects shots in The Incredibles and they were quite often the gag, such as the shattering when Bob angrily shuts the car door. In addition, the effects team improved upon the modeling of clouds, being able to model them for the first time with volumetric rendering.[12]
The skin of the characters gained a new level of realism from a technology to produce what is known as "subsurface scattering."[13] The challenges did not stop with modeling humans. Bird decided that in a shot near the end of the film, baby Jack-Jack would undergo a series of transformations, and in one of the five planned he would turn himself into a kind of goo. Technical directors believed it would take upwards of two months to work out the goo effect, and production was at a point where two months of their time was indescribably precious.[16] They petitioned to the film's producer, John Walker for help. Bird, who had brought Walker over from Warner Bros., took great exception to the idea that Jack-Jack could undergo a mere four transformations and that The Incredibles could do without the goo-baby. They argued over the issue in several invective-laced meetings for two months until Bird finally gave in.[16] Bird also insisted that the storyboards define the blocking of characters' movements, lighting, and camera moves, which had previously been left to other departments rather than storyboarded.[10]
Bird self-admitted that he "had the knees of [the studio] trembling under the weight" of The Incredibles, but called the film a testament to the talent of the animators at Pixar, who were admiring the challenges the film provoked.[12] He recalled, "Basically, I came into a wonderful studio, frightened a lot of people with how many presents I wanted for Christmas, and then got almost everything I asked for."[14]
The Incredibles is the first Pixar film to be scored by Michael Giacchino. Brad Bird was looking for a specific sound as inspired by the design of the film  the future as seen from the 1960s. John Barry was the first choice to do the film's score, with a trailer of the film given a rerecording of Barry's theme to On Her Majesty's Secret Service. However Barry did not wish to duplicate the sound of some of his earlier soundtracks;[17] the assignment was instead given to Michael Giacchino.[18] Giacchino noted that recording in the 1960s was largely different than modern day recording and Dan Wallin, the recording engineer, said that Bird wanted a very old feel, and as such the score was recorded on analogue tapes. Wallin noted that brass instruments, which are at the forefront of the score of The Incredibles, sound better on analog equipment rather than digital. Wallin came from an era in which music was recorded, according to Giacchino, "the right way," which consists of everyone in the same room, "playing against each other and feeding off each other's energy." Tim Simonec was the conductor/orchestrator for the recording of the score.[19]
The completely orchestral score was released on November 2, 2004, three days before the film opened in theaters. It won numerous awards for Best Score and was nominated for a Grammy Award.
Several film reviewers drew precise parallels between The Incredibles and certain superhero comic books, like Powers, Watchmen, and Fantastic Four. Indeed, the producers of the 2005 adaptation of the Fantastic Four were forced to make significant script changes and add more special effects because of similarities to The Incredibles.[20] Bird was not surprised that comparisons arose due to superheroes being "the most well-trod turf on the planet," but noted that he'd not been inspired by any comic books specifically, only having heard of Watchmen. He did comment that it was nice to be compared to something as highly regarded as Watchmen.[8]
Some commentators took Bob's frustration with celebrating mediocrity and Syndrome's comment that if "everyone is super, then no one is" as a reflection of views shared by German philosopher Friedrich Nietzsche or an extension of Russian-American novelist's Ayn Rand's Objectivism philosophy, which Bird felt was "ridiculous."[3][8] He stated that a large portion of the audience understood the satire whereas "two percent thought I was doing The Fountainhead or Atlas Shrugged." Some purported that The Incredibles exhibited a right-wing bias, which Bird found silly. "I think that's as silly of an analysis as saying The Iron Giant was left-wing. I'm definitely a centrist and feel like both parties can be absurd."[3]
The film also explored Bird's dislike for the tendency of the children's comics and Saturday morning cartoons of his youth to portray villains as unrealistic, ineffectual, and non-threatening. In the movie, Dash and Violet have to deal with villains who are perfectly willing to use deadly force against children. On another level, both Dash and Violet display no emotion or regret at the deaths of those who are trying to kill them, such as when Dash outruns pursuers who crash their vehicles while chasing him, or when both of them witness their parents destroy several attacking vehicles with people inside, in such a manner that the deaths of those piloting them is undeniable. Despite disagreeing with some analysis, Bird felt it gratifying for his work to be considered on many different levels, which was his intention: "The fact that it was written about in the op/ed section of the New York Times several times was really gratifying to me. Look, it's a mainstream animated movie, and how often are those considered thought provoking?"[3]
The Incredibles opened on November 5, 2004 as Pixar's first PG-rated film (due to its action sequences).[16] Its theatrical release was accompanied with a Pixar short film Boundin'.[21] While Pixar celebrated another triumph with The Incredibles, Steve Jobs was embroiled in a public feud with the head of its distribution partner, The Walt Disney Company.[22] This would eventually lead to the ousting of Michael Eisner and Disney's acquisition of Pixar the following year.
The Incredibles 2-disc Collector's Edition DVD set and VHS edition (the last VHS release of any Pixar film) were released on March 15, 2005. The DVD release of the film also includes Jack-Jack Attack and Mr. Incredible and Pals, two Pixar short films made especially for the release of The Incredibles, and Boundin', a Pixar short film which premiered with The Incredibles in theaters.[21] The Incredibles was the highest-selling DVD of 2005, with 17.38 million copies sold.[23] The film was also released on UMD for the Sony PSP.[24] It was released on Blu-ray in North America on April 12, 2011.[25]
The Incredibles received very positive reviews from film critics, receiving a 97% approval rating at Rotten Tomatoes[27] which made the movie the fifteenth greatest action film of all time and the only one of Top 20 with more than 100 reviews.[28] Metacritic indicates The Incredibles "universal acclaim" with a 90 out of 100 rating.[29] Critic Roger Ebert awarded the film 3 12 stars out of four, writing that the film "alternates breakneck action with satire of suburban sitcom life" and is "another example of Pixar's mastery of popular animation."[30] Rolling Stone gave the film 3 12 stars and called the movie "one of the year's best" and said that it "doesn't ring cartoonish, it rings true."[31] Also giving the film 3 12 stars, People magazine found that The Incredibles "boasts a strong, entertaining story and a truckload of savvy comic touches."[32]
Eleanor Ringel Gillespie of the Atlanta Journal-Constitution was bored by the film's recurring pastiches of earlier action films, concluding, "the Pixar whizzes do what they do excellently; you just wish they were doing something else."[33] Similarly, Jessica Winter of the Village Voice criticized the film for playing as a standard summer action film, despite being released in early November. Her review, titled as "Full Metal Racket," noted that "The Incredibles announces the studio's arrival in the vast yet overcrowded Hollywood lot of eardrum-bashing, metal-crunching action sludge."
Peter Travers of Rolling Stone named The Incredibles No.6 on his list of the best films of the decade, writing "Of all the Pixar miracles studded through the decade, The Incredibles still delights me the most. It's not every toon that deals with midlife crisis, marital dysfunction, child neglect, impotence fears, fashion faux pas and existential angst."[34]
Despite concerns that the film would receive underwhelming results,[35] the film grossed $70,467,623 in its opening weekend from 7,600 screens at 3,933 theaters, averaging $17,917 per theater or $9,272 per screen, the highest opening weekend gross for a Pixar film (the record was later broken in 2010 by Toy Story 3, with $110 million), the highest opening weekend for a non-sequel animated feature (the record was broken in 2007 by The Simpsons Movie, with $74 million), and the highest opening weekend for a non-franchise-based film for just over five years when Avatar opened with $77 million. The film was also No. 1 in its second weekend, grossing another $50,251,359, dropping just 29 percent, and easily out-grossing new animated opener The Polar Express. The film ultimately grossed $261,441,092, as the fourth highest-grossing Pixar film behind Toy Story 3 ($415 million), Finding Nemo ($380.6 million), and Up ($293 million), and the fifth highest-grossing film of 2004.[36] Worldwide, the film grossed $631,442,092, as the fourth highest-grossing Pixar film also behind Toy Story 3 ($1.063 billion), Finding Nemo ($921.5 million), and Up ($731.3 million), and ranked fourth for 2004.[37] The film was also the second highest-grossing 2004 animated film behind Shrek 2 ($919.8 million).
It had its network television premiere on Thanksgiving Day 2007 on NBC sponsored by Target and its basic cable premiere on ABC Family as part of The 25 Days of Christmas in December 2007, and its second cable showing on Disney Channel as part of the No Ordinary Friday on February 1, 2008.
The Incredibles won the Academy Award for Best Animated Feature, beating two DreamWorks films, Shrek 2 and Shark Tale, as well as Best Sound Editing at the 77th Academy Awards. It also received nominations for Best Original Screenplay (for writer/director Brad Bird) and Best Sound Mixing (Randy Thom, Gary Rizzo and Doc Kane).[38] It was Pixar's first feature film to win multiple Oscars, followed in 2010 by Up. Joe Morgenstern of the Wall Street Journal called The Incredibles the best picture of the year.[8]Premiere magazine released a cross-section of all the top critics in America and The Incredibles placed at number three, whereas review aggregator website Rotten Tomatoes cross-referenced reviews that suggested it was the highest-rated film of its year.[8]
The film also received the 2004 Annie Award for Best Animated Feature and the 2005 Hugo Award for Best Dramatic Presentation, Long Form, and it was nominated for the 2004 Golden Globe Award for Best Motion Picture  Musical or Comedy. It also won the Saturn Award for Best Animated Film.
The American Film Institute nominated The Incredibles for its Top 10 Animated Films list.[39]
Several companies released promotional products related to the movie. Dark Horse Comics released a limited series of comic books based on the movie. Kellogg's released an Incredibles-themed cereal, as well as promotional Pop Tarts and fruit snacks, all proclaiming an "Incrediberry Blast" of flavor. Pringles included potato chips featuring the superheroes and the movie quotes. Furthermore, in the weeks before the movie's opening, there were also promotional tie-ins with SBC Communications (using Dash to promote the "blazing-fast speed" of its SBC Yahoo! DSL service) Tide, Downy, Bounce and McDonald's. Toy maker Hasbro produced a series of action figures and toys based on the film, although the line was not as successful as the film itself.
In Europe, Kinder chocolate eggs contained small plastic toy characters from the film.
In Belgium, car manufacturer Opel sold special The Incredibles editions of their cars.
In the United Kingdom, Telewest promoted blueyonder internet services with branding from the film, including television adverts starring characters from the film.
In all merchandising outside of the film itself, Elastigirl is referred to as Mrs. Incredible. This is due to a licensing agreement between DisneyPixar and DC Comics, who has a character named Elasti-Girl (a member of the Doom Patrol). The DC Comics character is able to grow and shrink at will from microscopic size to thousands of feet tall.
In July 2008, it was announced that a series of comic books based on The Incredibles would be published by BOOM! Studios in collaboration with Disney Publishing by the end of the year.[40]
The first miniseries by BOOM! was The Incredibles: Family Matters by Mark Waid and Marcio Takara, which was published from March to June 2009, and collected into a trade paperback published in July of that year. An ongoing series written by both Mark Waid and Landry Walker, with art by Marcio Takara and Ramanda Kamarga, began later that same year, running for sixteen issues before being cancelled in October 2010. Marvel has begun a reprint of the series starting in August 2011 and possibly finish the storyline, which was abruptly cancelled despite scripts and art having been produced for a finale.
A video game based on the film was released for the PlayStation 2, Xbox, Nintendo GameCube, Game Boy Advance, PC, Apple Macintosh, and mobiles. Though based on the movie, several key scenes are altered from the original script.
A second game, The Incredibles: Rise of the Underminer, was released for PlayStation 2, GameCube, Xbox, Mac OS X, Game Boy Advance, Nintendo DS, and Windows. Taking place immediately after the movie, the sequel focuses on Mr. Incredible and Frozone as they do battle with the megalomaniacal mole, The Underminer.
A third game, The Incredibles: When Danger Calls, was released for Windows and Mac OS X. It is a collection of 10 games and activities for the playable characters to perform.
Another game, Kinect Rush: A Disney Pixar Adventure, was released on March 20, 2012, for Xbox 360. It features characters and missions from five Pixar's films: The Incredibles, Up, Cars, Ratatouille and Toy Story.[41]
In 2004, when Disney owned sequel rights, Disney announced plans to make sequels for The Incredibles and Finding Nemo without Pixar involvement. Those plans were subsequently scrapped.[citation needed]
When Disney acquired Pixar in 2006, the expectation of Disney was that Pixar would create more sequels and bankable franchises. Director Brad Bird stated in 2007 that he was open to the idea of an The Incredibles 2 if he could come up with an idea superior to the original film. "I have pieces that I think are good, but I don't have them all together," Bird said.[42]
October 27, 2004(2004-10-27) (London Film Festival)
November 5, 2004(2004-11-05) (United States)
1 Plot
2 Cast
3 Production

3.1 Writing
3.2 Animation
3.3 Music


3.1 Writing
3.2 Animation
3.3 Music
4 Themes
5 Release

5.1 Home media


5.1 Home media
6 Reception

6.1 Critical response
6.2 Box office
6.3 Accolades


6.1 Critical response
6.2 Box office
6.3 Accolades
7 Merchandising
8 Comics
9 Video game
10 Possible sequel
11 Notes
12 References
13 External links
3.1 Writing
3.2 Animation
3.3 Music
5.1 Home media
6.1 Critical response
6.2 Box office
6.3 Accolades
Craig T. Nelson as Bob Parr/Mr. Incredible, the patriarch of the Parr family, possessing super-strength.
Holly Hunter as Helen Parr/Elastigirl, Bob's wife, able to stretch her body like rubber.
Jason Lee as Buddy Pine/Syndrome, who has no super powers of his own but uses advanced technology to give himself equivalent abilities.
Spencer Fox as Dashiell Robert "Dash" Parr/The Dash, the Parrs' 4th-grader son, gifted with super-speed.
Sarah Vowell as Violet Parr, the Parrs' middle-schooler daughter, who possesses the ability to turn invisible and create a force shield around herself or other people.
Eli Fucile and Maeve Andrews as Jack-Jack Parr, Bob and Helen's infant son, who initially shows no powers but near the end of the film reveals himself to have an enormous range of abilities including shape-shifting, teleporting, laser vision, self-immolation, flight, etc.
Samuel L. Jackson as Lucius Best/Frozone, Bob's close friend, who has the ability to form ice from himself and from the humidity in the air.
Elizabeth Pea as Mirage, Syndrome's agent who lures Supers to the island.
Brad Bird as Edna Mode, fashion designer for the Supers.
Bud Luckey as Rick Dicker, the government agent overseeing the Relocation Program.
Wallace Shawn as Gilbert Huph, Bob's boss at his white-collar job.
John Ratzenberger as The Underminer, a new villain who appears at the end of the film.
Dominique Louis as Bomb Voyage, a villain from the past who uses Buddy's interference in Mr. Incredible's heroism to escape.
Micheal Bird as Tony Rydinger, a popular boy at Violet's school who develops a crush on Violet.
Kimberly Adair Clark as Honey, the wife of Frozone.
Bret Parker as Kari McKeen, the babysitter.
Lou Romano as Bernie Kropp, one of Dash's teachers who Dash is trouble to.
Wayne Canney as the principal of Dash's school.
Price, David (2008). The Pixar Touch. New York: Alfred A. Knopf. ISBN0-307-26575-7.
Official website
The Incredibles at the Internet Movie Database
The Incredibles at the Big Cartoon DataBase
The Incredibles at AllRovi
The Incredibles at Rotten Tomatoes
The Incredibles at Metacritic
The Incredibles at Box Office Mojo
.
#(`*Monsters, Inc.*`)#.
Monsters, Inc. is a 2001 American computer-animated comedy-adventure film directed by Pete Docter, released by Walt Disney Pictures, and the fourth film produced by Pixar Animation Studios. Co-directed by Lee Unkrich and David Silverman, the film stars two monsters who work for a company named Monsters, Inc.: top scarer James P. Sullivan (voiced by John Goodman)known as "Sulley"and his one-eyed assistant, Mike Wazowski (voiced by Billy Crystal). Monsters generate their city's power by scaring children, but they are terribly afraid themselves of being contaminated by children, so when one enters Monstropolis, Sulley finds his world disrupted.
Docter began developing the film in 1996 and wrote the story with Jill Culton, Jeff Pidgeon, and Ralph Eggleston. Fellow Pixar director Andrew Stanton wrote the screenplay with screenwriter Daniel Gerson. The characters went through many incarnations over the film's five-year production process. The technical team and animators found new ways to render fur and cloth realistically for the film. Randy Newman, who composed Pixar's three prior films, returned to compose their fourth.
Although the film suffered negative publicity in the form of two lawsuits against the filmmakers, filed by Lori Madrid and Stanley Mouse respectively, that were ultimately dismissed, Monsters, Inc. proved to be a major box office success from its release by Walt Disney Pictures on November 2, 2001, generating over $525 million worldwide.[1] In addition, the film received highly positive reviews from film critics and audiences, who praised both the humor and heart of the movie.
Monsters, Inc. will see a 3D re-release in theaters in 2012, followed by the release of a prequel, Monsters University, in 2013.
The parallel city of Monstropolis is inhabited by monsters and powered by the screams of children in the human world. At the Monsters, Inc. factory, employees called "Scarers" venture into children's bedrooms to scare them and collect their screams, using closet doors as portals. This is considered a dangerous task since the monsters believe children to be toxic and that touching them would be fatal. However, production is falling as children are becoming harder to scare and the company chairman Henry J. Waternoose III is determined to find a solution. The top Scarer is James P. "Sulley" Sullivan, who lives with his assistant Mike Wazowski and has a rivalry with the ever-determined chameleon-like monster Randall Boggs. During an ordinary day's work on the "Scarefloor", another scarer accidentally brings a child's sock into the factory, causing the Children Detection Agency (CDA) to arrive and cleanse him. Mike is harassed by Roz the clerk for not completing his paperwork on time.
While working late at the factory, Sulley discovers that Randall left an activated door on the Scarefloor and a young girl (voiced by Mary Gibbs) who has entered the factory, much to Sulley's horror. After a few failed attempts to put her back, he places her in his bag and hides when Randall arrives and returns the door to storage. Mike is at a restaurant on a date with his girlfriend Celia when Sulley comes to him for help, but chaos erupts when the girl is discovered in the restaurant, and the CDA is called. Sulley and Mike escape the CDA and take the girl home, discovering that she is not toxic after all. Sulley quickly grows attached to the girl and names her "Boo". The next day, they smuggle her into the factory and Mike attempts to return her through her door. Randall tries to kidnap Boo, but kidnaps Mike by mistake.
In the basement, Randall reveals to Mike he has built a torture machine ("Scream Extractor") to extract children's screams, which would make the company's current tactics redundant.Randall straps Mike to the chair for experimentation but Sulley stops Randall from experimenting the machine on Mike and reports him to Waternoose. However, Waternoose is revealed to be in allegiance with Randall and he exiles Mike and Sulley to the Himalayas. The two are taken in by the Abominable Snowman, who tells them they can return to the factory through the nearby village. Sulley heads out, but Mike refuses to follow him out of frustration. Sulley returns to the factory and rescues Boo from the Scream Extractor. Mike returns to apologise to Sulley and inadvertently helps Sulley defeat Randall in a fight.
Randall pursues Mike and Sulley as they race the factory and ride on the doors heading into storage, taking them into a giant vault where millions of closet doors are stored. Boo's laughter activates the doors and allows the chase to pass in and out of the human world. After Randall almost kills Sulley by pushing him out of an open door, Sulley and Mike trap him in the human world using a door to a trailer park, where he is mistaken for an alligator and beaten up by a pair of hillbillies.
They are finally able to access Boo's door, but Waternoose and the CDA send it back to the Scarefloor. Mike distracts the CDA, while Sulley escapes with Boo and her door while Waternoose follows. Waternoose is tricked into confessing his plan to kidnap children in the simulation bedroom and is arrested by the CDA. The CDA's leader is revealed to be Roz, who has been undercover for years trying to prove there was a scandal at Monsters Inc. Sulley and Mike say goodbye to Boo and return her home; on Rozs orders Boos door is then destroyed. Sulley becomes the new chairman of Monsters Inc., and thanks to his experience with Boo, he comes up with a plan to end the company's energy crisis.
Months later, Sulley's leadership has changed the company's workload. The monsters now enter children's bedrooms to entertain them, since laughter is ten times more powerful than screams. Mike takes Sulley aside, revealing he has almost rebuilt Boo's door, requiring only one more piece which Sulley took as a memento. Sulley enters and reunites with Boo.
The idea for Monsters, Inc. was conceived in a lunch in 1994 attended by John Lasseter, Pete Docter, Andrew Stanton and Joe Ranft during the production of Toy Story.[2] One of the ideas that came out of the brainstorming session was a film about monsters. "When we were making Toy Story", Pete Docter claimed, "everybody came up to me and said 'Hey I totally believed that my toys came to life when I left the room.' So when Disney asked us to do some more films, I wanted to tap into a child-like notion that was similar to that. I knew monsters were coming out of my closet when I was a kid. So I said 'Hey, lets do a film about monsters.'"[3]
Pete Docter began work on the film that would become Monsters, Inc. in 1996 while others focused on A Bug's Life and Toy Story 2. Its code name was Hidden City, so named for Docter's favorite restaurant in Point Richmond.[4] By early February 1997, Docter had drafted a treatment together with Harley Jessup, Jill Culton, and Jeff Pidgeon that bore some resemblance to the final film.
Docter pitched the story to Disney with some initial artwork on February 4, 1997. He and his story team left with some suggestions in hand and returned to pitch a refined version of the story on May 30, 1997. At this pitch meeting, longtime Disney animator Joe Grantwhose work stretched back to Snow White and the Seven Dwarfssuggested the title Monsters, Inc., which stuck.[5]
Docter's initial concept for the film went through many changes, but the notion of monsters living in their own world was found by Docter as an appealing and workable one.[6] Docter's original idea revolved around a 30-year-old man dealing with monsters (which he drew in a book as a child) coming back to bother him as an adult. Each monster represented a fear he had, and conquering those fears caused the monsters eventually to disappear.[7]
After Docter scrapped the initial concept of a 30-year-old terrified of monsters, he decided on a buddy story between a monster and a child titled simply Monsters, were the monster character of Sulley (known at this stage as Johnson) was an up-and-comer at his workplace, where the company's purpose was to scare children; his eventual sidekick, Mike Wazowski, had not yet been added.[8][6][8]
Between the years 1996 and 2000, the lead monster and child character went through radical changes as the story evolved. As the story continued to develop, the child's character varied in age and gender. Ultimately, the story team decided that a girl would be the best counterpart for a furry 8-foot co-star.[6] After a girl was settled apon, the character continued to undergo changes, at one point being from Ireland and at another time being an African-American character.[5] Originally the character of the little girl, known as Mary, became a fearless seven-year-old who had been toughened by years of teasing and pranks from four older brothers.[5] In stark contrast, Johnson was nervous about the possibility of losing his job after the boss at Monsters, Inc. announces a downsizing is on the way. He feels envious because another scarer, Ned (who later became Randall), is the top performer in the company.[5] Through various drafts, the ocupation of Johnson's occupation went back-and-forth from being a scarer and from working in another area of the company such as a janitor or a refinery worker, until the his final incarnation as the best scarer at Monsters, Inc.[5] Johnson was originally planned to have tentacles for feet, however, this caused many problems in early animation tests. The idea was later largely rejected, as it was thought that audience was destracted by the tenticles.[9] Mary's age also differed from draft-to-draft until the the writters settled on the age of 3. "We found that the younger she was, the more dependent she was on Sulley," Docter claimed.[3]
Eventually Johnson was renamed Sullivan, and was also planned to wear glasses throughout the film. However, the creators found it a dangerous idea because the eyes were a perfectly readable and clear way of expressing the personality of a character, thus, the idea was rejected.[9]
The idea of a monster buddy for the lead monster emerged at an April 6, 1998 "story summit" in Burbank with Disney and Pixar employees. The term coined by Lasseter, a "story summit" was a crash exercise that would yield a finished story in just two days.[10] Such a character, the group agreed, was give the lead monster someone to talk to about his predicament. Development artist Ricky Nierva drew a concept sketch of a rounded, one-eyed monster as a concept for the character, and everyone was generally receptive to it.[3] Docter named the character Mike for the father of his friend Frank Oz, a director and Muppet performer.[5] Jeff Pidgeon and Jason Katz story-boarded a test in which Mike was helping Sulley choose a tie for work and Mike Wazowski soon became a vital character in the movie.[3] Originally Mike had no arms, and had to use his legs as appendages, however due to technical difficulties arms were soon added.[3]
Screenwriter Dan Gerson joined Pixar in 1999 and worked on the film for almost two years with the filmmakers on a daily basis. Gerson considered it his first experience writing a feature film. Dan Gerson explains; "I would sit with Pete and David Silverman and we would talk about a scene and they would tell me what they were looking for. I would make some suggestions and then go off and write the sequence. We'd get together again and review it and then hand it off to a story artist. Here's where the collaborative process really kicked in. The board artist was not beholden to my work and could take liberties here and there. Sometimes I would suggest an idea about making the joke work better visually. Once the scene moved on to animation, the animators would plus the material even further."[6]
The voice role of James P. "Sulley" Sullivan went to John Goodman, the longtime co-star of the comedy series Roseanne and a regular in the films of the Coen brothers. Goodman interpreted the character to himself as the monster equivalent of a National Football League player. "He's like a seasoned lineman in the tenth year of his career," he said at the time. "He is totally dedicated and a total pro."[11] Billy Crystal, having regretted turning down the part of Buzz Lightyear years prior, accepted that of Mike Wazowski, Sulley's one-eyed best friend and scare assistant.[3][11][12][13]
In November 2000, early in the production of Monsters, Inc., Pixar packed up and moved for the second time since its Lucasfilm years.[11] The company's approximately 500 employees had become spread among three buildings, separated by a busy highway. The company moved from Point Richmond to a much bigger Emeryville campus, co-designed by Lasseter and Steve Jobs.[11]
In production, Monsters Inc. differed from earlier Pixar features in that each main character had its own lead animator: John Kahrs on Sulley, Andrew Gordon on Mike, and Dave DeVan on Boo.[14] Kahrs found that the "bearlike quality" of Goodman's voice provided an exceptionally good fit with the character. He faced a difficult challenge, however, in dealing with Sulley's sheer mass; traditionally, animators conveyed a figure's heaviness by giving it a slower, more belabored movement, but Kahrs was concerned that such an approach to a central character would give the film a sluggish feel.[14] Like Goodman, Kahrs came to think of Sulley as a football player, one whose athleticism enabled him to move quickly in spite of his size. To help the animators with Sulley and other large monsters, Pixar arranged for Rodger Kram, an expert at Berkeley, on the locomotion of heavy mammals, to come in and lecture on the subject.[14]
Adding to Sulley's lifelike appearance was an intense effort by the technical team to refine the rendering of fur. Other production houses had tackled realistic fur, most notably Rhythm & Hues in its 1993 polar bear commercials for Coca-Cola and in its talking animals' faces in Babe (1995).[14] Monsters, Inc., however, required fur on a far larger scale. From the standpoint of Pixar's engineers, the quest for fur posed several significant challenges. One was figuring out how to animate the huge numbers of hairs2,320,413 on Sulleyin a reasonably efficient way.[14] Another was making sure the hairs cast shadows on other hairs. Without self-shadowing, fur or hair takes on an unrealistic flat-colored look (The hair on Andy's toddler sister, as seen in the opening sequence of Toy Story, is an example of hair without self-shadowing.)[14]
The first fur test was with Sullivan running an obstacle course. Results were not sastifactory, as fur would get caught by objects and stretch the fur out because of the extreme amount of motion. Another simillar test was also unsuccessful with the fur going through the objects.[9]
Eventually Pixar set-up the Simulation department and created a new fur simulation program called Fizt (for "physics tool").[15] After a shot with Sulley had been animated, the Simulation department took the data for the shot and added his fur. Fizt allowed the fur to react in a natural way. When Sully moved the fur would automatically react to his movements, taking into account the effects of wind and gravity as well. The Fizt program also controlled movement on Boo's clothing, which provided another breakthrough.[15] The deceptively simple-sounding task of animating cloth was also a challenge to animate because of the hundreds of creases and rickles that automatically occured in the clothing when the wearer moved.[16] It also meant solving the complex problem of how to keep cloth untangledthat is, how to keep it from passing through itself when parts of it intersect.[17] With the Fizt program, it applyed the same system as Sulley's fur. Boo would first be animated shirtless and the Simulation department would use the Fizt program to apply the shirt over Boo's body and when she moved her cloths would react with her movements in a natural mannor.
To solve the problem of cloth-to-cloth collisions, Michael Kass, senior scientist at Pixar, was joined on Monsters, Inc. by David Baraff and Andrew Witkin and developed an algorithm they called "global intersection analysis" to handle the problem. The complexity of the shots in Monsters, Inc.  including elaborate sets such as the door vault  required more computing power to render than any of Pixar's earlier efforts combined. The render farm in place for Monsters, Inc. was made up of 3500 Sun Microsystems processors, compared with 1400 for Toy Story 2 and only 200 for Toy Story.[17]
The film was theatrically released on November 2, 2001 in the United States, in Australia on December 26, 2001, and in the United Kingdom on February 8, 2002. It was released on VHS and DVD on September 17, 2002,[18] and on Blu-ray on November 10, 2009.[19] After the success of the 3D re-release of The Lion King,[20] Disney and Pixar announced a 3D re-release of Monsters, Inc. for December 19, 2012[21] (originally a January 18, 2013 release). In addition, the film's prequel, Monsters University, will be also released in the 3-D format on June 21, 2013.
Monsters, Inc. ranked #1 at the box office its opening weekend, grossing $62,577,067 in North America alone. The film had a small drop-off of 27.2% over its second weekend, earning another $45,551,028. In its third weekend the film experienced a larger decline of 50.1%, placing itself in the second position just after Harry Potter and the Philosopher's Stone. In its fourth weekend, however, there was an increase of 5.9%. Making $24,055,001 that weekend, it is the seventh biggest (in US$) fourth weekend ever for a film.[22][23]
As of September 26, 2002, the film has a total of $255,873,250 in the United States and Canada and $269,493,347 in other territories for a worldwide gross of $525,366,597.[1] The film is Pixar's eighth highest-grossing film worldwide and fifth in North America.[24] For a time, the film went on to take the place of Toy Story 2 as the second highest-grossing animated film of all time, behind only The Lion King.[17]
In the UK, Ireland and Malta, it earned 37,264,502 ($53,335,579) in total, marking the 6th highest-grossing animated feature of all time in the country and the 32nd largest movie of all time.[25] In Japan, although earning $4,471,902 during its opening and ranking second behind The Lord of the Rings: The Fellowship of the Ring for the weekend, on subsequent weekends it moved to first place due to exceptionally small decreases or even increases and dominated for six weeks at the box office. It finally reached $74,437,612, standing as the third highest-grossing film of 2002 and the third largest US animated feature of all time in the country behind Toy Story 3 and Finding Nemo.[26]
The film received a very positive reception. Review aggregator Rotten Tomatoes reports that 95% of critics gave the film a positive review based on 167 reviews, with an average score of 7.9/10. The critical consensus was: "Even though Monsters, Inc. lacks the sophistication of the Toy Story series, it is a still delight for children of all ages."[27] Another review aggregator, Metacritic, which assigns a normalized rating out of 100 top reviews from mainstream critics, calculated a score of 78 based on 34 reviews.[28] Charles Taylor from Salon.com stated: "It's agreeable and often funny, and adults who take their kids to see it might be surprised to find themselves having a pretty good time."[29]
A. O. Scott from The New York Times gave a positive review, praising the film's use of "creative energy": "There hasn't been a film in years to use creative energy as efficiently as Monsters, Inc."[30] Although Mike Clark from USA Today thought the comedy was sometimes "more frenetic than inspired and viewer emotions are rarely touched to any notable degree," he thought the movie to be as "visually inventive as its Pixar predecessors."[31]
Reelviews film critic James Berardinelli, who gave the film 3 stars out of 4 wrote, saying that Monsters, Inc. was "one of those rare family films that parents can enjoy (rather than endure) along with their kids."[32] Roger Ebert, film critic from Chicago Sun-Times, who gave the film 3 out of 4 stars, called the film "cheerful, high-energy fun, and like the other Pixar movies, has a running supply of gags and references aimed at grownups."[33]
Lisa Schwarzbaum, a film critic for Entertainment Weekly, giving the film a B, praised the film's animation, stating "Everything from Pixar Animation Studios, the snazzy, cutting-edge computer animation outfit, looks really, really terrific, and unspools with a liberated, heppest-moms-and-dads-on-the-block iconoclasm."[34]
Monsters, Inc. won the Academy Award for Best Original Song (Randy Newman, after 15 previous nominations, for If I Didn't Have You). It was one of the first animated films to be nominated for an Academy Award for Best Animated Film (lost to Shrek). It was also nominated for Best Original Score (lost to The Lord of the Rings: The Fellowship of the Ring) and Best Sound Editing (lost to Pearl Harbor).
At the Kid's Choice Awards in 2002, it was nominated for "Favorite Voice in an Animated Movie" for Billy Crystal (who lost to Eddie Murphy in Shrek).
Monsters Inc. was Randy Newman's 4th feature film collaboration with Pixar. The end credits song "If I Didn't Have You" was sung by John Goodman and Billy Crystal.[6]
The album was nominated for the Academy Award for Best Original Score and a Grammy Award for Best Score Soundtrack Album for a Motion Picture, Television or Other Visual Media. The score lost both these awards to The Lord of the Rings: The Fellowship of the Ring, but after 16 nominations, the song "If I Didn't Have You" finally won Newman his first Academy Award for Best Original Song. It also won a Grammy for Best Song Written for a Motion Picture, Television or Other Visual Media.
All songs are written and composed by Randy Newman.
Shortly before the film's release, Pixar was sued by children's song writer Lori Madrid of Wyoming, claiming that the company had stolen her ideas from a 1997 story she penned, titled "There's a Boy in My Closet." Madrid mailed her story around to half-dozen publishers in October 1999, notably a San Francisco publishing house called Chronicle Books. No publishers expressed interest in the story, so she instead turned it into a local stage musical in the summer of 2001. As the summer came to a close, several of her friends and coworkers began urging her to see a trailer for Monsters, Inc., believing the film to be plainly based on her story. Madrid reached the same conclusions after seeing the trailer herself.[37]
After searching on the Internet, Madrid found that a book titled The Art of Monsters, Inc. had recently been published by Chronicle. Pixar had previously published books with Disney's in-house publishing arm, Hyperion. She concluded that Chronicle passed her story to Pixar in 1999, and Pixar had reciprocated by switching to Chronicle.[38]
After finding a lawyer, Madrid filed suit in October 2001 against Chronicle Books, Pixar, and Disney in a federal court in Cheyenne, Wyoming. Her lawyer asked the court to issue a preliminary injunction that would forbid Pixar and Disney from releasing the film while the suit was pending. Over their objections, however, the judge ordered a hearing on the motion for a preliminary injunction to take place on November 1, 2001  the day before the scheduled release of Monsters, Inc. on 5,800 screens in 3,200 theaters across the country.[38]
Docter and Walt Disney Motion Pictures Group chairman Dick Cook testified on Thursday, November 1 in Cheyenne as planned. Cook stated the effect of a preliminary injunction against the release of the film would be devastating, as Monsters, Inc. was one of the company's "tent-pole" films for the season. The 5,800-odd prints, he said, had already gone out from Technicolor's warehouses in California and Ohio and were sitting at theaters. Judge Clarence Brimmer did not issue the injunction and Monsters, Inc. opened as planned on November 2, nationwide. Brimmer ruled on June 26, 2002 that the film had simply nothing in common with the poem.[39]
A lawsuit was filled in a federal court in San Fransico a year after the film's release, which Stanley Mouse alleged that the characters of Mike and Sulley were based on drawings of Excuse My Dust that he had tried to sell to Hollywood in 1998. He said the film's main characters, Mike and Sully, were derived from a one-eyed creature called Wise G'Eye and a larger monster, who often appeared together in his cartoons going back to 1963.[40]
Dust would be set in Monster City, where the animated monsters worked for the Monster Corporation of America. The lawsuit also claimed that a story artist from Pixar visited Mouse in 2000 and discussed Mouse's work with him. In the film, Mike and Sully live in Monstropolis and work for Monsters, Inc.[40]
A Disney spokeswoman responded by saying only that the characters in Monsters, Inc. were "developed independently by the Pixar and Walt Disney Pictures creative teams, and do not infringe on anyone's copyrights".[41]
A prequel called Monsters University will be released on June 21, 2013. John Goodman, Billy Crystal, Steve Buscemi, Jennifer Tilly, Frank Oz and Daniel Gerson are reprising their roles of Sulley, Mike, Randall, Celia, Fungus and Smitty and Needleman, while Dan Scanlon is directing the movie. The prequel's plot focuses on Sulley and Mike's studies at the University of Fear, where they start off as rivals but soon become best friends. Boo will be absent in the film, as it takes place before they met her.
An animated short, Mike's New Car, was made by Pixar in 2002 in which the two main characters have assorted misadventures with a car Mike has just bought. This film was not screened in theaters, but is included with all home video releases of Monsters, Inc., and on Pixar's Dedicated Shorts DVD.
A manga version of Monsters, Inc. was made by Hiromi Yamafuji and distributed in Kodansha's Comic Bon Bon magazine in Japan; the manga was published in English by Tokyopop until it went out of print.[citation needed]
A series of video games, including a multi-platform video game were created based on the film.
Feld Entertainment toured a Monsters, Inc. edition of their Walt Disney's World on Ice skating tour from 2003 to 2007.
Monsters, Inc. has inspired three attractions at Disney theme parks around the world.
November 2, 2001(2001-11-02)
1 Plot
2 Voice cast
3 Production

3.1 Development
3.2 Writing
3.3 Casting
3.4 Animation


3.1 Development
3.2 Writing
3.3 Casting
3.4 Animation
4 Release
5 Reception

5.1 Box office
5.2 Critical reception
5.3 Accolades


5.1 Box office
5.2 Critical reception
5.3 Accolades
6 Music
7 Lawsuits

7.1 Lori Madrid lawsuit
7.2 Stanley Mouse lawsuit


7.1 Lori Madrid lawsuit
7.2 Stanley Mouse lawsuit
8 Prequel
9 Other media

9.1 Additional short film
9.2 Manga
9.3 Video games
9.4 Walt Disney's World on Ice
9.5 Theme park attractions


9.1 Additional short film
9.2 Manga
9.3 Video games
9.4 Walt Disney's World on Ice
9.5 Theme park attractions
10 See also
11 Notes
12 References
13 External links
3.1 Development
3.2 Writing
3.3 Casting
3.4 Animation
5.1 Box office
5.2 Critical reception
5.3 Accolades
7.1 Lori Madrid lawsuit
7.2 Stanley Mouse lawsuit
9.1 Additional short film
9.2 Manga
9.3 Video games
9.4 Walt Disney's World on Ice
9.5 Theme park attractions
John Goodman as James P. "Sulley" Sullivan  Sulley is a giant furry blue friendly and sweet monster with horns and purple spots. Even though he excels at scaring children, he is kind hearted and thoughtful by nature. Sulley is relatively laid-back, and has a relaxed, outgoing and happy personality. In the beginning of the film he is "The Best Scarer" for several months running.
Billy Crystal as Michael "Mike" Wazowski  Mike is a green monster with a ball-shaped body, a single big eyeball, and skinny arms and legs. He runs Sulley's station on the scare floor, and they are close friends and roommates. Mike has an outgoing personality and is dating Celia Mae. He has an ego that often makes him forget something obvious, such as how his face is obscured in advertisements for the company. He makes cameo appearances in Finding Nemo, Cars, WALL-E and Toy Story 3.
Mary Gibbs as Boo  A 2-year-old human girl who is unafraid of any monster except Randall, who regularly scares her at night. She refers to Sulley as "Kitty". The book based on the film gives Boo's "real" name as Mary Gibbs, the name of her voice actress. In the film, one of Boo's drawings is covered with the name "Mary."
Steve Buscemi as Randall Boggs  An impatient, multi-legged lizard-shaped monster with a chameleon-like ability to change skin color and blend in completely with his surroundings. He is Mike and Sulley's rival in scream collection.
Jennifer Tilly as Celia Mae  A gorgon-like monster with one eye, snakes for hair, and tentacle-like legs. She is Mike's girlfriend and the receptionist for Monsters, Inc.
James Coburn as Henry J. Waternoose III  A spider or crab-like monster with five eyes. At the start of the film, he is CEO of Monsters, Inc., the job having been in his family for three generations. He somewhat holds a mentor-like relationship with Sulley, believing him to be the best scarer.
Bob Peterson as Roz  A slug-like monster with a raspy voice, similar to Selma Diamond's. She is the administrative clerk for Scarefloor F and "number 1" in the CDA, who has been doing secret work around Monsters, Inc. for about 2 years.
Frank Oz as Jeff Fungus  Randall's red-skinned three-eyed assistant and reluctant participant in the plot.
John Ratzenberger as The Abominable Snowman  A yeti banished to the Himalayas. He is also a relative of Bigfoot who like him and the Loch Ness Monster were also banished.
Samuel Lord Black as George Sanderson  A furry monster with a horn on top of his head, he was frequently assisted by Charlie. He is the butt of a running gag in which he repeatedly contacts human artifacts by accident (due to the static cling of his fur), triggering "2319" incidents and humorously overblown reactions by the CDA resulting in the removal of his hair.
Phil Proctor as Charlie - George's assistant with blue skin, two octopus like arms, four tantacles as feet and snail-like eyes. He is very friendly and admires Sulley and Mike's work.
Dan Gerson as Smitty and Needleman  Two goofy monsters with cracking voices who work as janitors and operate the Door Shredder when required. They (almost excessively) idolize Sulley.
Bonnie Hunt as Ms. Flint  A snake-like monster who trains new monsters to scare children.
Jeff Pidgeon as Thaddeus "Phlegm" Bile  A trainee scarer for Monsters, Inc.
AFI's 100 Years...100 Songs:

If I Didn't Have You  Nominated[35]


If I Didn't Have You  Nominated[35]
AFI's 10 Top 10  Nominated Animated Film[36]
If I Didn't Have You  Nominated[35]
In 2006 Monsters, Inc. Mike & Sulley to the Rescue! opened at Disney California Adventure Park at the Disneyland Resort in Anaheim, California. The dark ride was developed to boost the theme park's lagging attendance, and was quite successful in doing so for a short time.
In 2007 Monsters, Inc. Laugh Floor opened at the Magic Kingdom at the Walt Disney World Resort in Lake Buena Vista, Florida, replacing The Timekeeper. The show is improvisational in nature, and features the opportunity for Guests to interact with the monster comedians, and even submit jokes of their own via text message. The attraction has been praised for its originality.
In 2009 Monsters, Inc.: Ride & Go Seek opened at Tokyo Disneyland at the Tokyo Disney Resort in Chiba, Japan.
List of animated feature-length films
List of computer-animated films
Price, David (2008). The Pixar Touch. New York: Alfred A. Knopf. ISBN0-307-26575-7.
Official website
Monsters, Inc. at the Internet Movie Database
Monsters, Inc. at the Big Cartoon DataBase
Monsters, Inc. at AllRovi
Monsters, Inc. at Rotten Tomatoes
Monsters, Inc. at Metacritic
Monsters, Inc. at Box Office Mojo
.
#(`*Brave*`)#.
Brave(s) or The Brave(s) may refer to:
1 Common meanings
2 Sports teams

2.1 Baseball
2.2 Basketball
2.3 American football
2.4 Ice hockey
2.5 University teams
2.6 Other


2.1 Baseball
2.2 Basketball
2.3 American football
2.4 Ice hockey
2.5 University teams
2.6 Other
3 Arts and entertainment

3.1 Films
3.2 Music
3.3 Other


3.1 Films
3.2 Music
3.3 Other
4 Ships
5 Other uses
6 See also
2.1 Baseball
2.2 Basketball
2.3 American football
2.4 Ice hockey
2.5 University teams
2.6 Other
3.1 Films
3.2 Music
3.3 Other
Bravery, the ability to confront fear, pain, danger, uncertainty, or intimidation
A Native American warrior
Atlanta Braves, an American Major League Baseball team (originally the Boston Braves, then the Milwaukee Braves), or their affiliates:

Anderson Braves, a former affiliate
Austin Braves, a former affiliate of the Milwaukee and Atlanta Braves
Danville Braves, a farm team
Dominican Summer League Braves
Evansville Braves, minor league team affiliated with the Boston and Milwaukee Braves in the 1940s and 1950s
Greenwood Braves, a former affiliate
Gulf Coast League Braves
Gwinnett Braves, a Triple-A affiliate previously the Richmond Braves (see below)
Jacksonville Braves, a former Class A affiliate of the Milwaukee Braves
Kingsport Braves, a former affiliate
Macon Braves, a Class A affiliate now the Rome Braves (see below)
Mississippi Braves, in Class AA
Richmond Braves, a defunct Triple-A affiliate
Rome Braves, in Class A
Savannah Braves, a former affiliate
Sumter Braves, a former affiliate
Utica Braves, a former affiliate of the Boston Braves and other teams
Ventura Braves, a former affiliate of the Boston Braves
Waycross Braves, a former affiliate of the Milwaukee Braves
Wichita Braves, a former Class AAA affiliate of the Milwaukee Braves


Anderson Braves, a former affiliate
Austin Braves, a former affiliate of the Milwaukee and Atlanta Braves
Danville Braves, a farm team
Dominican Summer League Braves
Evansville Braves, minor league team affiliated with the Boston and Milwaukee Braves in the 1940s and 1950s
Greenwood Braves, a former affiliate
Gulf Coast League Braves
Gwinnett Braves, a Triple-A affiliate previously the Richmond Braves (see below)
Jacksonville Braves, a former Class A affiliate of the Milwaukee Braves
Kingsport Braves, a former affiliate
Macon Braves, a Class A affiliate now the Rome Braves (see below)
Mississippi Braves, in Class AA
Richmond Braves, a defunct Triple-A affiliate
Rome Braves, in Class A
Savannah Braves, a former affiliate
Sumter Braves, a former affiliate
Utica Braves, a former affiliate of the Boston Braves and other teams
Ventura Braves, a former affiliate of the Boston Braves
Waycross Braves, a former affiliate of the Milwaukee Braves
Wichita Braves, a former Class AAA affiliate of the Milwaukee Braves
Orix BlueWave, formerly the Hankyu Braves and Orix Braves, a defunct Japanese professional baseball team
Salisbury Braves, a defunct affiliate of the Houston Colt .45s in 1961 and the New York Mets in 1962
Kilgore Braves, an East Texas League baseball team in 1936
Bourne Braves, a collegiate summer baseball team in Massachusetts
Staunton Braves, a collegiate summer baseball team in Virginia
Anderson Braves, a former affiliate
Austin Braves, a former affiliate of the Milwaukee and Atlanta Braves
Danville Braves, a farm team
Dominican Summer League Braves
Evansville Braves, minor league team affiliated with the Boston and Milwaukee Braves in the 1940s and 1950s
Greenwood Braves, a former affiliate
Gulf Coast League Braves
Gwinnett Braves, a Triple-A affiliate previously the Richmond Braves (see below)
Jacksonville Braves, a former Class A affiliate of the Milwaukee Braves
Kingsport Braves, a former affiliate
Macon Braves, a Class A affiliate now the Rome Braves (see below)
Mississippi Braves, in Class AA
Richmond Braves, a defunct Triple-A affiliate
Rome Braves, in Class A
Savannah Braves, a former affiliate
Sumter Braves, a former affiliate
Utica Braves, a former affiliate of the Boston Braves and other teams
Ventura Braves, a former affiliate of the Boston Braves
Waycross Braves, a former affiliate of the Milwaukee Braves
Wichita Braves, a former Class AAA affiliate of the Milwaukee Braves
Buffalo Braves, a National Basketball Association team now the Los Angeles Clippers
Bendigo Braves, in the Australian Basketball Association
Elizabeth Braves, a 1940s American Basketball League team
Washington Redskins, originally the Boston Braves, a National Football League team
Syracuse Braves, an American professional football team in 1936 and 1937
Burlington Braves, a junior football league team based in Ontario, Canada
Boston Braves (AHL), a former American Hockey League team
St. Louis Braves, a former affiliate of the National Hockey League Chicago Black Hawks
Brockville Braves, a Junior "A" hockey team from Ontario, Canada
Saanich Braves, a Junior "B" hockey team in British Columbia, Canada
Spokane Braves, a Junior "B" hockey team in Washington state
Tavistock Braves, a Junior hockey team from Ontario
Valleyfield Braves, a Junior "AAA" hockey team from Quebec, Canada
Bradley Braves, the athletics teams of Bradley University, Illinois
the athletics teams of the University of North Carolina at Pembroke
Ottawa Braves, the athletics teams of Ottawa University, Kansas
Indianapolis Braves, an American soccer team
Boston Braves (rugby league), an American National Rugby League team
Kitchener-Waterloo Braves, a Junior "A" box lacrosse team from Ontario, Canada
Brave (1994 film), a concept film based on the Marillion album (see below)
The Brave, a 1997 film starring Johnny Depp
Brave, a Thai film featuring Afdlin Shauki
Brave (2012 film), a computer-animated 3-D film produced by Pixar

Brave (video game), based on the 2012 film


Brave (video game), based on the 2012 film
Brave (video game), based on the 2012 film
Brave Entertainment, a South Korean record label
The Braves, an American hip hop group
Ysabella Brave (born 1979), American singer
Brave (Jamie O'Neal album), also the title song
Brave (Jennifer Lopez album), also the title song
Brave (Kate Ceberano album), or the title song
Brave (Marillion album), or the title song
Brave (Nichole Nordeman album), or the title song
"Brave" (Idina Menzel song)
"Brave" (Kelis song)
"Brave", a song by Katatonia from Brave Murder Day
"Brave", a song by Leona Lewis from Echo
The Brave, a novel by Gregory Mcdonald; basis for the 1997 film
Brave: Shaman's Challenge, a 2009 puzzle video game for Nintendo DS
Brave: The Search for Spirit Dancer, a 2005 platformer video game for PlayStation 2
Brave series, a toy and anime franchise
HMS Brave, at least five ships of the Royal Navy
HMS Arab (1798), a post ship formerly the French privateer Brave, captured in 1798
Brave class fast patrol boat, formerly employed by the Royal Navy Coastal Forces division
French ship Brave, numerous vessels
Brave Mountain, Labrador, Canada
Kansu Braves, an army division of Chinese Muslims from Kansu that fought in the Boxer Rebellion
List of people known as the Brave
All pages with titles containing "Brave"
.
#(`*Natural language processing*`)#.
Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages. As such, NLP is related to the area of humancomputer interaction. Many challenges in NLP involve natural language understanding -- that is, enabling computers to derive meaning from human or natural language input.
The history of NLP generally starts in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published his famous article "Computing Machinery and Intelligence" which proposed what is now called the Turing test as a criterion of intelligence. This criterion depends on the ability of a computer program to impersonate a human in a real-time written conversation with a human judge, sufficiently well that the judge is unable to distinguish reliably on the basis of the conversational content alone between the program and a real human.
The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten years long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.
Some notably successful NLP systems developed in the 1960s were SHRDLU, a natural language system working in restricted "blocks worlds" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 to 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the "patient" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to "My head hurts" with "Why do you say your head hurts?".
During the 70's many programmers began to write 'conceptual ontologies', which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.
Up to the 1980s, most NLP systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing. This was due both to the steady increase in computational power resulting from Moore's Law and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3] Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.
Many of the notable early successes occurred in the field of machine translation, due especially to work at IBM Research, where successively more complicated statistical models were developed. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.
Recent research has increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms are able to learn from data that has not been hand-annotated with the desired answers, or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results.
Modern NLP algorithms are based on machine learning, especially statistical machine learning. The paradigm of machine learning is different from that of most prior attempts at language processing. Prior implementations of language-processing tasks typically involved the direct hand coding of large sets of rules. The machine-learning paradigm calls instead for using general learning algorithms often, although not always, grounded in statistical inference to automatically learn such rules through the analysis of large corpora of typical real-world examples. A corpus (plural, "corpora") is a set of documents (or sometimes, individual sentences) that have been hand-annotated with the correct values to be learned.
Many different classes of machine learning algorithms have been applied to NLP tasks. These algorithms take as input a large set of "features" that are generated from the input data. Some of the earliest-used algorithms, such as decision trees, produced systems of hard if-then rules similar to the systems of hand-written rules that were then common. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature. Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.
Systems based on machine-learning algorithms have many advantages over hand-produced rules:
The subfield of NLP devoted to learning approaches is known as Natural Language Learning (NLL) and its conference CoNLL and peak body SIGNLL are sponsored by ACL, recognizing also their links with Computational Linguistics and Language Acquisition. When the aims of computational language learning research is to understand more about human language acquisition, or psycholinguistics, NLL overlaps into the related field of Computational Psycholinguistics.
The following is a list of some of the most commonly researched tasks in NLP. Note that some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks. What distinguishes these tasks from other potential and actual NLP tasks is not only the volume of research devoted to them but the fact that for each one there is typically a well-defined problem setting, a standard metric for evaluating the task, standard corpora on which the task can be evaluated, and competitions devoted to the specific task.
In some cases, sets of related tasks are grouped into subfields of NLP that are often considered separately from NLP as a whole. Examples include:
Other tasks include:
Statistical natural-language processing uses stochastic, probabilistic and statistical methods to resolve some of the difficulties discussed above, especially those which arise because longer sentences are highly ambiguous when processed with realistic grammars, yielding thousands or millions of possible analyses. Methods for disambiguation often involve the use of corpora and Markov models. Statistical NLP comprises all quantitative approaches to automated language processing, including probabilistic modeling, information theory, and linear algebra.[5] The technology for statistical NLP comes mainly from machine learning and data mining, both of which are fields of artificial intelligence that involve learning from data.
The goal of NLP evaluation is to measure one or more qualities of an algorithm or a system, in order to determine whether (or to what extent) the system answers the goals of its designers, or meets the needs of its users. Research in NLP evaluation has received considerable attention, because the definition of proper evaluation criteria is one way to specify precisely an NLP problem, going thus beyond the vagueness of tasks defined only as language understanding or language generation. A precise set of evaluation criteria, which includes mainly evaluation data and evaluation metrics, enables several teams to compare their solutions to a given NLP problem.
The first evaluation campaign on written texts seems to be a campaign dedicated to message understanding in 1987 (Pallet 1998). Then, the Parseval/GEIG project compared phrase-structure grammars (Black 1991). A series of campaigns within Tipster project were realized on tasks like summarization, translation and searching (Hirschman 1998). In 1994, in Germany, the Morpholympics compared German taggers. Then, the Senseval and Romanseval campaigns were conducted with the objectives of semantic disambiguation. In 1996, the Sparkle campaign compared syntactic parsers in four different languages (English, French, German and Italian). In France, the Grace project compared a set of 21 taggers for French in 1997 (Adda 1999). In 2004, during the Technolangue/Easy project, 13 parsers for French were compared. Large-scale evaluation of dependency parsers were performed in the context of the CoNLL shared tasks in 2006 and 2007. In Italy, the EVALITA campaign was conducted in 2007 and 2009 to compare various NLP and speech tools for Italian; the 2011 campaign is in full progress - EVALITA web site. In France, within the ANR-Passage project (end of 2007), 10 parsers for French were compared - passage web site.
Depending on the evaluation procedures, a number of distinctions are traditionally made in NLP evaluation.
Intrinsic evaluation considers an isolated NLP system and characterizes its performance mainly with respect to a gold standard result, pre-defined by the evaluators. Extrinsic evaluation, also called evaluation in use considers the NLP system in a more complex setting, either as an embedded system or serving a precise function for a human user. The extrinsic performance of the system is then characterized in terms of its utility with respect to the overall task of the complex system or the human user. For example, consider a syntactic parser that is based on the output of some new part of speech (POS) tagger. An intrinsic evaluation would run the POS tagger on some labelled data, and compare the system output of the POS tagger to the gold standard (correct) output. An extrinsic evaluation would run the parser with some other POS tagger, and then with the new POS tagger, and compare the parsing accuracy.
Black-box evaluation requires one to run an NLP system on a given data set and to measure a number of parameters related to the quality of the process (speed, reliability, resource consumption) and, most importantly, to the quality of the result (e.g. the accuracy of data annotation or the fidelity of a translation). Glass-box evaluation looks at the design of the system, the algorithms that are implemented, the linguistic resources it uses (e.g. vocabulary size), etc. Given the complexity of NLP problems, it is often difficult to predict performance only on the basis of glass-box evaluation, but this type of evaluation is more informative with respect to error analysis or future developments of a system.
In many cases, automatic procedures can be defined to evaluate an NLP system by comparing its output with the gold standard (or desired) one. Although the cost of producing the gold standard can be quite high, automatic evaluation can be repeated as often as needed without much additional costs (on the same input data). However, for many NLP problems, the definition of a gold standard is a complex task, and can prove impossible when inter-annotator agreement is insufficient. Manual evaluation is performed by human judges, which are instructed to estimate the quality of a system, or most often of a sample of its output, based on a number of criteria. Although, thanks to their linguistic competence, human judges can be considered as the reference for a number of language processing tasks, there is also considerable variation across their ratings. This is why automatic evaluation is sometimes referred to as objective evaluation, while the human kind appears to be more "subjective."
An ISO sub-committee is working in order to ease interoperability between lexical resources and NLP programs. The sub-committee is part of ISO/TC37 and is called ISO/TC37/SC4. Some ISO standards are already published but most of them are under construction, mainly on lexicon representation (see LMF), annotation and data category registry.
1 History
2 NLP using machine learning
3 Major tasks in NLP
4 Statistical NLP
5 Evaluation of natural language processing

5.1 Objectives
5.2 Short history of evaluation in NLP
5.3 Different types of evaluation


5.1 Objectives
5.2 Short history of evaluation in NLP
5.3 Different types of evaluation
6 Standardization in NLP
7 See also
8 References
9 Further reading
10 External links
5.1 Objectives
5.2 Short history of evaluation in NLP
5.3 Different types of evaluation
The learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not obvious at all where the effort should be directed.
Automatic learning procedures can make use of statistical inference algorithms to produce models that are robust to unfamiliar input (e.g. containing words or structures that have not been seen before) and to erroneous input (e.g. with misspelled words or words accidentally omitted). Generally, handling such input gracefully with hand-written rules or more generally, creating systems of hand-written rules that make soft decisions is extremely difficult, error-prone and time-consuming.
Systems based on automatically learning the rules can be made more accurate simply by supplying more input data. However, systems based on hand-written rules can only be made more accurate by increasing the complexity of the rules, which is a much more difficult task. In particular, there is a limit to the complexity of systems based on hand-crafted rules, beyond which the systems become more and more unmanageable. However, creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked, generally without significant increases in the complexity of the annotation process.
Automatic summarization: Produce a readable summary of a chunk of text. Often used to provide summaries of text of a known type, such as articles in the financial section of a newspaper.
Coreference resolution: Given a sentence or larger chunk of text, determine which words ("mentions") refer to the same objects ("entities"). Anaphora resolution is a specific example of this task, and is specifically concerned with matching up pronouns with the nouns or names that they refer to. For example, in a sentence such as "He entered John's house through the front door", "the front door" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John's house (rather than of some other structure that might also be referred to).
Discourse analysis: This rubric includes a number of related tasks. One task is identifying the discourse structure of connected text, i.e. the nature of the discourse relationships between sentences (e.g. elaboration, explanation, contrast). Another possible task is recognizing and classifying the speech acts in a chunk of text (e.g. yes-no question, content question, statement, assertion, etc.).
Machine translation: Automatically translate text from one human language to another. This is one of the most difficult problems, and is a member of a class of problems colloquially termed "AI-complete", i.e. requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.) in order to solve properly.
Morphological segmentation: Separate words into individual morphemes and identify the class of the morphemes. The difficulty of this task depends greatly on the complexity of the morphology (i.e. the structure of words) of the language being considered. English has fairly simple morphology, especially inflectional morphology, and thus it is often possible to ignore this task entirely and simply model all possible forms of a word (e.g. "open, opens, opened, opening") as separate words. In languages such as Turkish, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms.
Named entity recognition (NER): Given a stream of text, determine which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g. person, location, organization). Note that, although capitalization can aid in recognizing named entities in languages such as English, this information cannot aid in determining the type of named entity, and in any case is often inaccurate or insufficient. For example, the first word of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized. Furthermore, many other languages in non-Western scripts (e.g. Chinese or Arabic) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names. For example, German capitalizes all nouns, regardless of whether they refer to names, and French and Spanish do not capitalize names that serve as adjectives.
Natural language generation: Convert information from computer databases into readable human language.
Natural language understanding: Convert chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate. Natural language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural language expression which usually takes the form of organized notations of natural languages concepts. Introduction and creation of language metamodel and ontology are efficient however empirical solutions. An explicit formalization of natural languages semantics without confusions with implicit assumptions such as closed world assumption (CWA) vs. open world assumption, or subjective Yes/No vs. objective True/False is expected for the construction of a basis of semantics formalization.[4]
Optical character recognition (OCR): Given an image representing printed text, determine the corresponding text.
Part-of-speech tagging: Given a sentence, determine the part of speech for each word. Many words, especially common ones, can serve as multiple parts of speech. For example, "book" can be a noun ("the book on the table") or verb ("to book a flight"); "set" can be a noun, verb or adjective; and "out" can be any of at least five different parts of speech. Note that some languages have more such ambiguity than others. Languages with little inflectional morphology, such as English are particularly prone to such ambiguity. Chinese is prone to such ambiguity because it is a tonal language during verbalization. Such inflection is not readily conveyed via the entities employed within the orthography to convey intended meaning.
Parsing: Determine the parse tree (grammatical analysis) of a given sentence. The grammar for natural languages is ambiguous and typical sentences have multiple possible analyses. In fact, perhaps surprisingly, for a typical sentence there may be thousands of potential parses (most of which will seem completely nonsensical to a human).
Question answering: Given a human-language question, determine its answer. Typical questions have a specific right answer (such as "What is the capital of Canada?"), but sometimes open-ended questions are also considered (such as "What is the meaning of life?").
Relationship extraction: Given a chunk of text, identify the relationships among named entities (e.g. who is the wife of whom).
Sentence breaking (also known as sentence boundary disambiguation): Given a chunk of text, find the sentence boundaries. Sentence boundaries are often marked by periods or other punctuation marks, but these same characters can serve other purposes (e.g. marking abbreviations).
Sentiment analysis: Extract subjective information usually from a set of documents, often using online reviews to determine "polarity" about specific objects. It is especially useful for identifying trends of public opinion in the social media, for the purpose of marketing.
Speech recognition: Given a sound clip of a person or people speaking, determine the textual representation of the speech. This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed "AI-complete" (see above). In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below). Note also that in most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process.
Speech segmentation: Given a sound clip of a person or people speaking, separate it into words. A subtask of speech recognition and typically grouped with it.
Topic segmentation and recognition: Given a chunk of text, separate it into segments each of which is devoted to a topic, and identify the topic of the segment.
Word segmentation: Separate a chunk of continuous text into separate words. For a language like English, this is fairly trivial, since words are usually separated by spaces. However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language.
Word sense disambiguation: Many words have more than one meaning; we have to select the meaning which makes the most sense in context. For this problem, we are typically given a list of words and associated word senses, e.g. from a dictionary or from an online resource such as WordNet.
Information retrieval (IR): This is concerned with storing, searching and retrieving information. It is a separate field within computer science (closer to databases), but IR relies on some NLP methods (for example, stemming). Some current research and applications seek to bridge the gap between IR and NLP.
Information extraction (IE): This is concerned in general with the extraction of semantic information from text. This covers tasks such as named entity recognition, Coreference resolution, relationship extraction, etc.
Speech processing: This covers speech recognition, text-to-speech and related tasks.
Stemming
Text simplification
Text-to-speech
Text-proofing
Natural language search
Query expansion
Automated essay scoring
Truecasing
Intrinsic vs. extrinsic evaluation
Black-box vs. glass-box evaluation
Automatic vs. manual evaluation
List of natural language processing toolkits
Biomedical text mining
Compound term processing
Computer-assisted reviewing
Controlled natural language
Deep Linguistic Processing
Foreign language reading aid
Foreign language writing aid
Language technology
Latent semantic indexing
LRE Map
Natural language programming
Reification (linguistics)
Spoken dialogue system
Telligent Systems
Transderivational search
Bates, M. (1995). Models of natural language understanding. Proceedings of the National Academy of Sciences of the United States of America, Vol. 92, No. 22 (Oct. 24, 1995), pp.99779982.
Steven Bird, Ewan Klein, and Edward Loper (2009). Natural Language Processing with Python. O'Reilly Media. ISBN 978-0-596-51649-9.
Daniel Jurafsky and James H. Martin (2008). Speech and Language Processing, 2nd edition. Pearson Prentice Hall. ISBN 978-0-13-187321-6.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schtze (2008). Introduction to Information Retrieval. Cambridge University Press. ISBN 978-0-521-86571-5. Official html and pdf versions available without charge.
Christopher D. Manning and Hinrich Schtze (1999). Foundations of Statistical Natural Language Processing. The MIT Press. ISBN 978-0-262-13360-9.
Natural Language Toolkit
.
#(`*Computer*`)#.
A computer is a general purpose device that can be programmed to carry out a finite set of arithmetic or logical operations. Since a sequence of operations can be readily changed, the computer can solve more than one kind of problem.
Conventionally, a computer consists of at least one processing element, typically a central processing unit (CPU) and some form of memory. The processing element carries out arithmetic and logic operations, and a sequencing and control unit that can change the order of operations based on stored information. Peripheral devices allow information to be retrieved from an external source, and the result of operations saved and retrieved.
The first electronic digital computers were developed between 1940 and 1945 in the United Kingdom and United States. Originally they were the size of a large room, consuming as much power as several hundred modern personal computers (PCs).[1] In this era mechanical analog computers were used for military applications.
Modern computers based on integrated circuits are millions to billions of times more capable than the early machines, and occupy a fraction of the space.[2] Simple computers are small enough to fit into mobile devices, and mobile computers can be powered by small batteries. Personal computers in their various forms are icons of the Information Age and are what most people think of as "computers". However, the embedded computers found in many devices from mp3 players to fighter aircraft and from toys to industrial robots are the most numerous.
The first use of the word "computer" was recorded in 1613, referring to a person who carried out calculations, or computations, and the word continued with the same meaning until the middle of the 20th century. From the end of the 19th century the word began to take on its more familiar meaning, a machine that carries out computations.[3]
The history of the modern computer begins with two separate technologies, automated calculation and programmability, but no single device can be identified as the earliest computer, partly because of the inconsistent application of that term. A few devices are worth mentioning though, like some mechanical aids to computing, which were very successful and survived for centuries until the advent of the electronic calculator, like the Sumerian abacus, designed around 2500 BC[4] of which a descendant won a speed competition against a modern desk calculating machine in Japan in 1946,[5] the slide rules, invented in the 1620s, which were carried on five Apollo space missions, including to the moon[6] and arguably the astrolabe and the Antikythera mechanism, an ancient astronomical computer built by the Greeks around 80 BC.[7] The Greek mathematician Hero of Alexandria (c. 1070 AD) built a mechanical theater which performed a play lasting 10minutes and was operated by a complex system of ropes and drums that might be considered to be a means of deciding which parts of the mechanism performed which actions and when.[8] This is the essence of programmability.
Around the end of the 10th century, the French monk Gerbert d'Aurillac brought back from Spain the drawings of a machine invented by the Moors that answered either Yes or No to the questions it was asked.[9] Again in the 13th century, the monks Albertus Magnus and Roger Bacon built talking androids without any further development (Albertus Magnus complained that he had wasted forty years of his life when Thomas Aquinas, terrified by his machine, destroyed it).[10]
In 1642, the Renaissance saw the invention of the mechanical calculator,[11] a device that could perform all four arithmetic operations without relying on human intelligence.[12] The mechanical calculator was at the root of the development of computers in two separate ways. Initially, it was in trying to develop more powerful and more flexible calculators[13] that the computer was first theorized by Charles Babbage[14][15] and then developed.[16] Secondly, development of a low-cost electronic calculator, successor to the mechanical calculator, resulted in the development by Intel[17] of the first commercially available microprocessor integrated circuit.
In 1801, Joseph Marie Jacquard made an improvement to the textile loom by introducing a series of punched paper cards as a template which allowed his loom to weave intricate patterns automatically. The resulting Jacquard loom was an important step in the development of computers because the use of punched cards to define woven patterns can be viewed as an early, albeit limited, form of programmability.
It was the fusion of automatic calculation with programmability that produced the first recognizable computers. In 1837, Charles Babbage was the first to conceptualize and design a fully programmable mechanical computer, his analytical engine.[20] Limited finances and Babbage's inability to resist tinkering with the design meant that the device was never completednevertheless his son, Henry Babbage, completed a simplified version of the analytical engine's computing unit (the mill) in 1888. He gave a successful demonstration of its use in computing tables in 1906. This machine was given to the Science museum in South Kensington in 1910.
In the late 1880s, Herman Hollerith invented the recording of data on a machine-readable medium. Earlier uses of machine-readable media had been for control, not data. "After some initial trials with paper tape, he settled on punched cards..."[21] To process these punched cards he invented the tabulator, and the keypunch machines. These three inventions were the foundation of the modern information processing industry. Large-scale automated data processing of punched cards was performed for the 1890 United States Census by Hollerith's company, which later became the core of IBM. By the end of the 19th century a number of ideas and technologies, that would later prove useful in the realization of practical computers, had begun to appear: Boolean algebra, the vacuum tube (thermionic valve), punched cards and tape, and the teleprinter.
During the first half of the 20th century, many scientific computing needs were met by increasingly sophisticated analog computers, which used a direct mechanical or electrical model of the problem as a basis for computation. However, these were not programmable and generally lacked the versatility and accuracy of modern digital computers.
Alan Turing is widely regarded as the father of modern computer science. In 1936 Turing provided an influential formalisation of the concept of the algorithm and computation with the Turing machine, providing a blueprint for the electronic digital computer.[22] Of his role in the creation of the modern computer, Time magazine in naming Turing one of the 100 most influential people of the 20th century, states: "The fact remains that everyone who taps at a keyboard, opening a spreadsheet or a word-processing program, is working on an incarnation of a Turing machine".[22]
The AtanasoffBerry Computer (ABC) was the world's first electronic digital computer, albeit not programmable.[23] Atanasoff is considered to be one of the fathers of the computer.[24] Conceived in 1937 by Iowa State College physics professor John Atanasoff, and built with the assistance of graduate student Clifford Berry,[25] the machine was not programmable, being designed only to solve systems of linear equations. The computer did employ parallel computation. A 1973 court ruling in a patent dispute found that the patent for the 1946 ENIAC computer derived from the AtanasoffBerry Computer.
The first program-controlled computer was invented by Konrad Zuse, who built the Z3, an electromechanical computing machine, in 1941.[26] The first programmable electronic computer was the Colossus, built in 1943 by Tommy Flowers.
George Stibitz is internationally recognized as a father of the modern digital computer. While working at Bell Labs in November 1937, Stibitz invented and built a relay-based calculator he dubbed the "Model K" (for "kitchen table", on which he had assembled it), which was the first to use binary circuits to perform an arithmetic operation. Later models added greater sophistication including complex arithmetic and programmability.[27]
A succession of steadily more powerful and flexible computing devices were constructed in the 1930s and 1940s, gradually adding the key features that are seen in modern computers. The use of digital electronics (largely invented by Claude Shannon in 1937) and more flexible programmability were vitally important steps, but defining one point along this road as "the first digital electronic computer" is difficult.Shannon 1940 Notable achievements include:
Several developers of ENIAC, recognizing its flaws, came up with a far more flexible and elegant design, which came to be known as the "stored-program architecture" or von Neumann architecture. This design was first formally described by John von Neumann in the paper First Draft of a Report on the EDVAC, distributed in 1945. A number of projects to develop computers based on the stored-program architecture commenced around this time, the first of which was completed in 1948 at the University of Manchester in England, the Manchester Small-Scale Experimental Machine (SSEM or "Baby"). The Electronic Delay Storage Automatic Calculator (EDSAC), completed a year after the SSEM at Cambridge University, was the first practical, non-experimental implementation of the stored-program design and was put to use immediately for research work at the university. Shortly thereafter, the machine originally described by von Neumann's paperEDVACwas completed but did not see full-time use for an additional two years.
Nearly all modern computers implement some form of the stored-program architecture, making it the single trait by which the word "computer" is now defined. While the technologies used in computers have changed dramatically since the first electronic, general-purpose computers of the 1940s, most still use the von Neumann architecture.
Beginning in the 1950s, Soviet scientists Sergei Sobolev and Nikolay Brusentsov conducted research on ternary computers, devices that operated on a base three numbering system of 1, 0, and 1 rather than the conventional binary numbering system upon which most computers are based. They designed the Setun, a functional ternary computer, at Moscow State University. The device was put into limited production in the Soviet Union, but supplanted by the more common binary architecture.
Computers using vacuum tubes as their electronic elements were in use throughout the 1950s, but by the 1960s they had been largely replaced by transistor-based machines, which were smaller, faster, cheaper to produce, required less power, and were more reliable. The first transistorised computer was demonstrated at the University of Manchester in 1953.[31] In the 1970s, integrated circuit technology and the subsequent creation of microprocessors, such as the Intel 4004, further decreased size and cost and further increased speed and reliability of computers. By the late 1970s, many products such as video recorders contained dedicated computers called microcontrollers, and they started to appear as a replacement to mechanical controls in domestic appliances such as washing machines. The 1980s witnessed home computers and the now ubiquitous personal computer. With the evolution of the Internet, personal computers are becoming as common as the television and the telephone in the household[citation needed].
Modern smartphones are fully programmable computers in their own right, and as of 2009 may well be the most common form of such computers in existence[citation needed].
The defining feature of modern computers which distinguishes them from all other machines is that they can be programmed. That is to say that some type of instructions (the program) can be given to the computer, and it will process them. Modern computers based on the von Neumann architecture often have machine code in the form of an imperative programming language.
In practical terms, a computer program may be just a few instructions or extend to many millions of instructions, as do the programs for word processors and web browsers for example. A typical modern computer can execute billions of instructions per second (gigaflops) and rarely makes a mistake over many years of operation. Large computer programs consisting of several million instructions may take teams of programmers years to write, and due to the complexity of the task almost certainly contain errors.
This section applies to most common RAM machine-based computers.
In most cases, computer instructions are simple: add one number to another, move some data from one location to another, send a message to some external device, etc. These instructions are read from the computer's memory and are generally carried out (executed) in the order they were given. However, there are usually specialized instructions to tell the computer to jump ahead or backwards to some other place in the program and to carry on executing from there. These are called "jump" instructions (or branches). Furthermore, jump instructions may be made to happen conditionally so that different sequences of instructions may be used depending on the result of some previous calculation or some external event. Many computers directly support subroutines by providing a type of jump that "remembers" the location it jumped from and another instruction to return to the instruction following that jump instruction.
Program execution might be likened to reading a book. While a person will normally read each word and line in sequence, they may at times jump back to an earlier place in the text or skip sections that are not of interest. Similarly, a computer may sometimes go back and repeat the instructions in some section of the program over and over again until some internal condition is met. This is called the flow of control within the program and it is what allows the computer to perform tasks repeatedly without human intervention.
Comparatively, a person using a pocket calculator can perform a basic arithmetic operation such as adding two numbers with just a few button presses. But to add together all of the numbers from 1 to 1,000 would take thousands of button presses and a lot of time, with a near certainty of making a mistake. On the other hand, a computer may be programmed to do this with just a few simple instructions. For example:
Once told to run this program, the computer will perform the repetitive addition task without further human intervention. It will almost never make a mistake and a modern PC can complete the task in about a millionth of a second.[32]
Errors in computer programs are called "bugs". They may be benign and not affect the usefulness of the program, or have only subtle effects. But in some cases they may cause the program or the entire system to "hang" become unresponsive to input such as mouse clicks or keystrokes to completely fail, or to crash. Otherwise benign bugs may sometimes be harnessed for malicious intent by an unscrupulous user writing an exploit, code designed to take advantage of a bug and disrupt a computer's proper execution. Bugs are usually not the fault of the computer. Since computers merely execute the instructions they are given, bugs are nearly always the result of programmer error or an oversight made in the program's design.[33]
Grace Hopper is credited for having first used the term "bugs" in computing after a dead moth was found shorting a relay in the Harvard Mark II computer in September 1947.[34]
In most computers, individual instructions are stored as machine code with each instruction being given a unique number (its operation code or opcode for short). The command to add two numbers together would have one opcode, the command to multiply them would have a different opcode and so on. The simplest computers are able to perform any of a handful of different instructions; the more complex computers have several hundred to choose from, each with a unique numerical code. Since the computer's memory is able to store numbers, it can also store the instruction codes. This leads to the important fact that entire programs (which are just lists of these instructions) can be represented as lists of numbers and can themselves be manipulated inside the computer in the same way as numeric data. The fundamental concept of storing programs in the computer's memory alongside the data they operate on is the crux of the von Neumann, or stored program, architecture. In some cases, a computer might store some or all of its program in memory that is kept separate from the data it operates on. This is called the Harvard architecture after the Harvard Mark I computer. Modern von Neumann computers display some traits of the Harvard architecture in their designs, such as in CPU caches.
While it is possible to write computer programs as long lists of numbers (machine language) and while this technique was used with many early computers,[35] it is extremely tedious and potentially error-prone to do so in practice, especially for complicated programs. Instead, each basic instruction can be given a short name that is indicative of its function and easy to remember a mnemonic such as ADD, SUB, MULT or JUMP. These mnemonics are collectively known as a computer's assembly language. Converting programs written in assembly language into something the computer can actually understand (machine language) is usually done by a computer program called an assembler.
Programming languages provide various ways of specifying programs for computers to run. Unlike natural languages, programming languages are designed to permit no ambiguity and to be concise. They are purely written languages and are often difficult to read aloud. They are generally either translated into machine code by a compiler or an assembler before being run, or translated directly at run time by an interpreter. Sometimes programs are executed by a hybrid method of the two techniques.
Machine languages and the assembly languages that represent them (collectively termed low-level programming languages) tend to be unique to a particular type of computer. For instance, an ARM architecture computer (such as may be found in a PDA or a hand-held videogame) cannot understand the machine language of an Intel Pentium or the AMD Athlon 64 computer that might be in a PC.[36]
Though considerably easier than in machine language, writing long programs in assembly language is often difficult and is also error prone. Therefore, most practical programs are written in more abstract high-level programming languages that are able to express the needs of the programmer more conveniently (and thereby help reduce programmer error). High level languages are usually "compiled" into machine language (or sometimes into assembly language and then into machine language) using another computer program called a compiler.[37] High level languages are less related to the workings of the target computer than assembly language, and more related to the language and structure of the problem(s) to be solved by the final program. It is therefore often possible to use different compilers to translate the same high level language program into the machine language of many different types of computer. This is part of the means by which software like video games may be made available for different computer architectures such as personal computers and various video game consoles.
Program design of small programs is relatively simple and involves the analysis of the problem, collection of inputs, using the programming constructs within languages, devising or using established procedures and algorithms, providing data for output devices and solutions to the problem as applicable. As problems become larger and more complex, features such as subprograms, modules, formal documentation, and new paradigms such as object-oriented programming are encountered. Large programs involving thousands of line of code and more require formal software methodologies. The task of developing large software systems presents a significant intellectual challenge. Producing software with an acceptably high reliability within a predictable schedule and budget has historically been difficult; the academic and professional discipline of software engineering concentrates specifically on this challenge.
A general purpose computer has four main components: the arithmetic logic unit (ALU), the control unit, the memory, and the input and output devices (collectively termed I/O). These parts are interconnected by busses, often made of groups of wires.
Inside each of these parts are thousands to trillions of small electrical circuits which can be turned off or on by means of an electronic switch. Each circuit represents a bit (binary digit) of information so that when the circuit is on it represents a "1", and when off it represents a "0" (in positive logic representation). The circuits are arranged in logic gates so that one or more of the circuits may control the state of one or more of the other circuits.
The control unit, ALU, registers, and basic I/O (and often other hardware closely linked with these) are collectively known as a central processing unit (CPU). Early CPUs were composed of many separate components but since the mid-1970s CPUs have typically been constructed on a single integrated circuit called a microprocessor.
The control unit (often called a control system or central controller) manages the computer's various components; it reads and interprets (decodes) the program instructions, transforming them into a series of control signals which activate other parts of the computer.[38] Control systems in advanced computers may change the order of some instructions so as to improve performance.
A key component common to all CPUs is the program counter, a special memory cell (a register) that keeps track of which location in memory the next instruction is to be read from.[39]
The control system's function is as followsnote that this is a simplified description, and some of these steps may be performed concurrently or in a different order depending on the type of CPU:
Since the program counter is (conceptually) just another set of memory cells, it can be changed by calculations done in the ALU. Adding 100 to the program counter would cause the next instruction to be read from a place 100 locations further down the program. Instructions that modify the program counter are often known as "jumps" and allow for loops (instructions that are repeated by the computer) and often conditional instruction execution (both examples of control flow).
The sequence of operations that the control unit goes through to process an instruction is in itself like a short computer program, and indeed, in some more complex CPU designs, there is another yet smaller computer called a microsequencer, which runs a microcode program that causes all of these events to happen.
The ALU is capable of performing two classes of operations: arithmetic and logic.[40]
The set of arithmetic operations that a particular ALU supports may be limited to addition and subtraction, or might include multiplication, division, trigonometry functions such as sine, cosine, etc., and square roots. Some can only operate on whole numbers (integers) whilst others use floating point to represent real numbers, albeit with limited precision. However, any computer that is capable of performing just the simplest operations can be programmed to break down the more complex operations into simple steps that it can perform. Therefore, any computer can be programmed to perform any arithmetic operationalthough it will take more time to do so if its ALU does not directly support the operation. An ALU may also compare numbers and return boolean truth values (true or false) depending on whether one is equal to, greater than or less than the other ("is 64 greater than 65?").
Logic operations involve Boolean logic: AND, OR, XOR and NOT. These can be useful for creating complicated conditional statements and processing boolean logic.
Superscalar computers may contain multiple ALUs, allowing them to process several instructions simultaneously.[41] Graphics processors and computers with SIMD and MIMD features often contain ALUs that can perform arithmetic on vectors and matrices.
A computer's memory can be viewed as a list of cells into which numbers can be placed or read. Each cell has a numbered "address" and can store a single number. The computer can be instructed to "put the number 123 into the cell numbered 1357" or to "add the number that is in cell 1357 to the number that is in cell 2468 and put the answer into cell 1595". The information stored in memory may represent practically anything. Letters, numbers, even computer instructions can be placed into memory with equal ease. Since the CPU does not differentiate between different types of information, it is the software's responsibility to give significance to what the memory sees as nothing but a series of numbers.
In almost all modern computers, each memory cell is set up to store binary numbers in groups of eight bits (called a byte). Each byte is able to represent 256 different numbers (2^8 = 256); either from 0 to 255 or 128 to +127. To store larger numbers, several consecutive bytes may be used (typically, two, four or eight). When negative numbers are required, they are usually stored in two's complement notation. Other arrangements are possible, but are usually not seen outside of specialized applications or historical contexts. A computer can store any kind of information in memory if it can be represented numerically. Modern computers have billions or even trillions of bytes of memory.
The CPU contains a special set of memory cells called registers that can be read and written to much more rapidly than the main memory area. There are typically between two and one hundred registers depending on the type of CPU. Registers are used for the most frequently needed data items to avoid having to access main memory every time data is needed. As data is constantly being worked on, reducing the need to access main memory (which is often slow compared to the ALU and control units) greatly increases the computer's speed.
Computer main memory comes in two principal varieties: random-access memory or RAM and read-only memory or ROM. RAM can be read and written to anytime the CPU commands it, but ROM is pre-loaded with data and software that never changes, therefore the CPU can only read from it. ROM is typically used to store the computer's initial start-up instructions. In general, the contents of RAM are erased when the power to the computer is turned off, but ROM retains its data indefinitely. In a PC, the ROM contains a specialized program called the BIOS that orchestrates loading the computer's operating system from the hard disk drive into RAM whenever the computer is turned on or reset. In embedded computers, which frequently do not have disk drives, all of the required software may be stored in ROM. Software stored in ROM is often called firmware, because it is notionally more like hardware than software. Flash memory blurs the distinction between ROM and RAM, as it retains its data when turned off but is also rewritable. It is typically much slower than conventional ROM and RAM however, so its use is restricted to applications where high speed is unnecessary.[42]
In more sophisticated computers there may be one or more RAM cache memories, which are slower than registers but faster than main memory. Generally computers with this sort of cache are designed to move frequently needed data into the cache automatically, often without the need for any intervention on the programmer's part.
I/O is the means by which a computer exchanges information with the outside world.[43] Devices that provide input or output to the computer are called peripherals.[44] On a typical personal computer, peripherals include input devices like the keyboard and mouse, and output devices such as the display and printer. Hard disk drives, floppy disk drives and optical disc drives serve as both input and output devices. Computer networking is another form of I/O.
I/O devices are often complex computers in their own right, with their own CPU and memory. A graphics processing unit might contain fifty or more tiny computers that perform the calculations necessary to display 3D graphics[citation needed]. Modern desktop computers contain many smaller computers that assist the main CPU in performing I/O.
While a computer may be viewed as running one gigantic program stored in its main memory, in some systems it is necessary to give the appearance of running several programs simultaneously. This is achieved by multitasking i.e. having the computer switch rapidly between running each program in turn.[45]
One means by which this is done is with a special signal called an interrupt, which can periodically cause the computer to stop executing instructions where it was and do something else instead. By remembering where it was executing prior to the interrupt, the computer can return to that task later. If several programs are running "at the same time", then the interrupt generator might be causing several hundred interrupts per second, causing a program switch each time. Since modern computers typically execute instructions several orders of magnitude faster than human perception, it may appear that many programs are running at the same time even though only one is ever executing in any given instant. This method of multitasking is sometimes termed "time-sharing" since each program is allocated a "slice" of time in turn.[46]
Before the era of cheap computers, the principal use for multitasking was to allow many people to share the same computer.
Seemingly, multitasking would cause a computer that is switching between several programs to run more slowly, in direct proportion to the number of programs it is running, but most programs spend much of their time waiting for slow input/output devices to complete their tasks. If a program is waiting for the user to click on the mouse or press a key on the keyboard, then it will not take a "time slice" until the event it is waiting for has occurred. This frees up time for other programs to execute so that many programs may be run simultaneously without unacceptable speed loss.
Some computers are designed to distribute their work across several CPUs in a multiprocessing configuration, a technique once employed only in large and powerful machines such as supercomputers, mainframe computers and servers. Multiprocessor and multi-core (multiple CPUs on a single integrated circuit) personal and laptop computers are now widely available, and are being increasingly used in lower-end markets as a result.
Supercomputers in particular often have highly unique architectures that differ significantly from the basic stored-program architecture and from general purpose computers.[47] They often feature thousands of CPUs, customized high-speed interconnects, and specialized computing hardware. Such designs tend to be useful only for specialized tasks due to the large scale of program organization required to successfully utilize most of the available resources at once. Supercomputers usually see usage in large-scale simulation, graphics rendering, and cryptography applications, as well as with other so-called "embarrassingly parallel" tasks.
Computers have been used to coordinate information between multiple locations since the 1950s. The U.S. military's SAGE system was the first large-scale example of such a system, which led to a number of special-purpose commercial systems such as Sabre.[48]
In the 1970s, computer engineers at research institutions throughout the United States began to link their computers together using telecommunications technology. The effort was funded by ARPA (now DARPA), and the computer network that resulted was called the ARPANET.[49] The technologies that made the Arpanet possible spread and evolved.
In time, the network spread beyond academic and military institutions and became known as the Internet. The emergence of networking involved a redefinition of the nature and boundaries of the computer. Computer operating systems and applications were modified to include the ability to define and access the resources of other computers on the network, such as peripheral devices, stored information, and the like, as extensions of the resources of an individual computer. Initially these facilities were available primarily to people working in high-tech environments, but in the 1990s the spread of applications like e-mail and the World Wide Web, combined with the development of cheap, fast networking technologies like Ethernet and ADSL saw computer networking become almost ubiquitous. In fact, the number of computers that are networked is growing phenomenally. A very large proportion of personal computers regularly connect to the Internet to communicate and receive information. "Wireless" networking, often utilizing mobile phone networks, has meant networking is becoming increasingly ubiquitous even in mobile computing environments.
There are many types of computer architectures:
The quantum computer architecture holds the most promise to revolutionize computing.[50]
Logic gates are a common abstraction which can apply to most of the above digital or analog paradigms.
The ability to store and execute lists of instructions called programs makes computers extremely versatile, distinguishing them from calculators. The ChurchTuring thesis is a mathematical statement of this versatility: any computer with a minimum capability (being Turing-complete) is, in principle, capable of performing the same tasks that any other computer can perform. Therefore any type of computer (netbook, supercomputer, cellular automaton, etc.) is able to perform the same computational tasks, given enough time and storage capacity.
A computer does not need to be electronic, nor even have a processor, nor RAM, nor even a hard disk. While popular usage of the word "computer" is synonymous with a personal electronic computer, the modern[51] definition of a computer is literally "A device that computes, especially a programmable [usually] electronic machine that performs high-speed mathematical or logical operations or that assembles, stores, correlates, or otherwise processes information".[52] Any device which processes information qualifies as a computer, especially if the processing is purposeful.
Historically, computers evolved from mechanical computers and eventually from vacuum tubes to transistors. However, conceptually computational systems as flexible as a personal computer can be built out of almost anything. For example, a computer can be made out of billiard balls (billiard ball computer); an oft-quoted example[citation needed]. More realistically, modern computers are made out of transistors made of photolithographed semiconductors.
There is active research to make computers out of many promising new types of technology, such as optical computers, DNA computers, neural computers, and quantum computers. Most computers are universal, and are able to calculate any computable function, and are limited only by their memory capacity and operating speed. However different designs of computers can give very different performance for particular problems; for example quantum computers can potentially break some modern encryption algorithms (by quantum factoring) very quickly.
A computer will solve problems in exactly the way it is programmed to, without regard to efficiency, alternative solutions, possible shortcuts, or possible errors in the code. Computer programs that learn and adapt are part of the emerging field of artificial intelligence and machine learning.
The term hardware covers all of those parts of a computer that are tangible objects. Circuits, displays, power supplies, cables, keyboards, printers and mice are all hardware.
Software refers to parts of the computer which do not have a material form, such as programs, data, protocols, etc. When software is stored in hardware that cannot easily be modified (such as BIOS ROM in an IBM PC compatible), it is sometimes called "firmware" to indicate that it falls into an uncertain area somewhere between hardware and software.
There are thousands of different programming languagessome intended to be general purpose, others useful only for highly specialized applications.
As the use of computers has spread throughout society, there are an increasing number of careers involving computers.
The need for computers to work well together and to be able to exchange information has spawned the need for many standards organizations, clubs and societies of both a formal and informal nature.

1 History of computing

1.1 Limited-function early computers
1.2 First general-purpose computers
1.3 Stored-program architecture
1.4 Semiconductors and microprocessors


1.1 Limited-function early computers
1.2 First general-purpose computers
1.3 Stored-program architecture
1.4 Semiconductors and microprocessors
2 Programs

2.1 Stored program architecture
2.2 Bugs
2.3 Machine code
2.4 Programming language

2.4.1 Low-level languages
2.4.2 Higher-level languages


2.5 Program design


2.1 Stored program architecture
2.2 Bugs
2.3 Machine code
2.4 Programming language

2.4.1 Low-level languages
2.4.2 Higher-level languages


2.4.1 Low-level languages
2.4.2 Higher-level languages
2.5 Program design
3 Components

3.1 Control unit
3.2 Arithmetic logic unit (ALU)
3.3 Memory
3.4 Input/output (I/O)
3.5 Multitasking
3.6 Multiprocessing
3.7 Networking and the Internet
3.8 Computer architecture paradigms


3.1 Control unit
3.2 Arithmetic logic unit (ALU)
3.3 Memory
3.4 Input/output (I/O)
3.5 Multitasking
3.6 Multiprocessing
3.7 Networking and the Internet
3.8 Computer architecture paradigms
4 Misconceptions

4.1 Required technology


4.1 Required technology
5 Further topics

5.1 Artificial intelligence
5.2 Hardware

5.2.1 History of computing hardware
5.2.2 Other hardware topics


5.3 Software
5.4 Languages
5.5 Professions and organizations


5.1 Artificial intelligence
5.2 Hardware

5.2.1 History of computing hardware
5.2.2 Other hardware topics


5.2.1 History of computing hardware
5.2.2 Other hardware topics
5.3 Software
5.4 Languages
5.5 Professions and organizations
6 See also
7 Notes
8 References
9 External links
1.1 Limited-function early computers
1.2 First general-purpose computers
1.3 Stored-program architecture
1.4 Semiconductors and microprocessors
2.1 Stored program architecture
2.2 Bugs
2.3 Machine code
2.4 Programming language

2.4.1 Low-level languages
2.4.2 Higher-level languages


2.4.1 Low-level languages
2.4.2 Higher-level languages
2.5 Program design
2.4.1 Low-level languages
2.4.2 Higher-level languages
3.1 Control unit
3.2 Arithmetic logic unit (ALU)
3.3 Memory
3.4 Input/output (I/O)
3.5 Multitasking
3.6 Multiprocessing
3.7 Networking and the Internet
3.8 Computer architecture paradigms
4.1 Required technology
5.1 Artificial intelligence
5.2 Hardware

5.2.1 History of computing hardware
5.2.2 Other hardware topics


5.2.1 History of computing hardware
5.2.2 Other hardware topics
5.3 Software
5.4 Languages
5.5 Professions and organizations
5.2.1 History of computing hardware
5.2.2 Other hardware topics
Konrad Zuse's electromechanical "Z machines". The Z3 (1941) was the first working machine featuring binary arithmetic, including floating point arithmetic and a measure of programmability. In 1998 the Z3 was proved to be Turing complete, therefore being the world's first operational computer.[28]
The non-programmable AtanasoffBerry Computer (commenced in 1937, completed in 1941) which used vacuum tube based computation, binary numbers, and regenerative capacitor memory. The use of regenerative memory allowed it to be much more compact than its peers (being approximately the size of a large desk or workbench), since intermediate results could be stored and then fed back into the same set of computation elements.
The secret British Colossus computers (1943),[29] which had limited programmability but demonstrated that a device using thousands of tubes could be reasonably reliable and electronically reprogrammable. It was used for breaking German wartime codes.
The Harvard Mark I (1944), a large-scale electromechanical computer with limited programmability.[30]
The U.S. Army's Ballistic Research Laboratory ENIAC (1946), which used decimal arithmetic and is sometimes called the first general purpose electronic computer (since Konrad Zuse's Z3 of 1941 used electromagnets instead of electronics). Initially, however, ENIAC had an inflexible architecture which essentially required rewiring to change its programming.
Quantum computer vs Chemical computer
Scalar processor vs Vector processor
Non-Uniform Memory Access (NUMA) computers
Register machine vs Stack machine
Harvard architecture vs von Neumann architecture
Cellular architecture
Glossary of computers
Computability theory
Computer insecurity
Computer security
List of computer term etymologies
List of fictional computers
Pulse computation
a Kempf, Karl (1961). Historical Monograph: Electronic Computers Within the Ordnance Corps. Aberdeen Proving Ground (United States Army). http://ed-thelen.org/comp-hist/U-S-Ord-61.html.
a Phillips, Tony (2000). "The Antikythera Mechanism I". American Mathematical Society. http://www.math.sunysb.edu/~tony/whatsnew/column/antikytheraI-0400/kyth1.html. Retrieved 5 April 2006.
a Shannon, Claude Elwood (1940). A symbolic analysis of relay and switching circuits. Massachusetts Institute of Technology. http://hdl.handle.net/1721.1/11173.
Digital Equipment Corporation (1972) (PDF). PDP-11/40 Processor Handbook. Maynard, MA: Digital Equipment Corporation. http://bitsavers.vt100.net/dec/www.computer.museum.uq.edu.au_mirror/D-09-30_PDP11-40_Processor_Handbook.pdf.
Verma, G.; Mielke, N. (1988). Reliability performance of ETOX based flash memories. IEEE International Reliability Physics Symposium.
Meuer, Hans; Strohmaier, Erich; Simon, Horst; Dongarra, Jack (13 November 2006). "Architectures Share Over Time". TOP500. http://www.top500.org/lists/2006/11/overtime/Architectures. Retrieved 27 November 2006.
Lavington, Simon (1998). A History of Manchester Computers (2 ed.). Swindon: The British Computer Society. ISBN978-0-902505-01-8.
Stokes, Jon (2007). Inside the Machine: An Illustrated Introduction to Microprocessors and Computer Architecture. San Francisco: No Starch Press. ISBN978-1-59327-104-6.
Felt, Dorr E. (1916). Mechanical arithmetic, or The history of the counting machine. Chicago: Washington Institute. http://www.archive.org/details/mechanicalarithm00feltrich.
Ifrah, Georges (2001). The Universal History of Computing: From the Abacus to the Quantum Computer. New York: John Wiley & Sons. ISBN0-471-39671-0.
Berkeley, Edmund (1949). Giant Brains, or Machines That Think. John Wiley & Sons.
A Brief History of Computing  slideshow by Life magazine
.
#(`*Computer programming*`)#.
Computer programming (often shortened to programming, scripting, or coding) is the process of designing, writing, testing, debugging, and maintaining the source code of computer programs. This source code is written in one or more programming languages (such as Java, C++, C#, Python, etc.). The purpose of programming is to create a set of instructions that computers use to perform specific operations or to exhibit desired behaviors. The process of writing source code often requires expertise in many different subjects, including knowledge of the application domain, specialized algorithms and formal logic.
Within software engineering, programming (the implementation) is regarded as one phase in a software development process.
There is an ongoing debate on the extent to which the writing of programs is an art form, a craft, or an engineering discipline.[1] In general, good programming is considered to be the measured application of all three, with the goal of producing an efficient and evolvable software solution (the criteria for "efficient" and "evolvable" vary considerably). The discipline differs from many other technical professions in that programmers, in general, do not need to be licensed or pass any standardized (or governmentally regulated) certification tests in order to call themselves "programmers" or even "software engineers." Because the discipline covers many areas, which may or may not include critical applications, it is debatable whether licensing is required for the profession as a whole. In most cases, the discipline is self-governed by the entities which require the programming, and sometimes very strict environments are defined (e.g. United States Air Force use of AdaCore and security clearance). However, representing oneself as a "Professional Software Engineer" without a license from an accredited institution is illegal in many parts of the world.
Another ongoing debate is the extent to which the programming language used in writing computer programs affects the form that the final program takes. This debate is analogous to that surrounding the SapirWhorf hypothesis[2] in linguistics and cognitive science, which postulates that a particular spoken language's nature influences the habitual thought of its speakers. Different language patterns yield different patterns of thought. This idea challenges the possibility of representing the world perfectly with language, because it acknowledges that the mechanisms of any language condition the thoughts of its speaker community.
Ancient cultures had no conception of computing beyond simple arithmetic. The only mechanical device that existed for numerical computation at the beginning of human history was the abacus, invented in Sumeria circa 2500 BC. Later, the Antikythera mechanism, invented some time around 100 AD in ancient Greece, was the first mechanical calculator utilizing gears of various sizes and configuration to perform calculations,[3] which tracked the metonic cycle still used in lunar-to-solar calendars, and which is consistent for calculating the dates of the Olympiads.[4] The Kurdish medieval scientist Al-Jazari built programmable Automata in 1206 AD. One system employed in these devices was the use of pegs and cams placed into a wooden drum at specific locations, which would sequentially trigger levers that in turn operated percussion instruments. The output of this device was a small drummer playing various rhythms and drum patterns.[5][6] The Jacquard Loom, which Joseph Marie Jacquard developed in 1801, uses a series of pasteboard cards with holes punched in them. The hole pattern represented the pattern that the loom had to follow in weaving cloth. The loom could produce entirely different weaves using different sets of cards. Charles Babbage adopted the use of punched cards around 1830 to control his Analytical Engine. The first computer program was written for the Analytical Engine by mathematician Ada Lovelace to calculate a sequence of Bernoulli Numbers.[7] The synthesis of numerical calculation, predetermined operation and output, along with a way to organize and input instructions in a manner relatively easy for humans to conceive and produce, led to the modern development of computer programming. Development of computer programming accelerated through the Industrial Revolution.
In the late 1880s, Herman Hollerith invented the recording of data on a medium that could then be read by a machine. Prior uses of machine readable media, above, had been for control, not data. "After some initial trials with paper tape, he settled on punched cards..."[8] To process these punched cards, first known as "Hollerith cards" he invented the tabulator, and the keypunch machines. These three inventions were the foundation of the modern information processing industry. In 1896 he founded the Tabulating Machine Company (which later became the core of IBM). The addition of a control panel (plugboard) to his 1906 Type I Tabulator allowed it to do different jobs without having to be physically rebuilt. By the late 1940s, there were a variety of control panel programmable machines, called unit record equipment, to perform data-processing tasks.
The invention of the von Neumann architecture allowed computer programs to be stored in computer memory. Early programs had to be painstakingly crafted using the instructions (elementary operations) of the particular machine, often in binary notation. Every model of computer would likely use different instructions (machine language) to do the same task. Later, assembly languages were developed that let the programmer specify each instruction in a text format, entering abbreviations for each operation code instead of a number and specifying addresses in symbolic form (e.g., ADD X, TOTAL). Entering a program in assembly language is usually more convenient, faster, and less prone to human error than using machine language, but because an assembly language is little more than a different notation for a machine language, any two machines with different instruction sets also have different assembly languages.
In 1954, FORTRAN was invented; it was the first high level programming language to have a functional implementation, as opposed to just a design on paper.[9][10] (A high-level language is, in very general terms, any programming language that allows the programmer to write programs in terms that are more abstract than assembly language instructions, i.e. at a level of abstraction "higher" than that of an assembly language.) It allowed programmers to specify calculations by entering a formula directly (e.g. Y = X*2 + 5*X + 9). The program text, or source, is converted into machine instructions using a special program called a compiler, which translates the FORTRAN program into machine language. In fact, the name FORTRAN stands for "Formula Translation". Many other languages were developed, including some for commercial programming, such as COBOL. Programs were mostly still entered using punched cards or paper tape. (See computer programming in the punch card era). By the late 1960s, data storage devices and computer terminals became inexpensive enough that programs could be created by typing directly into the computers. Text editors were developed that allowed changes and corrections to be made much more easily than with punched cards. (Usually, an error in punching a card meant that the card had to be discarded and a new one punched to replace it.)
As time has progressed, computers have made giant leaps in the area of processing power. This has brought about newer programming languages that are more abstracted from the underlying hardware. Popular programming languages of the modern era include C++, C#, Objective-C, Visual Basic, SQL, HTML with PHP, ActionScript, Perl, Java, JavaScript, Ruby, Python, Haskell and dozens more.[11] Although these high-level languages usually incur greater overhead, the increase in speed of modern computers has made the use of these languages much more practical than in the past. These increasingly abstracted languages typically are easier to learn and allow the programmer to develop applications much more efficiently and with less source code. However, high-level languages are still impractical for a few programs, such as those where low-level hardware control is necessary or where maximum processing speed is vital. Computer programming has become a popular career in the developed world, particularly in the United States, Europe, Scandinavia, and Japan. Due to the high labor cost of programmers in these countries, some forms of programming have been increasingly subject to offshore outsourcing (importing software and services from other countries, usually at a lower wage), making programming career decisions in developed countries more complicated, while increasing economic opportunities for programmers in less developed areas, particularly China and India.
Whatever the approach to software development may be, the final program must satisfy some fundamental properties. The following properties are among the most relevant:
In computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. It affects the aspects of quality above, including portability, usability and most importantly maintainability.
Readability is important because programmers spend the majority of their time reading, trying to understand and modifying existing source code, rather than writing new source code. Unreadable code often leads to bugs, inefficiencies, and duplicated code. A study[12] found that a few simple readability transformations made code shorter and drastically reduced the time to understand it.
Following a consistent programming style often helps readability. However, readability is more than just programming style. Many factors, having little or nothing to do with the ability of the computer to efficiently compile and execute the code, contribute to readability.[13] Some of these factors include:
The academic field and the engineering practice of computer programming are both largely concerned with discovering and implementing the most efficient algorithms for a given class of problem. For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input. Expert programmers are familiar with a variety of well-established algorithms and their respective complexities and use this knowledge to choose algorithms that are best suited to the circumstances.
The first step in most formal software development processes is requirements analysis, followed by testing to determine value modeling, implementation, and failure elimination (debugging). There exist a lot of differing approaches for each of those tasks. One approach popular for requirements analysis is Use Case analysis. Nowadays many programmers use forms of Agile software development where the various stages of formal software development are more integrated together into short cycles that take a few weeks rather than years. There are many approaches to the Software development process.
Popular modeling techniques include Object-Oriented Analysis and Design (OOAD) and Model-Driven Architecture (MDA). The Unified Modeling Language (UML) is a notation used for both the OOAD and MDA.
A similar technique used for database design is Entity-Relationship Modeling (ER Modeling).
Implementation techniques include imperative languages (object-oriented or procedural), functional languages, and logic languages.
It is very difficult to determine what are the most popular of modern programming languages. Some languages are very popular for particular kinds of applications (e.g., COBOL is still strong in the corporate data center[citation needed], often on large mainframes, FORTRAN in engineering applications, scripting languages in Web development, and C in embedded applications), while some languages are regularly used to write many different kinds of applications. Also many applications use a mix of several languages in their construction and use. New languages are generally designed around the syntax of a previous language with new functionality added (for example C++ adds object-orientedness to C, and Java adds memory management and bytecode to C++).
Methods of measuring programming language popularity include: counting the number of job advertisements that mention the language,[14] the number of books sold and courses teaching the language (this overestimates the importance of newer languages), and estimates of the number of existing lines of code written in the language (this underestimates the number of users of business languages such as COBOL).
Debugging is a very important task in the software development process, because an incorrect program can have significant consequences for its users. Some languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages. Use of a static code analysis tool can help detect some possible problems.
Debugging is often done with IDEs like Eclipse, Kdevelop, NetBeans, Code::Blocks, and Visual Studio. Standalone debuggers like gdb are also used, and these often provide less of a visual environment, usually using a command line.
Different programming languages support different styles of programming (called programming paradigms). The choice of language used is subject to many considerations, such as company policy, suitability to task, availability of third-party packages, or individual preference. Ideally, the programming language best suited for the task at hand will be selected. Trade-offs from this ideal involve finding enough programmers who know the language to build a team, the availability of compilers for that language, and the efficiency with which programs written in a given language execute. Languages form an approximate spectrum from "low-level" to "high-level"; "low-level" languages are typically more machine-oriented and faster to execute, whereas "high-level" languages are more abstract and easier to use but execute less quickly. It is usually easier to code in "high-level" languages than in "low-level" ones.
Allen Downey, in his book How To Think Like A Computer Scientist, writes:
Many computer languages provide a mechanism to call functions provided by libraries such as in a .so. Provided the functions in a library follow the appropriate run time conventions (e.g., method of passing arguments), then these functions may be written in any other language.
Computer programmers are those who write computer software. Their jobs usually involve:
Requirements
Specification
Architecture
Design
Implementation
Testing
Debugging
Deployment
Maintenance
Waterfall
Prototype model
Incremental
Iterative
V-Model
Spiral
Scrum
Cleanroom
RAD
DSDM
RUP
XP
Agile
Lean
Dual Vee Model
TDD
FDD
Configuration management
Documentation
Quality assurance (SQA)
Project management
User experience design
Compiler
Debugger
Profiler
GUI designer
IDE
Build automation
v
t
e
1 History
2 Modern programming

2.1 Quality requirements
2.2 Readability of source code
2.3 Algorithmic complexity
2.4 Methodologies
2.5 Measuring language usage
2.6 Debugging


2.1 Quality requirements
2.2 Readability of source code
2.3 Algorithmic complexity
2.4 Methodologies
2.5 Measuring language usage
2.6 Debugging
3 Programming languages
4 Programmers
5 See also
6 References
7 Further reading
8 External links
2.1 Quality requirements
2.2 Readability of source code
2.3 Algorithmic complexity
2.4 Methodologies
2.5 Measuring language usage
2.6 Debugging
Reliability: how often the results of a program are correct. This depends on conceptual correctness of algorithms, and minimization of programming mistakes, such as mistakes in resource management (e.g., buffer overflows and race conditions) and logic errors (such as division by zero or off-by-one errors).
Robustness: how well a program anticipates problems not due to programmer error. This includes situations such as incorrect, inappropriate or corrupt data, unavailability of needed resources such as memory, operating system services and network connections, and user error.
Usability: the ergonomics of a program: the ease with which a person can use the program for its intended purpose, or in some cases even unanticipated purposes. Such issues can make or break its success even regardless of other issues. This involves a wide range of textual, graphical and sometimes hardware elements that improve the clarity, intuitiveness, cohesiveness and completeness of a program's user interface.
Portability: the range of computer hardware and operating system platforms on which the source code of a program can be compiled/interpreted and run. This depends on differences in the programming facilities provided by the different platforms, including hardware and operating system resources, expected behaviour of the hardware and operating system, and availability of platform specific compilers (and sometimes libraries) for the language of the source code.
Maintainability: the ease with which a program can be modified by its present or future developers in order to make improvements or customizations, fix bugs and security holes, or adapt it to new environments. Good practices during initial development make the difference in this regard. This quality may not be directly apparent to the end user but it can significantly affect the fate of a program over the long term.
Efficiency/performance: the amount of system resources a program consumes (processor time, memory space, slow devices such as disks, network bandwidth and to some extent even user interaction): the less, the better. This also includes correct disposal of some resources, such as cleaning up temporary files and lack of memory leaks.
Different indentation styles (whitespace)
Comments
Decomposition
Naming conventions for objects (such as variables, classes, procedures, etc.)
input: Gather data from the keyboard, a file, or some other device.
output: Display data on the screen or send data to a file or other device.
arithmetic: Perform basic arithmetical operations like addition and multiplication.
conditional execution: Check for certain conditions and execute the appropriate sequence of statements.
repetition: Perform some action repeatedly, usually with some variation.
Coding
Compilation
Debugging
Documentation
Integration
Maintenance
Requirements analysis
Software architecture
Software testing
Specification
Book: Programming
ACCU
Association for Computing Machinery
Computer networking
Computer programming in the punch card era
Computer science
Computing
Hello world program
Institution of Analysts and Programmers
Programming paradigms
Software engineering
The Art of Computer Programming
A.K. Hartmann, Practical Guide to Computer Simulations, Singapore: World Scientific (2009)
A. Hunt, D. Thomas, and W. Cunningham, The Pragmatic Programmer. From Journeyman to Master, Amsterdam: Addison-Wesley Longman (1999)
Brian W. Kernighan, The Practice of Programming, Pearson (1999)
Weinberg, Gerald M., The Psychology of Computer Programming, New York: Van Nostrand Reinhold (1971)
Software engineering at the Open Directory Project
Programming Wikia
How to Think Like a Computer Scientist - by Jeffrey Elkner, Allen B. Downey and Chris Meyers
.
#(`*Named-entity recognition*`)#.
Named-entity recognition (NER) (also known as entity identification and entity extraction) is a subtask of information extraction that seeks to locate and classify atomic elements in text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.
Most research on NER systems has been structured as taking an unannotated block of text, such as this one:
And producing an annotated block of text, such as this one:
In this example, the annotations have been done using so-called ENAMEX tags that were developed for the Message Understanding Conference in the 1990s.
State-of-the-art NER systems for English produce near-human performance. For example, the best system entering MUC-7 scored 93.39% of F-measure while human annotators scored 97.60% and 96.95%.[1][2] These algorithms had roughly twice the error rate (6.61%) of human annotators (2.40% and 3.05%).
NER systems have been created that use linguistic grammar-based techniques as well as statistical models. Hand-crafted grammar-based systems typically obtain better precision, but at the cost of lower recall and months of work by experienced computational linguists. Statistical NER systems typically require a large amount of manually annotated training data.
Research indicates that even state-of-the-art NER systems are brittle, meaning that NER systems developed for one domain do not typically perform well on other domains.[3] Considerable effort is involved in tuning NER systems to perform well in a new domain; this is true for both rule-based and trainable statistical systems.
Early work in NER systems in the 1990s was aimed primarily at extraction from journalistic articles. Attention then turned to processing of military dispatches and reports. Later stages of the automatic content extraction (ACE) evaluation also included several types of informal text styles, such as weblogs and text transcripts from conversational telephone speech conversations. Since about 1998, there has been a great deal of interest in entity identification in the molecular biology, bioinformatics, and medical natural language processing communities. The most common entity of interest in that domain has been names of genes and gene products.
In the expression named entity, the word named restricts the task to those entities for which one or many rigid designators, as defined by Kripke, stands for the referent. For instance, the automotive company created by Henry Ford in 1903 is referred to as Ford or Ford Motor Company. Rigid designators include proper names as well as certain natural kind terms like biological species and substances.
There is a general agreement to include temporal expressions and some numerical expressions (i.e., money, percentages, etc.) as instances of named entities in the context of the NER task. While some instances of these types are good examples of rigid designators (e.g., the year 2001) there are also many invalid ones (e.g., I take my vacations in June). In the first case, the year 2001 refers to the 2001st year of the Gregorian calendar. In the second case, the month June may refer to the month of an undefined year (past June, next June, June 2020, etc.). It is arguable that the named entity definition is loosened in such cases for practical reasons. The definition of the term named entity is therefore not strict and often has to be explained in the context it is used.[4]
At least two hierarchies of named entity types have been proposed in the literature. BBN categories, proposed in 2002, is used for Question Answering and consists of 29 types and 64 subtypes.[5] Sekine's extended hierarchy, proposed in 2002, is made of 200 subtypes.[6]
Despite the high F1 numbers reported on the MUC-7 dataset, the problem of Named Entity Recognition is far from being solved. The main efforts are directed to reducing the annotation labor [7] ,[8] robust performance across domains[9][10] and scaling up to fine-grained entity types.[11][12]
A recently emerging task of identifying "important expressions" in text and cross-linking them to Wikipedia [13] [14][15] can be seen as an instance of extremely fine-grained named entity recognition, where the types are the actual Wikipedia pages describing the (potentially ambiguous) concepts. Below is an example output of a Wikification system:
Several systems are available online. For traditional NER, the most popular publicly available systems are: OpenNLP NameFinder, Illinois NER system, Stanford NER system, and Lingpipe NER system. The Illinois NER reports 90.6 F1 on the CoNLL03 NER shared task data and the Stanford NER reports 86.86 F1.[16][17]
There are also several publicly available Wikification systems for identifying important expressions in the text and cross-linking them to Wikipedia. Most notably, Illinois Wikification system WM Wikifier and TAGME . An overview of these system can be found here
Evaluation of NER systems is critical to scientific progress of this field.
Most evaluation of these systems has been performed at conferences or contests put on by government organizations, sometimes acting in concert with contractors or academics.
1 Approaches
2 Problem domains
3 Named entity types
4 Current challenges and research
5 Available technology
6 NER evaluation forums
7 See also
8 References
9 External links
Information extraction
Knowledge extraction
Smart tag (Microsoft)
Named entity recognition for Arabic  Issues and challenges in morphologically rich languages such as Arabic
Farhad Abedini, Fariborz Mahmoudi, and Amir Hossein Jadidinejad, "From Text to Knowledge: Semantic Entity Extraction using YAGO Ontology," International Journal of Machine Learning and Computing vol. 1, no. 2, pp. 113-119 , 2011.
Farhad Abedini, Fariborz Mahmoudi, and Seyedeh Masoumeh Mirhashem, "Using Semantic Entity Extraction Method for a New Application," International Journal of Machine Learning and Computing vol. 2, no. 2, pp. 178-182, 2012.
.
#(`*Massachusetts Institute of Technology*`)#.
Coordinates: 422135N 710532W / 42.35982N 71.09211W / 42.35982; -71.09211
Massachusetts Institute of Technology (MIT) is a private research university located in Cambridge, Massachusetts, United States. MIT has five schools and one college, containing a total of 32 academic departments, with a strong emphasis on scientific, engineering, and technological education and research.
Founded in 1861 in response to the increasing industrialization of the United States, the institute adopted the European polytechnic university model and emphasized laboratory instruction from an early date.[9] MIT's early emphasis on applied technology at the undergraduate and graduate levels led to close cooperation with industry. Curricular reforms under Karl Compton and Vannevar Bush in the 1930s re-emphasized basic scientific research.[10] MIT was elected to the Association of American Universities in 1934. Researchers were involved in efforts to develop computers, radar, and inertial guidance in connection with defense research during World War II and the Cold War. Post-war defense research contributed to the rapid expansion of the faculty and campus under James Killian.
The current 168-acre (68.0ha) campus opened in 1916 and extends over 1 mile (1.6km) along the northern bank of the Charles River basin.[5] In the past 60 years, MIT's educational disciplines have expanded beyond the physical sciences and engineering into fields such as biology, economics, linguistics, political science, and management.
MIT enrolled 4,384 undergraduates and 6,510 graduate students for the 20112012 school year.[4] MIT received 18,109 undergraduate applicants for the class of 2016, with only 1,620 offered admittance, an acceptance rate of 8.9%.[11] It employs around 1,000 faculty members.[3] 78 Nobel laureates, 52 National Medal of Science recipients, 45 Rhodes Scholars, and 38 MacArthur Fellows are currently or have previously been affiliated with the university.[3][6][12]
MIT has a strong entrepreneurial culture. The aggregated revenues of companies founded by MIT alumni would rank as the eleventh-largest economy in the world.[13][14] MIT managed $718.2 million in research expenditures and an $8.0 billion endowment in 2009.[15][16]
The "Engineers"[17] sponsor 33 sports, most teams of which compete in the NCAA Division III's New England Women's and Men's Athletic Conference; the Division I rowing programs compete as part of the EARC and EAWRC.
[18], Act to Incorporate the Massachusetts Institute of Technology, Acts of 1861, Chapter 183
In 1859, various individuals and Bostonian organizations submitted to the Massachusetts General Court a proposal to use newly opened lands in Back Bay, Boston for a "Conservatory of Art and Science", but the proposal failed.[19][20] A later proposal by William Barton Rogers led to a charter for the incorporation of the Massachusetts Institute of Technology, which was signed by the governor of Massachusetts on April 10, 1861.[21]
Rogers sought to establish a new institution of higher education to address the challenges posed by the mid-19th century's rapid scientific and technological advances.[22][23] Though he valued first-hand experience as part of a proper education, he did not intend to found a professional school, but advocated instruction in useful knowledge that combined elements of both professional and liberal education,[24] writing that "The true and only practicable object of a polytechnic school is, as I conceive, the teaching, not of the minute details and manipulations of the arts, which can be done only in the workshop, but the inculcation of those scientific principles which form the basis and explanation of them, and along with this, a full and methodical review of all their leading processes and operations in connection with physical laws."[25] The Rogers Plan, as it came to be known, reflected the German research university model, emphasizing an independent faculty engaged in research as well as instruction oriented around seminars and laboratories.[26]
Just two days after the issuance of the charter, the first battle of the Civil War broke out. After years of delay caused by wartime funding and staffing difficulties, MIT's first classes were held in rented space at the Mercantile Building in downtown Boston in 1865.[27] Though it was a private institution to be located in the middle of urban Boston, the new institute had a mission that matched the intent of the 1862 Morrill Land-Grant Colleges Act to fund institutions "to promote the liberal and practical education of the industrial classes", and was thus named a land-grant school.[28][b] In 1866, the proceeds from land sales went toward new buildings in the Back Bay neighborhood.[29]
MIT soon came to be called "Boston Tech".[29] After surviving a period of financial uncertainties, the institute saw significant expansion in the last two decades of the 19th century under President Francis Amasa Walker.[30] Programs in electrical, chemical, marine, and sanitary engineering were introduced,[31][32] new buildings were built, and the size of the student body increased to well over 1000.[30] The curriculum became increasingly vocational; less and less focus was placed on theoretical topics.[33] During these "Boston Tech" years, MIT faculty and alumni repeatedly rejected overtures from former MIT faculty turned Harvard University president Charles W. Eliot, to merge MIT with Harvard College's Lawrence Scientific School.[34]
MIT reinforced its independence in 1916 when it moved to its new campus on a mile-long tract on the Cambridge side of the Charles River,[36] almost entirely on landfill.[c] The neoclassical "New Technology" campus was funded by donations from industrialist George Eastman and designed by William W. Bosworth.[38]
In the 1930s, President Karl Taylor Compton and Vice-President (effectively Provost) Vannevar Bush reformed the applied technology curriculum by re-emphasizing the importance of "pure" sciences like physics and chemistry and by reducing the vocational practice required in shops and drafting studios.[10] Despite the challenges of the Great Depression, the Compton reforms "renewed confidence in the ability of the Institute to develop leadership in science as well as in engineering."[39] The expansion and reforms cemented MIT's academic reputation,[10] though unlike Ivy League schools, MIT catered more to middle-class families, and depended more on tuition than on endowments or grants.[40] The school was elected to the Association of American Universities in 1934.[41]
Still, as late as 1949, the Lewis Committee lamented in its report on the state of education at MIT that "the Institute is widely conceived as basically a vocational school", a "partly unjustified" perception the committee sought to change. The report comprehensively reviewed the undergraduate curriculum, recommended offering a broader education, and warned against letting engineering and government-sponsored research detract from the sciences and humanities.[42][43] The School of Humanities, Arts, and Social Sciences and the MIT Sloan School of Management were formed in 1950 to compete with the powerful Schools of Science and Engineering. Previously marginalized faculties in the areas of economics, management, political science, and linguistics emerged into cohesive and assertive departments by attracting respected professors and launching competitive graduate programs.[44][45] The School of Humanities, Arts, and Social Sciences continued to develop under the successive terms of the more humanistically oriented presidents Howard W. Johnson and Jerome Wiesner between 1966 and 1980.[46]
MIT's involvement in military research surged during World War II. In 1941, Vannevar Bush was appointed head of the federal Office of Scientific Research and Development and directed funding to only a select group of universities, including MIT.[47] Engineers and scientists from across the country gathered at MIT's Radiation Laboratory, established in 1940 to assist the British military in developing microwave radar. The work done there significantly impacted both the war and subsequent research in the area.[48] Other defense projects included gyroscope-based and other complex control systems for gunsight, bombsight, and inertial navigation under Charles Stark Draper's Instrumentation Laboratory;[49][50] the development of a digital computer for flight simulations under Project Whirlwind;[51] and high-speed and high-altitude photography under Harold Edgerton.[52][53] By the end of the war, MIT became the nation's largest wartime R&D contractor (attracting some criticism of Bush),[47] employing nearly 4000 in the Radiation Laboratory alone[48] and receiving in excess of $100 million ($1.2 billion in 2012 dollars) before 1946.[39] Work on defense projects continued even after then. Post-war government-sponsored research at MIT included SAGE and guidance systems for ballistic missiles and Project Apollo.[54]
These activities affected MIT profoundly. A 1949 report noted the lack of "any great slackening in the pace of life at the Institute" to match the return to peacetime, remembering the "academic tranquillity of the prewar years", though acknowledging the significant contributions of military research to the increased emphasis on graduate education and rapid growth of personnel and facilities.[55] Indeed, the faculty doubled and the graduate student body quintupled during the terms of Karl Taylor Compton, president of MIT between 1930 and 1948, James Rhyne Killian, president from 1948 to 1957, and Julius Adams Stratton, chancellor from 1952 to 1957, whose institution-building strategies shaped the expanding university. By the 1950s, MIT no longer merely benefited the industries it had worked so closely with three decades prior, and was much closer to its new patrons, philanthropic foundations and the federal government.[56]
In late 1960s and early 1970s, student and faculty activists protested against the Vietnam War and MIT's defense research.[57][58] The Union of Concerned Scientists was founded on March 4, 1969 during a meeting of faculty members and students seeking to shift the emphasis on military research towards environmental and social problems.[59] MIT ultimately divested itself from the Instrumentation Laboratory and moved all classified research off-campus to the Lincoln Laboratory facility in 1973 in response to the protests,[60][61] and the student body, faculty, and administration remained comparatively unpolarized during what was a tumultuous time for many other universities.[57][62]
MIT has kept pace with and helped to advance the digital age. In addition to developing the predecessors to modern computing and networking technologies,[63][64] students, staff, and faculty members at Project MAC, the Artificial Intelligence Laboratory, and the Tech Model Railroad Club wrote some of the earliest interactive computer video games like Spacewar! and created much of modern hacker slang and culture.[65] Several major computer-related organizations have originated at MIT since the 1980s: Richard Stallman's GNU Project and the subsequent Free Software Foundation were founded in the mid-1980s at the AI Lab; the MIT Media Lab was founded in 1985 by Nicholas Negroponte and Jerome Wiesner to promote research into novel uses of computer technology;[66] the World Wide Web Consortium standards organization was founded at the Laboratory for Computer Science in 1994 by Tim Berners-Lee;[67] the OpenCourseWare project has made course materials for over 2,000 MIT classes available online free of charge since 2002;[68] and the One Laptop per Child initiative to expand computer education and connectivity to children worldwide was launched in 2005.[69]
MIT was named a sea-grant college in 1976 to support its programs in oceanography and marine sciences and was named a space-grant college in 1989 to support its aeronautics and astronautics programs.[70][71] Despite diminishing government financial support over the past quarter century, MIT launched several successful development campaigns to significantly expand the campus: new dormitories and athletics buildings on west campus; the Tang Center for Management Education; several buildings in the northeast corner of campus supporting research into biology, brain and cognitive sciences, genomics, biotechnology, and cancer research; and a number of new "backlot" buildings on Vassar Street including the Stata Center.[72] Construction on campus in the 2000s included expansions of the Media Lab, the Sloan School's eastern campus, and graduate residences in the northwest.[73][74] In 2006, President Hockfield launched the MIT Energy Research Council to investigate the interdisciplinary challenges posed by increasing global energy consumption.[75]
In 2001, inspired by the open source and open access movements,[76] MIT launched OpenCourseWare to make the lecture notes, problem sets, syllabuses, exams, and lectures from the great majority of its courses available online for no charge, though without any formal accreditation for coursework completed.[77] While the cost of supporting and hosting the project is high,[78] OCW expanded in 2005 to include other universities as a part of the OpenCourseWare Consortium, which currently includes more than 250 academic institutions with content available in least six languages.[79] In 2011, MIT announced it would offer formal certification (but not credits or degrees) to online participants completing coursework in its "MITx" program, for a modest fee.[80] The "edX" online platform supporting MITx will initially be developed in partnership with Harvard and its analogous "Harvardx" initiative. The courseware platform is open source, and other universities will be encouraged to add their own course content.[81]
MIT's 168-acre (68.0ha) campus spans approximately a mile of the north side of the Charles River basin in the city of Cambridge.[82] The campus is divided roughly in half by Massachusetts Avenue, with most dormitories and student life facilities to the west and most academic buildings to the east. The bridge closest to MIT is the Harvard Bridge, which is known for being marked off in a non-standard unit of length  the smoot.[83][84] The Kendall MBTA Red Line station is located on the far northeastern edge of the campus in Kendall Square. The Cambridge neighborhoods surrounding MIT are a mixture of high tech companies occupying both modern office and rehabilitated industrial buildings as well as socio-economically diverse residential neighborhoods.[85][86]
Each building at has a number (possibly preceded by a W, N, E, or NW) designation and most have a name as well. Typically, academic and office buildings are referred to primarily by number while residence halls are referred to by name. The organization of building numbers roughly corresponds to the order in which the buildings were built and their location relative (north, west, and east) to the original center cluster of Maclaurin buildings.[87] Many of the buildings are connected above ground as well as through an extensive network of underground tunnels, providing protection from the Cambridge weather as well as a venue for roof and tunnel hacking.[88][89]
MIT's on-campus nuclear reactor[90] is one of the most powerful university-based nuclear reactors in the United States. The prominence of the reactor's containment building in a densely populated area has been controversial,[91] but MIT maintains that it is well-secured.[92] Other notable campus facilities include a pressurized wind tunnel and a towing tank for testing ship and ocean structure designs.[93][94] MIT's campus-wide wireless network was completed in the fall of 2005 and consists of nearly 3,000 access points covering 9,400,000 square feet (870,000m2) of campus.[95]
In 2001, the Environmental Protection Agency sued MIT for violating Clean Water Act and Clean Air Act with regard to its hazardous waste storage and disposal procedures.[96] MIT settled the suit by paying a $155,000 fine and launching three environmental projects.[97] In connection with capital campaigns to expand the campus, the Institute has also extensively renovated existing buildings to improve their energy efficiency. MIT has also taken steps to reduce its environmental impact by running alternative fuel campus shuttles, subsidizing public transportation passes, and building a low-emission cogeneration plant that serves most of the campus electricity, heating, and cooling requirements.[98]
Between 2007 and 2009,[dated info] campus security and local police received reports of 8 forcible sex offences, 5 robberies, 9 aggravated assaults, 409 burglaries, 2 cases of arson, and 15 cases of motor vehicle theft, affecting a community of around 13,000 students and employees.[99]
MIT's School of Architecture, now the School of Architecture and Planning, was the first in the United States,[100] and it has a history of commissioning progressive buildings.[101][102] The first buildings constructed on the Cambridge campus, completed in 1916, are sometimes called the "Maclaurin buildings" after Institute president Richard Maclaurin who oversaw their construction. Designed by William Welles Bosworth, these imposing buildings were built of reinforced concrete, a first for a non-industrial  much less university  building in the US.[103] Bosworth's design was influenced by the City Beautiful Movement of the early 1900s,[103] and features the Pantheon-esque Great Dome housing the Barker Engineering Library. The Great Dome overlooks Killian Court, where commencement is held each year. The friezes of the limestone-clad buildings around Killian Court are engraved with the names of important scientists and philosophers.[d] The imposing Building 7 atrium along Massachusetts Avenue is regarded as the entrance to the Infinite Corridor and the rest of the campus.[86]
Alvar Aalto's Baker House (1947), Eero Saarinen's Chapel and Auditorium (1955), and I.M. Pei's Green, Dreyfus, Landau, and Wiesner buildings represent high forms of post-war modernist architecture.[106][107][108] More recent buildings like Frank Gehry's Stata Center (2004), Steven Holl's Simmons Hall (2002), Charles Correa's Building 46 (2005), Fumihiko Maki's Media Lab Extension (2009) stand out among the Boston area's classical architecture and serve as examples of contemporary campus "starchitecture".[101][109] These buildings have not always been well received;[110][111] in 2010, The Princeton Review included MIT in a list of twenty schools whose campuses are "tiny, unsightly, or both".[112]
Undergraduates are guaranteed four-year housing in one of MIT's 12 undergraduate dormitories,[113] Those living on campus can receive support and mentoring from live-in graduate student tutors, resident advisors, and faculty housemasters.[114] Because housing assignments are made based on the preferences of the students themselves, diverse social atmospheres can be sustained in different living groups; for example, according to the Yale Daily News Staff's The Insider's Guide to the Colleges, 2010, "The split between East Campus and West Campus is a significant characteristic of MIT. East Campus has gained a reputation as a thriving counterculture."[115] MIT also has 5 dormitories for single graduate students and 2 apartment buildings on campus for married student families.[116]
MIT has a very active Greek and co-op housing system which includes 36 fraternities, sororities, and independent living groups (FSILGs).[117] In 2009, 92% of all undergraduates lived on MIT-affiliated housing, 50 percent of the men in fraternities and 34% of the women in sororities.[118] Most FSILGs are located across the river in the Back Bay owing to MIT's history there, and there is also a cluster of fraternities on MIT's West Campus.[119] After the 1997 death of Scott Krueger, a new member at the Phi Gamma Delta fraternity, MIT required all freshmen to live in the dormitory system starting in 2002.[120] Because FSILGs had previously housed as many as 300 freshmen off-campus, the new policy did not take effect until 2002 after Simmons Hall opened.[121]
MIT is chartered as a non-profit organization and is owned and governed by a privately appointed board of trustees known as the MIT Corporation.[122] The current board consists of 43 members elected to five-year terms,[123] 25 life members who vote until their 75th birthday,[124] 3 elected officers (President, Treasurer, and Secretary),[125] and 4 ex officio members (the president of the alumni association, the Governor of Massachusetts, the Massachusetts Secretary of Education, and the Chief Justice of the Massachusetts Supreme Judicial Court).[126][127] The board is chaired by John S. Reed, the former chairman of the New York Stock Exchange and Citigroup.[128][129] The corporation approves the budget, new programs, degrees, and faculty appointments as well as electing the President to serve as the chief executive officer of the university and presiding over the Institute's faculty.[86][130] MIT's endowment and other financial assets are managed through a subsidiary MIT Investment Management Company (MITIMCo).[131] Valued at $9.7 billion in 2011, MIT's endowment is the sixth-largest among American colleges and universities.[15][132]
MIT has five schools (Science, Engineering, Architecture and Planning, Management, and Humanities, Arts, and Social Sciences) and one college (Whitaker College of Health Sciences and Technology), but no schools of law or medicine.[133][e] While faculty committees assert substantial control over many areas of MIT's curriculum, research, student life, and administrative affairs,[135] the chair of each of MIT's 32 academic departments reports to the dean of that department's school, who in turn reports to the Provost under the President.[136] The current president is L. Rafael Reif, who formerly served as provost under President Susan Hockfield, the first woman to hold the post.[137][138]
The university historically pioneered research and training collaborations between academia, industry and government.[139][140] In 1946 President Compton, Harvard Business School professor Georges Doriot, and Massachusetts Investor Trust chairman Merrill Grisswold founded American Research & Development Corp., the first American venture-capital firm.[141][142] In 1948, Compton established the MIT Industrial Liaison Program.[143] Throughout the late 1980s and early 1990s, American politicians and business leaders accused MIT and other universities of contributing to a declining economy by transferring taxpayer-funded research and technology to international  especially Japanese  firms that were competing with struggling American businesses.[144][145] On the other hand, MIT's extensive collaboration with the federal government on research projects has led to several MIT leaders serving as Presidential scientific advisers since 1940.[f] MIT established a Washington Office in 1991 to continue to lobby for research funding and national science policy.[147][148]
The Justice Department began an antitrust investigation in 1989, and in 1991 filed an antitrust suit against MIT, the eight Ivy League colleges, and eleven other institutions for allegedly engaging in price-fixing in their annual "Overlap Meetings", which were held to prevent bidding wars over promising prospective students from consuming funds for need-based scholarships.[149][150] While the Ivy League institutions settled,[151] MIT contested the charges, arguing that the practice was not anti-competitive because it ensured the availability of aid for the greatest number of students.[152][153] MIT ultimately prevailed when the Justice Department dropped the case in 1994.[154][155]
MIT's proximity[g] to Harvard University ("the other school up the river") has led to a substantial number of research collaborations such as the Harvard-MIT Division of Health Sciences and Technology and Broad Institute.[156] In addition, students at the two schools can cross-register for credits toward their own school's degrees without any additional fees.[156] A cross-registration program between MIT and Wellesley College has also existed since 1969, and in 2002 the CambridgeMIT Institute launched an undergraduate exchange program between MIT and the University of Cambridge.[156] MIT has more modest cross-registration programs with Boston University, Brandeis University, Tufts University, Massachusetts College of Art, and the School of the Museum of Fine Arts, Boston.[156] MIT maintains substantial research and faculty ties with independent research organizations in the Boston-area like the Charles Stark Draper Laboratory, Whitehead Institute for Biomedical Research, and Woods Hole Oceanographic Institution as well as international research and educational collaborations through the Singapore-MIT Alliance, MIT-Politecnico di Milano,[156][157] MIT-Zaragoza International Logistics Program, and other countries through the MIT International Science and Technology Initiatives (MISTI) program.[156][158]
The mass-market magazine Technology Review is published by MIT through a subsidiary company, as is a special edition that also serves as an alumni magazine.[159][160] The MIT Press is a major university press, publishing over 200 books and 30 journals annually emphasizing science and technology as well as arts, architecture, new media, current events, and social issues.[161]
MIT is a large, highly residential, research university with a majority of enrollments in graduate and professional programs.[168] The university has been accredited by the New England Association of Schools and Colleges since 1929.[169][170] MIT operates on a 414 academic calendar with the fall semester beginning after Labor Day and ending in mid-December, a 4-week "Independent Activities Period" in the month of January, and the spring semester beginning in early February and ending in late May.[171]
MIT places among the top ten in many overall rankings of universities (see right) and rankings based on students' revealed preferences.[172][173] For several years, U.S.News & World Report, the QS World University Rankings, and the Academic Ranking of World Universities have ranked MIT's School of Engineering first, as did the 1995 National Research Council report. In the same lists, MIT's strongest showings apart from in engineering are in computer science, the natural sciences, business, economics, linguistics, mathematics, and, to a lesser extent, political science and philosophy.[174][175][176][177][178][179]
MIT students refer to both their majors and classes using numbers or acronyms alone.[180] Departments and their corresponding majors are numbered in the approximate order of their foundation; for example, Civil and Environmental Engineering is Course 1, while Linguistics and Philosophy is Course 24.[181] Students majoring in Electrical Engineering and Computer Science (EECS), the most popular department, collectively identify themselves as "Course 6". MIT students use a combination of the department's course number and the number assigned to the class to identify their subjects; the introductory calculus-based classical mechanics course is simply "8.01" at MIT.[182][h]
The four year, full-time undergraduate program maintains a balance between professional majors and those in the arts and sciences, and is selective, admitting few transfer students[168] and 9.7% of its applicants in the 20102011 application season.[184] MIT offers 44 undergraduate degrees across its five schools.[185] In the 20102011 academic year, 1,161 bachelor of science (abbreviated SB) degrees were granted, the only type of undergraduate degree MIT now awards.[186][187] In the 2011 fall term, among students who had designated a major, the School of Engineering was the most popular division, enrolling 62.7% of students in its 19 degree programs, followed by the School of Science (28.5%), School of Humanities, Arts, & Social Sciences (3.7%), Sloan School of Management (3.3%), and School of Architecture and Planning (1.8%). The largest undergraduate degree programs were in Electrical Engineering and Computer Science (Course 6-2), Computer Science and Engineering (Course 6-3), Mechanical Engineering (Course 2), Physics (Course 8), and Mathematics (Course 18).[4]
All undergraduates are required to complete a core curriculum called the General Institute Requirements (GIRs).[188] The Science Requirement, generally completed during freshman year as prerequisites for classes in science and engineering majors, comprises two semesters of physics, two semesters of calculus, one semester of chemistry, and one semester of biology. There is a Laboratory Requirement, usually satisfied by an appropriate class in a course major. The Humanities, Arts, and Social Sciences (HASS) Requirement consists of eight semesters of classes in the humanities, arts, and social sciences, including at least one semester from each division as well as the courses required for a designated concentration in a HASS division. Under the Communication Requirement, two of the HASS classes, plus two of the classes taken in the designated major must be "communication-intensive",[189] including "substantial instruction and practice in oral presentation".[190] Finally, all students are required to complete a swimming test; non-varsity athletes must also take four quarters of physical education classes.[188]
Most classes rely on a combination of lectures, recitations led by associate professors or graduate students, weekly problem sets ("p-sets"), and tests. Although keeping up with the pace and difficulty of MIT coursework has been compared to "drinking from a fire hose",[191][192] the freshmen retention rate at MIT is similar to that at other national research universities.[193] The "pass/no-record" grading system relieves some of the pressure for first-year undergraduates. For each class taken in the fall term, freshmen transcripts will either report only that the class was passed, or otherwise not have any record of it. In the spring term, passing grades (A, B, C) appear on the transcript while non-passing grades are again not recorded.[194] (Grading had previously been "pass/no record" all freshman year, but was amended for the Class of 2006 to prevent students from gaming the system by completing required major classes in their freshman year.[195]) Also, freshmen may choose to join alternative learning communities, such as Experimental Study Group, Concourse, or Terrascope.[194]
In 1969, Margaret MacVicar founded the Undergraduate Research Opportunities Program (UROP) to enable undergraduates to collaborate directly with faculty members and researchers. Students join or initiate research projects ("UROPs") for academic credit, pay, or on a volunteer basis through postings on the UROP website or by contacting faculty members directly.[196] A substantial majority of undergraduates participate.[197][198] Students often become published, file patent applications, and/or launch start-up companies based upon their experience in UROPs.[199][200]
In 1970, the then-Dean of Institute Relations, Benson R. Snyder, published The Hidden Curriculum, arguing that education at MIT was often slighted in favor of following a set of unwritten expectations, and that graduating with good grades was more often the product of figuring out the system rather than a solid education. The successful student, according to Snyder, was the one who was able to discern which of the formal requirements were to be ignored in favor of which unstated norms. For example, organized student groups had compiled "course bibles", collections of problem-set and examination questions and answers to be used as references for later students. This sort of gamesmanship, Snyder argued, hindered the development of a creative intellect and contributed to student discontent and unrest.[201][202]
MIT's graduate program has high coexistence with the undergraduate program and offers a comprehensive doctoral program with degrees in the humanities, social sciences, and STEM fields as well as professional degrees.[168] The Institute offers graduate programs leading to academic degrees such as the Master of Science (SM), various Engineer's Degrees, Doctor of Philosophy (PhD), and Doctor of Science (ScD); professional degrees such as Master of Architecture (MArch),[203] Master of Business Administration (MBA),[204] Master of City Planning (MCP),[205] Master of Engineering (MEng),[206] and Master of Finance (MFin); and interdisciplinary graduate programs such as the MD/PhD (with Harvard Medical School).[207][208] Admission to graduate programs is decentralized; applicants apply directly to the department or degree program. More than 90% of doctoral students are supported by fellowships, research assistantships, or teaching assistantships.[209] MIT awarded 1,547 master's degrees and 609 doctoral degrees in the 201011 academic year.[186] In the 2011 fall term, the School of Engineering was the most popular academic division, enrolling 45.0% of graduate students, followed by the Sloan School of Management (19%), School of Science (16.9%), School of Architecture and Planning (9.2%), Whitaker College of Health Sciences (5.1%),[i] and School of Humanities, Arts, and Social Sciences (4.7%). The largest graduate degree programs were the Sloan MBA, Electrical Engineering and Computer Science, and Mechanical Engineering.[4]
The MIT library system consists of five subject libraries: Barker (Engineering), Dewey (Economics), Hayden (Humanities and Science), Lewis (Music), and Rotch (Arts and Architecture). There are also various specialized libraries and archives. The libraries contain more than 2.9 million printed volumes, 2.4 million microforms, 49,000 print or electronic journal subscriptions, and 670 reference databases. The past decade has seen a trend of increased focus on digital over print resources in the libraries.[210] Notable collections include the Lewis Music Library with an emphasis on 20th and 21st-century music and electronic music,[211] the List Visual Arts Center's rotating exhibitions of contemporary art,[212] and the Compton Gallery's cross-disciplinary exhibitions.[213] MIT allocates a percentage of the budget for all new construction and renovation to commission and support its extensive public art and outdoor sculpture collection.[214][215] The MIT Museum was founded in 1971 and collects, preserves, and exhibits artifacts significant to the life and history of MIT as well as collaborating with the nearby Museum of Science.[216]
MIT was elected to the Association of American Universities in 1934 and remains a research university with a very high level of research activity;[41][168] research expenditures totaled $718.2 million in 2009.[16] The federal government was the largest source of sponsored research, with the Department of Health and Human Services granting $255.9 million, Department of Defense $97.5 million, Department of Energy $65.8 million, National Science Foundation $61.4 million, and NASA $27.4 million.[16] MIT employs approximately 1300 researchers in addition to faculty.[217] In 2011, MIT faculty and researchers disclosed 632 inventions, were issued 153 patents, earned $85.4 million in cash income, and received $69.6 million in royalties.[218] Through programs like the Deshpande Center, MIT faculty leverage their research and discoveries into multi-million-dollar commercial ventures.[219]
In electronics, magnetic core memory, radar, single electron transistors, and inertial guidance controls were invented or substantially developed by MIT researchers.[220][221] Harold Eugene Edgerton was a pioneer in high speed photography.[222] Claude E. Shannon developed much of modern information theory and discovered the application of Boolean logic to digital circuit design theory.[223] In the domain of computer science, MIT faculty and researchers made fundamental contributions to cybernetics, artificial intelligence, computer languages, machine learning, robotics, and cryptography.[221][224] At least nine Turing Award laureates and seven recipients of the Draper Prize in engineering have been or are currently associated with MIT.[225][226]
Current and previous physics faculty have won eight Nobel Prizes,[227] four Dirac Medals,[228] and three Wolf Prizes predominantly for their contributions to subatomic and quantum theory.[229] Members of the chemistry department have been awarded three Nobel Prizes and one Wolf Prize for the discovery of novel syntheses and methods.[227] MIT biologists have been awarded six Nobel Prizes for their contributions to genetics, immunology, oncology, and molecular biology.[227] Professor Eric Lander was one of the principal leaders of the Human Genome Project.[230][231] Positronium atoms,[232] synthetic penicillin,[233] synthetic self-replicating molecules,[234] and the genetic bases for Lou Gehrig's disease and Huntington's disease were first discovered at MIT.[235] Jerome Lettvin transformed the study of cognitive science with his paper "What the frog's eye tells the frog's brain".[236]
In the domain of humanities, arts, and social sciences, MIT economists have been awarded five Nobel Prizes and nine John Bates Clark Medals.[227][237] Linguists Noam Chomsky and Morris Halle authored seminal texts on generative grammar and phonology.[238][239] The MIT Media Lab, founded in 1985 within the School of Architecture and Planning and known for its unconventional research,[240][241] has been home to influential researchers such as constructivist educator and Logo creator Seymour Papert.[242]
Spanning many of the above fields, MacArthur Fellowships (the so-called "Genius Grants") have been awarded to 38 people associated with MIT.[243] Four Pulitzer Prize winning writers currently work at or have retired from MIT.[244] Four current or former faculty are members of the American Academy of Arts and Letters.[245]
Given MIT's prominence, allegations of research misconduct or improprieties have received substantial press coverage. Professor David Baltimore, a Nobel Laureate, became embroiled in a misconduct investigation starting in 1986 that led to Congressional hearings in 1991.[246][247] Professor Ted Postol has accused the MIT administration since 2000 of attempting to whitewash potential research misconduct at the Lincoln Lab facility involving a ballistic missile defense test, though a final investigation into the matter has not been completed.[248][249] Associate Professor Luk Van Parijs was dismissed in 2005 following allegations of scientific misconduct and found guilty of the same by the United States Office of Research Integrity in 2009.[250][251]
The faculty and student body highly value meritocracy and technical proficiency.[252][253] MIT has never awarded an honorary degree, nor does it award athletic scholarships, ad eundem degrees, or Latin honors upon graduation.[254] However, MIT has twice awarded honorary professorships: to Winston Churchill in 1949 and Salman Rushdie in 1993.[255]
Many upperclass students and alumni wear a large, heavy, distinctive class ring known as the "Brass Rat".[256][257] Originally created in 1929, the ring's official name is the "Standard Technology Ring."[258] The undergraduate ring design (a separate graduate student version exists as well) varies slightly from year to year to reflect the unique character of the MIT experience for that class, but always features a three-piece design, with the MIT seal and the class year each appearing on a separate face, flanking a large rectangular bezel bearing an image of a beaver.[256] The initialism IHTFP, representing the informal school motto "I Hate This Fucking Place" and jocularly euphemized as "I Have Truly Found Paradise," "Institute Has The Finest Professors," "It's Hard to Fondle Penguins," and other variations, has occasionally been featured on the ring given its historical prominence in student culture.[259]
MIT has over 380 recognized student activity groups,[260] including a campus radio station, The Tech student newspaper, an annual entrepreneurship competition, and weekly screenings of popular films by the Lecture Series Committee. Less traditional activities include the "world's largest open-shelf collection of science fiction" in English, a model railroad club, and a vibrant folk dance scene. Students, faculty, and staff are involved in over 50 educational outreach and public service programs through the MIT Museum, Edgerton Center, and MIT Public Service Center.[261]
The Independent Activities Period is a four-week long "term" offering hundreds of optional classes, lectures, demonstrations, and other activities throughout the month of January between the Fall and Spring semesters. Some of the most popular recurring IAP activities are the 6.270, 6.370, and MasLab competitions,[262] the annual "mystery hunt",[263] and Charm School.[264][265] More than 250 students pursue externships annually at companies in the US and abroad.[266][267]
Many MIT students also engage in "hacking," which encompasses both the physical exploration of areas that are generally off-limits (such as rooftops and steam tunnels), as well as elaborate practical jokes.[268][269] Recent high-profile hacks have included the theft of Caltech's cannon,[270] reconstructing a Wright Flyer atop the Great Dome,[271] and adorning the John Harvard statue with the Master Chief's Spartan Helmet.[272]
MIT sponsors 31 varsity sports and has one of the three broadest NCAA Division III athletic programs.[273][274] MIT participates in the NCAA's Division III, the New England Women's and Men's Athletic Conference, the New England Football Conference, the Pilgrim League for men's lacrosse and NCAA's Division I Eastern Association of Women's Rowing Colleges (EAWRC) for women's crew. Men's crew competes outside the NCAA in the Eastern Association of Rowing Colleges (EARC). In April 2009, budget cuts lead to MIT eliminating eight of its 41 sports, including the mixed mens and womens teams in alpine skiing and pistol; separate teams for men and women in ice hockey and gymnastics; and mens programs in golf and wrestling.[275][276]
The Institute's sports teams are called the Engineers, their mascot since 1914 being a beaver, "nature's engineer." Lester Gardner, a member of the Class of 1898, provided the following justification:
The beaver not only typifies the Tech, but his habits are particularly our own. The beaver is noted for his engineering and mechanical skills and habits of industry. His habits are nocturnal. He does his best work in the dark.[277]
MIT fielded several dominant intercollegiate Tiddlywinks teams through 1980, winning national and world championships.[278] MIT has produced 188 Academic All-Americans, the third largest membership in the country for any division and the highest number of members for Division III.[274]
The Zesiger sports and fitness center (Z-Center) which opened in 2002, significantly expanded the capacity and quality of MIT's athletics, physical education, and recreation offerings to 10 buildings and 26 acres (110,000m2) of playing fields. The 124,000-square-foot (11,500m2) facility features an Olympic-class swimming pool, international-scale squash courts, and a two-story fitness center.[274]
MIT enrolled 4,384 undergraduates and 6,510 graduate students in 20112012.[4] Women constituted 45 percent of undergraduate students.[4][281] Undergraduate and graduate students are drawn from all 50 states as well as 115 foreign countries in the 20112012 school year.[282]
MIT received 17,909 applications for admission to the undergraduate Class of 2015; 1,742 were admitted (9.7 percent) and 1128 enrolled (64.8 percent).[118] 19,446 applications were received for graduate and advanced degree program across all departments; 2,991 were admitted (15.4 percent) and 1,880 enrolled (62.8 percent).[283] The interquartile range on the SAT was 20302320 and 95 percent of students ranked in the top tenth of their high school graduating class.[118] 97 percent of the Class of 2012 returned as sophomores; 82.3 percent of the Class of 2007 graduated within 4 years, and 91.3 percent (92 percent of the men and 96 percent of the women) graduated within 6 years.[118][284]
Undergraduate tuition and fees total $40,732 and annual expenses are estimated at $52,507 as of 2012. 62 percent of students received need-based financial aid in the form of scholarships and grants from federal, state, institutional, and external sources averaging $38,964 per student.[285] MIT awarded $87.6 million in scholarships and grants, the vast majority ($73.4 million) coming from institutional support.[118] The annual increase in expenses has led to a student tradition (dating back to the 1960s) of tongue-in-cheek "tuition riots".[286]
MIT has been nominally coeducational since admitting Ellen Swallow Richards in 1870. Richards also became the first female member of MIT's faculty, specializing in sanitary chemistry.[287] Female students remained a very small minority (less than 3 percent) prior to the completion of the first wing of a women's dormitory, McCormick Hall, in 1962.[288][289] Between 1993 and 2009, the proportion of women rose from 34 percent to 45 percent of undergraduates and from 20 percent to 31 percent of graduate students.[4][290] Women currently outnumber men in Biology, Brain & Cognitive Sciences, Architecture, Urban Planning, and Biological Engineering.[4][281]
A number of student deaths in the late 1990s and early 2000s resulted in considerable media attention to MIT's culture and student life.[291][292] After the alcohol-related death of Scott Krueger in September 1997 as a new member at the Phi Gamma Delta fraternity,[293] MIT began requiring all freshmen to live in the dormitory system.[293][294] The 2000 suicide of MIT undergraduate Elizabeth Shin drew attention to suicides at MIT and created a controversy over whether MIT had an unusually high suicide rate.[295][296] In late 2001 a task force's recommended improvements in student mental health services were implemented,[297][298] including expanding staff and operating hours at the mental health center.[299] These and later cases were significant as well because they sought to prove the negligence and liability of university administrators in loco parentis.[295]
MIT has 1,018 faculty members, of whom 217 are women.[3] Faculty are responsible for lecturing classes, advising both graduate and undergraduate students, and sitting on academic committees, as well as conducting original research. Between 1964 and 2009, a total of seventeen faculty and staff members affiliated with MIT were awarded Nobel Prizes (thirteen in the last 25 years).[300] MIT faculty members past or present have won a total of twenty-seven Nobel Prizes, the majority in Economics or Physics.[301] Among current faculty and teaching staff, there are eighty Guggenheim Fellows, six Fulbright Scholars, and twenty-nine MacArthur Fellows.[3] Faculty members who have made extraordinary contributions to their research field as well as the MIT community are granted appointments as Institute Professors for the remainder of their tenures.
A 1998 MIT study concluded that a systemic bias against female faculty existed in its college of science,[302] although the study's methods were controversial.[303][304] Since the study, though, women have headed departments within the Schools of Science and Engineering, and MIT has appointed several female vice presidents, although allegations of sexism continue to be made.[305] Susan Hockfield, a molecular neurobiologist, was MIT's president from 2004 to 2012 and was the first woman to hold the post.[138]
Tenure outcomes have vaulted MIT into the national spotlight on several occasions. The 1984 dismissal of David F. Noble, a historian of technology, became a cause clbre about the extent to which academics are granted freedom of speech after he published several books and papers critical of MIT's and other research universities' reliance upon financial support from corporations and the military.[306] Former materials science professor Gretchen Kalonji sued MIT in 1994 alleging that she was denied tenure because of sexual discrimination.[305][307] In 1997, the Massachusetts Commission Against Discrimination issued a probable cause finding supporting James Jennings' allegations of racial discrimination after a senior faculty search committee in the Department of Urban Studies and Planning did not offer him reciprocal tenure.[308] In 20062007, MIT's denial of tenure to African-American biological engineering professor James Sherley reignited accusations of racism in the tenure process, eventually leading to a protracted public dispute with the administration, a brief hunger strike, and the resignation of Professor Frank L. Douglas in protest.[309][310]
MIT faculty members have often been recruited to lead other colleges and universities; former provost Robert A. Brown is president of Boston University, former provost Mark Wrighton is chancellor of Washington University in St. Louis, former associate provost Alice Gast is president of Lehigh University, former dean of the School of Science Robert J. Birgeneau is the chancellor of the University of California, Berkeley, and former professor David Baltimore had been president of Caltech. In addition, faculty members have been recruited to lead governmental agencies; for example, former professor Marcia McNutt is the director of the United States Geological Survey,[311] urban studies professor Xavier de Souza Briggs is currently the associate director of the White House Office of Management and Budget,[312] and biology professor Eric Lander is a co-chair of the President's Council of Advisors on Science and Technology.[313]
Many of MIT's over 120,000 alumni have had considerable success in scientific research, public service, education, and business. As of 2011, twenty-four MIT alumni have won the Nobel Prize, forty-four have been selected as Rhodes Scholars, and fifty-five have been selected as Marshall Scholars.[314]
Alumni in American politics and public service include Chairman of the Federal Reserve Ben Bernanke, former MA-1 Representative John Olver, Chief Economic Adviser of India Raghuram Rajan, former CA-13 Representative Pete Stark, former National Economic Council chairman Lawrence H. Summers, and former Council of Economic Advisors chairwoman Christina Romer. MIT alumni in international politics include Israeli Prime Minister Benjamin Netanyahu, President of the European Central Bank Mario Draghi, physicist Richard Feynman, former British Foreign Minister David Miliband, former Greek Prime Minister Lucas Papademos, former UN Secretary General Kofi Annan, and former Iraqi Deputy Prime Minister Ahmed Chalabi.
MIT alumni founded or co-founded many notable companies, such as Intel, McDonnell Douglas, Texas Instruments, 3Com, Qualcomm, Bose, Raytheon, Koch Industries, Rockwell International, Genentech, Dropbox, and Campbell Soup. According to the British newspaper, The Guardian, "a survey of living MIT alumni found that they have formed 25,800 companies, employing more than three million people including about a quarter of the workforce of Silicon Valley. Those firms between them generate global revenues of about $1.9tn (1.2tn) a year. If MIT was a country, it would have the 11th highest GDP of any nation in the world."[315]
Prominent institutions of higher education have been led by MIT alumni, including the University of California system, Harvard University, Johns Hopkins University, Carnegie Mellon University, Tufts University, Rochester Institute of Technology, Rhode Island School of Design (RISD), Northeastern University, Lahore University of Management Sciences, Rensselaer Polytechnic Institute, Tecnolgico de Monterrey, Purdue University, Virginia Polytechnic Institute and Quaid-e-Azam University.
More than one third of the United States' manned spaceflights have included MIT-educated astronauts (among them Apollo 11 Lunar Module Pilot Buzz Aldrin), more than any university excluding the United States service academies.[316]
Noted alumni in non-scientific fields include author Hugh Lofting,[317] sculptor Daniel Chester French, Boston guitarist Tom Scholz, The New York Times columnist and Nobel Prize Winning economist Paul Krugman, The Bell Curve author Charles Murray, United States Supreme Court building architect Cass Gilbert, Pritzker Prize-winning architects I.M. Pei and Gordon Bunshaft.
Apollo 11 astronaut Buzz Aldrin, ScD 1963 (Aero & Astro)
Former UN Secretary-General Kofi Annan, SM 1972 (Management)
Federal Reserve Bank Chairman Ben Bernanke, PhD 1979 (Economics)
Physicist Nobel laureate Richard Feynman, SB 1939 (Physics)
Economics Nobel laureate Paul Krugman, PhD 1977 (Economics)
Biologist, suffragist, philanthropist Katherine Dexter McCormick (left), SB 1904 (Biology)
Astronaut physicist Ronald McNair, PhD 1976 (Physics)
Israeli Prime Minister Benjamin Netanyahu, SB 1975 (Architecture), SM 1976 (Management)
Architect I. M. Pei, BArch 1940 (Architecture)
CEO of General Motors Alfred P. Sloan, SB 1895 (Electrical Engineering)

Explanatory notes
Citations
Bibliography
1 History

1.1 Foundation and vision
1.2 Early developments
1.3 Curricular reforms
1.4 Defense research
1.5 Recent history


1.1 Foundation and vision
1.2 Early developments
1.3 Curricular reforms
1.4 Defense research
1.5 Recent history
2 Campus

2.1 Architecture
2.2 Housing


2.1 Architecture
2.2 Housing
3 Organization and administration

3.1 Collaborations


3.1 Collaborations
4 Academics

4.1 Undergraduate program
4.2 Graduate program
4.3 Libraries, collections, and museums


4.1 Undergraduate program
4.2 Graduate program
4.3 Libraries, collections, and museums
5 Research
6 Traditions and student activities

6.1 Activities
6.2 Athletics


6.1 Activities
6.2 Athletics
7 People

7.1 Students
7.2 Faculty
7.3 Alumni


7.1 Students
7.2 Faculty
7.3 Alumni
8 See also
9 References
10 External links
1.1 Foundation and vision
1.2 Early developments
1.3 Curricular reforms
1.4 Defense research
1.5 Recent history
2.1 Architecture
2.2 Housing
3.1 Collaborations
4.1 Undergraduate program
4.2 Graduate program
4.3 Libraries, collections, and museums
6.1 Activities
6.2 Athletics
7.1 Students
7.2 Faculty
7.3 Alumni






Apollo 11 astronaut Buzz Aldrin, ScD 1963 (Aero & Astro)









Former UN Secretary-General Kofi Annan, SM 1972 (Management)









Federal Reserve Bank Chairman Ben Bernanke, PhD 1979 (Economics)









Physicist Nobel laureate Richard Feynman, SB 1939 (Physics)









Economics Nobel laureate Paul Krugman, PhD 1977 (Economics)









Biologist, suffragist, philanthropist Katherine Dexter McCormick (left), SB 1904 (Biology)









Astronaut physicist Ronald McNair, PhD 1976 (Physics)









Israeli Prime Minister Benjamin Netanyahu, SB 1975 (Architecture), SM 1976 (Management)









Architect I. M. Pei, BArch 1940 (Architecture)









CEO of General Motors Alfred P. Sloan, SB 1895 (Electrical Engineering)



 Boston portal
 Massachusetts portal
 University portal
Abelmann, Walter H. (2004). The Harvard-MIT Division of Health Sciences and Technology: The First 25 Years, 1970-1995. Cambridge, Mass.: Harvard-MIT Division of Health Sciences and Technology. ISBN9780674014589.
Angulo, A. J. (2007). "The Initial Reception of MIT, 1860s1880s". History of Higher Education Annual 26: 128.
Etzkowitz, Henry (2006). MIT and the Rise of Entrepreneurial Science. London: Routledge. ISBN9780415435055.
Hapgood, Fred (1992). Up the Infinite Corridor: MIT and the Technical Imagination. Reading, Mass.: Addison-Wesley. ISBN9780201082937.
Jarzombek, Mark (2004). Designing MIT: Bosworth's New Tech. Boston, Mass.: Northeastern University Press. ISBN9781555536190.
Keyser, Samuel Jay (2011). Mens et Mania: The MIT Nobody Knows. Cambridge, Mass.: MIT Press. ISBN9780262015943.
Lecuyer, Christophe (1992). "The Making of a Science Based Technological University: Karl Compton, James Killian, and the Reform of MIT, 19301957". Historical Studies in the Physical & Biological Sciences 23 (1): 153180.
Leslie, Stuart W. (1993). The Cold War and American Science: The Military-Industrial-Academic Complex at MIT and Stanford. New York: Columbia University Press. ISBN9780231079587.
Lewis, Warren K., Ronald H. Robnett, C. Richard Soderberg, Julius A. Stratton et al. (1949) (PDF). Report of the Committee on Educational Survey (Lewis Report). Cambridge, Massachusetts: MIT Press. http://libraries.mit.edu/archives/mithistory/pdf/lewis.pdf. Retrieved May 28, 2012.
Mitchell, William J. (2007). Imagining MIT: Designing a Campus for the Twenty-first Century. Cambridge, Mass.: MIT Press. ISBN9780262134798.
Peterson, T. F. (2003). Nightwork: A History of Hacks and Pranks at MIT. Cambridge, Mass.: MIT Press. ISBN9780262661379.
Prescott, Samuel C. (1954). When MIT was "Boston Tech", 1861-1916 (Reprint. ed.). MIT Press. ISBN9780262661393.
Servos, John W. (December 1980). "The Industrial Relations of Science: Chemical Engineering at MIT, 19001939". Isis (The University of Chicago Press on behalf of The History of Science Society) 71 (4): 531549. JSTOR230499.
Shrock, Robert Rakes (1982). Geology at MIT 1865-1965: A History of the First Hundred Years of Geology at Massachusetts Institute of Technology. Cambridge, Mass.: MIT Press. ISBN9780262192118.
Simha, O. Robert (2003). MIT Campus Planning, 1960-2000: An Annotated Chronology. Cambridge, Mass.: MIT Press. ISBN9780262692946.
Snyder, Benson R. (1971). The Hidden Curriculum. Cambridge, Mass.: MIT Press. ISBN9780262690430.
Stratton, Julius A. (2005). Mind and Hand: The Birth of MIT. Cambridge, Mass.: MIT Press. ISBN9780262195249.
Vest, Charles M. (2004). Pursuing the Endless Frontier: Essays on MIT and the Role of Research Universities. Cambridge, Mass.: MIT Press. ISBN9780262220729.
Wildes, Karl L.; Lindgren, Nilo A. (1985). A Century of Electrical Engineering and Computer Science at MIT, 1882-1982. Cambridge, Mass.: MIT Press. ISBN9780262231190.
Official website
"Massachusetts Institute of Technology, The". Encyclopedia Americana. 1920.
.
#(`*Pearl Harbor*`)#.
Pearl Harbor is a lagoon harbor on the island of Oahu, Hawaii, west of Honolulu. Much of the harbor and surrounding lands is a United States Navy deep-water naval base. It is also the headquarters of the United States Pacific Fleet. The attack on Pearl Harbor by the Empire of Japan on Sunday, December 7, 1941 brought the United States into World War II.[3][4][5]
Pearl Harbor was originally an extensive deep embayment called Wai Nomi (meaning, pearl water) or Puuloa (meaning, long hill) by the Hawaiians. Puuloa was regarded as the neighbor of the dolphin god, Kaahupahau, and his brother (or father), Kahiuka, in Hawaiian legends. According to tradition, Keaunui, the head of the powerful Ewu chiefs, is credited with cutting a navigable channel near the present Puuloa saltworks, by which made the estuary, known as "Pearl Lake," accessible to navigation. Making due allowance for legendary amplification, the estuary already had an outlet for its waters where the present gap is; but Keaunui is typically given the credit for widening and deepening it.[6]
During the early 19th century, Pearl Harbor was not used for large ships due to its shallow entrance. The interest of United States in the Hawaiian Islands followed its whaling and trading ships in the Pacific. As early as 1820, an "Agent of the United States for Commerce and Seamen" was appointed to look after American business in the Port of Honolulu. These commercial ties to the American continent were accompanied by the work of the American Board of Commissioners for Foreign Missions. American missionaries and their families became an integral part of the Hawaiian political body.
Throughout the 1820s and 1830s, many American warships visited Honolulu. In most cases, the commanding officers carried letters from the U.S. Government giving advice on governmental affairs and of the relations of the island nation with foreign powers. In 1841, the newspaper Polynesian, printed in Honolulu, advocated that the U.S. establish a naval base in Hawaii for protection of American citizens engaged in the whaling industry. The British Hawaiian Minister of Foreign Affairs Robert Crichton Wyllie, remarked in 1840 that "...my opinion is that the tide of events rushes on to annexation to the United States."
With the conclusion of the Civil War, the purchase of Alaska, the increased importance of the Pacific states, the projected trade with the Orient and the desire for a duty free market for Hawaiian staples, Hawaiian trade expanded. In 1865, the North Pacific Squadron was formed to embrace the western coast and Hawaii. Lackawanna in the following year was assigned to cruise among the islands, "a locality of great and increasing interest and importance." This vessel surveyed the Northwestern Hawaiian Islands toward Japan. As a result the United States claimed Midway Island. The Secretary of the Navy was able to write in his annual report of 1868, that in November 1867, 42 American flags flew over whaleships and merchant vessels in Honolulu to only six of other nations. This increased activity caused the permanent assignment of at least one warship to Hawaiian waters. It also praised Midway Island as possessing a harbor surpassing Honolulu's. In the following year, Congress approved an appropriation of $50,000 on March 1, 1869, to deepen the approaches to this harbor.
After 1868, when the Commander of the Pacific Fleet visited the islands to look after American interests, naval officers played an important role in internal affairs. They served as arbitrators in business disputes, negotiators of trade agreements and defenders of law and order. Periodic voyages among the islands and to the mainland aboard U.S. warships were arranged for members of the Hawaiian royal family and important island government officials. When King Lunalilo died in 1873, negotiations were underway for the cessation of Pearl Harbor as a port for the duty-free export of sugar to the U.S.[citation needed] With the election of King Kalkaua in March 1874, riots prompted landing of sailors from USS Tuscorora and Portsmouth. The British warship, HMSTenedos, also landed a token force. During the reign of King Kalkaua the United States was granted exclusive rights to enter Pearl Harbor and to establish "a coaling and repair station."
This treaty continued in force until August 1898, the U.S. did not fortify Pearl Harbor as a naval base. The shallow entrance constituted a formidable barrier against the use of the deep protected waters of the inner harbor as it had for 60 years.
The United States and the Hawaiian Kingdom signed the Reciprocity Treaty of 1875 as supplemented by Convention on December 6, 1884, the Reciprocity Treaty was made by James Carter and ratified it in 1887. On January 20, 1887, the United States Senate allowed the Navy to exclusive right to maintain a coaling and repair station at Pearl Harbor. (The US took possession on November 9 that year). The Spanish-American War of 1898 and the desire for the United States to have a permanent presence in the Pacific both contributed to the decision.
Following the annexation of Hawaii, Pearl Harbor was refitted to allow for more navy ships. In May 1899, Commander F. Merry was made naval representative with authority to transact business for the Navy Department and its Bureaus. He immediately assumed control of the Coal Depot and its equipment. To supplement his facilities, he was assigned the Navy tug Iroquois and two coal barges. Inquiries that commenced in June culminated in the establishment of the "Naval Station, Honolulu" on November 17, 1899. On February 2, 1900, this title was changed to "Naval Station, Hawaii."
The creation of the Naval Station allowed the Navy Department to explore territorial outposts. In October 1899, Nero and Iroquois made extensive surveys and sounding of the waterways to Midway and Guam. One of the reasons for these explorations was to select a possible cable route to Luzon.
A coal famine and an outbreak of the bubonic plague were the only two incidents that hindered the Commandant from fulfilling his duties. Because of the severe coal shortage in September 1899, the Commandant sold coal to the Oahu Railway and Land Company and the Inter-Island Steam Navigation Company, Ltd. Although this indicated the affinity of economic ties with the Navy, it was to a certain extent counteracted by the quarantine of the naval establishment from December 1899-February 1900, because of the bubonic plague. Approximately 61 deaths were recorded in Honolulu for this period. Work was consequently delayed on nascent Navy projects in Honolulu Harbor.
From 1900-1908, the Navy devoted its time to improving the facilities of the 85 acres (34ha) that constituted the naval reservation in Honolulu. Under the Appropriation Act of March 3, 1901, this tract of land was improved with the erection of additional sheds and housing. Improvements included a machine shop, smithery and foundry, Commandant's house and stables, cottage for the watchman, fencing, 10-ton wharf crane, and water-pipe system. The harbor was dredged and the channel enlarged to accommodate larger ships. On May 28, 1903, the first battleship, Wisconsin, entered the harbor for coal and water. However, when the vessels of the Asiatic station visited Honolulu in January 1904, Rear Admiral Silas Terry complained that they were inadequately accommodated with dockage and water.
Under the above Appropriation Act, Congress approved the acquisition of lands for the development of a naval station at Pearl Harbor and the improvement of the channel to the Lochs. The Commandant, under the direction of the Bureau of Equipment, attempted to obtain options on lands surrounding Pearl Harbor that were recommended for naval use. This endeavor was unsuccessful when the owners of the property refused to accept what was deemed to be a fair price. Condemnation proceedings, under the Hawaiian law of eminent domain, were begun on July 6, 1901. The land acquired by this suit included the present Navy Yard, Kauhua Island, and a strip on the southeast coast of Ford Island. The work of dredging the coral reef that blocked Pearl Harbor progressed rapidly enough to allow the gunboat Petrel to proceed to the upper part of Main Loch in January 1905.
One of the early concerns of the growing station was that the Army would make claims on its property. Because of their facilities, as wharves, cranes, artesian wells, and coal supplies, many requests were made by the Army for their use. By February 1901, the Army had made application for the privilege of establishing on Navy docks movable cranes for handling coal and other stores, a saluting battery and a flag staff on the naval reservation, and an artesian well of its own. All these requests were rejected by the Bureau of Equipment on the theory that, once granted, they "will practically constitute a permanent foothold on the property, and end in dividing it between the two Departments, or in the entire exclusion of the Navy Department on the ground of military expediency as established by frequency of use." However, the Army Depot Quartermaster at Honolulu contracted for the sinking of an artesian well on the Naval Station with the Commandant's approval, who, in turn, acted on a recommendation of the Bureau of Yards and Docks. The flow of water obtained amounted to over 1.5 million gallons per day, sufficient for all purposes of the Army and Navy. The Bureau of Equipment felt that its word of caution was justified when the Depot Quartermaster in 1902 let it be known that any water used by the Navy from the artesian well was "only given by courtesy of the Army."
Despite the warnings of the Bureau of Equipment, the War Department, the Department of Labor and Commerce, and the Department of Agriculture had secured permission to settle on the naval reservation. By 1906, the Commandant believed that it was necessary for the Bureau of Yards and Docks to develop a policy on the future of the station. The docks were being used to a greater extent by the Army transports, than by Navy ships, and the Army was actually attempting to get possession of Quarantine Wharf (which was built by the Territorial Government on the Naval Reservation, with the understanding that it could be taken over at any time by the Navy Department upon the payment of its appraised value.) In 1903, the Department of Labor and Commerce received about 7 acres (2.8ha) for an Immigration Station. The Department of Agriculture had, in the meanwhile, secured part of the site intended for a hospital as an experimental station. The Commandant felt that, if the station was going to develop beyond a mere coaling depot, these territorial encroachments on the part of other departments should be stopped, particularly when they were enjoying the benefits of naval appropriations. "On the other hand," he wrote, "if it is the intention to improve Pearl Harbor and eventually abandon this station every effort should be made to begin work there as soon as possible... I am informed that important commercial interests will make a strong effort next year to have Pearl Harbor improved, and I think that will be an opportune time for the Navy Department to make efforts in the same direction."
In 1908, the Pearl Harbor Naval Shipyard was established. The period from 1908-1919 was one of steady and continuous growth of the Naval Station, Pearl Harbor, with the exception of the discouraging collapse of the drydock in 1913. The Act of May 13, 1908 authorized the enlargement and dredging of the Pearl Harbor channel and lochs "to admit the largest ships," the building of shops and supply houses for the Navy Yard, and the construction of a drydock. Work on the dock started on September 21, 1909. In April 1910, the barquentine Amaranth became the fourth deep-sea, cargo-carrying vessel to venture into the newly dredged harbor, having been preceded by the three-masted schooner W.H. Marston on March 8, and the schooner Ariel and bark Marston a few days later. Amaranth delivered materials for construction of the dry dock facility.[7] Work progressed satisfactorily on all projects, except the drydock. After much wrangling with Congress to secure an appropriation of over three million dollars for its construction, the drydock was wrecked by "underground pressure." "On February 17, 1913, the entire drydock structure rumbled, rocked, and caved in." The drydock was ceremonially opened to flooding on August 21, 1919, by Mrs. Josephus Daniels, wife of the Secretary of the Navy. In 1917, Ford Island in the middle of Pearl Harbor was purchased for joint Army and Navy use in the development of military aviation in the Pacific.
As the Japanese military pressed its war in China, concern over Japan's intentions caused the U.S. to begin taking defensive measures. On February 1, 1933, the U.S. Navy staged a mock attack on the base at Pearl Harbor as part of a preparedness exercise.[citation needed] The attack "succeeded" and the defense was deemed a "failure".
The actual attack on Pearl Harbor by the Empire of Japan on December 7, 1941 brought the United States into World War II.
Aircraft and midget submarines of the Imperial Japanese Navy began an attack on the U.S naval base. Through earlier code breaking activity, the Americans had determined that an attack was likely to occur. However, while the Americans failed to discover Japan's target location, it was believed that the Philippines was the most likely target.[8] Under the command of Admiral Chuichi Nagumo,[8] the attack was devastating in loss of life and damage to the U.S. fleet. At 06:05 on December 7, the six Japanese carriers launched a first wave of 183 aircraft composed mainly of dive bombers, horizontal bombers and fighters.[9]
The Japanese hit American ships and military installations at 07:51. The first wave attacked military airfields of Ford Island. At 08:30, a second wave of 170 Japanese aircraft, mostly torpedo bombers, attacked the fleet anchored in Pearl Harbor. The battleship Arizona was hit with an armor-piercing bomb which penetrated the forward ammunition compartment, blowing the ship apart and sinking it within seconds. It was one of eight US battleships at the dock, five of which were sunk and the remaining three were badly damaged. Overall, 9 ships of the U.S. fleet were sunk and 21 ships were severely damaged. 3 of the 21 would be irreparable. The overall death toll reached 2,402 [10] and 1,282 wounded, including 68 civilians. Of the military personnel lost at Pearl Harbor, 1,177 were from the Arizona. The first shots fired were from the destroyer Ward on a midget submarine that surfaced outside of Pearl Harbor; Ward sank the midget sub at approximately 06:55, about an hour before the attack on Pearl Harbor. Japan would lose 29 out of the 350 aircraft they attacked with.
Pearl Harbor remained a main base for the US Pacific Fleet after World War II along with Naval Base San Diego. In 2010, the Navy and the Air Force merged their two nearby bases; Pearl Harbor joined with Hickham Air Force Base to create Joint Base Pearl Harbor-Hickam.
The Navy base itself was recognized on January 29, 1964 as a National Historic Landmark district and with the National Register of Historic Places since 1976. Within its bounds, it contains several other National Historic Landmarks associated with the attack on Pearl Harbor, including the Arizona, Bowfin, and Utah. As an active Navy base, many of the historic buildings that contributed to the NHL designation are under threat of demolition and rebuilding.[2]
1 History

1.1 19th century
1.2 18991941
1.3 Sunday, December 7, 1941
1.4 West Loch Explosion, 1944
1.5 Naval base, 1945present


1.1 19th century
1.2 18991941
1.3 Sunday, December 7, 1941
1.4 West Loch Explosion, 1944
1.5 Naval base, 1945present
2 National Historic Landmark
3 See also
4 References
5 External links
1.1 19th century
1.2 18991941
1.3 Sunday, December 7, 1941
1.4 West Loch Explosion, 1944
1.5 Naval base, 1945present
Pearl Harbor National Wildlife Refuge
Official website
British Path Online archive of Pearl Harbor and related footage
Pearl Harbor at the Open Directory Project
Pearl Harbor on The History Channel
.
#(`*World War I*`)#.
British Empire
 France
Russia (191417)
Italy (191518)
United States (191718)
Romania (191618)
Japan
Serbia
Belgium
Greece (191718)
Portugal (191618)
and others
Germany
Austria-Hungary
Ottoman Empire
Bulgaria (191518)
Co-belligerents
 Jabal Shammar
...and others
 H. H. Asquith
 David Lloyd George
 Douglas Haig
 Raymond Poincar
 Georges Clemenceau
 Ferdinand Foch
 Nicholas II
 Nicholas Nikolaevich
 Victor Emanuel III
 Antonio Salandra
 Vittorio Orlando
 Luigi Cadorna
 Woodrow Wilson
 John J. Pershing
 Peter I, King of Serbia
and others
 Wilhelm II
 Paul von Hindenburg
 Erich Ludendorff
 Franz Joseph I
 Karl I
 Conrad von Htzendorf
 Mehmed V
 Enver Pasha
 Mustafa Kemal
 Ferdinand I
 Nikola Zhekov
and others
 12,000,000
 8,841,541[2][3]
 8,660,000[4]
 5,615,140
 4,743,826
 1,234,000
 800,000
 707,343
 380,000
 250,000
Total: 42,959,850
 13,250,000
 7,800,000
 2,998,321
 1,200,000
Total: 25,248,321
World War I (WWI) was a global war centred in Europe that began on 28 July 1914 and lasted until 11 November 1918. It was predominantly called the World War or the Great War from its occurrence until the start of World War II in 1939, and the First World War or World War I thereafter. It involved all the world's great powers,[5] which were assembled in two opposing alliances: the Allies (based on the Triple Entente of the United Kingdom, France and Russia) and the Central Powers (originally the Triple Alliance of Germany, Austria-Hungary and Italy; but, as AustriaHungary had taken the offensive against the agreement, Italy did not enter into the war).[6] These alliances both reorganised (Italy fought for the Allies) and expanded as more nations entered the war. Ultimately, more than 70million military personnel, including 60 million Europeans, were mobilised in one of the largest wars in history.[7][8] More than 9million combatants were killed, largely because of technological advancements that led to enormous increases in the lethality of weapons without corresponding improvements in protection or mobility. It was the sixth-deadliest conflict in world history, subsequently paving the way for various political changes, such as revolutions in many of the nations involved.[9]
Long-term causes of the war included the imperialistic foreign policies of the great powers of Europe, including the German Empire, the Austro-Hungarian Empire, the Ottoman Empire, the Russian Empire, the British Empire, the French Republic, and Italy. The assassination on 28 June 1914 of Archduke Franz Ferdinand of Austria, the heir to the throne of Austria-Hungary, by a Yugoslav nationalist in Sarajevo, Bosnia was the proximate trigger of the war. It resulted in a Habsburg ultimatum against the Kingdom of Serbia.[10][11] Several alliances formed over the previous decades were invoked, so, within weeks, the major powers were at war; via their colonies, the conflict soon spread around the world.
On 28 July, the conflict opened with the Austro-Hungarian invasion of Serbia,[12][13] followed by the German invasion of Belgium, Luxembourg and France; and a Russian attack against Germany. After the German march on Paris was brought to a halt, the Western Front settled into a static battle of attrition with a trench line that changed little until 1917. In the East, the Russian army successfully fought against the Austro-Hungarian forces, but was forced back from East Prussia and Poland by the German army. Additional fronts opened after the Ottoman Empire joined the war in 1914, Italy and Bulgaria in 1915 and Romania in 1916. The Russian Empire collapsed in March 1917, and Russia left the war after the October Revolution later that year. After a 1918 German offensive along the western front, the Allies drove back the German armies in a series of successful offensives and United States forces began entering the trenches. Germany, which had its own trouble with revolutionaries at this point, agreed to a cease-fire on 11 November 1918, later known as Armistice Day. The war had ended in victory for the Allies.
Events on the home fronts were as tumultuous as on the battle fronts, as the participants tried to mobilize their manpower and economic resources to fight a total war. By the end of the war, four major imperial powersthe German, Russian, Austro-Hungarian and Ottoman empiresceased to exist. The successor states of the former two lost a great amount of territory, while the latter two were dismantled entirely. The map of central Europe was redrawn into several smaller states.[14] The League of Nations was formed in the hope of preventing another such conflict. The European nationalism spawned by the war and the breakup of empires, the repercussions of Germany's defeat and problems with the Treaty of Versailles are agreed to be factors contributing to World War II.[15]
In Canada, Maclean's Magazine in October 1914 said, "Some wars name themselves. This is the Great War."[16] A history of the origins and early months of the war published in New York in late 1914 was titled The World War.[17] During the Interwar period, the war was most often called the World War and the Great War in English-speaking countries.
After the onset of the Second World War in 1939, the terms World War I or the First World War became standard, with British and Canadian historians favouring the First World War, and Americans World War I. Both of these terms had also been used during the Interwar period. The term "First World War" was first used in September 1914 by the German philosopher Ernst Haeckel, who claimed that "there is no doubt that the course and character of the feared 'European War' ... will become the first world war in the full sense of the word."[18] The First World War was also the title of a 1920 history by the officer and journalist Charles  Court Repington.
In the 19th century, the major European powers had gone to great lengths to maintain a balance of power throughout Europe, resulting in the existence of a complex network of political and military alliances throughout the continent by 1900.[6] These had started in 1815, with the Holy Alliance between Prussia, Russia, and Austria. Then, in October 1873, German Chancellor Bismarck negotiated the League of the Three Emperors (German: Dreikaiserbund) between the monarchs of AustriaHungary, Russia and Germany. This agreement failed because AustriaHungary and Russia could not agree over Balkan policy, leaving Germany and AustriaHungary in an alliance formed in 1879, called the Dual Alliance. This was seen as a method of countering Russian influence in the Balkans as the Ottoman Empire continued to weaken.[6] In 1882, this alliance was expanded to include Italy in what became the Triple Alliance.[19]
After 1870, European conflict was averted largely through a carefully planned network of treaties between the German Empire and the remainder of Europe orchestrated by Bismarck. He especially worked to hold Russia at Germany's side to avoid a two-front war with France and Russia. When Wilhelm II ascended to the throne as German Emperor (Kaiser), Bismarck was compelled to retire and his system of alliances was gradually de-emphasised. For example, the Kaiser refused to renew the Reinsurance Treaty with Russia in 1890. Two years later, the Franco-Russian Alliance was signed to counteract the force of the Triple Alliance. In 1904, the United Kingdom signed a series of agreements with France, the Entente Cordiale, and in 1907, the United Kingdom and Russia signed the Anglo-Russian Convention. While these agreements did not formally ally the United Kingdom with France or Russia, they made British entry into any future conflict involving France or Russia probable, and the system of interlocking bilateral agreements became known as the Triple Entente.[6]
German industrial and economic power had grown greatly after unification and the foundation of the Empire in 1871. From the mid-1890s on, the government of Wilhelm II used this base to devote significant economic resources for building up the Kaiserliche Marine (Imperial German Navy), established by Admiral Alfred von Tirpitz, in rivalry with the British Royal Navy for world naval supremacy.[20] As a result, each nation strove to out-build the other in terms of capital ships. With the launch of HMSDreadnought in 1906, the British Empire expanded on its significant advantage over its German rival.[20] The arms race between Britain and Germany eventually extended to the rest of Europe, with all the major powers devoting their industrial base to producing the equipment and weapons necessary for a pan-European conflict.[21] Between 1908 and 1913, the military spending of the European powers increased by 50percent.[22]
Austria-Hungary precipitated the Bosnian crisis of 19081909 by officially annexing the former Ottoman territory of Bosnia and Herzegovina, which it had occupied since 1878. This angered the Kingdom of Serbia and its patron, the Pan-Slavic and Orthodox Russian Empire.[23] Russian political manoeuvring in the region destabilised peace accords, which were already fracturing in what was known as "the powder keg of Europe".[23]
In 1912 and 1913, the First Balkan War was fought between the Balkan League and the fracturing Ottoman Empire. The resulting Treaty of London further shrank the Ottoman Empire, creating an independent Albanian State while enlarging the territorial holdings of Bulgaria, Serbia, Montenegro, and Greece. When Bulgaria attacked both Serbia and Greece on 16 June 1913, it lost most of Macedonia to Serbia and Greece and Southern Dobruja to Romania in the 33-day Second Balkan War, further destabilising the region.[24]
On 28 June 1914, Gavrilo Princip, a Bosnian Serb student and member of Young Bosnia, assassinated the heir to the Austro-Hungarian throne, Archduke Franz Ferdinand of Austria in Sarajevo, Bosnia.[25] This began a month of diplomatic manoeuvring between Austria-Hungary, Germany, Russia, France, and Britain called the July Crisis. Wanting to finally end Serbian interference in Bosnia, Austria-Hungary delivered the July Ultimatum to Serbia, a series of ten demands intentionally made unacceptable, intending to provoke a war with Serbia.[26] When Serbia agreed to only eight of the ten demands, Austria-Hungary declared war on 28 July 1914. Strachan argues, "Whether an equivocal and early response by Serbia would have made any difference to Austria-Hungary's behaviour must be doubtful. Franz Ferdinand was not the sort of personality who commanded popularity, and his demise did not cast the empire into deepest mourning".[27]
The Russian Empire, unwilling to allow AustriaHungary to eliminate its influence in the Balkans, and in support of its longtime Serb protgs, ordered a partial mobilisation one day later.[19] The German Empire mobilized on 30 July 1914, ready to apply the "Schlieffen Plan", which planned a quick, massive invasion of France to eliminate the French army, then to turn east against Russia. The French cabinet resisted military pressure to commence immediate mobilisation, and ordered its troops to withdraw 10 km from the border to avoid any incident. France only mobilized on the evening of 2 August, when Germany invaded Belgium and attacked French troops. Germany declared war on Russia on the same day.[28] The United Kingdom declared war on Germany on 4 August 1914, following an "unsatisfactory reply" to the British ultimatum that Belgium must be kept neutral.[29]
The strategy of the Central Powers suffered from miscommunication. Germany had promised to support Austria-Hungary's invasion of Serbia, but interpretations of what this meant differed. Previously tested deployment plans had been replaced early in 1914, but the replacements had never been tested in exercises. Austro-Hungarian leaders believed Germany would cover its northern flank against Russia.[30] Germany, however, envisioned Austria-Hungary directing most of its troops against Russia, while Germany dealt with France. This confusion forced the Austro-Hungarian Army to divide its forces between the Russian and Serbian fronts.
On 9 September 1914, the Septemberprogramm, a possible plan that detailed Germany's specific war aims and the conditions that Germany sought to force on the Allied Powers, was outlined by German Chancellor Theobald von Bethmann-Hollweg. It was never officially adopted.
Some of the first clashes of the war involved British, French, and German colonial forces in Africa. On 7 August, French and British troops invaded the German protectorate of Togoland. On 10 August, German forces in South-West Africa attacked South Africa; sporadic and fierce fighting continued for the rest of the war. The German colonial forces in German East Africa, led by Colonel Paul Emil von Lettow-Vorbeck, fought a guerrilla warfare campaign during World War I and only surrendered two weeks after the armistice took effect in Europe.[31]
Austria invaded and fought the Serbian army at the Battle of Cer and Battle of Kolubara beginning on 12 August. Over the next two weeks, Austrian attacks were thrown back with heavy losses, which marked the first major Allied victories of the war and dashed Austro-Hungarian hopes of a swift victory. As a result, Austria had to keep sizable forces on the Serbian front, weakening its efforts against Russia.[32] Serbias defeat of the Austro-Hungarian invasion of 1914 counts among the major upset victories of the last century.[33]
At the outbreak of the First World War, the German army (consisting in the West of seven field armies) carried out a modified version of the Schlieffen Plan. This marched German armies through neutral Belgium and into France, before turning southwards to encircle the French army on the German border.[10]. Since France had declared that it would "keep full freedom of acting in case of a war between Germany and Russia", Germany had to expect the possibility of an attack by France on one front and by Russia on the other. To meet such a scenario, the Schlieffen Plan stated that Germany must try to defeat France quickly (as had happened in the Franco-Prussian War of 1870-71). It further suggested that to repeat a fast victory in the west, Germany should not attack through the difficult terrain of Alsace-Lorraine (which had a direct border west of the river Rhine), instead, the idea was to try to quickly cut Paris off from the English Channel and British assistance, and take Paris, thus winning the war. Then the armies would be moved over to the east to meet Russia. Russia was believed to need a long period of mobilization before they could become a real threat to the Central Powers.
The only existing German plan for a two-front war had German armies marching through Belgium. Germany wanted free escort through Belgium (and originally Holland as well, which plan Kaiser Wilhelm II rejected) to invade France. Neutral Belgium rejected this idea, so the Germans decided to invade through Belgium instead. France also wanted to move their troops into Belgium, but Belgium originally rejected this "suggestion" as well, in the hope of avoiding any war on Belgian soil. In the end, after the German invasion, Belgium did try to join their army with the French (but a large part of the Belgian army retreated to Antwerp where they were forced to surrender when all hope of help was gone).
The plan called for the right flank of the German advance to bypass the French armies (which were concentrated on the Franco-German border, leaving the Belgian border without significant French forces) and move south to Paris. Initially the Germans were successful, particularly in the Battle of the Frontiers (1424 August). By 12 September, the French, with assistance from the British forces, halted the German advance east of Paris at the First Battle of the Marne (512 September), and pushed the German forces back some 50 km. The last days of this battle signified the end of mobile warfare in the west.[10] The French offensive into Southern Alsace, launched on 20 August with the Battle of Mulhouse, had limited success.
In the east, the Russians invaded with two armies, surprising the German staff who had not expected the Russians to move so early. A field army, the 8th, was rapidly moved from its previous role as reserve for the invasion of France, to East Prussia by rail across the German Empire. This army, led by general Paul von Hindenburg defeated Russia in a series of battles collectively known as the First Battle of Tannenberg (17 August 2 September). But the failed Russian invasion, causing the fresh German troops to move to the east, allowed the tactical Allied victory at the First Battle of the Marne. The Central Powers were denied a quick victory in France and forced to fight a war on two fronts. The German army had fought its way into a good defensive position inside France and had permanently incapacitated 230,000 more French and British troops than it had lost itself. Despite this, communications problems and questionable command decisions cost Germany the chance of early victory.[34]
New Zealand occupied German Samoa (later Western Samoa) on 30 August 1914. On 11 September, the Australian Naval and Military Expeditionary Force landed on the island of Neu Pommern (later New Britain), which formed part of German New Guinea. On 28 October, the cruiser SMS Emden sunk the Russian cruiser Zhemchug in the Battle of Penang. Japan seized Germany's Micronesian colonies and, after the Siege of Tsingtao, the German coaling port of Qingdao in the Chinese Shandong peninsula. Within a few months, the Allied forces had seized all the German territories in the Pacific; only isolated commerce raiders and a few holdouts in New Guinea remained.[35][36]
Military tactics before World War I had failed to keep pace with advances in technology. These advances allowed for impressive defence systems, which out-of-date military tactics could not break through for most of the war. Barbed wire was a significant hindrance to massed infantry advances. Artillery, vastly more lethal than in the 1870s, coupled with machine guns, made crossing open ground extremely difficult.[37] The Germans introduced poison gas; it soon became used by both sides, though it never proved decisive in winning a battle. Its effects were brutal, causing slow and painful death, and poison gas became one of the most-feared and best-remembered horrors of the war.[38] Commanders on both sides failed to develop tactics for breaching entrenched positions without heavy casualties. In time, however, technology began to produce new offensive weapons, such as the tank.[39]
After the First Battle of the Marne (512 September 1914), both Entente and German forces began a series of outflanking manoeuvres, in the so-called "Race to the Sea". Britain and France soon found themselves facing entrenched German forces from Lorraine to Belgium's coast.[10] Britain and France sought to take the offensive, while Germany defended the occupied territories. Consequently, German trenches were much better constructed than those of their enemy; Anglo-French trenches were only intended to be "temporary" before their forces broke through German defences.[40]
Both sides tried to break the stalemate using scientific and technological advances. On 22 April 1915, at the Second Battle of Ypres, the Germans (violating the Hague Convention) used chlorine gas for the first time on the Western Front. Algerian troops retreated when gassed and a six-kilometre (four-mile) hole opened in the Allied lines, which the Germans quickly exploited, taking Kitcheners' Wood, before Canadian soldiers closed the breach.[41] Tanks were first used in combat by the British during the Battle of Flers-Courcelette (part of the wider Somme offensive) on 15 September 1916 with only partial success; the French introduced the revolving turret of the Renault FT in late 1917; the Germans employed captured Allied tanks and small numbers of their own design.
Neither side proved able to deliver a decisive blow for the next two years. Around 1.1 to 1.2 million soldiers from the British and Dominion armies were on the Western Front at any one time.[42] A thousand battalions, occupying sectors of the line from the North Sea to the Orne River, operated on a month-long four-stage rotation system, unless an offensive was underway. The front contained over 9,600 kilometres (5,965mi) of trenches. Each battalion held its sector for about a week before moving back to support lines and then further back to the reserve lines before a week out-of-line, often in the Poperinge or Amiens areas.
Throughout 191517, the British Empire and France suffered more casualties than Germany, because of both the strategic and tactical stances chosen by the sides. Strategically, while the Germans only mounted a single main offensive at Verdun, the Allies made several attempts to break through German lines.
On 1 July 1916, the British Army endured the bloodiest day in its history, suffering 57,470 casualties, including 19,240 dead, on the first day of the Battle of the Somme. Most of the casualties occurred in the first hour of the attack. The entire Somme offensive cost the British Army almost half a million men.[43]
Protracted German action at Verdun throughout 1916,[44] combined with the bloodletting at the Somme (July and August 1916), brought the exhausted French army to the brink of collapse. Futile attempts at frontal assault came at a high price for both the British and the French poilu and led to widespread mutinies in 1917, after the costly Nivelle Offensive (April and May 1917).[45]
Tactically, German commander Erich Ludendorff's doctrine of "elastic defence" was well suited for trench warfare. This defence had a lightly defended forward position and a more powerful main position farther back beyond artillery range, from which an immediate and powerful counter-offensive could be launched.[46][47]
Ludendorff wrote on the fighting in 1917,
On the battle of the Menin Road Ridge, Ludendorff wrote,
In the 1917 Battle of Arras, the only significant British military success was the capture of Vimy Ridge by the Canadian Corps under Sir Arthur Currie and Julian Byng. The assaulting troops could  for the first time  overrun, rapidly reinforce, and hold the ridge defending the coal-rich Douai plain.[50][51]
At the start of the war, the German Empire had cruisers scattered across the globe, some of which were subsequently used to attack Allied merchant shipping. The British Royal Navy systematically hunted them down, though not without some embarrassment from its inability to protect Allied shipping. For example, the German detached light cruiser SMS Emden, part of the East-Asia squadron stationed at Tsingtao, seized or destroyed 15 merchantmen, as well as sinking a Russian cruiser and a French destroyer. However, most of the German East-Asia squadronconsisting of the armoured cruisers Scharnhorst and Gneisenau, light cruisers Nrnberg and Leipzig and two transport shipsdid not have orders to raid shipping and was instead underway to Germany when it met British warships. The German flotilla and Dresden sank two armoured cruisers at the Battle of Coronel, but was almost destroyed at the Battle of the Falkland Islands in December 1914, with only Dresden and a few auxiliaries escaping, but at the Battle of Ms a Tierra these too were destroyed or interned.[52]
Soon after the outbreak of hostilities, Britain began a naval blockade of Germany. The strategy proved effective, cutting off vital military and civilian supplies, although this blockade violated accepted international law codified by several international agreements of the past two centuries.[53] Britain mined international waters to prevent any ships from entering entire sections of ocean, causing danger to even neutral ships.[54] Since there was limited response to this tactic, Germany expected a similar response to its unrestricted submarine warfare.[55]
The 1916 Battle of Jutland (German: Skagerrakschlacht, or "Battle of the Skagerrak") developed into the largest naval battle of the war, the only full-scale clash of battleships during the war, and one of the largest in history. It took place on 31 May 1 June 1916, in the North Sea off Jutland. The Kaiserliche Marine's High Seas Fleet, commanded by Vice Admiral Reinhard Scheer, squared off against the Royal Navy's Grand Fleet, led by Admiral Sir John Jellicoe. The engagement was a stand off, as the Germans, outmanoeuvred by the larger British fleet, managed to escape and inflicted more damage to the British fleet than they received. Strategically, however, the British asserted their control of the sea, and the bulk of the German surface fleet remained confined to port for the duration of the war.[56]
German U-boats attempted to cut the supply lines between North America and Britain.[57] The nature of submarine warfare meant that attacks often came without warning, giving the crews of the merchant ships little hope of survival.[57][58] The United States launched a protest, and Germany changed its rules of engagement. After the sinking of the passenger ship RMS Lusitania in 1915, Germany promised not to target passenger liners, while Britain armed its merchant ships, placing them beyond the protection of the "cruiser rules", which demanded warning and placing crews in "a place of safety" (a standard that lifeboats did not meet).[59] Finally, in early 1917, Germany adopted a policy of unrestricted submarine warfare, realising that the Americans would eventually enter the war.[57][60] Germany sought to strangle Allied sea lanes before the U.S. could transport a large army overseas, but could maintain only five long-range U-boats on station, to limited effect.[57]
The U-boat threat lessened in 1917, when merchant ships began travelling in convoys, escorted by destroyers. This tactic made it difficult for U-boats to find targets, which significantly lessened losses; after the hydrophone and depth charges were introduced, accompanying destroyers might attack a submerged submarine with some hope of success. Convoys slowed the flow of supplies, since ships had to wait as convoys were assembled. The solution to the delays was an extensive program of building new freighters. Troopships were too fast for the submarines and did not travel the North Atlantic in convoys.[61] The U-boats had sunk more than 5,000 Allied ships, at a cost of 199 submarines.[62]
World War I also saw the first use of aircraft carriers in combat, with HMS Furious launching Sopwith Camels in a successful raid against the Zeppelin hangars at Tondern in July 1918, as well as blimps for antisubmarine patrol.[63]
Faced with Russia, Austria-Hungary could spare only one-third of its army to attack Serbia. After suffering heavy losses, the Austrians briefly occupied the Serbian capital, Belgrade. A Serbian counterattack in the battle of Kolubara, however, succeeded in driving them from the country by the end of 1914. For the first ten months of 1915, Austria-Hungary used most of its military reserves to fight Italy. German and Austro-Hungarian diplomats, however, scored a coup by persuading Bulgaria to join the attack on Serbia. The Austro-Hungarian provinces of Slovenia, Croatia and Bosnia provided troops for Austria-Hungary, invading Serbia as well as fighting Russia and Italy. Montenegro allied itself with Serbia.[65]
Serbia was conquered in a little more than a month, as the Central Powers, now including Bulgaria, sent in 600,000 troops. The Serbian army, fighting on two fronts and facing certain defeat, retreated into northern Albania (which they had invaded at the beginning of the war[dubious  discuss]). The Serbs suffered defeat in the Battle of Kosovo. Montenegro covered the Serbian retreat towards the Adriatic coast in the Battle of Mojkovac in 67 January 1916, but ultimately the Austrians conquered Montenegro, too. The 70,000 surviving Serbian soldiers were evacuated by ship to Greece.[66]
In late 1915, a Franco-British force landed at Salonica in Greece, to offer assistance and to pressure the government to declare war against the Central Powers. Unfortunately for the Allies, the pro-German King Constantine I dismissed the pro-Allied government of Eleftherios Venizelos before the Allied expeditionary force could arrive.[67] The friction between the King of Greece and the Allies continued to accumulate with the National Schism, which effectively divided Greece between regions still loyal to the king and the new provisional government of Venizelos in Salonica. After intensive diplomatic negotiations and an armed confrontation in Athens between Allied and royalist forces (an incident known as Noemvriana), the King of Greece resigned, and his second son Alexander took his place. Venizelos returned to Athens on 29 May 1917 and Greece, now unified, officially joined the war on the side of the Allies. The entire Greek army was mobilized and began to participate in military operations against the Central Powers on the Macedonian front.
After conquest, Serbia was divided between Austro-Hungary and Bulgaria. In 1917, the Serbs launched the Toplica Uprising and, for a short time, liberated the area between the Kopaonik mountains and the South Morava river. The uprising was crushed by the joint effort of Bulgarian and Austrian forces at the end of March 1917.
In the beginning, the Macedonian Front was mostly static. French and Serbian forces retook limited areas of Macedonia by recapturing Bitola on 19 November 1916 following the costly Monastir Offensive, which brought stabilization of the front.
Serbian and French troops finally made a breakthrough, after most of the German and Austro-Hungarian troops had withdrawn. This breakthrough was significant in defeating Bulgaria and Austro-Hungary, which led to the final victory of WWI. The Bulgarians suffered their only defeat of the war at the Battle of Dobro Pole, but, days later, they decisively defeated British and Greek forces at the Battle of Doiran, avoiding occupation. After the Serbian breakthrough of Bulgarian lines, Bulgaria capitulated on 29 September 1918.[68] Hindenburg and Ludendorff concluded that the strategic and operational balance had now shifted decidedly against the Central Powers and a day after the Bulgarian collapse, during a meeting with government officials, insisted on an immediate peace settlement.[69]
The disappearance of the Macedonian front meant that the road to Budapest and Vienna was now opened for the 670,000-strong army of general Franchet d'Esperey as the Bulgarian surrender deprived the Central Powers of the 278 infantry battalions and 1,500 guns (the equivalent of some 25 to 30 German divisions) that were previously holding the line.[70] The German high command responded by sending only seven infantry and one cavalry division, but these forces were far to weak to reestablish a front.[70]
The Ottoman Empire joined the Central Powers in the war, the secret Ottoman-German Alliance having been signed in August 1914.[71] It threatened Russia's Caucasian territories and Britain's communications with India via the Suez Canal. The British and French opened overseas fronts with the Gallipoli (1915) and Mesopotamian campaigns. In Gallipoli, the Ottoman Empire successfully repelled the British, French, and Australian and New Zealand Army Corps (ANZACs). In Mesopotamia, by contrast, after the disastrous Siege of Kut (191516), British Imperial forces reorganised and captured Baghdad in March 1917.
Further to the west, the Suez Canal was successfully defended from Ottoman attacks in 1915 and 1916; in August, a joint German and Ottoman force was defeated at the Battle of Romani by the Anzac Mounted and the 52nd (Lowland) Infantry Divisions. Following this victory, a British Empire Egyptian Expeditionary Force advanced across the Sinai Peninsula, pushing Ottoman forces back in the Battle of Magdhaba in December and the Battle of Rafa on the border between the Egyptian Sinai and Ottoman Palestine in January 1917.
Russian armies generally had the best of it in the Caucasus. Enver Pasha, supreme commander of the Ottoman armed forces, was ambitious and dreamed of re-conquering central Asia and areas that had been lost to Russia previously. He was, however, a poor commander.[72] He launched an offensive against the Russians in the Caucasus in December 1914 with 100,000 troops; insisting on a frontal attack against mountainous Russian positions in winter, he lost 86% of his force at the Battle of Sarikamish.[73]
General Yudenich, the Russian commander from 1915 to 1916, drove the Turks out of most of the southern Caucasus with a string of victories.[73] In 1917, Russian Grand Duke Nicholas assumed command of the Caucasus front. Nicholas planned a railway from Russian Georgia to the conquered territories, so that fresh supplies could be brought up for a new offensive in 1917. However, in March 1917 (February in the pre-revolutionary Russian calendar), the Czar was overthrown in the February Revolution and the Russian Caucasus Army began to fall apart.
Instigated by the Arab bureau of the British Foreign Office, the Arab Revolt started with the help of Britain in June 1916 at the Battle of Mecca, led by Sherif Hussein of Mecca, and ended with the Ottoman surrender of Damascus. Fakhri Pasha, the Ottoman commander of Medina, resisted for more than two and half years during the Siege of Medina.[74]
Along the border of Italian Libya and British Egypt, the Senussi tribe, incited and armed by the Turks, waged a small-scale guerrilla war against Allied troops. The British were forced to dispatch 12,000 troops to oppose them in the Senussi Campaign. Their rebellion was finally crushed in mid-1916.[75]
Italy had been allied with the German and Austro-Hungarian Empires since 1882 as part of the Triple Alliance. However, the nation had its own designs on Austrian territory in Trentino, Istria, and Dalmatia. Rome had a secret 1902 pact with France, effectively nullifying its alliance.[76] At the start of hostilities, Italy refused to commit troops, arguing that the Triple Alliance was defensive and that AustriaHungary was an aggressor. The Austro-Hungarian government began negotiations to secure Italian neutrality, offering the French colony of Tunisia in return. The Allies made a counter-offer in which Italy would receive the Southern Tyrol, Julian March and territory on the Dalmatian coast after the defeat of Austria-Hungary. This was formalised by the Treaty of London. Further encouraged by the Allied invasion of Turkey in April 1915, Italy joined the Triple Entente and declared war on Austria-Hungary on 23 May. Fifteen months later, Italy declared war on Germany.
Militarily, the Italians had numerical superiority. This advantage, however, was lost, not only because of the difficult terrain in which fighting took place, but also because of the strategies and tactics employed. Field Marshal Luigi Cadorna, a staunch proponent of the frontal assault, had dreams of breaking into the Slovenian plateau, taking Ljubljana and threatening Vienna. Cadorna's plan did not take into account the difficulties of the rugged Alpine terrain, or the technological changes that created trench warfare, giving rise to a series of bloody and inconclusive stalemated offensives.
On the Trentino front, the Austro-Hungarians took advantage of the mountainous terrain, which favoured the defender. After an initial strategic retreat, the front remained largely unchanged, while Austrian Kaiserschtzen and Standschtzen engaged Italian Alpini in bitter hand-to-hand combat throughout the summer. The Austro-Hungarians counterattacked in the Altopiano of Asiago, towards Verona and Padua, in the spring of 1916 (Strafexpedition), but made little progress.
Beginning in 1915, the Italians under Cadorna mounted eleven offensives on the Isonzo front along the Isonzo River, northeast of Trieste. All eleven offensives were repelled by the Austro-Hungarians, who held the higher ground. In the summer of 1916, the Italians captured the town of Gorizia. After this minor victory, the front remained static for over a year, despite several Italian offensives. In the autumn of 1917, thanks to the improving situation on the Eastern front, the Austro-Hungarian troops received large numbers of reinforcements, including German Stormtroopers and the elite Alpenkorps.
The Central Powers launched a crushing offensive on 26 October 1917, spearheaded by the Germans. They achieved a victory at Caporetto. The Italian Army was routed and retreated more than 100 kilometres (62mi) to reorganise, stabilising the front at the Piave River. Since the Italian Army had suffered heavy losses in the Battle of Caporetto, the Italian Government called to arms the so-called '99 Boys (Ragazzi del '99): that is, all males who were 18 years old. In 1918, the Austro-Hungarians failed to break through in a series of battles on the Piave River, and were finally decisively defeated in the Battle of Vittorio Veneto in October of that year. From 56 November 1918, Italian forces were reported to have reached Lissa, Lagosta, Sebenico, and other localities on the Dalmatian coast.[77] By the end of hostilities in November 1918, the Italian military had seized control of the entire portion of Dalmatia that had been guaranteed to Italy by the London Pact.[78] In 1918, Admiral Enrico Millo declared himself Italy's Governor of Dalmatia.[78] Austria-Hungary surrendered in early November 1918.[79][80]
Romania had been allied with the Central Powers since 1882. When the war began, however, it declared its neutrality, arguing that because Austria-Hungary had itself declared war on Serbia, Romania was under no obligation to join the war. When the Entente Powers promised Romania large territories of eastern Hungary (Transylvania and Banat), which had a large Romanian population, in exchange for Romania's declaring war on the Central Powers, the Romanian government renounced its neutrality and, on 27 August 1916, the Romanian Army launched an attack against Austria-Hungary, with limited Russian support. The Romanian offensive was initially successful, pushing back the Austro-Hungarian troops in Transylvania, but a counterattack by the forces of the Central Powers drove back the Russo-Romanian forces. As a result of the Battle of Bucharest, the Central Powers occupied Bucharest on 6 December 1916. Fighting in Moldova continued in 1917, resulting in a costly stalemate for the Central Powers.[81][82] Russian withdrawal from the war in late 1917 as a result of the October Revolution meant that Romania was forced to sign an armistice with the Central Powers on 9 December 1917.
In January 1918, Romanian forces established control over Bessarabia as the Russian Army abandoned the province. Although a treaty was signed by the Romanian and the Bolshevik Russian government following talks from 59 March 1918 on the withdrawal of Romanian forces from Bessarabia within two months, on 27 March 1918 Romania attached Bessarabia to its territory, formally based on a resolution passed by the local assembly of the territory on the unification with Romania.
Romania officially made peace with the Central Powers by signing the Treaty of Bucharest on 7 May 1918. Under that treaty, Romania was obliged to end the war with the Central Powers and make small territorial concessions to Austria-Hungary, ceding control of some passes in the Carpathian Mountains, and grant oil concessions to Germany. In exchange, the Central Powers recognised the sovereignty of Romania over Bessarabia. The treaty was renounced in October 1918 by the Alexandru Marghiloman government, and Romania nominally re-entered the war on 10 November 1918. The next day, the Treaty of Bucharest was nullified by the terms of the Armistice of Compigne.[83][84] Total Romanian deaths from 1914 to 1918, military and civilian, within contemporary borders, were estimated at 748,000.[85]
Contrary to British fears of a revolt in India, the outbreak of the war saw an unprecedented outpouring of loyalty and goodwill towards the United Kingdom.[86][87] Indian political leaders from the Indian National Congress and other groups were eager to support the British war effort, since they believed that strong support for the war effort would further the cause of Indian Home Rule. The Indian Army in fact outnumbered the British Army at the beginning of the war; about 1.3million Indian soldiers and labourers served in Europe, Africa, and the Middle East, while both the central government and the princely states sent large supplies of food, money, and ammunition. In all, 140,000men served on the Western Front and nearly 700,000 in the Middle East. Casualties of Indian soldiers totalled 47,746 killed and 65,126 wounded during World War I.[88] The suffering engendered by the war, as well as the failure of the British government to grant self-government to India after the end of hostilities, bred disillusionment and fuelled the campaign for full independence that would be led by Mohandas Karamchand Gandhi and others.
While the Western Front had reached stalemate, the war continued in East Europe. Initial Russian plans called for simultaneous invasions of Austrian Galicia and German East Prussia. Although Russia's initial advance into Galicia was largely successful, it was driven back from East Prussia by Hindenburg and Ludendorff at Tannenberg and the Masurian Lakes in August and September 1914.[89][90] Russia's less developed industrial base and ineffective military leadership was instrumental in the events that unfolded. By the spring of 1915, the Russians had retreated to Galicia, and, in May. the Central Powers achieved a remarkable breakthrough on Poland's southern frontiers.[91] On 5 August, they captured Warsaw and forced the Russians to withdraw from Poland.
Despite the success of the June 1916 Brusilov Offensive in eastern Galicia,[92] dissatisfaction with the Russian government's conduct of the war grew. The offensive's success was undermined by the reluctance of other generals to commit their forces to support the victory. Allied and Russian forces were revived only temporarily by Romania's entry into the war on 27 August. German forces came to the aid of embattled Austro-Hungarian units in Transylvania, and Bucharest fell to the Central Powers on 6 December. Meanwhile, unrest grew in Russia, as the Tsar remained at the front. Empress Alexandra's increasingly incompetent rule drew protests and resulted in the murder of her favourite, Rasputin, at the end of 1916.
In March 1917, demonstrations in Petrograd culminated in the abdication of Tsar Nicholas II and the appointment of a weak Provisional Government, which shared power with the Petrograd Soviet socialists. This arrangement led to confusion and chaos both at the front and at home. The army became increasingly ineffective.[91]
Discontent and the weaknesses of the Provisional Government led to a rise in the popularity of the Bolshevik Party, led by Vladimir Lenin, which demanded an immediate end to the war. The successful armed uprising by the Bolsheviks of November was followed in December by an armistice and negotiations with Germany. At first, the Bolsheviks refused the German terms, but when German troops began marching across the Ukraine unopposed, the new government acceded to the Treaty of Brest-Litovsk on 3 March 1918. The treaty ceded vast territories, including Finland, the Baltic provinces, parts of Poland and Ukraine to the Central Powers.[93] Despite this enormous apparent German success, the manpower required for German occupation of former Russian territory may have contributed to the failure of the Spring Offensive and secured relatively little food or other materiel.
With the adoption of the Treaty of Brest-Litovsk, the Entente no longer existed. The Allied powers led a small-scale invasion of Russia, partly to stop Germany from exploiting Russian resources and, to a lesser extent, to support the "Whites" (as opposed to the "Reds") in the Russian Civil War.[94] Allied troops landed in Arkhangelsk and in Vladivostok.
In December 1916, after ten brutal months of the Battle of Verdun and a successful offensive against Romania, the Germans attempted to negotiate a peace with the Allies. Soon after, U.S. President Woodrow Wilson attempted to intervene as a peacemaker, asking in a note for both sides to state their demands. Lloyd George's War Cabinet considered the German offer to be a ploy to create divisions amongst the Allies. After initial outrage and much deliberation, they took Wilson's note as a separate effort, signalling that the U.S. was on the verge of entering the war against Germany following the "submarine outrages". While the Allies debated a response to Wilson's offer, the Germans chose to rebuff it in favour of "a direct exchange of views". Learning of the German response, the Allied governments were free to make clear demands in their response of 14 January. They sought restoration of damages, the evacuation of occupied territories, reparations for France, Russia and Romania, and a recognition of the principle of nationalities. This included the liberation of Italians, Slavs, Romanians, Czecho-Slovaks, and the creation of a "free and united Poland". On the question of security, the Allies sought guarantees that would prevent or limit future wars, complete with sanctions, as a condition of any peace settlement.[95] The negotiations failed and the Entente powers rejected the German offer, because Germany did not state any specific proposals. To Wilson, the Entente powers stated that they would not start peace negotiations until the Central powers evacuated all occupied Allied territories and provided indemnities for all damage which had been done.[96]
Events of 1917 proved decisive in ending the war, although their effects were not fully felt until 1918.
The British naval blockade began to have a serious impact on Germany. In response, in February 1917, the German General Staff convinced Chancellor Theobald von Bethmann-Hollweg to declare unrestricted submarine warfare, with the goal of starving Britain out of the war. German planners estimated that unrestricted submarine warfare would cost Britain a monthly shipping loss of 600,000 tons. The General Staff acknowledged that the policy would almost certainly bring the United States into the conflict, but calculated that British shipping losses would be so high that they would be forced to sue for peace after 5 to 6 months, before American intervention could make an impact. In reality, tonnage sunk rose above 500,000tons per month from February to July. It peaked at 860,000tons in April. After July, the newly re-introduced convoy system became extremely effective in reducing the U-boat threat. Britain was safe from starvation, while German industrial output fell and the United States troops joined the war in large numbers far earlier than Germany had anticipated.
On 3 May 1917, during the Nivelle Offensive, the weary French 2nd Colonial Division, veterans of the Battle of Verdun, refused their orders, arriving drunk and without their weapons. Their officers lacked the means to punish an entire division, and harsh measures were not immediately implemented. Then, mutinies afflicted an additional 54 French divisions and saw 20,000 men desert. The other Allied forces attacked, but sustained tremendous casualties.[97] However, appeals to patriotism and duty, as well as mass arrests and trials, encouraged the soldiers to return to defend their trenches, although the French soldiers refused to participate in further offensive action.[98] Robert Nivelle was removed from command by 15 May, replaced by General Philippe Ptain, who suspended bloody large-scale attacks.
The victory of AustriaHungary and Germany at the Battle of Caporetto, led the Allies to convenve the Rapallo Conference at which they formed the Supreme War Council to coordinate planning. Previously, British and French armies had operated under separate commands.
In December, the Central Powers signed an armistice with Russia. This released large numbers of German troops for use in the west. With German reinforcements and new American troops pouring in, the outcome was to be decided on the Western Front. The Central Powers knew that they could not win a protracted war, but they held high hopes for success based on a final quick offensive. Furthermore, the leaders of the Central Powers and the Allies became increasingly fearful of social unrest and revolution in Europe. Thus, both sides urgently sought a decisive victory.[99]
In March and April 1917, at the First and Second Battles of Gaza, German and Ottoman forces stopped the advance of the Egyptian Expeditionary Force, which had begun in August 1916 at Romani. At the end of October, the Sinai and Palestine Campaign resumed, when General Edmund Allenby's XXth Corps, XXI Corps and Desert Mounted Corps won the Battle of Beersheba. Two Ottoman armies were defeated a few weeks later at the Battle of Mughar Ridge and, early in December, Jerusalem was captured following another Ottoman defeat at the Battle of Jerusalem (1917). About this time, Friedrich Freiherr Kress von Kressenstein was relieved of his duties as the Eighth Army's commander, replaced by Djevad Pasha, and a few months later the commander of the Ottoman Army in Palestine, Erich von Falkenhayn, was replaced by Otto Liman von Sanders.
At the outbreak of the war, the United States pursued a policy of non-intervention, avoiding conflict while trying to broker a peace. When a German U-boat sank the British liner RMS Lusitania on 7 May 1915 with 128 Americans among the dead, President Woodrow Wilson insisted that "America is too proud to fight" but demanded an end to attacks on passenger ships. Germany complied. Wilson unsuccessfully tried to mediate a settlement. However, he also repeatedly warned that the U.S.A. would not tolerate unrestricted submarine warfare, in violation of international law. Former president Theodore Roosevelt denounced German acts as "piracy".[100] Wilson was narrowly reelected in 1916 as his supporters emphasized "he kept us out of war".
In January 1917, Germany resumed unrestricted submarine warfare, realizing it would mean American entry. The German Foreign Minister, in the Zimmermann Telegram, invited Mexico to join the war as Germany's ally against the United States. In return, the Germans would finance Mexico's war and help it recover the territories of Texas, New Mexico, and Arizona.[101] Wilson released the Zimmerman note to the public, and Americans saw it as casus bellia cause for war. Wilson called on antiwar elements to end all wars, by winning this one and eliminating militarism from the globe. He argued that the war was so important that the U.S. had to have a voice in the peace conference.[102]
After the sinking of seven U.S. merchant ships by submarines and the publication of the Zimmerman telegram, Wilson called for war on Germany,[103] which the U.S. Congress declared on 6 April 1917.
The United States was never formally a member of the Allies but became a self-styled "Associated Power". The United States had a small army, but, after the passage of the Selective Service Act, it drafted 2.8 million men,[104] and, by summer 1918, was sending 10,000 fresh soldiers to France every day. In 1917, the U.S. Congress gave U.S. citizenship to Puerto Ricans when they were drafted to participate in World War I, as part of the Jones Act. Germany had miscalculated, believing it would be many more months before American soldiers would arrive and that their arrival could be stopped by U-boats.[105]
The United States Navy sent a battleship group to Scapa Flow to join with the British Grand Fleet, destroyers to Queenstown, Ireland, and submarines to help guard convoys. Several regiments of U.S. Marines were also dispatched to France. The British and French wanted U.S. units used to reinforce their troops already on the battle lines and not waste scarce shipping on bringing over supplies. The U.S. rejected the first proposition and accepted the second. General John J. Pershing, American Expeditionary Forces (AEF) commander, refused to break up U.S. units to be used as reinforcements for British Empire and French units. As an exception, he did allow African-American combat regiments to be used in French divisions. The Harlem Hellfighters fought as part of the French 16th Division, earning a unit Croix de Guerre for their actions at Chateau-Thierry, Belleau Wood, and Sechault.[106] AEF doctrine called for the use of frontal assaults, which had long since been discarded by British Empire and French commanders because of the large loss of life.[107]
In 1917, Emperor Charles I of Austria secretly attempted separate peace negotiations with Clemenceau, with his wife's brother Sixtus in Belgium as an intermediary, without the knowledge of Germany. When the negotiations failed, his attempt was revealed to Germany, resulting in a diplomatic catastrophe.[108][109]
German General Erich Ludendorff drew up plans (codenamed Operation Michael) for the 1918 offensive on the Western Front. The Spring Offensive sought to divide the British and French forces with a series of feints and advances. The German leadership hoped to strike a decisive blow before significant U.S. forces arrived. The operation commenced on 21 March 1918, with an attack on British forces near Amiens. German forces achieved an unprecedented advance of 60 kilometres (37mi).[110]
British and French trenches were penetrated using novel infiltration tactics, also named Hutier tactics, after General Oskar von Hutier. Previously, attacks had been characterised by long artillery bombardments and massed assaults. However, in the Spring Offensive of 1918, Ludendorff used artillery only briefly and infiltrated small groups of infantry at weak points. They attacked command and logistics areas and bypassed points of serious resistance. More heavily armed infantry then destroyed these isolated positions. German success relied greatly on the element of surprise.[111]
The front moved to within 120 kilometres (75mi) of Paris. Three heavy Krupp railway guns fired 183shells on the capital, causing many Parisians to flee. The initial offensive was so successful that Kaiser Wilhelm II declared 24 March a national holiday. Many Germans thought victory was near. After heavy fighting, however, the offensive was halted. Lacking tanks or motorised artillery, the Germans were unable to consolidate their gains. This situation was not helped by the supply lines now being stretched as a result of their advance.[112] The sudden stop was also a result of the four Australian Imperial Force (AIF) divisions that were "rushed" down, thus doing what no other army had done: stopping the German advance in its tracks. During that time, the 1st Australian Division was hurriedly sent back north to stop the second German breakthrough.
General Foch pressed to use the arriving American troops as individual replacements, whereas Pershing sought to field American units as an independent force. These units were assigned to the depleted French and British Empire commands on 28 March. A Supreme War Council of Allied forces was created at the Doullens Conference on 5 November 1917.[113] General Foch was appointed as supreme commander of the allied forces. Haig, Petain, and Pershing retained tactical control of their respective armies; Foch assumed a coordinating rather than a directing role, and the British, French, and U.S. commands operated largely independently.[113]
Following Operation Michael, Germany launched Operation Georgette against the northern English Channel ports. The Allies halted the drive after limited territorial gains by Germany. The German Army to the south then conducted Operations Blcher and Yorck, pushing broadly towards Paris. Operation Marne was launched on 15 July, attempting to encircle Reims and beginning the Second Battle of the Marne. The resulting counterattack, starting the Hundred Days Offensive, marked the first successful Allied offensive of the war.
By 20 July, the Germans were back across the Marne at their Kaiserschlacht starting lines,[114] having achieved nothing. Following this last phase of the war in the West, the German Army never regained the initiative. German casualties between March and April 1918 were 270,000, including many highly trained storm troopers.
Meanwhile, Germany was falling apart at home. Anti-war marches became frequent and morale in the army fell. Industrial output was 53percent of 1913 levels.
Early in 1918, the front line was extended into the Jordan Valley, which continued to be occupied, following the First Transjordan and the Second Transjordan attack by British Empire forces in March and April 1918, into the summer. During March, most of the Egyptian Expeditionary Force's British infantry and Yeomanry cavalry were sent to fight on the Western Front as a consequence of the Spring Offensive. They were replaced by Indian Army units. During several months of reorganisation and training during the summer, a number of attacks were carried out on sections of the Ottoman front line. These pushed the front line north to more advantageous positions in preparation for an attack and to acclimatise the newly arrived Indian Army infantry. It was not until the middle of September that the integrated force was ready for large-scale operations.
The reorganised Egyptian Expeditionary Force, with an additional mounted division, broke Ottoman forces at the Battle of Megiddo in September 1918. In two days the British and Indian infantry supported by a creeping barrage broke the Ottoman front line and captured the headquarters of the Eighth Army (Ottoman Empire) at Tulkarm, the continuous trench lines at Tabsor, Arara and the Seventh Army (Ottoman Empire) headquarters at Nablus. The Desert Mounted Corps rode through the break in the front line created by the infantry and, during virtually continuous operations by Australian Light Horse, British mounted Yeomanry, Indian Lancers and New Zealand Mounted Rifle brigades in the Jezreel Valley, they captured Nazareth, Afulah and Beisan, Jenin, along with Haifa on the Mediterranean coast and Daraa east of the Jordan River on the Hejaz railway. Samakh and Tiberias on the Sea of Galilee, were captured on the way northwards to Damascus. Meanwhile, Chaytor's Force of Australian light horse, New Zealand mounted rifles, Indian, British West Indies and Jewish infantry captured the crossings of the Jordan River, Es Salt, Amman and at Ziza most of the Fourth Army (Ottoman Empire). The Armistice of Mudros, signed at the end of October ended hostilities with the Ottoman Empire when fighting was continuing north of Aleppo.
In the late spring of 1918, three new states were formed in the South Caucasus: the Democratic Republic of Armenia, the Azerbaijan Democratic Republic, and the Democratic Republic of Georgia, which declared their independence from the Russian Empire.[115] Two other minor entities were established, the Centrocaspian Dictatorship and South West Caucasian Republic (the former was liquidated by Azerbaijan in the autumn of 1918 and the latter by a joint Armenian-British task force in early 1919). With the withdrawal of the Russian armies from the Caucasus front in the winter of 191718, the three major republics braced for an imminent Ottoman advance, which commenced in the early months of 1918. Solidarity was briefly maintained when the Transcaucasian Federative Republic was created in the spring of 1918, but this collapsed in May, when the Georgians asked and received protection from Germany and the Azerbaijanis concluded a treaty with the Ottoman Empire that was more akin to a military alliance. Armenia was left to fend for itself and struggled for five months against the threat of a full-fledged occupation by the Ottoman Turks.[116]
The Allied counteroffensive, known as the Hundred Days Offensive, began on 8 August 1918. The Battle of Amiens developed with III Corps British Fourth Army on the left, the French First Army on the right, and the Australian and Canadian Corps spearheading the offensive in the centre through Harbonnires.[117][118] It involved 414 tanks of the Mark IV and Mark V type, and 120,000 men. They advanced 12 kilometres (7.5mi) into German-held territory in just seven hours. Erich Ludendorff referred to this day as the "Black Day of the German army".[117][119]
The Australian-Canadian spearhead at Amiens, a battle that was the beginning of Germany's downfall,[49] helped pull forward the British armies to the north and the French armies to the south. On the British Fourth Army front at Amiens, after an advance as far as 14 miles (23km), German resistance stiffened, and the battle there concluded. But the French Third Army lengthened the Amiens front on 10August, when it was thrown in on the right of the French First Army, and advanced 4 miles (6km), liberating Lassigny in fighting which lasted until 16August. South of the French Third Army, General Charles Mangin (The Butcher) drove his French Tenth Army forward at Soissons on 20 August to capture eight thousand prisoners, two hundred guns, and the Aisne heights overlooking and menacing the German position north of the Vesle.[120] Another "Black day", as described by Erich Ludendorff.
Meanwhile, General Byng of the British Third Army, reporting that the enemy on his front was thinning in a limited withdrawal, was ordered to attack with 200 tanks towards Bapaume, opening the Battle of Albert, with specific orders "To break the enemy's front, in order to outflank the enemy's present battle front" (opposite the British Fourth Army at Amiens).[49] Allied leaders had now realised that to continue an attack after resistance had hardened was a waste of lives, and it was better to turn a line than to try to roll over it. They began to undertake attacks in quick order to take advantage of successful advances on the flanks, then broke them off when each attack lost its initial impetus.[120]
The British Third Army's 15-mile (24km) front north of Albert progressed after stalling for a day against the main resistance line to which the enemy had withdrawn.[121] Rawlinson's British Fourth Army was able to push its left flank forward between Albert and the Somme, straightening the line between the advanced positions of the Third Army and the Amiens front, which resulted in recapturing Albert at the same time.[120] On 26August the British First Army on the left of the Third Army was drawn into the battle, extending it northward to beyond Arras. The Canadian Corps, already back in the vanguard of the First Army, fought its way from Arras eastward 5 miles (8km) astride the heavily defended Arras-Cambrai area before reaching the outer defences of the Hindenburg Line, breaching them on the 28 and 29 August. Bapaume fell on 29August to the New Zealand Division of the Third Army, and the Australians, still leading the advance of the Fourth Army, were again able to push forward at Amiens to take Peronne and Mont Saint-Quentin on 31August. Further south, the French First and Third Armies had slowly fought forward while the Tenth Army, which had by now crossed the Ailette and was east of the Chemin des Dames, neared the Alberich position of the Hindenburg Line.[122] During the last week of August the pressure along a 70-mile (113km) front against the enemy was heavy and unrelenting. From German accounts, "Each day was spent in bloody fighting against an ever and again on-storming enemy, and nights passed without sleep in retirements to new lines."[120] Even to the north in Flanders the British Second and Fifth Armies during August and September were able to make progress, taking prisoners and positions that had previously been denied them.[122]
On 2 September, the Canadian Corps' outflanking of the Hindenburg line, with the breaching of the Wotan Position, made it possible for the Third Army to advance, which sent repercussions all along the Western Front. That same day, Oberste Heeresleitung (OHL) had no choice but to issue orders to six armies to withdraw back into the Hindenburg Line in the south, behind the Canal du Nord on the Canadian-First Army's front and back to a line east of the Lys in the north. This ceded without a fight the salient seized the previous April.[123] According to Ludendorff "We had to admit the necessity...to withdraw the entire front from the Scarpe to the Vesle."[124]
In nearly four weeks of fighting beginning 8August, over 100,000 German prisoners were taken, 75,000 by the BEF and the rest by the French. As of "The Black Day of the German Army", the German High Command realised that the war was lost and made attempts to reach a satisfactory end. The day after that battle, Ludenforff told Colonel Mertz: "We cannot win the war any more, but we must not lose it either." On 11August he offered his resignation to the Kaiser, who refused it, replying, "I see that we must strike a balance. We have nearly reached the limit of our powers of resistance. The war must be ended." On 13 August, at Spa, Hindenburg, Ludendorff, the Chancellor, and Foreign Minister Hintz agreed that the war could not be ended militarily and, on the following day, the German Crown Council decided that victory in the field was now most improbable. Austria and Hungary warned that they could only continue the war until December, and Ludendorff recommended immediate peace negotiations, to which the Kaiser responded by instructing Hintz to seek the mediation of the Queen of the Netherlands. Prince Rupprecht warned Prince Max of Baden: "Our military situation has deteriorated so rapidly that I no longer believe we can hold out over the winter; it is even possible that a catastrophe will come earlier." On 10September Hindenburg urged peace moves to Emperor Charles of Austria, and Germany appealed to the Netherlands for mediation. On 14September Austria sent a note to all belligerents and neutrals suggesting a meeting for peace talks on neutral soil, and on 15September Germany made a peace offer to Belgium. Both peace offers were rejected, and on 24September OHL informed the leaders in Berlin that armistice talks were inevitable.[122]
September saw the Germans continuing to fight strong rear-guard actions and launching numerous counterattacks on lost positions, but only a few succeeded, and then only temporarily. Contested towns, villages, heights, and trenches in the screening positions and outposts of the Hindenburg Line continued to fall to the Allies, with the BEF alone taking 30,441prisoners in the last week of September. Further small advances eastward would follow the Third Army's victory at Ivincourt on 12September, the Fourth Army's at Epheny on 18September, and the French gain of Essigny-le-Grand a day later. On 24September a final assault by both the British and French on a 4-mile (6.4km) front would come within 2 miles (3.2km) of St. Quentin.[122] With the outposts and preliminary defensive lines of the Siegfried and Alberich Positions eliminated, the Germans were now completely back in the Hindenburg Line. With the Wotan position of that line already breached and the Siegfried position in danger of being turned from the north, the time had now come for an Allied assault on the whole length of the line.
The Allied attack on the Hindenburg Line, begun on 26September, included U.S. soldiers. The still-green American troops suffered problems coping with supply trains for large units on a difficult landscape.[125] The following week, cooperating French and American units broke through in Champagne at the Battle of Blanc Mont Ridge, forcing the Germans off the commanding heights, and closing towards the Belgian frontier.[126] The last Belgian town to be liberated before the armistice was Ghent, which the Germans held as a pivot until the Allies brought up artillery.[127][128] The German army had to shorten its front and use the Dutch frontier as an anchor to fight rear-guard actions.
When Bulgaria signed a separate armistice on 29September, the Allies gained control of Serbia and Greece. Ludendorff, having been under great stress for months, suffered something similar to a breakdown. It was evident that Germany could no longer mount a successful defence.[129][130]
Meanwhile, news of Germany's impending military defeat spread throughout the German armed forces. The threat of mutiny was rife. Admiral Reinhard Scheer and Ludendorff decided to launch a last attempt to restore the "valour" of the German Navy. Knowing the government of Prince Maximilian of Baden would veto any such action, Ludendorff decided not to inform him. Nonetheless, word of the impending assault reached sailors at Kiel. Many, refusing to be part of a naval offensive, which they believed to be suicidal, rebelled and were arrested. Ludendorff took the blame; the Kaiser dismissed him on 26October. The collapse of the Balkans meant that Germany was about to lose its main supplies of oil and food. Its reserves had been used up, even as U.S. troops kept arriving at the rate of 10,000 per day.[131]
Having suffered over 6million casualties, Germany moved towards peace. Prince Maximilian of Baden took charge of a new government as Chancellor of Germany to negotiate with the Allies. Telegraphic negotiations with President Wilson began immediately, in the vain hope that he would offer better terms than the British and French. Instead, Wilson demanded the abdication of the Kaiser. There was no resistance when the Social Democrat Philipp Scheidemann on 9November declared Germany to be a republic. Imperial Germany was dead; a new Germany had been born: the Weimar Republic.[132]
The collapse of the Central Powers came swiftly. Bulgaria was the first to sign an armistice, on 29 September 1918 at Saloniki.[134] On 30 October, the Ottoman Empire capitulated at Moudros (Armistice of Mudros).[134]
On 24 October, the Italians began a push that rapidly recovered territory lost after the Battle of Caporetto. This culminated in the Battle of Vittorio Veneto, which marked the end of the Austro-Hungarian Army as an effective fighting force. The offensive also triggered the disintegration of the Austro-Hungarian Empire. During the last week of October, declarations of independence were made in Budapest, Prague, and Zagreb. On 29 October, the imperial authorities asked Italy for an armistice. But the Italians continued advancing, reaching Trento, Udine, and Trieste. On 3 November, AustriaHungary sent a flag of truce to ask for an Armistice. The terms, arranged by telegraph with the Allied Authorities in Paris, were communicated to the Austrian commander and accepted. The Armistice with Austria was signed in the Villa Giusti, near Padua, on 3 November. Austria and Hungary signed separate armistices following the overthrow of the Habsburg Monarchy.
Following the outbreak of the German Revolution of 19181919, a republic was proclaimed on 9 November. The Kaiser fled to the Netherlands.
On 11 November, at 5:00 am, an armistice with Germany was signed in a railroad carriage at Compigne. At 11am on 11 November 1918  "the eleventh hour of the eleventh day of the eleventh month"  a ceasefire came into effect. During the six hours between the signing of the armistice and its taking effect, opposing armies on the Western Front began to withdraw from their positions, but fighting continued along many areas of the front, as commanders wanted to capture territory before the war ended. Canadian Private George Lawrence Price was shot by a German sniper at 10:57 and died at 10:58.[135] American Henry Gunther was killed 60 seconds before the armistice came into force while charging astonished German troops who were aware the Armistice was nearly upon them.[136] The last British soldier to die was Pte George Edwin Ellison. The last casualty of the war was a German, Lieutenant Thomas, who, after 11am, was walking towards the line to inform Americans who had not yet been informed of the Armistice that they would be vacating the buildings behind them.[137] The occupation of the Rhineland took place following the Armistice. The occupying armies consisted of American, Belgian, British and French forces.
In November 1918, the Allies had ample supplies of men and materiel to invade Germany. Yet at the time of the armistice, no Allied force had crossed the German frontier; the Western Front was still almost 900mi (1,400km) from Berlin; and the Kaiser's armies had retreated from the battlefield in good order. These factors enabled Hindenburg and other senior German leaders to spread the story that their armies had not really been defeated. This resulted in the stab-in-the-back legend,[138][139] which attributed Germany's defeat not to its inability to continue fighting (even though up to a million soldiers were suffering from the 1918 flu pandemic and unfit to fight), but to the public's failure to respond to its "patriotic calling" and the supposed intentional sabotage of the war effort, particularly by Jews, Socialists, and Bolsheviks.
A formal state of war between the two sides persisted for another seven months, until the signing of the Treaty of Versailles with Germany on 28 June 1919. However, the American public opposed ratification of the treaty, mainly because of the League of Nations the treaty created; the U.S. did not formally end its involvement in the war until the KnoxPorter Resolution was signed in 1921. After the Treaty of Versailles, treaties with Austria, Hungary, Bulgaria, and the Ottoman Empire were signed. However, the negotiation of the latter treaty with the Ottoman Empire was followed by strife (the Turkish War of Independence), and a final peace treaty between the Allied Powers and the country that would shortly become the Republic of Turkey was not signed until 24 July 1923, at Lausanne.
Some war memorials date the end of the war as being when the Versailles Treaty was signed in 1919, which was when many of the troops serving abroad finally returned to their home countries; by contrast, most commemorations of the war's end concentrate on the armistice of 11 November 1918. Legally, the formal peace treaties were not complete until the last, the Treaty of Lausanne, was signed. Under its terms, the Allied forces divested Constantinople on 23 August 1923.
The First World War began as a clash of 20th-century technology and 19th-century tactics, with the inevitably large ensuing casualties. By the end of 1917, however, the major armies, now numbering millions of men, had modernised and were making use of telephone, wireless communication,[140] armoured cars, tanks,[141] and aircraft. Infantry formations were reorganised, so that 100-man companies were no longer the main unit of manoeuvre; instead, squads of 10 or so men, under the command of a junior NCO, were favoured.
Artillery also underwent a revolution. In 1914, cannons were positioned in the front line and fired directly at their targets. By 1917, indirect fire with guns (as well as mortars and even machine guns) was commonplace, using new techniques for spotting and ranging, notably aircraft and the often overlooked field telephone. Counter-battery missions became commonplace, also, and sound detection was used to locate enemy batteries.
Germany was far ahead of the Allies in utilising heavy indirect fire. The German Army employed 150 and 210mm howitzers in 1914, when typical French and British guns were only 75 and 105mm. The British had a 6inch (152mm) howitzer, but it was so heavy it had to be hauled to the field in pieces and assembled. The Germans also fielded Austrian 305mm and 420mm guns and, even at the beginning of the war, had inventories of various calibers of Minenwerfer, which were ideally suited for trench warfare.[142]
Much of the combat involved trench warfare, in which hundreds often died for each yard gained. Many of the deadliest battles in history occurred during the First World War. Such battles include Ypres, the Marne, Cambrai, the Somme, Verdun, and Gallipoli. The Germans employed the Haber process of nitrogen fixation to provide their forces with a constant supply of gunpowder despite the British naval blockade.[143] Artillery was responsible for the largest number of casualties[144] and consumed vast quantities of explosives. The large number of head wounds caused by exploding shells and fragmentation forced the combatant nations to develop the modern steel helmet, led by the French, who introduced the Adrian helmet in 1915. It was quickly followed by the Brodie helmet, worn by British Imperial and U.S. troops, and in 1916 by the distinctive German Stahlhelm, a design, with improvements, still in use today.
The widespread use of chemical warfare was a distinguishing feature of the conflict. Gases used included chlorine, mustard gas and phosgene. Few war casualties were caused by gas,[146] as effective countermeasures to gas attacks were quickly created, such as gas masks. The use of chemical warfare and small-scale strategic bombing were both outlawed by the 1907 Hague Conventions, and both proved to be of limited effectiveness,[147] though they captured the public imagination.[148]
The most powerful land-based weapons were railway guns weighing hundreds of tons apiece. These were nicknamed Big Berthas, even though the namesake was not a railway gun. Germany developed the Paris Gun, able to bombard Paris from over 100 kilometres (62mi), though shells were relatively light at 94kilograms (210lb). While the Allies also had railway guns, German models severely out-ranged and out-classed them.
Fixed-wing aircraft were first used militarily by the Italians in Libya on 23 October 1911 during the Italo-Turkish War for reconnaissance, soon followed by the dropping of grenades and aerial photography the next year. By 1914, their military utility was obvious. They were initially used for reconnaissance and ground attack. To shoot down enemy planes, anti-aircraft guns and fighter aircraft were developed. Strategic bombers were created, principally by the Germans and British, though the former used Zeppelins as well.[150] Towards the end of the conflict, aircraft carriers were used for the first time, with HMS Furious launching Sopwith Camels in a raid to destroy the Zeppelin hangars at Tondern in 1918.[151]
Manned observation balloons, floating high above the trenches, were used as stationary reconnaissance platforms, reporting enemy movements and directing artillery. Balloons commonly had a crew of two, equipped with parachutes,[152] so that if there was an enemy air attack the crew could parachute to safety. (At the time, parachutes were too heavy to be used by pilots of aircraft (with their marginal power output), and smaller versions were not developed until the end of the war; they were also opposed by British leadership, who feared they might promote cowardice.)[153]
Recognised for their value as observation platforms, balloons were important targets for enemy aircraft. To defend them against air attack, they were heavily protected by antiaircraft guns and patrolled by friendly aircraft; to attack them, unusual weapons such as air-to-air rockets were even tried. Thus, the reconnaissance value of blimps and balloons contributed to the development of air-to-air combat between all types of aircraft, and to the trench stalemate, because it was impossible to move large numbers of troops undetected. The Germans conducted air raids on England during 1915 and 1916 with airships, hoping to damage British morale and cause aircraft to be diverted from the front lines, and indeed the resulting panic led to the diversion of several squadrons of fighters from France.[150][153]
Germany deployed U-boats (submarines) after the war began. Alternating between restricted and unrestricted submarine warfare in the Atlantic, the Kaiserliche Marine employed them to deprive the British Isles of vital supplies. The deaths of British merchant sailors and the seeming invulnerability of U-boats led to the development of depth charges (1916), hydrophones (passive sonar, 1917), blimps, hunter-killer submarines (HMS R-1, 1917), forward-throwing anti-submarine weapons, and dipping hydrophones (the latter two both abandoned in 1918).[154] To extend their operations, the Germans proposed supply submarines (1916). Most of these would be forgotten in the interwar period until World War II revived the need.
Trenches, machine guns, air reconnaissance, barbed wire, and modern artillery with fragmentation shells helped bring the battle lines of World War I to a stalemate. The British and the French sought a solution with the creation of the tank and mechanised warfare. The British first tanks were used during the Battle of the Somme on 15 September 1916. Mechanical reliability was an issue, but the experiment proved its worth. Within a year, the British were fielding tanks by the hundreds, and they showed their potential during the Battle of Cambrai in November 1917, by breaking the Hindenburg Line, while combined arms teams captured 8,000 enemy soldiers and 100guns. Meanwhile, the French introduced the first tanks with a rotating turret, the Renault FT-A7, which became a decisive tool of the victory. The conflict also saw the introduction of Light automatic weapons and submachine guns, such as the Lewis Gun, the Browning automatic rifle, and the Bergmann MP18.
Another new weapon, the flamethrower, was first used by the German army and later adopted by other forces. Although not of high tactical value, the flamethrower was a powerful, demoralising weapon that caused terror on the battlefield. It was a dangerous weapon to wield, as its heavy weight made operators vulnerable targets.
Trench railways evolved to supply the enormous quantities of food, water, and ammunition required to support large numbers of soldiers in areas where conventional transportation systems had been destroyed. Internal combustion engines and improved traction systems for automobiles and trucks/lorries eventually rendered trench railways obsolete.
The ethnic cleansing of the Ottoman Empire's Armenian population, including mass deportations and executions, during the final years of the Ottoman Empire is considered genocide.[155] The Ottomans saw the entire Armenian population as an enemy[156] that had chosen to side with Russia at the beginning of the war.[157] In early 1915, a number of Armenians joined the Russian forces, and the Ottoman government used this as a pretext to issue the Tehcir Law (Law on Deportation). This authorized the deportation of Armenians from the Empire's eastern provinces to Syria between 1915 and 1917. The exact number of deaths is unknown: while Balakian gives a range of 250,000 to 1.5 million for the deaths of Armenians,[158] the International Association of Genocide Scholars estimates over 1 million.[155][159] The government of Turkey has consistently rejected charges of genocide, arguing that those who died were victims of inter-ethnic fighting, famine, or disease during the First World War.[160] Other ethnic groups were similarly attacked by the Ottoman Empire during this period, including Assyrians and Greeks, and some scholars consider those events to be part of the same policy of extermination.[161][162][163]
Many pogroms accompanied the Russian Revolution of 1917 and the ensuing Russian Civil War. 60,000200,000 civilian Jews were killed in the atrocities throughout the former Russian Empire.[165]
The German invaders treated any resistancesuch as sabotaging rail linesas illegal and immoral, and shot the offenders and burned buildings in retaliation. In addition, they tended to suspect that most civilians were potential "franc-tireurs" and, accordingly, took and sometimes killed hostages from among the civilian population. The German army executed over 6,500 French and Belgian civilians between August and November 1914, usually in near-random large-scale shootings of civilians ordered by junior German officers. The German Army destroyed 15,00020,000 buildingsmost famously the university library at Louvainand generated a refugee wave of over a million people. Over half the German regiments in Belgium were involved in major incidents.[166] Thousands of workers were shipped to Germany to work in factories. British propaganda dramatizing the "Rape of Belgium" attracted much attention in the U.S., while Berlin said it was legal and necessary because of the threat of "franc-tireurs" (guerrillas) like those in France in 1870.[167] The British and French magnified the reports and disseminated them at home and in the U.S., where they played a major role in dissolving support for Germany.[168][169]
The British soldiers of the war were initially volunteers but increasingly were conscripted into service. Britain's Imperial War Museum has collected more than 2,500recordings of soldiers' personal accounts, and selected transcripts, edited by military author Max Arthur, have been published. The Museum believes that historians have not taken full account of this material, and accordingly has made the full archive of recordings available to authors and researchers.[170] Surviving veterans, returning home, often found that they could only discuss their experiences amongst themselves. Grouping together, they formed "veterans' associations" or "Legions".
About 8 million men surrendered and were held in POW camps during the war. All nations pledged to follow the Hague Conventions on fair treatment of prisoners of war. POWs' rate of survival was generally much higher than that of their peers at the front.[171] Individual surrenders were uncommon; large units usually surrendered en masse. At the Battle of Tannenberg 92,000Russians surrendered. When the besieged garrison of Kaunas surrendered in 1915, some 20,000Russians became prisoners. Over half of Russian losses (as a proportion of those captured, wounded, or killed) were to prisoner status; for Austria-Hungary 32%, for Italy 26%, for France 12%, for Germany 9%; for Britain 7%. Prisoners from the Allied armies totalled about 1.4million (not including Russia, which lost 2.-3.5[clarification needed] million men as prisoners.) From the Central Powers about 3.3million men became prisoners.[172]
Germany held 2.5million prisoners; Russia held 2.9million; while Britain and France held about 720,000. Most were captured just prior to the Armistice. The U.S. held 48,000. The most dangerous moment was the act of surrender, when helpless soldiers were sometimes gunned down.[173][174] Once prisoners reached a camp, conditions were, in general, satisfactory (and much better than in World War II), thanks in part to the efforts of the International Red Cross and inspections by neutral nations. However, conditions were terrible in Russia: starvation was common for prisoners and civilians alike; about 1520% of the prisoners in Russia died. In Germany, food was scarce, but only 5% died.[175][176][177]
The Ottoman Empire often treated POWs poorly.[178] Some 11,800 British Empire soldiers, most of them Indians, became prisoners after the Siege of Kut in Mesopotamia in April 1916; 4,250 died in captivity.[179] Although many were in very bad condition when captured, Ottoman officers forced them to march 1,100 kilometres (684mi) to Anatolia. A survivor said: "We were driven along like beasts; to drop out was to die."[180] The survivors were then forced to build a railway through the Taurus Mountains.
In Russia, when the prisoners from the Czech Legion of the Austro-Hungarian army were released in 1917, they re-armed themselves and briefly became a military and diplomatic force during the Russian Civil War.
While the Allied prisoners of the Central Powers were quickly sent home at the end of active hostilities, the same treatment was not granted to Central Power prisoners of the Allies and Russia, many of whom served as forced labor, e.g., in France until 1920. They were released only after many approaches by the Red Cross to the Allied Supreme Council.[181] German prisoners were still being held in Russia as late as 1924.[182]
Military and civilian observers from every major power closely followed the course of the war. Many were able to report on events from a perspective somewhat akin to modern "embedded" positions within the opposing land and naval forces. These military attachs and other observers prepared voluminous first-hand accounts of the war and analytical papers.
For example, former U.S. Army Captain Granville Fortescue followed the developments of the Gallipoli Campaign from an embedded perspective within the ranks of the Turkish defenders; and his report was passed through Turkish censors before being printed in London and New York.[183] However, this observer's role was abandoned when the U.S. entered the war, as Fortescue immediately re-enlisted, sustaining wounds at Forest of Argonne in the Meuse-Argonne Offensive, September 1918.[184]
In-depth observer narratives of the war and more narrowly focused professional journal articles were written soon after the war; and these post-war reports conclusively illustrated the battlefield destructiveness of this conflict. This was not the first time the tactics of entrenched positions for infantry defended with machine guns and artillery became vitally important. The Russo-Japanese War had been closely observed by military attachs, war correspondents and other observers; but, from a 21st century perspective, it is now apparent that a range of tactical lessons were disregarded or not used in the preparations for war in Europe and throughout the Great War.[185]
In the Balkans, Yugoslav nationalists such as the leader Ante Trumbi in the Balkans strongly supported the war, desiring the freedom of Yugoslavs from Austria-Hungary and other foreign powers and the creation of an independent Yugoslavia.[186] The Yugoslav Committee was formed in Paris on 30 April 1915 but shortly moved its office to London; Trumbi led the Committee.[186]
In the Middle East, Arab nationalism soared in Ottoman territories in response to the rise of Turkish nationalism during the war, with Arab nationalist leaders advocating the creation of a pan-Arab state.[187] In 1916, the Arab Revolt began in Ottoman-controlled territories of the Middle East in an effort to achieve independence.[187]
Italian nationalism was stirred by the outbreak of the war and was initially strongly supported by a variety of political factions. One of the most prominent and popular Italian nationalist supporters of the war was Gabriele d'Annunzio, who promoted Italian irredentism and helped sway the Italian public to support intervention in the war.[188] The Italian Liberal Party, under the leadership of Paolo Boselli, promoted intervention in the war on the side of the Allies and utilised the Dante Aligheri Society to promote Italian nationalism.[189]
A number of socialist parties initially supported the war when it began in August 1914.[190] But European socialists split on national lines, with the concept of class conflict held by radical socialists such as Marxists and syndicalists being overborne by their patriotic support for war.[191] Once the war began, Austrian, British, French, German, and Russian socialists followed the rising nationalist current by supporting their countries' intervention in the war.[192]
Italian socialists were divided on whether to support the war or oppose it; some were militant supporters of the war, including Benito Mussolini and Leonida Bissolati.[193] However, the Italian Socialist Party decided to oppose the war after anti-militarist protestors were killed, resulting in a general strike called Red Week.[194] The Italian Socialist Party purged itself of pro-war nationalist members, including Mussolini.[194] Mussolini, a syndicalist who supported the war on grounds of irredentist claims on Italian-populated regions of Austria-Hungary, formed the pro-interventionist Il Popolo d'Italia and the Fasci Riviluzionario d'Azione Internazionalista ("Revolutionary Fasci for International Action") in October 1914 that later developed into the Fasci di Combattimento in 1919, the origin of fascism.[195] Mussolini's nationalism enabled him to raise funds from Ansaldo (an armaments firm) and other companies to create Il Popolo d'Italia to convince socialists and revolutionaries to support the war.[196]
In April 1918, the Rome Congress of Oppressed Nationalities met, including Czechoslovak, Italian, Polish, Transylvanian, and Yugoslav representatives who urged the Allies to support national self-determination for the peoples residing within Austria-Hungary.[190]
The trade union and socialist movements had long voiced their opposition to a war, which they argued would only mean that workers would kill other workers in the interest of capitalism. Once war was declared, however, many socialists and trade unions backed their governments. Among the exceptions were the Bolsheviks, the Socialist Party of America, and the Italian Socialist Party, and individuals such as Karl Liebknecht, Rosa Luxemburg, and their followers in Germany. There were also small anti-war groups in Britain and France.
Benedict XV, elected to the papacy less than three months into World War I, made the war and its consequences the main focus of his early pontificate. In stark contrast to his predecessor,[197] five days after his election he spoke of his determination to do what he could to bring peace. His first encyclical, Ad Beatissimi Apostolorum, given 1 November 1914, was concerned with this subject. Seen as being biased in favour of the other and resented for weakening national morale, Benedict XV found his abilities and unique position as a religious emissary of peace ignored by the belligerent powers.
The 1915 Treaty of London between Italy and the Triple Entente included secret provisions whereby the Allies agreed with Italy to ignore papal peace moves towards the Central Powers. Consequently, the publication of Benedict's proposed seven-point Peace Note of August 1917 was roundly ignored by all parties except Austria-Hungary.[198]
In Britain, in 1914, the Public Schools Officers' Training Corps annual camp was held at Tidworth Pennings, near Salisbury Plain. Head of the British Army Lord Kitchener was to review the cadets, but the imminence of the war prevented him. General Horace Smith-Dorrien was sent instead. He surprised the two-or-three thousand cadets by declaring (in the words of Donald Christopher Smith, a Bermudian cadet who was present), that war should be avoided at almost any cost, that war would solve nothing, that the whole of Europe and more besides would be reduced to ruin, and that the loss of life would be so large that whole populations would be decimated. In our ignorance I, and many of us, felt almost ashamed of a British General who uttered such depressing and unpatriotic sentiments, but during the next four years, those of us who survived the holocaustprobably not more than one-quarter of uslearned how right the General's prognosis was and how courageous he had been to utter it.[199] Voicing these sentiments did not hinder Smith-Dorien's career, or prevent him from doing his duty in World War I to the best of his abilities.
Many countries jailed those who spoke out against the conflict. These included Eugene Debs in the United States and Bertrand Russell in Britain. In the U.S., the Espionage Act of 1917 and Sedition Act of 1918 made it a federal crime to oppose military recruitment or make any statements deemed "disloyal". Publications at all critical of the government were removed from circulation by postal censors,[102] and many served long prison sentences for statements of fact deemed unpatriotic.
A number of nationalists opposed intervention, particularly within states that the nationalists were hostile to. Although the vast majority of Irish people consented to participate in the war in 1914 and 1915, a minority of advanced Irish nationalists staunchly opposed taking part.[200] The war began amid the Home Rule crisis in Ireland that had resurfaced in 1912, and, by July 1914, there was a serious possibility of an outbreak of civil war in Ireland.[201] Irish nationalists and Marxists attempted to pursue Irish independence, culminating in the Easter Rising of 1916, with Germany sending 20,000 rifles to Ireland in order to stir unrest in the United Kingdom.[201] The UK government placed Ireland under martial law in response to the Easter Rising, although, once the immediate threat of revolution had dissipated, the authorities did try to make concessions to nationalist feeling.[202]
Other opposition came from conscientious objectors some socialist, some religious who refused to fight. In Britain, 16,000 people asked for conscientious objector status.[203] Some of them, most notably prominent peace activist Stephen Henry Hobhouse, refused both military and alternative service.[204] Many suffered years of prison, including solitary confinement and bread and water diets. Even after the war, in Britain many job advertisements were marked "No conscientious objectors need apply".
The Central Asian Revolt started in the summer of 1916, when the Russian Empire government ended its exemption of Muslims from military service.[205]
In 1917, a series of mutinies in the French army led to dozens of soldiers being executed and many more imprisoned.
In Milan, in May 1917, Bolshevik revolutionaries organised and engaged in rioting calling for an end to the war, and managed to close down factories and stop public transportation.[206] The Italian army was forced to enter Milan with tanks and machine guns to face Bolsheviks and anarchists, who fought violently until 23 May when the army gained control of the city. Almost 50 people (including three Italian soldiers) were killed and over 800 people arrested.[206]
The Conscription Crisis of 1917 in Canada erupted when conservative Prime Minister Robert Borden brought in compulsory military service over the objection of French-speaking Quebecers.[207] Out of approximately 625,000 Canadians who served, about 60,000 were killed and another 173,000 wounded.[208]
In 1917, Emperor Charles I of Austria secretly entered into peace negotiations with the Allied powers, with his brother-in-law Sixtus as intermediary, without the knowledge of his ally Germany. He failed, however, because of the resistance of Italy.[209]
In September 1917, Russian soldiers in France began questioning why they were fighting for the French at all and mutinied.[210] In Russia, opposition to the war led to soldiers also establishing their own revolutionary committees, which helped foment the October Revolution of 1917, with the call going up for "bread, land, and peace". The Bolsheviks agreed to a peace treaty with Germany, the peace of Brest-Litovsk, despite its harsh conditions.
In northern Germany, the end of October 1918, saw the beginning of the German Revolution of 19181919. Units of the German Navy refused to set sail for a last, large-scale operation in a war which they saw as good as lost; this initiated the uprising. The sailors' revolt which then ensued in the naval ports of Wilhelmshaven and Kiel spread across the whole country within days and led to the proclamation of a republic on 9 November 1918 and shortly thereafter to the abdication of Kaiser Wilhelm II.
As the war slowly turned into a war of attrition, conscription was implemented in some countries. This issue was particularly explosive in Canada and Australia. In the former, it opened a political gap between French Canadians, who believed their true loyalty should be to Canada and not to the British Empire, and members of the Anglophone majority, who saw the war as a duty to both Britain and Canada. Prime Minister Robert Borden pushed through a Military Service Act, provoking the Conscription Crisis of 1917. In Australia, a sustained pro-conscription campaign by Prime Minister Billy Hughes caused a split in the Australian Labor Party, so Hughes formed the Nationalist Party of Australia in 1917 to pursue the matter. Nevertheless, the labour movement, the Catholic Church, and Irish nationalist expatriates successfully opposed Hughes' push, which was rejected in two plebiscites.
Conscription put into uniform nearly every physically fit man in Britain, six of ten million eligible. Of these, about 750,000 lost their lives and 1,700,000 were wounded. Most deaths were to young unmarried men; however, 160,000 wives lost husbands and 300,000 children lost fathers.[211]
No other war had changed the map of Europe so dramatically. Four empires disappeared: the German, Austro-Hungarian, Ottoman, and Russian. Four dynasties, together with their ancillary aristocracies, all fell after the war: the Hohenzollerns, the Habsburgs, the Romanovs, and the Ottomans. Belgium and Serbia were badly damaged, as was France, with 1.4million soldiers dead,[212] not counting other casualties. Germany and Russia were similarly affected.[213]
The war had profound economic consequences. Of the 60million European soldiers who were mobilised from 1914 to 1918, 8million were killed, 7million were permanently disabled, and 15million were seriously injured. Germany lost 15.1% of its active male population, AustriaHungary lost 17.1%, and France lost 10.5%.[214] About 750,000 German civilians died from starvation caused by the British blockade during the war.[215] By the end of the war, famine had killed approximately 100,000people in Lebanon.[216] The best estimates of the death toll from the Russian famine of 1921 run from 5million to 10million people.[217] By 1922, there were between 4.5 million and 7 million homeless children in Russia as a result of nearly a decade of devastation from World War I, the Russian Civil War, and the subsequent famine of 19201922.[218] Numerous anti-Soviet Russians fled the country after the Revolution; by the 1930s, the northern Chinese city of Harbin had 100,000Russians.[219] Thousands more emigrated to France, England, and the United States.
In Australia, the effects of the war on the economy were no less severe. The then Prime Minister Hughes wrote to the British Prime Minister Lloyd George, "You have assured us that you cannot get better terms. I much regret it, and hope even now that some way may be found of securing agreement for demanding reparation commensurate with the tremendous sacrifices made by the British Empire and her Allies."[220] Australia received 5,571,720 war reparations, but the direct cost of the war to Australia had been 376,993,052, and, by the mid-1930s, repatriation pensions, war gratuities, interest and sinking fund charges were 831,280,947.[220] Of about 416,000 Australians who served, about 60,000 were killed and another 152,000 were wounded.[221]
Diseases flourished in the chaotic wartime conditions. In 1914 alone, louse-borne epidemic typhus killed 200,000 in Serbia.[222] From 1918 to 1922, Russia had about 25million infections and 3 million deaths from epidemic typhus.[223] Whereas before World War I Russia had about 3.5 million cases of malaria, its people suffered more than 13 million cases in 1923.[224] In addition, a major influenza epidemic spread around the world. Overall, the 1918 flu pandemic killed at least 50 million people.[225][226]
Lobbying by Chaim Weizmann and fear that American Jews would encourage the USA to support Germany culminated in the British government's Balfour Declaration of 1917, endorsing creation of a Jewish homeland in Palestine.[227] A total of more than 1,172,000 Jewish soldiers served in the Allied and Central Power forces in World War I, including 275,000 in Austria-Hungary and 450,000 in Czarist Russia.[228]
The social disruption and widespread violence of the Revolution of 1917 and the ensuing Russian Civil War sparked more than 2,000 pogroms in the former Russian Empire, mostly in the Ukraine.[229] An estimated 60,000200,000civilian Jews were killed in the atrocities.[230]
In the aftermath of World War I, Greece fought against Turkish nationalists led by Mustafa Kemal, a war which resulted in a massive population exchange between the two countries under the Treaty of Lausanne.[231] According to various sources,[232] several hundred thousand Pontic Greeks died during this period.[233]
After the war, the Paris Peace Conference imposed a series of peace treaties on the Central Powers. The 1919 Treaty of Versailles officially ended the war. Building on Wilson's 14th point, the Treaty of Versailles also brought into being the League of Nations on 28 June 1919.[234][235]
In signing the treaty, Germany acknowledged responsibility for the war, and agreed to pay enormous war reparations and award territory to the victors. The "Guilt Thesis" became a controversial explanation of later events among analysts in Britain and the United States. The Treaty of Versailles caused enormous bitterness in Germany, which nationalist movements, especially the Nazis, exploited with a conspiracy theory they called the Dolchstosslegende (Stab-in-the-back legend). The Weimar Republic lost the former colonial possessions and was saddled with accepting blame for the war, as well as paying punitive reparations for it. Unable to pay them with exports (as a result of territorial losses and postwar recession),[236] Germany did so by borrowing from the United States. Runaway inflation in the 1920s contributed to the economic collapse of the Weimar Republic, and the payment of reparations was suspended in 1931 following the Stock Market Crash of 1929 and the beginnings of the Great Depression worldwide.
AustriaHungary was partitioned into several successor states, including Austria, Hungary, Czechoslovakia, and Yugoslavia, largely but not entirely along ethnic lines. Transylvania was shifted from Hungary to Greater Romania. The details were contained in the Treaty of Saint-Germain and the Treaty of Trianon. As a result of the Treaty of Trianon, 3.3 million Hungarians came under foreign rule. Although the Hungarians made up 54% of the population of the pre-war Kingdom of Hungary, only 32% of its territory was left to Hungary. Between 1920 and 1924, 354,000 Hungarians fled former Hungarian territories attached to Romania, Czechoslovakia, and Yugoslavia.
The Russian Empire, which had withdrawn from the war in 1917 after the October Revolution, lost much of its western frontier as the newly independent nations of Estonia, Finland, Latvia, Lithuania, and Poland were carved from it. Bessarabia was re-attached to Greater Romania, as it had been a Romanian territory for more than a thousand years.[237]
The Ottoman Empire disintegrated, and much of its non-Anatolian territory was awarded to various Allied powers as protectorates. The Turkish core was reorganised as the Republic of Turkey. The Ottoman Empire was to be partitioned by the Treaty of Svres of 1920. This treaty was never ratified by the Sultan and was rejected by the Turkish republican movement, leading to the Turkish Independence War and, ultimately, to the 1923 Treaty of Lausanne.
"None," said the other, "Save the undone years"...
The first tentative efforts to comprehend the meaning and consequences of modern warfare began during the initial phases of the war, and this process continued throughout and after the end of hostilities.
Memorials were erected in thousands of villages and towns. Close to battlefields, those buried in improvised burial grounds were gradually moved to formal graveyards under the care of organisations such as the Commonwealth War Graves Commission, the American Battle Monuments Commission, the German War Graves Commission, and Le Souvenir franais. Many of these graveyards also have central monuments to the missing or unidentified dead, such as the Menin Gate memorial and the Thiepval Memorial to the Missing of the Somme.
On 3 May 1915, during the Second Battle of Ypres, Lieutenant Alexis Helmer was killed. At his graveside, his friend John McCrae, M.D., of Guelph, Ontario, Canada, wrote the memorable poem In Flanders Fields as a salute to those who perished in the Great War. Published in Punch on 8 December 1915, it is still recited today, especially on Remembrance Day and Memorial Day.[238][239]
Liberty Memorial in Kansas City, Missouri, is a United States memorial dedicated to all Americans who served in World War I. The site for the Liberty Memorial was dedicated on 1 November 1921. On this day, the supreme Allied commanders spoke to a crowd of more than 100,000 people. It was the only time in history these leaders were together in one place. In attendance were Lieutenant General Baron Jacques of Belgium; General Armando Diaz of Italy; Marshal Ferdinand Foch of France; General Pershing of the United States; and Admiral D. R. Beatty of Great Britain. After three years of construction, the Liberty Memorial was completed and President Calvin Coolidge delivered the dedication speech to a crowd of 150,000 people in 1926.
Liberty Memorial is also home to The National World War I Museum, the only museum dedicated solely to World War I in the United States.
The First World War had a lasting impact on social memory. It was seen by many in Britain as signalling the end of an era of stability stretching back to the Victorian period, and across Europe many regarded it as a watershed.[240] Historian Samuel Hynes explained:
This has become the most common perception of the First World War, perpetuated by the art, cinema, poems, and stories published subsequently. Films such as All Quiet on the Western Front, Paths of Glory and King & Country have perpetuated the idea, while war-time films including Camrades, Flanders Poppies, and Shoulder Arms indicate that the most contemporary views of the war were overall far more positive.[242] Likewise, the art of Paul Nash, John Nash, Christopher Nevison, and Henry Tonks in Britain painted a negative view of the conflict in keeping with the growing perception, while popular war-time artists such as Muirhead Bone painted more serene and pleasant interpretations subsequently rejected as inaccurate.[241] Several historians like John terriane, Niall Ferguson and Gary Sheffield have challenged these interpretations as partial and polemical views:
These beliefs did not become widely shared because they offered the only accurate interpretation of wartime events. In every respect, the war was much more complicated than they suggest. In recent years, historians have argued persuasively against almost every popular clich of the First World War. It has been pointed out that, although the losses were devastating, their greatest impact was socially and geographically limited. The many emotions other than horror experienced by soldiers in and out of the front line, including comradeship, boredom, and even enjoyment, have been recognised. The war is not now seen as a 'fight about nothing', but as a war of ideals, a struggle between aggressive militarism and more or less liberal democracy. It has been acknowledged that British generals were often capable men facing difficult challenges, and that it was under their command that the British army played a major part in the defeat of the Germans in 1918: a great forgotten victory.[242]
Though these historians have discounted as "myths"[241][243] these perceptions of the war, they are common.[citation needed] They have dynamically changed according to contemporary influences, reflecting in the 1950s perceptions of the war as 'aimless' following the contrasting Second World War and emphasising conflict within the ranks during times of class conflict in the 1960s.[242] The majority of additions to the contrary are often rejected.[242]
The social trauma caused by unprecedented rates of casualties manifested itself in different ways, which have been the subject of subsequent historical debate.[244] Some people[who?] were revolted by nationalism and its results, and began to work towards a more internationalist world, supporting organisations such as the League of Nations. Pacifism became increasingly popular. Others had the opposite reaction, feeling that only strength and military might could be relied upon in a chaotic and inhumane world. Anti-modernist views were an outgrowth of the many changes taking place in society.
The experiences of the war led to a collective trauma shared by many from all participating countries. The optimism of la belle poque was destroyed, and those who had fought in the war were referred to as the Lost Generation.[245] For years afterwards, people mourned the dead, the missing, and the many disabled.[246] Many soldiers returned with severe trauma, suffering from shell shock (also called neurasthenia, a condition related to posttraumatic stress disorder).[247] Many more returned home with few after-effects; however, their silence about the war contributed to the conflict's growing mythological status.[244] In the United Kingdom, mass mobilisation, large casualty rates, and the collapse of the Edwardian era made a strong impression on society. Though many participants did not share in the experiences of combat or spend any significant time at the front, or had positive memories of their service, the images of suffering and trauma became the widely shared perception.[244] Such historians as Dan Todman, Paul Fussell, and Samuel Heyns have all published works since the 1990s arguing that these common perceptions of the war are factually incorrect.[244]
The rise of Nazism and fascism included a revival of the nationalist spirit and a rejection of many post-war changes. Similarly, the popularity of the Stab-in-the-back legend (German: Dolchstolegende) was a testament to the psychological state of defeated Germany and was a rejection of responsibility for the conflict. This conspiracy theory of betrayal became common, and the German populace came to see themselves as victims. The Dolchstolegende's popular acceptance in Germany played a significant role in the rise of Nazism. A sense of disillusionment and cynicism became pronounced, with nihilism growing. Many believed the war heralded the end of the world as they had known it because of the high fatalities among a generation of men, the dissolution of governments and empires, and the collapse of capitalism and imperialism.
Communist and socialist movements around the world drew strength from this theory and enjoyed a new level of popularity. These feelings were most pronounced in areas directly or harshly affected by the war. Out of German discontent with the still controversial Treaty of Versailles, Adolf Hitler was able to gain popularity and power.[248][249] World War II was in part a continuation of the power struggle never fully resolved by the First World War; in fact, it was common for Germans in the 1930s and 1940s to justify acts of international aggression because of perceived injustices imposed by the victors of the First World War.[250][251][252] American historian William Rubinstein wrote that:
"The 'Age of Totalitarianism' included nearly all of the infamous examples of genocide in modern history, headed by the Jewish Holocaust, but also comprising the mass murders and purges of the Communist world, other mass killings carried out by Nazi Germany and its allies, and also the Armenian genocide of 1915. All these slaughters, it is argued here, had a common origin, the collapse of the elite structure and normal modes of government of much of central, eastern and southern Europe as a result of the First World War, without which surely neither Communism nor Fascism would have existed except in the minds of unknown agitators and crackpots".[253]
The establishment of the modern state of Israel and the roots of the continuing Israeli-Palestinian Conflict are partially found in the unstable power dynamics of the Middle East that resulted from World War I.[254] Prior to the end of the war, the Ottoman Empire had maintained a modest level of peace and stability throughout the Middle East.[255] With the fall of the Ottoman government, power vacuums developed and conflicting claims to land and nationhood began to emerge.[256] The political boundaries drawn by the victors of the First World War were quickly imposed, sometimes after only cursory consultation with the local population. In many cases, these continue to be problematic in the 21st-century struggles for national identity.[257][258] While the dissolution of the Ottoman Empire at the end of World War I was pivotal in contributing to the modern political situation of the Middle East, including the Arab-Israeli conflict,[259][260][261] the end of Ottoman rule also spawned lesser known disputes over water and other natural resources.[262]
U.S. intervention in the war, as well as the Wilson administration itself, became deeply unpopular. This was reflected in the U.S. Senate's rejection of the Versailles Treaty and membership in the League of Nations. In the interwar era, a consensus arose that U.S. intervention had been a mistake, and the Congress passed laws in an attempt to preserve U.S. neutrality in any future conflict. Polls taken in 1937 and the opening months of World War II established that nearly 60% regarded intervention in WWI as a mistake, with only 28% opposing that view. But, in the period between the fall of France and the attack on Pearl Harbor, public opinion changed dramatically and, for the first time, a narrow plurality rejected the idea that the war had been a mistake.[263]
Poland reemerged as an independent country, after more than a century. As a "minor Entente nation" and the country with the most casualties per capita,[264][265][266] the Kingdom of Serbia and its dynasty became the backbone of the new multinational state, the Kingdom of Serbs, Croats and Slovenes (later renamed Yugoslavia). Czechoslovakia, combining the Kingdom of Bohemia with parts of the Kingdom of Hungary, became a new nation. Russia became the Soviet Union and lost Finland, Estonia, Lithuania, and Latvia, which became independent countries. The Ottoman Empire was soon replaced by Turkey and several other countries in the Middle East.
In the British Empire, the war unleashed new forms of nationalism. In Australia and New Zealand the Battle of Gallipoli became known as those nations' "Baptism of Fire". It was the first major war in which the newly established countries fought, and it was one of the first times that Australian troops fought as Australians, not just subjects of the British Crown. Anzac Day, commemorating the Australian and New Zealand Army Corps, celebrates this defining moment.[267][268]
After the Battle of Vimy Ridge, where the Canadian divisions fought together for the first time as a single corps, Canadians began to refer to theirs as a nation "forged from fire".[269] Having succeeded on the same battleground where the "mother countries" had previously faltered, they were for the first time respected internationally for their own accomplishments. Canada entered the war as a Dominion of the British Empire and remained so, although it emerged with a greater measure of independence.[270][271] When Britain declared war in 1914, the dominions were automatically at war; at the conclusion, Canada, Australia, New Zealand, and South Africa were individual signatories of the Treaty of Versailles.[272]
One of the most dramatic effects of the war was the expansion of governmental powers and responsibilities in Britain, France, the United States, and the Dominions of the British Empire. In order to harness all the power of their societies, governments created new ministries and powers. New taxes were levied and laws enacted, all designed to bolster the war effort; many have lasted to this day. Similarly, the war strained the abilities of some formerly large and bureaucratised governments, such as in AustriaHungary and Germany; however, any analysis of the long-term effects were clouded by the defeat of these governments.
Gross domestic product (GDP) increased for three Allies (Britain, Italy, and U.S.), but decreased in France and Russia, in neutral Netherlands, and in the three main Central Powers. The shrinkage in GDP in Austria, Russia, France, and the Ottoman Empire reached 30 to 40%. In Austria, for example, most pigs were slaughtered, so at war's end there was no meat.
In all nations, the government's share of GDP increased, surpassing fifty percent in both Germany and France and nearly reaching that level in Britain. To pay for purchases in the United States, Britain cashed in its extensive investments in American railroads and then began borrowing heavily on Wall Street. President Wilson was on the verge of cutting off the loans in late 1916, but allowed a great increase in U.S. government lending to the Allies. After 1919, the U.S. demanded repayment of these loans. The repayments were, in part, funded by German reparations, which, in turn, were supported by American loans to Germany. This circular system collapsed in 1931 and the loans were never repaid. In 1934, Britain owed the US $4.4 billion[275] of World War I debt.[276]
Macro- and micro-economic consequences devolved from the war. Families were altered by the departure of many men. With the death or absence of the primary wage earner, women were forced into the workforce in unprecedented numbers. At the same time, industry needed to replace the lost labourers sent to war. This aided the struggle for voting rights for women.[277]
World War I further compounded the gender imbalance, adding to the phenomenon of surplus women. The deaths of nearly one million men during the war increased the gender gap by almost a million; from 670,000 to 1,700,000. The number of unmarried women seeking economic means grew dramatically. In addition, demobilisation and economic decline following the war caused high unemployment. The war increased female empolyment; however, the return of demoblised men displaced many from the workforce, as did the closure of many of the wartime factories. Hence women who had worked during the war found themselves struggling to find jobs and those approaching working age were not offered the opportunity.
In Britain, rationing was finally imposed in early 1918, limited to meat, sugar, and fats (butter and oleo), but not bread. The new system worked smoothly. From 1914 to 1918, trade union membership doubled, from a little over four million to a little over eight million. Work stoppages and strikes became frequent in 19171918 as the unions expressed grievances regarding prices, alcohol control, pay disputes, fatigue from overtime and working on Sundays, and inadequate housing.
Britain turned to her colonies for help in obtaining essential war materials whose supply had become difficult from traditional sources. Geologists such as Albert Ernest Kitson were called on to find new resources of precious minerals in the African colonies. Kitson discovered important new deposits of manganese, used in munitions production, in the Gold Coast.[278]
Article 231 of the Treaty of Versailles (the so-called "war guilt" clause) declared Germany and its allies responsible for all "loss and damage" suffered by the Allies during the war and provided the basis for reparations. The total reparations demanded was 132 billion gold marks, which was far more than the total German gold or foreign exchange. The economic problems that the payments brought, and German resentment at their imposition, are usually cited as one of the more significant factors that led to the end of the Weimar Republic and the beginning of the dictatorship of Adolf Hitler. After Germany's defeat in World War II, payment of the reparations was not resumed. There was, however, outstanding German debt that the Weimar Republic had used to pay the reparations. Germany finished paying off the reparations in October 2010.[279]
             
End of the German, Russian, Ottoman, and Austro-Hungarian empires
Formation of new countries in Europe and the Middle East
Transfer of German colonies and regions of the former Ottoman Empire to other powers
Establishment of the League of Nations. (more...)
Australia
Canada
India
Newfoundland
New Zealand
South Africa
Southern Rhodesia
1 Names
2 Background
3 Theatres of conflict

3.1 Opening hostilities

3.1.1 Confusion among the Central Powers
3.1.2 African campaigns
3.1.3 Serbian campaign
3.1.4 German forces in Belgium and France
3.1.5 Asia and the Pacific


3.2 Western Front

3.2.1 Trench warfare begins (19141915)
3.2.2 Trench warfare continues (19161917)


3.3 Naval war
3.4 Southern theatres

3.4.1 War in the Balkans
3.4.2 Ottoman Empire
3.4.3 Italian participation
3.4.4 Romanian participation
3.4.5 The role of India


3.5 Eastern Front

3.5.1 Initial actions
3.5.2 Russian Revolution


3.6 Central Powers proposal for starting peace negotiations
3.7 19171918

3.7.1 Developments in 1917
3.7.2 Ottoman Empire conflict in 1917
3.7.3 Entry of the United States

3.7.3.1 Non-intervention
3.7.3.2 U.S. declaration of war on Germany
3.7.3.3 First active U.S. participation


3.7.4 Austrian offer of separate peace
3.7.5 German Spring Offensive of 1918
3.7.6 Ottoman Empire conflict 1918
3.7.7 New states under war zone
3.7.8 Allied victory: summer and autumn 1918
3.7.9 Armistices and capitulations

3.7.9.1 Allied superiority and the stab-in-the-back legend, November 1918




3.8 Treaty of Versailles, June 1919


3.1 Opening hostilities

3.1.1 Confusion among the Central Powers
3.1.2 African campaigns
3.1.3 Serbian campaign
3.1.4 German forces in Belgium and France
3.1.5 Asia and the Pacific


3.1.1 Confusion among the Central Powers
3.1.2 African campaigns
3.1.3 Serbian campaign
3.1.4 German forces in Belgium and France
3.1.5 Asia and the Pacific
3.2 Western Front

3.2.1 Trench warfare begins (19141915)
3.2.2 Trench warfare continues (19161917)


3.2.1 Trench warfare begins (19141915)
3.2.2 Trench warfare continues (19161917)
3.3 Naval war
3.4 Southern theatres

3.4.1 War in the Balkans
3.4.2 Ottoman Empire
3.4.3 Italian participation
3.4.4 Romanian participation
3.4.5 The role of India


3.4.1 War in the Balkans
3.4.2 Ottoman Empire
3.4.3 Italian participation
3.4.4 Romanian participation
3.4.5 The role of India
3.5 Eastern Front

3.5.1 Initial actions
3.5.2 Russian Revolution


3.5.1 Initial actions
3.5.2 Russian Revolution
3.6 Central Powers proposal for starting peace negotiations
3.7 19171918

3.7.1 Developments in 1917
3.7.2 Ottoman Empire conflict in 1917
3.7.3 Entry of the United States

3.7.3.1 Non-intervention
3.7.3.2 U.S. declaration of war on Germany
3.7.3.3 First active U.S. participation


3.7.4 Austrian offer of separate peace
3.7.5 German Spring Offensive of 1918
3.7.6 Ottoman Empire conflict 1918
3.7.7 New states under war zone
3.7.8 Allied victory: summer and autumn 1918
3.7.9 Armistices and capitulations

3.7.9.1 Allied superiority and the stab-in-the-back legend, November 1918




3.7.1 Developments in 1917
3.7.2 Ottoman Empire conflict in 1917
3.7.3 Entry of the United States

3.7.3.1 Non-intervention
3.7.3.2 U.S. declaration of war on Germany
3.7.3.3 First active U.S. participation


3.7.3.1 Non-intervention
3.7.3.2 U.S. declaration of war on Germany
3.7.3.3 First active U.S. participation
3.7.4 Austrian offer of separate peace
3.7.5 German Spring Offensive of 1918
3.7.6 Ottoman Empire conflict 1918
3.7.7 New states under war zone
3.7.8 Allied victory: summer and autumn 1918
3.7.9 Armistices and capitulations

3.7.9.1 Allied superiority and the stab-in-the-back legend, November 1918


3.7.9.1 Allied superiority and the stab-in-the-back legend, November 1918
3.8 Treaty of Versailles, June 1919
4 Technology

4.1 Aviation
4.2 Naval
4.3 Ground warfare
4.4 Flamethrowers and subterranean transport


4.1 Aviation
4.2 Naval
4.3 Ground warfare
4.4 Flamethrowers and subterranean transport
5 War crimes

5.1 Genocide and ethnic cleansing

5.1.1 Russian Empire


5.2 "Rape of Belgium"


5.1 Genocide and ethnic cleansing

5.1.1 Russian Empire


5.1.1 Russian Empire
5.2 "Rape of Belgium"
6 Soldiers' experiences

6.1 Prisoners of war
6.2 Military attachs and war correspondents


6.1 Prisoners of war
6.2 Military attachs and war correspondents
7 Support and opposition to the war

7.1 Support
7.2 Opposition

7.2.1 Conscription




7.1 Support
7.2 Opposition

7.2.1 Conscription


7.2.1 Conscription
8 Aftermath

8.1 Health and economic effects
8.2 Peace treaties and national boundaries


8.1 Health and economic effects
8.2 Peace treaties and national boundaries
9 Legacy

9.1 Memorials
9.2 Cultural memory
9.3 Social trauma
9.4 Discontent in Germany
9.5 Views in the United States
9.6 New national identities
9.7 Economic effects


9.1 Memorials
9.2 Cultural memory
9.3 Social trauma
9.4 Discontent in Germany
9.5 Views in the United States
9.6 New national identities
9.7 Economic effects
10 See also

10.1 Media


10.1 Media
11 Notes
12 References
13 External links

13.1 Animated maps


13.1 Animated maps
3.1 Opening hostilities

3.1.1 Confusion among the Central Powers
3.1.2 African campaigns
3.1.3 Serbian campaign
3.1.4 German forces in Belgium and France
3.1.5 Asia and the Pacific


3.1.1 Confusion among the Central Powers
3.1.2 African campaigns
3.1.3 Serbian campaign
3.1.4 German forces in Belgium and France
3.1.5 Asia and the Pacific
3.2 Western Front

3.2.1 Trench warfare begins (19141915)
3.2.2 Trench warfare continues (19161917)


3.2.1 Trench warfare begins (19141915)
3.2.2 Trench warfare continues (19161917)
3.3 Naval war
3.4 Southern theatres

3.4.1 War in the Balkans
3.4.2 Ottoman Empire
3.4.3 Italian participation
3.4.4 Romanian participation
3.4.5 The role of India


3.4.1 War in the Balkans
3.4.2 Ottoman Empire
3.4.3 Italian participation
3.4.4 Romanian participation
3.4.5 The role of India
3.5 Eastern Front

3.5.1 Initial actions
3.5.2 Russian Revolution


3.5.1 Initial actions
3.5.2 Russian Revolution
3.6 Central Powers proposal for starting peace negotiations
3.7 19171918

3.7.1 Developments in 1917
3.7.2 Ottoman Empire conflict in 1917
3.7.3 Entry of the United States

3.7.3.1 Non-intervention
3.7.3.2 U.S. declaration of war on Germany
3.7.3.3 First active U.S. participation


3.7.4 Austrian offer of separate peace
3.7.5 German Spring Offensive of 1918
3.7.6 Ottoman Empire conflict 1918
3.7.7 New states under war zone
3.7.8 Allied victory: summer and autumn 1918
3.7.9 Armistices and capitulations

3.7.9.1 Allied superiority and the stab-in-the-back legend, November 1918




3.7.1 Developments in 1917
3.7.2 Ottoman Empire conflict in 1917
3.7.3 Entry of the United States

3.7.3.1 Non-intervention
3.7.3.2 U.S. declaration of war on Germany
3.7.3.3 First active U.S. participation


3.7.3.1 Non-intervention
3.7.3.2 U.S. declaration of war on Germany
3.7.3.3 First active U.S. participation
3.7.4 Austrian offer of separate peace
3.7.5 German Spring Offensive of 1918
3.7.6 Ottoman Empire conflict 1918
3.7.7 New states under war zone
3.7.8 Allied victory: summer and autumn 1918
3.7.9 Armistices and capitulations

3.7.9.1 Allied superiority and the stab-in-the-back legend, November 1918


3.7.9.1 Allied superiority and the stab-in-the-back legend, November 1918
3.8 Treaty of Versailles, June 1919
3.1.1 Confusion among the Central Powers
3.1.2 African campaigns
3.1.3 Serbian campaign
3.1.4 German forces in Belgium and France
3.1.5 Asia and the Pacific
3.2.1 Trench warfare begins (19141915)
3.2.2 Trench warfare continues (19161917)
3.4.1 War in the Balkans
3.4.2 Ottoman Empire
3.4.3 Italian participation
3.4.4 Romanian participation
3.4.5 The role of India
3.5.1 Initial actions
3.5.2 Russian Revolution
3.7.1 Developments in 1917
3.7.2 Ottoman Empire conflict in 1917
3.7.3 Entry of the United States

3.7.3.1 Non-intervention
3.7.3.2 U.S. declaration of war on Germany
3.7.3.3 First active U.S. participation


3.7.3.1 Non-intervention
3.7.3.2 U.S. declaration of war on Germany
3.7.3.3 First active U.S. participation
3.7.4 Austrian offer of separate peace
3.7.5 German Spring Offensive of 1918
3.7.6 Ottoman Empire conflict 1918
3.7.7 New states under war zone
3.7.8 Allied victory: summer and autumn 1918
3.7.9 Armistices and capitulations

3.7.9.1 Allied superiority and the stab-in-the-back legend, November 1918


3.7.9.1 Allied superiority and the stab-in-the-back legend, November 1918
3.7.3.1 Non-intervention
3.7.3.2 U.S. declaration of war on Germany
3.7.3.3 First active U.S. participation
3.7.9.1 Allied superiority and the stab-in-the-back legend, November 1918
4.1 Aviation
4.2 Naval
4.3 Ground warfare
4.4 Flamethrowers and subterranean transport
5.1 Genocide and ethnic cleansing

5.1.1 Russian Empire


5.1.1 Russian Empire
5.2 "Rape of Belgium"
5.1.1 Russian Empire
6.1 Prisoners of war
6.2 Military attachs and war correspondents
7.1 Support
7.2 Opposition

7.2.1 Conscription


7.2.1 Conscription
7.2.1 Conscription
8.1 Health and economic effects
8.2 Peace treaties and national boundaries
9.1 Memorials
9.2 Cultural memory
9.3 Social trauma
9.4 Discontent in Germany
9.5 Views in the United States
9.6 New national identities
9.7 Economic effects
10.1 Media
13.1 Animated maps
Book: World War I
European Civil War
List of last surviving World War I veterans by country
List of people associated with World War I
List of wars and anthropogenic disasters by death toll
Lists of wars
Lists of World War I topics
World War I medal abbreviations
Death rates in the 20th century
<Please add first missing authors to populate metadata.> (1938), American Armies and Battlefields in Europe: A History, Guide, and Reference Book, U.S. Government Printing Office, OCLC59803706, http://www.secstate.wa.gov/history/ww1/maps.aspx
<Please add first missing authors to populate metadata.> (1993), Army Art of World War I, United States Army Center of Military History: Smithsonian Institution, National Museum of American History, OCLC28608539, http://www.secstate.wa.gov/history/publications_detail.aspx?p=28
Asghar, Syed Birjees (12 June 2005), A Famous Uprising, Dawn Group, archived from the original on 3 August 2007, http://web.archive.org/web/20070803144344/http://www.dawn.com/weekly/dmag/archive/050612/dmag14.htm, retrieved 2 November 2007
Ashworth, Tony (2000) [1980], Trench warfare, 191418: the live and let live system, London: Pan, ISBN0-330-48068-5, OCLC247360122
Bade, Klaus J; Brown, Allison (tr.) (2003), Migration in European History, The making of Europe, Oxford: Blackwell, ISBN0-631-18939-4, OCLC52695573 (translated from the German)
Baker, Kevin (June 2006), "Stabbed in the Back! The past and future of a right-wing myth", Harper's Magazine
Balakian, Peter (2003), The Burning Tigris: The Armenian Genocide and America's Response, New York: HarperCollins, ISBN978-0-06-019840-4, OCLC56822108
Ball, Alan M (1996), And Now My Soul Is Hardened: Abandoned Children in Soviet Russia, 19181930, Berkeley: University of California Press, ISBN978-0-520-20694-6, reviewed in Hegarty, Thomas J (MarchJune 1998), "And Now My Soul Is Hardened: Abandoned Children in Soviet Russia, 19181930", Canadian Slavonic Papers, http://findarticles.com/p/articles/mi_qa3763/is_/ai_n8801575[dead link]
Bass, Gary Jonathan (2002), Stay the Hand of Vengeance: The Politics of War Crimes Tribunals, Princeton, New Jersey: Princeton University Press, pp.424pp, ISBN0-691-09278-8, OCLC248021790
Blair, Dale (2005), No Quarter: Unlawful Killing and Surrender in the Australian War Experience, 19151918, Charnwood, Australia: Ginninderra Press, ISBN1-74027-291-9, OCLC62514621
Brands, Henry William (1997), T. R.: The Last Romantic, New York: Basic Books, ISBN0-465-06958-4, OCLC36954615
Brown, Judith M. (1994), Modern India: The Origins of an Asian Democracy, Oxford and New York: Oxford University Press. Pp. xiii, 474, ISBN0-19-873113-2.
Chickering, Rodger (2004), Imperial Germany and the Great War, 19141918, Cambridge: Cambridge University Press, ISBN0-521-83908-4, OCLC55523473
Clark, Charles Upson (1927), Bessarabia, Russia and Roumania on the Black Sea, New York: Dodd, Mead, OCLC150789848, http://depts.washington.edu/cartah/text_archive/clark/meta_pag.shtml
Cockfield, Jamie H (1997), With snow on their boots: The tragic odyssey of the Russian Expeditionary Force in France during World War 1, Palgrave Macmillan, ISBN0-312-22082-0
Conlon, Joseph M (PDF), The historical impact of epidemic typhus, Montana State University, http://entomology.montana.edu/historybug/TYPHUS-Conlon.pdf, retrieved 21 April 2009
Cook, Tim (2006), "The politics of surrender: Canadian soldiers and the killing of prisoners in the First World War", The Journal of Military History 70 (3): 637665, doi:10.1353/jmh.2006.0158
Cross, Wilbur L (1991), Zeppelins of World War I, New York: Paragon Press, ISBN978-1-55778-382-0, OCLC22860189
Djoki, Dejan (2003), Yugoslavism: histories of a failed idea, 19181992, London: Hurst, OCLC51093251
Dignan, Don K (February 1971), "The Hindu Conspiracy in Anglo-American Relations during World War I", The Pacific Historical Review (University of California Press) 40 (1): 5776, ISSN0030-8684, JSTOR3637829
Doughty, Robert A. (2005), Pyrrhic victory: French strategy and operations in the Great War, Harvard University Press, ISBN978-0-674-01880-8, http://books.google.com/books?id=vZRmHkdGk44C
Duffy, Michael, Somme, First World War.com, ISBN0-297-84689-2, http://www.firstworldwar.com/battles/somme.htm, retrieved 25 February 2007
Evans, David (2004), The First World War, Teach yourself, London: Hodder Arnold, ISBN0-340-88489-4, OCLC224332259
Evans, Leslie (27 May 2005), Future of Iraq, Israel-Palestine Conflict, and Central Asia Weighed at International Conference, UCLA International Institute, http://www.international.ucla.edu/article.asp?parentid=24920, retrieved 30 December 2008
Falls, Cyril Bentham (1960), The First World War, London: Longmans, ISBN1-84342-272-7, OCLC460327352
Farwell, Byron (1989), The Great War in Africa, 19141918, W.W. Norton, ISBN978-0-393-30564-7
Ferguson, Niall (1999), The Pity of War, New York: Basic Books, pp.563pp, ISBN0-465-05711-X, OCLC41124439
Ferguson, Niall (2006), The War of the World: Twentieth-Century Conflict and the Descent of the West, New York: Penguin Press, ISBN1-59420-100-5
Findley, Carter Vaughn; Rothney, J.A. (2006), Twentieth Century World (6th ed.), Boston: Houghton Mifflin
Fortescue, Granville Roland (28 October 1915), "London in Gloom over Gallipoli; Captain Fortescue in Book and Ashmead-Bartlett in Lecture Declare Campaign Lost. Say Allies Can't Advance; Attack on Allied Diplomacy in Correspondent's Doleful Talk Passed by Censor", New York Times, http://query.nytimes.com/gst/abstract.html?res=9907E3DE1E38E633A2575BC2A9669D946496D6CF
Fraser, Thomas G (April 1977), "Germany and Indian Revolution, 191418", Journal of Contemporary History (Sage Publications) 12 (2): 255272, doi:10.1177/002200947701200203, ISSN0022-0094
Fromkin, David (2001), A Peace to End All Peace: The Fall of the Ottoman Empire and the Creation of the Modern Middle East, New York: Owl Books, p.119, ISBN0-8050-6884-8, OCLC53814831
Fromkin, David (2004), Europe's Last Summer: Who Started the Great War in 1914?, New York: Alfred A. Knopf, ISBN0-375-41156-9, OCLC53937943
Gelvin, James L (2005), The Israel-Palestine Conflict: One Hundred Years of War, Cambridge: Cambridge University Press, ISBN0-521-85289-7, OCLC59879560
Gibbs, Phillip (26 October 1918 published 30 October 1918), "Fall of Ghent Near, German Flank in Peril", New York Times, http://query.nytimes.com/gst/abstract.html?res=9F05E4D61539E13ABC4850DFB6678383609EDE&scp=4&sq=Ghent+1918&st=p
Gibbs, Phillip (15 November 1918), "Ghent Burghers Hail Liberators" (PDF), New York Times, http://query.nytimes.com/mem/archive-free/pdf?res=940DE1DC1239E13ABC4D52DFB7678383609EDE
Gray, Randal; Argyle, Christopher (1990), Chronicle of the First World War, New York: Facts on File, ISBN978-0-8160-2595-4, OCLC19398100
Gilbert, Martin (2004), The First World War: A Complete History, Clearwater, Florida: Owl Books, p.306, ISBN0-8050-7617-4, OCLC34792651
Goodspeed, Donald James (1985), The German Wars 19141945, New York: Random House; Bonanza, ISBN978-0-517-46790-9
Gray, Randal (1991), Kaiserschlacht 1918: the final German offensive, Osprey, ISBN978-1-85532-157-1
Green, John Frederick Norman (1938), "Obituary: Albert Ernest Kitson", Geological Society Quarterly Journal (Geological Society) 94
Haber, Lutz Fritz (1986), The Poisonous Cloud: Chemical Warfare in the First World War, Oxford: Clarendon, ISBN0-19-858142-4, OCLC12051072
Halpern, Paul G (1995), A Naval History of World War I, New York: Routledge, ISBN1-85728-498-4, OCLC60281302
Harrach, Franz, "Archduke Franz Ferdinand's Assassination, 28 June 1914: Memoir of Count Franz von Harrach", Primary Documents (First World War.com)
Hartcup, Guy (1988), The War of Invention; Scientific Developments, 191418, Brassey's Defence Publishers, ISBN0-08-033591-8
Havighurst, Alfred F (1985), Britain in transition: the twentieth century (4 ed.), University of Chicago Press, ISBN978-0-226-31971-1
Heer, Germany (2009), German and Austrian Tactical Studies, ISBN978-1-110-76516-4
Heller, Charles E (1984), Chemical warfare in World War I: the American experience, 19171918, Fort Leavenworth, Kansas: Combat Studies Institute, OCLC123244486, http://www-cgsc.army.mil/carl/resources/csi/Heller/HELLER.asp
Herbert, Edwin (2003), Small Wars and Skirmishes 19021918: Early Twentieth-century Colonial Campaigns in Africa, Asia and the Americas, Nottingham: Foundry Books Publications, ISBN1-901543-05-6
Heyman, Neil M (1997), World War I, Guides to historic events of the twentieth century, Westport, Connecticut: Greenwood Press, ISBN0-313-29880-7, OCLC36292837
Hickey, Michael (2003), The Mediterranean Front 19141923, The First World War, 4, New York: Routledge, pp.6065, ISBN0-415-96844-5, OCLC52375688
Hinterhoff, Eugene (1984), Young, Peter, ed., "The Campaign in Armenia", Marshall Cavendish Illustrated Encyclopedia of World War I (New York: Marshall Cavendish) ii, ISBN0-86307-181-3
Hooker, Richard (1996), The Ottomans, Washington State University, http://www.wsu.edu/~dee/OTTOMAN/OTTOMAN1.HTM, retrieved 30 December 2008[dead link]
Hoover, Herbert; Wilson, Woodrow (1958), Ordeal of Woodrow Wilson, New York: McGraw-Hill, ISBN0-943875-41-2, OCLC254607345
Hughes, Thomas L (October 2002), "The German Mission to Afghanistan, 19151916", German Studies Review (German Studies Association) 25 (3): 447476, doi:10.2307/1432596, ISSN0149-7952, JSTOR1432596
Hull, Isabel Virginia (2006), Absolute destruction: military culture and the practices of war in Imperial Germany, Cornell University Press, ISBN978-0-8014-7293-0
Isaac, Jad; Hosh, Leonardo (79 May 1992), Roots of the Water Conflict in the Middle East, University of Waterloo, archived from the original on 28 September 2006, http://web.archive.org/web/20060928053605/http://www.oranim.ac.il/courses/meast/water/Roots+of+the+Water+Conflict+in+the+Middle+East.htm
Jenkins, Burris A (2009), Facing the Hindenburg Line, BiblioBazaar, ISBN978-1-110-81238-7
Johnson, Douglas Wilson (1921), Battlefields of the World War, Western and Southern Fronts, New York: Oxford University Press, ISBN1-4326-3739-8, OCLC688071, http://openlibrary.org/b/OL23383739M/Battlefields_of_the_World_War_western_and_southern_fronts
Johnson, James Edgar (2001), Full Circle: The Story of Air Fighting, London: Cassell, ISBN0-304-35860-6, OCLC45991828
Jones, Howard (2001), Crucible of Power: A History of U.S. Foreign Relations Since 1897, Wilmington, Delaware: Scholarly Resources Books, ISBN0-8420-2918-4, OCLC46640675
Kaplan, Robert D (February 1993), "Syria: Identity Crisis", The Atlantic, http://www.theatlantic.com/doc/199302/kaplan, retrieved 30 December 2008
Karp, Walter (1979), The Politics of War (1st ed.), ISBN0-06-012265-X, OCLC4593327, Wilson's maneuvering U.S. into war
Keegan, John (1998), The First World War, Hutchinson, ISBN0-09-180178-8, general military history
Keene, Jennifer D (2006), World War I, Westport, Connecticut: Greenwood Press, p.5, ISBN0-313-33181-2, OCLC70883191
Kennedy, David M (2004), Over here: the First World War and American society, Oxford University Press, ISBN978-0-19-517399-4
Kernek, Sterling (December 1970), "The British Government's Reactions to President Wilson's 'Peace' Note of December 1916", The Historical Journal 13 (4): 721766, doi:10.1017/S0018246X00009481, JSTOR2637713
Keynes, John Maynard (1920), The Economic Consequences of the Peace, New York: Harcourt, Brace and Howe, ISBN0-521-22095-5, OCLC213487540
Kitchen, Martin (2000) [1980], Europe Between the Wars, New York: Longman, ISBN0-582-41869-0, OCLC247285240
Knobler, Stacey L, ed. (2005), The Threat of Pandemic Influenza: Are We Ready? Workshop Summary, Washington DC: National Academies Press, ISBN0-309-09504-2, OCLC57422232, http://www.nap.edu/books/0309095042/html/7.html
Kurlander, Eric (2006) (Book review), Steffen Bruendel. Volksgemeinschaft oder Volksstaat: Die "Ideen von 1914" und die Neuordnung Deutschlands im Ersten Weltkrieg, H-net, http://www.h-net.org/reviews/showrev.cgi?path=101921145898314, retrieved 17 November 2009
<Please add first missing authors to populate metadata.> (1999), Lehmann, Hartmut; van der Veer, Peter, eds., Nation and religion: perspectives on Europe and Asia, Princeton, New Jersey: Princeton University Press, ISBN0-691-01232-6, OCLC39727826
Lewy, Guenter (2005), The Armenian Massacres in Ottoman Turkey: A Disputed Genocide, Salt Lake City, Utah: University of Utah Press, ISBN0-87480-849-9, OCLC61262401
Love, Dave (May 1996), "The Second Battle of Ypres, April 1915", Sabretasche 26 (4), http://www.worldwar1.com/sf2ypres.htm
Lyons, Michael J (1999), World War I: A Short History (2nd ed.), Prentice Hall, ISBN0-13-020551-6
Ludendorff, Erich (1919), My War Memories, 19141918, OCLC60104290 also published by Harper as "Ludendorff's Own Story, August 1914 November 1918: The Great War from the Siege of Liege to the Signing of the Armistice as Viewed from the Grand Headquarters of the German Army" OCLC561160 (original title Meine Kriegserinnerungen, 19141918)
Magliveras, Konstantinos D (1999), Exclusion from Participation in International Organisations: The Law and Practice behind Member States' Expulsion and Suspension of Membership, Martinus Nijhoff Publishers, ISBN90-411-1239-1
Maurice, Frederick Barton (18 August 1918), "Foe's reserves now only 16 divisions; Allies' Counteroffensive has reduced them from 60, Gen. Maurice says Ludendorff in dilemma; he must choose between giving up offensive projects and shortening his line", New York Times, http://query.nytimes.com/gst/abstract.html?res=9B02EFD6103BEE3ABC4052DFBE668383609EDE&scp=8&sq=Ludendorff+Amiens+1918&st=p
Martel, Gordon (2003), The Origins of the First World War, Pearson Longman, Harlow
Mawdsley, Evan (2008), The Russian Civil War (Edinburgh ed.), Birlinn location, ISBN1-84341-041-9
McDermott, T. P., USA's Boy Scouts and World War I Liberty Loan Bonds, http://www.sossi.org/journal/scouts-ww1-liberty-bonds.pdf
McLellan, Edwin N, The United States Marine Corps in the World War, http://www.ibiblio.org/hyperwar/AMH/XX/WWI/USMC/USMC-WWI.html#XIV
Meyer, Gerald J (2006), A World Undone: The Story of the Great War 1914 to 1918, Random House, ISBN978-0-553-80354-9
Millett, Allan Reed; Murray, Williamson (1988), Military Effectiveness, Boston: Allen Unwin, ISBN0-04-445053-2, OCLC220072268
Moon, John Ellis van Courtland (July 1996), "United States Chemical Warfare Policy in World War II: A Captive of Coalition Policy?", The Journal of Military History (Society for Military History) 60 (3): 495511, doi:10.2307/2944522, JSTOR2944522
Morton, Desmond; Granatstein, Jack L (1989), Marching to Armageddon: Canadians and the Great War 19141919, ISBN0-88619-209-9, OCLC21449019
Morton, Desmond (1992), Silent Battle: Canadian Prisoners of War in Germany, 19141919, Toronto: Lester Publishing, ISBN1-895555-17-5, OCLC29565680
Mosier, John (2001), "Germany and the Development of Combined Arms Tactics", Myth of the Great War: How the Germans Won the Battles and How the Americans Saved the Allies, New York: Harper Collins, ISBN0-06-019676-9
Muller, Jerry Z (March/April 2008), "Us and Them The Enduring Power of Ethnic Nationalism", Foreign Affairs (Council on Foreign Relations), http://www.foreignaffairs.com/20080301faessay87203/jerry-z-muller/us-and-them.html, retrieved 30 December 2008
Neiberg, Michael S (2005), Fighting the Great War: A Global History, Cambridge, Mass: Harvard University Press, ISBN0-674-01696-3, OCLC56592292
Nicholson, Gerald WL (1962), Canadian Expeditionary Force, 19141919: Official History of the Canadian Army in the First World War (1st ed.), Ottawa: Queens Printer and Controller of Stationary, OCLC2317262, http://www.censol.ca/research/greatwar/nicholson/index.htm
Northedge, FS (1986), The League of Nations: Its Life and Times, 19201946, New York: Holmes & Meier, ISBN0-7185-1316-9
Page, Thomas Nelson, Italy and the World War, Brigham Young University, Chapter XI, http://net.lib.byu.edu/estu/wwi/comment/Italy/Page04.htm cites "Cf. articles signed XXX in La Revue de Deux Mondes, 1 and 15 March 1920"
Perry, Frederick W (1988), The Commonwealth armies: manpower and organisation in two world wars, Manchester University Press, ISBN978-0-7190-2595-2
Phillimore, George Grenville; Bellot, Hugh HL (1919), "Treatment of Prisoners of War", Transactions of the Grotius Society 5: 4764, OCLC43267276
Pitt, Barrie (2003), 1918: The Last Act, Barnsley: Pen and Sword, ISBN0-85052-974-3, OCLC56468232
Price, Alfred (1980), Aircraft versus Submarine: the Evolution of the Anti-submarine Aircraft, 1912 to 1980, London: Jane's Publishing, ISBN0-7106-0008-9, OCLC10324173 Deals with technical developments, including the first dipping hydrophones
Prior, Robin (1999), The First World War, London: Cassell, ISBN0-304-35256-X
Raudzens, George (October 1990), "War-Winning Weapons: The Measurement of Technological Determinism in Military History", The Journal of Military History (Society for Military History) 54 (4): 403434, doi:10.2307/1986064, JSTOR1986064
Repington, Charles  Court (1920), The First World War, 19141918, 2, London: Constable, ISBN1-113-19764-1, http://www.archive.org/details/firstworldwar19102repiuoft
Rickard, J (5 March 2001), "Erich von Ludendorff, 18651937, German General", Military History Encyclopedia on the Web (HistoryOfWar.org), http://www.historyofwar.org/articles/people_ludendorff.html, retrieved 6 February 2008
Rickard, J (27 August 2007), The Ludendorff Offensives, 21 March-18 July 1918, http://www.historyofwar.org/scripts/fluffy/fcp.pl?words=20+July+1918&d=/battles_ludendorff.html
Roden, Mike, "The Lost Generation myth and reality", Aftermath when the boys came home, http://www.aftermathww1.com/lostgen.asp, retrieved 6 November 2009
Ross, Stewart Halsey (1996), Propaganda for War: How the United States was Conditioned to Fight the Great War of 19141918, Jefferson, North Carolina: McFarland, ISBN0-7864-0111-7, OCLC185807544
Saadi, Abdul-Ilah, Dreaming of Greater Syria, Al Jazeera, http://english.aljazeera.net/focus/arabunity/2008/02/2008525183842614205.html, retrieved 17 November 2009
Sachar, Howard Morley (1970), The emergence of the Middle East, 19141924, Allen Lane, ISBN0-7139-0158-6, OCLC153103197
Safire, William (2008), Safire's Political Dictionary, Oxford University Press, ISBN978-0-19-534334-2, http://books.google.com/?id=jK-0NPoMiYoC
Salibi, Kamal Suleiman (1993), "How it all began A concise history of Lebanon", A House of Many Mansions the history of Lebanon reconsidered, I.B. Tauris, ISBN1-85043-091-8, OCLC224705916, http://almashriq.hiof.no/lebanon/900/902/Kamal-Salibi/
Schindler, J (2003), "Steamrollered in Galicia: The Austro-Hungarian Army and the Brusilov Offensive, 1916", War in History 10 (1): 2759, doi:10.1191/0968344503wh260oa
Shanafelt, Gary W (1985), The secret enemy: Austria-Hungary and the German alliance, 19141918, East European Monographs, ISBN978-0-88033-080-0
Shapiro, Fred R; Epstein, Joseph (2006), The Yale Book of Quotations, Yale University Press, ISBN0-300-10798-6
Sheffield, G (2001). Forgotten Victory: The First World War: Myths and Realities (2002 ed.). London: Headline Book Publishing. ISBN0-74727-157-7.
Souter, Gavin (2000). Lion & Kangaroo: the initiation of Australia. Melbourne: Text Publishing. OCLC222801639.
Singh, Jaspal, History of the Ghadar Movement, panjab.org.uk, http://www.panjab.org.uk/english/histGPty.html, retrieved 31 October 2007
Sisemore, James D (2003), The Russo-Japanese War, Lessons Not Learned, U.S. Army Command and General Staff College, http://cgsc.cdmhost.com/cdm4/item_viewer.php?CISOROOT=/p4013coll2&CISOPTR=113
Smele, Jonathan, "War and Revolution in Russia 19141921", World Wars in-depth (BBC), archived from the original on 9 November 2011, http://www.webcitation.org/635RR9gbC, retrieved 12 November 2009
Speed, Richard B, III (1990), Prisoners, Diplomats and the Great War: A Study in the Diplomacy of Captivity, New York: Greenwood Press, ISBN0-313-26729-4, OCLC20694547
Stevenson, David (1996), Armaments and the Coming of War: Europe, 19041914, New York: Oxford University Press, ISBN0-19-820208-3, OCLC33079190
Stevenson, David (2004), Cataclysm: The First World War As Political Tragedy, New York: Basic Books, pp.560pp, ISBN0-465-08184-3, OCLC54001282, major reinterpretation
Stevenson, David (2005), The First World War and International Politics, Oxford: Clarendon, ISBN0-19-820281-4, OCLC248297941
Gilbert, Martin (1994), First World War, Stoddart Publishing, ISBN978-0-7737-2848-6
Strachan, Hew (2004), The First World War: Volume I: To Arms, New York: Viking, ISBN0-670-03295-6, OCLC53075929: the major scholarly synthesis. Thorough coverage of 1914
Strachan, Hew (1998), The Oxford Illustrated History of the First World War, New York: Oxford University Press, ISBN0-19-820614-3
Stumpp, Karl; Weins, Herbert; Smith, Ingeborg W (trans) (1997), A People on the Move: Germans in Russia and in the Former Soviet Union: 17631997, North Dakota State University Libraries, http://lib.ndsu.nodak.edu/grhc/history_culture/history/people.html
Swietochowski, Tadeusz (2004), Russian Azerbaijan, 19051920: The Shaping of a National Identity in a Muslim Community, 42, Cambridge University Press, ISBN978-0-521-52245-8, reviewed at JSTOR1866737
Taylor, Alan John Percivale (1963), The First World War: An Illustrated History, Hamish Hamilton, ISBN0-39-950260-2, OCLC2054370
Taylor, Alan John Percivale (1998), The First World War and its aftermath, 19141919, London: Folio Society, OCLC49988231
Taylor, John M (Summer 2007), "Audacious Cruise of the Emden", The Quarterly Journal of Military History 19 (4): 3847, doi:10.1353/jmh.2007.0331 (inactive 2010-07-26), ISSN0899-3718
Terraine, John (1963), Ordeal of Victory, Philadelphia: J. B. Lippincott, pp.508pp, ISBN0-09-068120-7, OCLC1345833
Tschanz, David W, Typhus fever on the Eastern front in World War I, Montana State University, http://www.entomology.montana.edu/historybug/WWI/TEF.htm, retrieved 12 November 2009
Tuchman, Barbara Wertheim (1962), The Guns of August, New York: Macmillan, ISBN978-0-02-620310-4, OCLC192333, tells of the opening diplomatic and military manoeuvres
Tuchman, Barbara Wertheim (1966), The Zimmerman Telegram (2nd ed.), New York: Macmillan, ISBN0-02-620320-0, OCLC233392415
Tucker, Spencer C (1999), European Powers in the First World War: An Encyclopedia, ISBN0-8153-3351-X, OCLC40417794
Tucker, Spencer C; Roberts, Priscilla Mary (2005), Encyclopedia of World War I, Santa Barbara: ABC-Clio, ISBN1-85109-420-2, OCLC61247250
Tucker, Spencer C; Wood, Laura Matysek; Murphy, Justin D (1999), The European powers in the First World War: an encyclopedia, Taylor & Francis, ISBN978-0-8153-3351-7
von der Porten, Edward P (1969), German Navy in World War II, New York: T. Y. Crowell, ISBN0-213-17961-X, OCLC164543865
Westwell, Ian (2004), World War I Day by Day, St. Paul, Minnesota: MBI Publishing, pp.192pp, ISBN0-7603-1937-5, OCLC57533366
Wilgus, William John (1931), Transporting the A. E. F. in Western Europe, 19171919, New York: Columbia University Press, OCLC1161730
Willmott, H.P. (2003), World War I, New York: Dorling Kindersley, ISBN0-7894-9627-5, OCLC52541937
Winegard, Timothy, "Here at Vimy: A Retrospective The 90th Anniversary of the Battle of Vimy Ridge", Canadian Military Journal 8 (2), http://www.journal.forces.gc.ca/vo8/no2/winegard-eng.asp
Winter, Denis (1983), The First of the Few: Fighter Pilots of the First World War, Penguin, ISBN978-0-14-005256-5
Wohl, Robert (1979), The Generation of 1914 (3 ed.), Harvard University Press, ISBN978-0-674-34466-2
Zieger, Robert H (2001), America's Great War: World War I and the American experience, Lanham, Maryland: Rowman & Littlefield, p.50, ISBN0-8476-9645-6
<Please add first missing authors to populate metadata.> (28 July 2005), "Country Briefings: Israel", The Economist, http://www.economist.com/countries/Israel/profile.cfm?folder=History%20in%20brief, retrieved 30 December 2008
Israeli Foreign Ministry, Ottoman Rule, Jewish Virtual Library, http://www.jewishvirtuallibrary.org/jsource/History/Ottoman.html, retrieved 30 December 2008
The Heritage of the Great War / First World War. Graphic color photos, pictures and music
A multimedia history of World War I
British Path Online film archive containing extensive coverage of World War I
The Heritage of the Great War, Netherlands
The World War I Document Archive Wiki, Brigham Young University
Maps of Europe covering the history of World War I at omniatlas.com
The Great War Association  World War One Reenacting
Your Family History of World War I - Europeana 1914-1918(Crowd-sourcing project)
EFG1914 - Film digitisation project on First World War
An animated map "Europe plunges into war"
An animated map of Europe at the end of the war
.
#(`*World War II*`)#.
Soviet Union (194145)[nb 1]
United States (194145)
British Empire
 China (at war 193745)
France[nb 2]
Poland
Canada
Australia
New Zealand
South Africa
Yugoslavia (194145)
Greece (194045)
Norway (194045)
Netherlands (194045)
Belgium (194045)
Czechoslovakia
Brazil (194245)
...and others
Client and puppet states
 Philippines (194145)
 Mongolia (194145)
...and others
Germany
Japan (at war 193745)
Italy (194043)
Hungary (194045)
Romania (194144)
Bulgaria (194144)
Co-belligerents
Finland (194144)
Thailand (194245)
Iraq (1941)
Client and puppet states
Manchukuo
Italian Social Republic (194345)
Croatia (194145)
Slovakia
...and others
 Winston Churchill
 Franklin D. Roosevelt
 Joseph Stalin
 Chiang Kai-shek
...and others
 Adolf Hitler
 Hirohito
 Benito Mussolini
...and others
World WarII, or the Second World War (often abbreviated as WWII or WW2), was a global war that was underway by 1939 and ended in 1945. It involved a vast majority of the world's nationsincluding all of the great powerseventually forming two opposing military alliances: the Allies and the Axis. It was the most widespread war in history, with more than 100 million people serving in military units. In a state of "total war", the major participants placed their entire economic, industrial, and scientific capabilities at the service of the war effort, erasing the distinction between civilian and military resources. Marked by significant events involving the mass death of civilians, including the Holocaust and the only use of nuclear weapons in warfare, it resulted in 50 million to over 73 million fatalities. These deaths make World War II by far the deadliest conflict in all of human history.[1]
The Empire of Japan aimed to dominate East Asia and was already at war with the Republic of China in 1937,[2] but the world war is generally said to have begun on 1September 1939 with the invasion of Poland by Germany and subsequent declarations of war on Germany by France and Britain. From late 1939 to early 1941, in a series of campaigns and treaties, Germany formed the Axis alliance with Italy, conquering or subduing much of continental Europe. Following the MolotovRibbentrop Pact, Germany and the Soviet Union partitioned and annexed territories between themselves of their European neighbours, including Poland. The United Kingdom, with its empire and Commonwealth, remained the only major Allied force continuing the fight against the Axis, with battles taking place in North Africa as well as the long-running Battle of the Atlantic. In June 1941, the European Axis launched an invasion of the Soviet Union, giving a start to the largest land theatre of war in history, which tied down the major part of the Axis' military forces for the rest of the war. In December 1941, Japan joined the Axis, attacked the United States and European territories in the Pacific Ocean, and quickly conquered much of the West Pacific.
The Axis advance was stopped in 1942, after Japan lost a series of naval battles and European Axis troops were defeated in North Africa and, decisively, at Stalingrad. In 1943, with a series of German defeats in Eastern Europe, the Allied invasion of Italy, and American victories in the Pacific, the Axis lost the initiative and undertook strategic retreat on all fronts. In 1944, the Western Allies invaded France, while the Soviet Union regained all of its territorial losses and invaded Germany and its allies. During 1944 and 1945 the United States defeated the Japanese Navy and captured key West Pacific islands.
The war in Europe ended with the capture of Berlin by Soviet and Polish troops and the subsequent German unconditional surrender on 8May 1945. In August 1945, the United States dropped atomic bombs on the Japanese as the invasion of the Japanese archipelago became imminent, and the Soviet Union declared war on Japan, invading Manchuria. The Empire of Japan surrendered on 15August 1945, ending the war in Asia and cementing the total victory of the Allies over the Axis.
World WarII altered the political alignment and social structure of the world. The United Nations (UN) was established to foster international cooperation and prevent future conflicts. The great powers that were the victors of the warthe United States, Soviet Union, China, the United Kingdom, and Francebecame the permanent members of the United Nations Security Council.[3] The Soviet Union and the United States emerged as rival superpowers, setting the stage for the Cold War, which lasted for the next 46 years. Meanwhile, the influence of European great powers started to decline, while the decolonisation of Asia and Africa began. Most countries whose industries had been damaged moved towards economic recovery. Political integration, especially in Europe, emerged as an effort to stabilise postwar relations.
The start of the war is generally held to be 1September 1939, beginning with the German invasion of Poland; Britain and France declared war on Germany two days later. Other dates for the beginning of war include the start of the Second Sino-Japanese War on 7July 1937.[4][5]
Others follow British historian A. J. P. Taylor, who held that the Sino-Japanese War and war in Europe and its colonies occurred simultaneously and the two wars merged in 1941. This article uses the conventional dating. Other starting dates sometimes used for World WarII include the Italian invasion of Abyssinia on 3 October 1935.[6] British historian Antony Beevor views the beginning of the Second World War as the Japanese invasion of Manchuria in August 1939.[7]
The exact date of the war's end is also not universally agreed upon. It has been suggested that the war ended at the armistice of 14August 1945 (V-J Day), rather than the formal surrender of Japan (2September 1945); in some European histories, it ended on V-E Day (8May 1945). However, the Treaty of Peace with Japan was not signed until 1951,[8] and that with Germany not until 1990.[9]
World WarI radically altered the political map, with the defeat of the Central Powers, including Austria-Hungary, Germany and the Ottoman Empire; and the 1917 Bolshevik seizure of power in Russia. Meanwhile, existing victorious Allies such as France, Belgium, Italy, Greece and Romania gained territories, while new states were created out of the collapse of Austria-Hungary and the Russian and Ottoman Empires.
Despite the pacific movement in the aftermath of the war,[10][11] the losses still caused irredentist and revanchist nationalism to became important in a number of European states. Irredentism and revanchism were strong in Germany because of the significant territorial, colonial, and financial losses incurred by the Treaty of Versailles. Under the treaty, Germany lost around 13 percent of its home territory and all of its overseas colonies, while German annexation of other states was prohibited, reparations were imposed, and limits were placed on the size and capability of the country's armed forces.[12] Meanwhile, the Russian Civil War had led to the creation of the Soviet Union.[13]
The German Empire was dissolved in the German Revolution of 19181919, and a democratic government, later known as the Weimar Republic, was created. The interwar period saw strife between supporters of the new republic and hardline opponents on both the right and left. Although Italy as an Entente ally made some territorial gains, Italian nationalists were angered that the promises made by Britain and France to secure Italian entrance into the war were not fulfilled with the peace settlement. From 1922 to 1925, the Fascist movement led by Benito Mussolini seized power in Italy with a nationalist, totalitarian, and class collaborationist agenda that abolished representative democracy, repressed socialist, left wing and liberal forces, and pursued an aggressive foreign policy aimed at forcefully forging Italy as a world powera "New Roman Empire".[14]
In Germany, the Nazi Party led by Adolf Hitler sought to establish a fascist government in Germany. With the onset of the Great Depression, domestic support for the Nazis rose and, in 1933, Hitler was appointed Chancellor of Germany. In the aftermath of the Reichstag fire, Hitler created a totalitarian single-party state led by the Nazis.[15]
The Kuomintang (KMT) party in China launched a unification campaign against regional warlords and nominally unified China in the mid-1920s, but was soon embroiled in a civil war against its former Chinese communist allies.[16] In 1931, an increasingly militaristic Japanese Empire, which had long sought influence in China[17] as the first step of what its government saw as the country's right to rule Asia, used the Mukden Incident as a pretext to launch an invasion of Manchuria and establish the puppet state of Manchukuo.[18]
Too weak to resist Japan, China appealed to the League of Nations for help. Japan withdrew from the League of Nations after being condemned for its incursion into Manchuria. The two nations then fought several battles, in Shanghai, Rehe and Hebei, until the Tanggu Truce was signed in 1933. Thereafter, Chinese volunteer forces continued the resistance to Japanese aggression in Manchuria, and Chahar and Suiyuan.[19]
Adolf Hitler, after an unsuccessful attempt to overthrow the German government in 1923, became the Chancellor of Germany in 1933. He abolished democracy, espousing a radical, racially motivated revision of the world order, and soon began a massive rearmament campaign.[20] Meanwhile, France, to secure its alliance, allowed Italy a free hand in Ethiopia, which Italy desired as a colonial possession. The situation was aggravated in early 1935 when the Territory of the Saar Basin was legally reunited with Germany and Hitler repudiated the Treaty of Versailles, accelerated his rearmament programme and introduced conscription.[21]
Hoping to contain Germany, the United Kingdom, France and Italy formed the Stresa Front. The Soviet Union, concerned due to Germany's goals of capturing vast areas of eastern Europe, wrote a treaty of mutual assistance with France. Before taking effect though, the Franco-Soviet pact was required to go through the bureaucracy of the League of Nations, which rendered it essentially toothless.[22][23] However, in June 1935, the United Kingdom made an independent naval agreement with Germany, easing prior restrictions. The United States, concerned with events in Europe and Asia, passed the Neutrality Act in August.[24] In October, Italy invaded Ethiopia, and Germany was the only major European nation to support the invasion. Italy subsequently dropped its objections to Germany's goal of absorbing Austria.[25]
Hitler defied the Versailles and Locarno treaties by remilitarizing the Rhineland in March 1936. He received little response from other European powers.[26] When the Spanish Civil War broke out in July, Hitler and Mussolini supported the fascist and authoritarian Nationalist forces in their civil war against the Soviet-supported Spanish Republic. Both sides used the conflict to test new weapons and methods of warfare,[27] with the Nationalists winning the war in early 1939. In October 1936, Germany and Italy formed the Rome-Berlin Axis. A month later, Germany and Japan signed the Anti-Comintern Pact, which Italy would join in the following year. In China, after the Xi'an Incident the Kuomintang and communist forces agreed on a ceasefire in order to present a united front to oppose Japan.[28]
The Second ItaloAbyssinian War was a brief colonial war that began in October 1935 and ended in May 1936. The war was fought between the armed forces of the Kingdom of Italy (Regno d'Italia) and the armed forces of the Ethiopian Empire (also known as Abyssinia). The war resulted in the military occupation of Ethiopia and its annexation into the newly created colony of Italian East Africa (Africa Orientale Italiana, or AOI); in addition, it exposed the weakness of the League of Nations as a force to preserve peace. Both Italy and Ethiopia were member nations, but the League did nothing when the former clearly violated the League's own Article X.[29]
Germany and Italy lent support to the Nationalist insurrection led by general Francisco Franco in Spain. The Soviet Union supported the existing government, the Spanish Republic, which showed leftist tendencies. Both Germany and the USSR used this proxy war as an opportunity to test improved weapons and tactics. The deliberate Bombing of Guernica by the German Condor Legion in April 1937 contributed to widespread concerns that the next major war would include extensive terror bombing attacks on civilians.[30][31]
In July1937, Japan captured the former Chinese imperial capital of Beijing after instigating the Marco Polo Bridge Incident, which culminated in the Japanese campaign to invade all of China.[32] The Soviets quickly signed a non-aggression pact with China to lend materiel support, effectively ending China's prior cooperation with Germany. Generalissimo Chiang Kai-shek deployed his best army to defend Shanghai, but after three months of fighting, Shanghai fell. The Japanese continued to push the Chinese forces back, capturing the capital Nanking in December 1937 and committed the Nanking Massacre.
In June1938, Chinese forces stalled the Japanese advance by flooding the Yellow River; this manoeuvre bought time for the Chinese to prepare their defenses at Wuhan, but the city was taken by October.[33] Japanese military victories did not bring about the collapse of Chinese resistance that Japan had hoped to achieve, instead the Chinese government relocated inland to Chongqing and continued the war.[34]
On 29July 1938, the Japanese invaded the USSR and were checked at the Battle of Lake Khasan. Although the battle was a Soviet victory, the Japanese dismissed it as an inconclusive draw, and on 11May 1939 decided to move the Japanese-Mongolian border up to the Khalkhin Gol River by force. After initial successes the Japanese assault on Mongolia was checked by the Red Army that inflicted the first major defeat on the Japanese Kwantung Army.[35][36]
These clashes convinced some factions in the Japanese government that they should focus on conciliating the Soviet government to avoid interference in the war against China and instead turn their military attention southward, towards the US and European holdings in the Pacific, and also prevented the sacking of experienced Soviet military leaders such as Georgy Zhukov, who would later play a vital role in the defence of Moscow.[37]
In Europe, Germany and Italy were becoming bolder. In March 1938, Germany annexed Austria, again provoking little response from other European powers.[38] Encouraged, Hitler began pressing German claims on the Sudetenland, an area of Czechoslovakia with a predominantly ethnic German population; and soon France and Britain conceded this territory to Germany in the Munich Agreement, which was made against the wishes of the Czechoslovak government, in exchange for a promise of no further territorial demands.[39] Soon after that, however, Germany and Italy forced Czechoslovakia to cede additional territory to Hungary and Poland.[40] In March 1939, Germany invaded the remainder of Czechoslovakia and subsequently split it into the German Protectorate of Bohemia and Moravia and the pro-German client state, the Slovak Republic.[41]
Alarmed, and with Hitler making further demands on Danzig, France and Britain guaranteed their support for Polish independence; when Italy conquered Albania in April 1939, the same guarantee was extended to Romania and Greece.[42] Shortly after the Franco-British pledge to Poland, Germany and Italy formalised their own alliance with the Pact of Steel.[43]
In August 1939, Germany and the Soviet Union signed the MolotovRibbentrop Pact,[44] a non-aggression treaty with a secret protocol. The parties gave each other rights, "in the event of a territorial and political rearrangement," to "spheres of influence" (western Poland and Lithuania for Germany, and eastern Poland, Finland, Estonia, Latvia and Bessarabia for the USSR). It also raised the question of continuing Polish independence.[45]
On 1September 1939, Germany and Slovakiaa client state in 1939attacked Poland.[46] On 3September France and Britain, followed by the countries of the Commonwealth,[47] declared war on Germany but provided little support to Poland other than a small French attack into the Saarland.[48] Britain and France also began a naval blockade of Germany on 3September which aimed to damage the country's economy and war effort.[49][50]
On 17September, after signing a cease-fire with Japan, the Soviets also invaded Poland.[51] Poland's territory was divided between Germany and the Soviet Union, with Lithuania and Slovakia also receiving small shares. The Poles did not surrender; they established a Polish Underground State and an underground Home Army, and continued to fight with the Allies on all fronts outside Poland.[52]
About 100,000 Polish military personnel were evacuated to Romania and the Baltic countries; many of these soldiers later fought against the Germans in other theatres of the war.[53] Poland's Enigma codebreakers were also evacuated to France.[54] During this time, Japan launched its first attack against Changsha, a strategically important Chinese city, but was repulsed by late September.[55]
Following the invasion of Poland and a German-Soviet treaty governing Lithuania, the Soviet Union forced the Baltic countries to allow it to station Soviet troops in their countries under pacts of "mutual assistance."[56][57][58] Finland rejected territorial demands and was invaded by the Soviet Union in November 1939.[59] The resulting conflict ended in March 1940 with Finnish concessions.[60] France and the United Kingdom, treating the Soviet attack on Finland as tantamount to entering the war on the side of the Germans, responded to the Soviet invasion by supporting the USSR's expulsion from the League of Nations.[58]
In Western Europe, British troops deployed to the Continent, but in a phase nicknamed the Phoney War by the British and "Sitzkrieg" (sitting war) by the Germans, neither side launched major operations against the other until April 1940.[61] The Soviet Union and Germany entered a trade pact in February 1940, pursuant to which the Soviets received German military and industrial equipment in exchange for supplying raw materials to Germany to help circumvent the Allied blockade.[62]
In April 1940, Germany invaded Denmark and Norway to secure shipments of iron ore from Sweden, which the Allies were about to disrupt.[63] Denmark immediately capitulated, and despite Allied support, Norway was conquered within two months.[64] In May 1940 Britain invaded Iceland to preempt a possible German invasion of the island.[65] British discontent over the Norwegian campaign led to the replacement of Prime Minister Neville Chamberlain with Winston Churchill on 10May 1940.[66]
Germany invaded France, Belgium, the Netherlands, and Luxembourg on 10May 1940.[67] The Netherlands and Belgium were overrun using blitzkrieg tactics in a few days and weeks, respectively.[68] The French-fortified Maginot Line and the Allied forces in Belgium were circumvented by a flanking movement through the thickly wooded Ardennes region,[69] mistakenly perceived by French planners as an impenetrable natural barrier against armoured vehicles.[70]
British troops were forced to evacuate the continent at Dunkirk, abandoning their heavy equipment by early June.[71] On 10June, Italy invaded France, declaring war on both France and the United Kingdom;[72] twelve days later France surrendered and was soon divided into German and Italian occupation zones,[73] and an unoccupied rump state under the Vichy Regime. On 3July, the British attacked the French fleet in Algeria to prevent its possible seizure by Germany.[74]
In June, during the last days of the Battle of France, the Soviet Union forcibly annexed Estonia, Latvia and Lithuania,[57] and then annexed the disputed Romanian region of Bessarabia. Meanwhile, Nazi-Soviet political rapprochement and economic cooperation[75][76] gradually stalled,[77][78] and both states began preparations for war.[79]
With France neutralized, Germany began an air superiority campaign over Britain (the Battle of Britain) to prepare for an invasion.[80] The campaign failed, and the invasion plans were canceled by September.[80] Using newly captured French ports, the German Navy enjoyed success against an over-extended Royal Navy, using U-boats against British shipping in the Atlantic.[81] Italy began operations in the Mediterranean, initiating a siege of Malta in June, conquering British Somaliland in August, and making an incursion into British-held Egypt in September 1940. Japan increased its blockade of China in September by seizing several bases in the northern part of the now-isolated French Indochina.[82]
Throughout this period, the neutral United States took measures to assist China and the Western Allies. In November 1939, the American Neutrality Act was amended to allow "cash and carry" purchases by the Allies.[83] In 1940, following the German capture of Paris, the size of the United States Navy was significantly increased and, after the Japanese incursion into Indochina, the United States embargoed iron, steel and mechanical parts against Japan.[84] In September, the United States further agreed to a trade of American destroyers for British bases.[85] Still, a large majority of the American public continued to oppose any direct military intervention into the conflict well into 1941.[86]
At the end of September 1940, the Tripartite Pact united Japan, Italy and Germany to formalize the Axis Powers. The Tripartite Pact stipulated that any country, with the exception of the Soviet Union, not in the war which attacked any Axis Power would be forced to go to war against all three.[87] During this time, the United States continued to support the United Kingdom and China by introducing the Lend-Lease policy authorizing the provision of materiel and other items[88] and creating a security zone spanning roughly half of the Atlantic Ocean where the United States Navy protected British convoys.[89] As a result, Germany and the United States found themselves engaged in sustained naval warfare in the North and Central Atlantic by October 1941, even though the United States remained officially neutral.[90][91]
The Axis expanded in November 1940 when Hungary, Slovakia and Romania joined the Tripartite Pact.[92] Romania would make the large contribution into the Axis war against the USSR, partially to recapture territory ceded to the USSR, partially to pursue its leader Ion Antonescu's desire to combat communism.[93] In October 1940, Italy invaded Greece but within days was repulsed and pushed back into Albania, where a stalemate soon occurred.[94] In December 1940, British Commonwealth forces began counter-offensives against Italian forces in Egypt and Italian East Africa.[95] By early 1941, with Italian forces having been pushed back into Libya by the Commonwealth, Churchill ordered a dispatch of troops from Africa to bolster the Greeks.[96] The Italian Navy also suffered significant defeats, with the Royal Navy putting three Italian battleships out of commission by a carrier attack at Taranto, and neutralising several more warships at the Battle of Cape Matapan.[97]
The Germans soon intervened to assist Italy. Hitler sent German forces to Libya in February, and by the end of March they had launched an offensive against the diminished Commonwealth forces.[98] In under a month, Commonwealth forces were pushed back into Egypt with the exception of the besieged port of Tobruk.[99] The Commonwealth attempted to dislodge Axis forces in May and again in June, but failed on both occasions.[100] In early April, following Bulgaria's signing of the Tripartite Pact, the Germans intervened in the Balkans by invading Greece and Yugoslavia following a coup; here too they made rapid progress, eventually forcing the Allies to evacuate after Germany conquered the Greek island of Crete by the end of May.[101]
The Allies did have some successes during this time. In the Middle East, Commonwealth forces first quashed a coup in Iraq which had been supported by German aircraft from bases within Vichy-controlled Syria,[102] then, with the assistance of the Free French, invaded Syria and Lebanon to prevent further such occurrences.[103] In the Atlantic, the British scored a much-needed public morale boost by sinking the German flagship Bismarck.[104] Perhaps most importantly, during the Battle of Britain the Royal Air Force had successfully resisted the Luftwaffe's assault, and the German bombing campaign largely ended in May 1941.[105]
In Asia, despite several offensives by both sides, the war between China and Japan was stalemated by 1940. In order to increase pressure on China by blocking supply routes, and to better position Japanese forces in the event of a war with the Western powers, Japan had seized military control of southern Indochina[106] In August of that year, Chinese communists launched an offensive in Central China; in retaliation, Japan instituted harsh measures (the Three Alls Policy) in occupied areas to reduce human and material resources for the communists.[107] Continued antipathy between Chinese communist and nationalist forces culminated in armed clashes in January 1941, effectively ending their co-operation.[108]
With the situation in Europe and Asia relatively stable, Germany, Japan, and the Soviet Union made preparations. With the Soviets wary of mounting tensions with Germany and the Japanese planning to take advantage of the European War by seizing resource-rich European possessions in Southeast Asia, the two powers signed the SovietJapanese Neutrality Pact in April 1941.[109] By contrast, the Germans were steadily making preparations for an attack on the Soviet Union, amassing forces on the Soviet border.[110]
On 22June 1941, Germany, along with other European Axis members and Finland, invaded the Soviet Union in Operation Barbarossa. The primary targets of this surprise offensive[111] were the Baltic region, Moscow and Ukraine, with an ultimate goal of ending the 1941 campaign near the Arkhangelsk-Astrakhan line, connecting the Caspian and White Seas. Hitler's objectives were to eliminate the Soviet Union as a military power, exterminate Communism, generate Lebensraum ("living space")[112] by dispossessing the native population[113] and guarantee access to the strategic resources needed to defeat Germany's remaining rivals.[114]
Although the Red Army was preparing for strategic counter-offensives before the war,[115] Barbarossa forced the Soviet supreme command to adopt a strategic defence. During the summer, the Axis made significant gains into Soviet territory, inflicting immense losses in both personnel and materiel. By the middle of August, however, the German Army High Command decided to suspend the offensive of a considerably depleted Army Group Centre, and to divert the 2nd Panzer Group to reinforce troops advancing towards central Ukraine and Leningrad.[116] The Kiev offensive was overwhelmingly successful, resulting in encirclement and elimination of four Soviet armies, and made further advance into Crimea and industrially developed Eastern Ukraine (the First Battle of Kharkov) possible.[117]
The diversion of three quarters of the Axis troops and the majority of their air forces from France and the central Mediterranean to the Eastern Front[118] prompted Britain to reconsider its grand strategy.[119] In July, the UK and the Soviet Union formed a military alliance against Germany[120] The British and Soviets invaded Iran to secure the Persian Corridor and Iran's oil fields.[121] In August, the United Kingdom and the United States jointly issued the Atlantic Charter.[122]
By October, when Axis operational objectives in Ukraine and the Baltic region were achieved, with only the sieges of Leningrad[123] and Sevastopol continuing,[124] a major offensive against Moscow had been renewed. After two months of fierce battles, the German army almost reached the outer suburbs of Moscow, where the exhausted troops[125] were forced to suspend their offensive.[126] Large territorial gains were made by Axis forces, but their campaign had failed to achieve its main objectives: two key cities remained in Soviet hands, the Soviet capability to resist was not broken, and the Soviet Union retained a considerable part of its military potential. The blitzkrieg phase of the war in Europe had ended.[127]
By early December, freshly mobilised reserves[128] allowed the Soviets to achieve numerical parity with Axis troops.[129] This, as well as intelligence data that established a minimal number of Soviet troops in the East sufficient to prevent any attack by the Japanese Kwantung Army,[130] allowed the Soviets to begin a massive counter-offensive that started on 5 December along a 1,000 kilometres (620mi) front and pushed German troops 100250 kilometres (62160 mi) west.[131]
German successes in Europe encouraged Japan to increase pressure on European governments in south-east Asia. The Dutch government agreed to provide Japan oil supplies from the Dutch East Indies, while refusing to hand over political control of the colonies. Vichy France, by contrast, agreed to a Japanese occupation of French Indochina.[132] In July 1941, the United States, United Kingdom and other Western governments reacted to the seizure of Indochina with a freeze on Japanese assets, while the United States (which supplied 80 percent of Japan's oil[133]) responded by placing a complete oil embargo.[134] That meant Japan was essentially forced to choose between abandoning its ambitions in Asia and the prosecution of the war against China, or seizing the natural resources it needed by force; the Japanese military did not consider the former an option, and many officers considered the oil embargo an unspoken declaration of war.[135]
Japan planned to rapidly seize European colonies in Asia to create a large defensive perimeter stretching into the Central Pacific; the Japanese would then be free to exploit the resources of Southeast Asia while exhausting the over-stretched Allies by fighting a defensive war.[136] To prevent American intervention while securing the perimeter it was further planned to neutralise the United States Pacific Fleet from the outset.[137] On 7 December (8 December in Asian time zones), 1941, Japan attacked British and American holdings with near-simultaneous offensives against Southeast Asia and the Central Pacific.[138] These included an attack on the American fleet at Pearl Harbor, landings in Thailand and Malaya[138] and the battle of Hong Kong.
These attacks led the U.S., Britain, China, Australia and several other states to formally declare war on Japan, whereas the Soviet Union, being heavily involved in large scale hostilities with European Axis countries, preferred to maintain a neutrality agreement with Japan.[139][140] Germany and the Axis states responded by declaring war on the United States. In January, the United States, Britain, Soviet Union, China, and 22 smaller or exiled governments issued the Declaration by United Nations, thereby affirming the Atlantic Charter,[141] and taking an obligation not to sign separate peace with the Axis powers. From 1941, Stalin persistently asked Churchill, and then Roosevelt, to open a 'second front' in France.[142] The Eastern front became the major theatre of war in Europe and the many millions of Soviet casualties dwarfed the few hundred thousand of the Western Allies; Churchill and Roosevelt said they needed more preparation time, leading to claims they stalled to save Western lives at the expense of Soviet lives.[143]
Meanwhile, by the end of April 1942, Japan and its ally Thailand had almost fully conquered Burma, Malaya, the Dutch East Indies, Singapore,[144] and Rabaul, inflicting severe losses on Allied troops and taking a large number of prisoners. Despite a stubborn resistance in Corregidor, the Philippines was eventually captured in May 1942, forcing the government of the Philippine Commonwealth into exile.[145] Japanese forces also achieved naval victories in the South China Sea, Java Sea and Indian Ocean,[146] and bombed the Allied naval base at Darwin, Australia. The only real Allied success against Japan was a Chinese victory at Changsha in early January 1942.[147] These easy victories over unprepared opponents left Japan overconfident, as well as overextended.[148]
Germany retained the initiative as well. Exploiting dubious American naval command decisions, the German navy ravaged Allied shipping off the American Atlantic coast.[149] Despite considerable losses, European Axis members stopped a major Soviet offensive in Central and Southern Russia, keeping most territorial gains they achieved during the previous year.[150] In North Africa, the Germans launched an offensive in January, pushing the British back to positions at the Gazala Line by early February,[151] followed by a temporary lull in combat which Germany used to prepare for their upcoming offensives.[152]
In early May 1942, Japan initiated operations to capture Port Moresby by amphibious assault and thus sever communications and supply lines between the United States and Australia. The Allies, however, prevented the invasion by intercepting and defeating the Japanese naval forces in the Battle of the Coral Sea.[153] Japan's next plan, motivated by the earlier Doolittle Raid, was to seize Midway Atoll and lure American carriers into battle to be eliminated; as a diversion, Japan would also send forces to occupy the Aleutian Islands in Alaska.[154] In early June, Japan put its operations into action but the Americans, having broken Japanese naval codes in late May, were fully aware of the plans and force dispositions and used this knowledge to achieve a decisive victory at Midway over the Imperial Japanese Navy.[155]
With its capacity for aggressive action greatly diminished as a result of the Midway battle, Japan chose to focus on a belated attempt to capture Port Moresby by an overland campaign in the Territory of Papua.[156] The Americans planned a counter-attack against Japanese positions in the southern Solomon Islands, primarily Guadalcanal, as a first step towards capturing Rabaul, the main Japanese base in Southeast Asia.[157]
Both plans started in July, but by mid-September, the Battle for Guadalcanal took priority for the Japanese, and troops in New Guinea were ordered to withdraw from the Port Moresby area to the northern part of the island, where they faced Australian and United States troops in the Battle of Buna-Gona.[158] Guadalcanal soon became a focal point for both sides with heavy commitments of troops and ships in the battle for Guadalcanal. By the start of 1943, the Japanese were defeated on the island and withdrew their troops.[159] In Burma, Commonwealth forces mounted two operations. The first, an offensive into the Arakan region in late 1942, went disastrously, forcing a retreat back to India by May 1943.[160] The second was the insertion of irregular forces behind Japanese front-lines in February which, by the end of April, had achieved dubious results.[161]
On Germany's eastern front, the Axis defeated Soviet offensives in the Kerch Peninsula and at Kharkov,[162] and then launched their main summer offensive against southern Russia in June 1942, to seize the oil fields of the Caucasus and occupy Kuban steppe, while maintaining positions on the northern and central areas of the front. The Germans split the Army Group South into two groups: Army Group A struck lower Don River while Army Group B struck south-east to the Caucasus, towards Volga River.[163] The Soviets decided to make their stand at Stalingrad, which was in the path of the advancing German armies.
By mid-November the Germans had nearly taken Stalingrad in bitter street fighting when the Soviets began their second winter counter-offensive, starting with an encirclement of German forces at Stalingrad[164] and an assault on the Rzhev salient near Moscow, though the latter failed disastrously.[165] By early February 1943, the German Army had taken tremendous losses; German troops at Stalingrad had been forced to surrender[166] and the front-line had been pushed back beyond its position before the summer offensive. In mid-February, after the Soviet push had tapered off, the Germans launched another attack on Kharkov, creating a salient in their front line around the Russian city of Kursk.[167]
By November 1941, Commonwealth forces had launched a counter-offensive, Operation Crusader, in North Africa, and reclaimed all the gains the Germans and Italians had made.[168] In the West, concerns the Japanese might use bases in Vichy-held Madagascar caused the British to invade the island in early May 1942.[169] This success was offset soon after by an Axis offensive in Libya which pushed the Allies back into Egypt until Axis forces were stopped at El Alamein.[170] On the Continent, raids of Allied commandos on strategic targets, culminating in the disastrous Dieppe Raid,[171] demonstrated the Western Allies' inability to launch an invasion of continental Europe without much better preparation, equipment, and operational security.[172]
In August 1942, the Allies succeeded in repelling a second attack against El Alamein[173] and, at a high cost, managed to deliver desperately needed supplies to the besieged Malta.[174] A few months later, the Allies commenced an attack of their own in Egypt, dislodging the Axis forces and beginning a drive west across Libya.[175] This attack was followed up shortly after by an Anglo-American invasion of French North Africa, which resulted in the region joining the Allies.[176] Hitler responded to the French colony's defection by ordering the occupation of Vichy France;[176] although Vichy forces did not resist this violation of the armistice, they managed to scuttle their fleet to prevent its capture by German forces.[177] The now pincered Axis forces in Africa withdrew into Tunisia, which was conquered by the Allies in May 1943.[178]
Following the Guadalcanal Campaign, the Allies initiated several operations against Japan in the Pacific. In May 1943, Allied forces were sent to eliminate Japanese forces from the Aleutians,[179] and soon after began major operations to isolate Rabaul by capturing surrounding islands, and to breach the Japanese Central Pacific perimeter at the Gilbert and Marshall Islands.[180] By the end of March 1944, the Allies had completed both of these objectives, and additionally neutralised the major Japanese base at Truk in the Caroline Islands. In April, the Allies then launched an operation to retake Western New Guinea.[181]
In the Soviet Union, both the Germans and the Soviets spent the spring and early summer of 1943 making preparations for large offensives in Central Russia. On 4 July 1943, Germany attacked Soviet forces around the Kursk Bulge. Within a week, German forces had exhausted themselves against the Soviets' deeply echeloned and well-constructed defences[182][183] and, for the first time in the war, Hitler cancelled the operation before it had achieved tactical or operational success.[184] This decision was partially affected by the Western Allies' invasion of Sicily launched on 9 July which, combined with previous Italian failures, resulted in the ousting and arrest of Mussolini later that month.[185]
On 12 July 1943, the Soviets launched their own counter-offensives, thereby dispelling any hopes of the German Army for victory or even stalemate in the east. The Soviet victory at Kursk heralded the downfall of German superiority,[186] giving the Soviet Union the initiative on the Eastern Front.[187][188] The Germans attempted to stabilise their eastern front along the hastily fortified Panther-Wotan line, however, the Soviets broke through it at Smolensk and by the Lower Dnieper Offensives.[189]
In early September 1943, the Western Allies invaded the Italian mainland, following an Italian armistice with the Allies.[190] Germany responded by disarming Italian forces, seizing military control of Italian areas,[191] and creating a series of defensive lines.[192] German special forces then rescued Mussolini, who then soon established a new client state in German occupied Italy named the Italian Social Republic.[193] The Western Allies fought through several lines until reaching the main German defensive line in mid-November.[194]
German operations in the Atlantic also suffered. By May 1943, as Allied counter-measures became increasingly effective, the resulting sizable German submarine losses forced a temporary halt of the German Atlantic naval campaign.[195] In November 1943, Franklin D. Roosevelt and Winston Churchill met with Chiang Kai-shek in Cairo[196] and then with Joseph Stalin in Tehran.[197] The former conference determined the post-war return of Japanese territory,[196] while the latter included agreement that the Western Allies would invade Europe in 1944 and that the Soviet Union would declare war on Japan within three months of Germany's defeat.[197]
From November 1943, during the seven-week Battle of Changde, the Chinese forced Japan to fight a costly war of attrition, while awaiting Allied relief.[198][199] In January 1944, the Allies launched a series of attacks in Italy against the line at Monte Cassino and attempted to outflank it with landings at Anzio.[200] By the end of January, a major Soviet offensive expelled German forces from the Leningrad region,[201] ending the longest and most lethal siege in history.
The following Soviet offensive was halted on the pre-war Estonian border by the German Army Group North aided by Estonians hoping to re-establish national independence. This delay slowed subsequent Soviet operations in the Baltic Sea region.[202] By late May 1944, the Soviets had liberated Crimea, largely expelled Axis forces from Ukraine, and made incursions into Romania, which were repulsed by the Axis troops.[203] The Allied offensives in Italy had succeeded and, at the expense of allowing several German divisions to retreat, on 4June Rome was captured.[204]
The Allies experienced mixed fortunes in mainland Asia. In March 1944, the Japanese launched the first of two invasions, an operation against British positions in Assam, India,[205] and soon besieged Commonwealth positions at Imphal and Kohima.[206] In May 1944, British forces mounted a counter-offensive that drove Japanese troops back to Burma,[206] and Chinese forces that had invaded northern Burma in late 1943 besieged Japanese troops in Myitkyina.[207] The second Japanese invasion attempted to destroy China's main fighting forces, secure railways between Japanese-held territory and capture Allied airfields.[208] By June, the Japanese had conquered the province of Henan and begun a renewed attack against Changsha in the Hunan province.[209]
On 6 June 1944 (known as D-Day), after three years of Soviet pressure,[143] the Western Allies invaded northern France. After reassigning several Allied divisions from Italy, they also attacked southern France.[210] These landings were successful, and led to the defeat of the German Army units in France. Paris was liberated by the local resistance assisted by the Free French Forces on 25 August[211] and the Western Allies continued to push back German forces in Western Europe during the latter part of the year. An attempt to advance into northern Germany spearheaded by a major airborne operation in the Netherlands ended with a failure.[212] After that, the Western Allies slowly pushed into Germany, unsuccessfully trying to cross the Rur river in a large offensive. In Italy the Allied advance also slowed down, when they ran into the last major German defensive line.
On 22 June, the Soviets launched a strategic offensive in Belarus (known as "Operation Bagration") that resulted in the almost complete destruction of the German Army Group Centre.[213] Soon after that, another Soviet strategic offensive forced German troops from Western Ukraine and Eastern Poland. The successful advance of Soviet troops prompted resistance forces in Poland to initiate several uprisings, though the largest of these, in Warsaw, as well as a Slovak Uprising in the south, were not assisted by the Soviets and were put down by German forces.[214] The Red Army's strategic offensive in eastern Romania cut off and destroyed the considerable German troops there and triggered a successful coup d'tat in Romania and in Bulgaria, followed by those countries' shift to the Allied side.[215]
In September 1944, Soviet Red Army troops advanced into Yugoslavia and forced the rapid withdrawal of the German Army Groups E and F in Greece, Albania and Yugoslavia to rescue them from being cut off.[216] By this point, the Communist-led Partisans under Marshal Josip Broz Tito, who had led an increasingly successful guerrilla campaign against the occupation since 1941, controlled much of the territory of Yugoslavia and were engaged in delaying efforts against the German forces further south. In northern Serbia, the Red Army, with limited support from Bulgarian forces, assisted the Partisans in a joint liberation of the capital city of Belgrade on 20 October. A few days later, the Soviets launched a massive assault against German-occupied Hungary that lasted until the fall of Budapest in February 1945.[217] In contrast with impressive Soviet victories in the Balkans, the bitter Finnish resistance to the Soviet offensive in the Karelian Isthmus denied the Soviets occupation of Finland and led to the signing of Soviet-Finnish armistice on relatively mild conditions,[218][219] with a subsequent shift to the Allied side by Finland.
By the start of July, Commonwealth forces in Southeast Asia had repelled the Japanese sieges in Assam, pushing the Japanese back to the Chindwin River[220] while the Chinese captured Myitkyina. In China, the Japanese were having greater successes, having finally captured Changsha in mid-June and the city of Hengyang by early August.[221] Soon after, they further invaded the province of Guangxi, winning major engagements against Chinese forces at Guilin and Liuzhou by the end of November[222] and successfully linking up their forces in China and Indochina by the middle of December.[223]
In the Pacific, American forces continued to press back the Japanese perimeter. In mid-June 1944 they began their offensive against the Mariana and Palau islands, and decisively defeated Japanese forces in the Battle of the Philippine Sea. These defeats led to the resignation of Japanese Prime Minister Tj and provided the United States with air bases to launch intensive heavy bomber attacks on the Japanese home islands. In late October, American forces invaded the Filipino island of Leyte; soon after, Allied naval forces scored another large victory during the Battle of Leyte Gulf, one of the largest naval battles in history.[224]
On 16 December 1944, Germany attempted its last desperate measure for success on the Western Front by using most of its remaining reserves to launch a massive counter-offensive in the Ardennes to attempt to split the Western Allies, encircle large portions of Western Allied troops and capture their primary supply port at Antwerp in order to prompt a political settlement.[225] By January, the offensive had been repulsed with no strategic objectives fulfilled.[225] In Italy, the Western Allies remained stalemated at the German defensive line. In mid-January 1945, the Soviets attacked in Poland, pushing from the Vistula to the Oder river in Germany, and overran East Prussia.[226] On 4 February, U.S., British, and Soviet leaders met for the Yalta Conference. They agreed on the occupation of post-war Germany,[227] and on when the Soviet Union would join the war against Japan.[228]
In February, the Soviets invaded Silesia and Pomerania, while Western Allies entered Western Germany and closed to the Rhine river. By March, the Western Allies crossed the Rhine north and south of the Ruhr, encircling the German Army Group B,[229] while the Soviets advanced to Vienna. In early April, the Western Allies finally pushed forward in Italy and swept across Western Germany, while Soviet forces stormed Berlin in late April; the two forces linked up on Elbe river on 25 April. On 30 April 1945, the Reichstag was captured, signalling the military defeat of the Third Reich.[230]
Several changes in leadership occurred during this period. On 12 April, U.S. President Roosevelt died and was succeeded by Harry Truman. Benito Mussolini was killed by Italian partisans on 28 April.[231] Two days later, Hitler committed suicide, and was succeeded by Grand Admiral Karl Dnitz.[232]
German forces surrendered in Italy on 29 April. The German instrument of surrender was signed on 7 May in Reims,[233] and ratified on 8 May in Berlin.[234] German Army Group Centre resisted in Prague until 11 May.[235]
In the Pacific theatre, American forces accompanied by the forces of the Philippine Commonwealth advanced in the Philippines, clearing Leyte by the end of April 1945. They landed on Luzon in January 1945 and captured Manila in March following a battle which reduced the city to ruins. Fighting continued on Luzon, Mindanao and other islands of the Philippines until the end of the war.[236]
In May 1945, Australian troops landed in Borneo, overrunning the oilfields there. British, American and Chinese forces defeated the Japanese in northern Burma in March, and the British pushed on to reach Rangoon by 3 May.[237] Chinese forces started to counterattack in Battle of West Hunan that occurred between 6 April and 7 June 1945. American forces also moved towards Japan, taking Iwo Jima by March, and Okinawa by the end of June.[238] American bombers destroyed Japanese cities, and American submarines cut off Japanese imports.[239]
On 11 July, the Allied leaders met in Potsdam, Germany. They confirmed earlier agreements about Germany,[240] and reiterated the demand for unconditional surrender of all Japanese forces by Japan, specifically stating that "the alternative for Japan is prompt and utter destruction".[241] During this conference the United Kingdom held its general election, and Clement Attlee replaced Churchill as Prime Minister.[242]
As Japan continued to ignore the Potsdam terms, the United States dropped atomic bombs on the Japanese cities of Hiroshima and Nagasaki in early August. Between the two bombings, the Soviets, pursuant to the Yalta agreement, invaded Japanese-held Manchuria, and quickly defeated the Kwantung Army, which was the largest Japanese fighting force.[243][244] The Red Army also captured Sakhalin Island and the Kuril Islands. On 15 August 1945 Japan surrendered, with the surrender documents finally signed aboard the deck of the American battleship USS Missouri on 2 September 1945, ending the war.[233]
American and Soviet troops meet in April 1945, east of the Elbe River.
A devastated Berlin street in the city centre post Battle of Berlin, taken 3 July 1945.
Atomic explosion at Nagasaki, 9 August 1945.
The Allies established occupation administrations in Austria and Germany. The former became a neutral state, non-aligned with any political bloc. The latter was divided into western and eastern occupation zones controlled by the Western Allies and the USSR, accordingly. A denazification program in Germany led to the prosecution of Nazi war criminals and the removal of ex-Nazis from power, although this policy moved towards amnesty and re-integration of ex-Nazis into West German society.[245]
Germany lost a quarter of its pre-war (1937) territory, the eastern territories: Silesia, Neumark and most of Pomerania were taken over by Poland; East Prussia was divided between Poland and the USSR, followed by the expulsion of the 9 million Germans from these provinces, as well as of 3 million Germans from the Sudetenland in Czechoslovakia, to Germany. By the 1950s, every fifth West German was a refugee from the east. The USSR also took over the Polish provinces east of the Curzon line (from which 2 million Poles were expelled),[246] Eastern Romania,[247][248] and part of eastern Finland[249] and three Baltic states.[250][251]
In an effort to maintain peace,[252] the Allies formed the United Nations, which officially came into existence on 24 October 1945,[253] and adopted The Universal Declaration of Human Rights in 1948, as a common standard for all member nations.[254] The great powers that were the victors of the warthe United States, Soviet Union, China, Britain, and Franceformed the permanent members of the UN's Security Council.[3] The five permanent members remain so to the present, although there have been two seat changes, between the Republic of China and the People's Republic of China in 1971, and between the Soviet Union and its successor state, the Russian Federation, following the dissolution of the Soviet Union. The alliance between the Western Allies and the Soviet Union had begun to deteriorate even before the war was over.[255]
Germany had been de facto divided, and two independent states, Federal Republic of Germany and German Democratic Republic[256] were created within the borders of Allied and Soviet occupation zones, accordingly. The rest of Europe was also divided onto Western and Soviet spheres of influence.[257] Most eastern and central European countries fell into the Soviet sphere, which led to establishment of Communist led regimes, with full or partial support of the Soviet occupation authorities. As a result, Poland, Hungary,[258] Czechoslovakia,[259] Romania, Albania,[260] and East Germany became Soviet Satellite states. Communist Yugoslavia conducted a fully independent policy causing tension with the USSR.[261]
Post-war division of the world was formalised by two international military alliances, the United States-led NATO and the Soviet-led Warsaw Pact;[262] the long period of political tensions and military competition between them, the Cold War, would be accompanied by an unprecedented arms race and proxy wars.[263]
In Asia, the United States led the occupation of Japan and administrated Japan's former islands in the Western Pacific, while the Soviets annexed Sakhalin and the Kuril Islands.[264] Korea, formerly under Japanese rule, was divided and occupied by the US in the South and the Soviet Union in the North between 1945 and 1948. Separate republics emerged on both sides of the 38th parallel in 1948, each claiming to be the legitimate government for all of Korea, which led ultimately to the Korean War.[265]
In China, nationalist and communist forces resumed the civil war in June 1946. Communist forces were victorious and established the People's Republic of China on the mainland, while nationalist forces retreated to Taiwan in 1949.[266] In the Middle East, the Arab rejection of the United Nations Partition Plan for Palestine and the creation of Israel marked the escalation of the Arab-Israeli conflict. While European colonial powers attempted to retain some or all of their colonial empires, their losses of prestige and resources during the war rendered this unsuccessful, leading to decolonisation.[267][268]
The global economy suffered heavily from the war, although participating nations were affected differently. The US emerged much richer than any other nation; it had a baby boom and by 1950 its gross domestic product per person was much higher than that of any of the other powers and it dominated the world economy.[269][270] The UK and US pursued a policy of industrial disarmament in Western Germany in the years 19451948.[271] Due to international trade interdependencies this led to European economic stagnation and delayed European recovery for several years.[272][273]
Recovery began with the mid 1948 currency reform in Western Germany, and was sped up by the liberalization of European economic policy that the Marshall plan (19481951) both directly and indirectly caused.[274][275] The post 1948 West German recovery has been called the German economic miracle.[276] Also the Italian[277][278] and French economies rebounded.[279] By contrast, the United Kingdom was in a state of economic ruin,[280] and continued relative economic decline for decades.[281]
The Soviet Union, despite enormous human and material losses, also experienced rapid increase in production in the immediate post-war era.[282] Japan experienced incredibly rapid economic growth, becoming one of the most powerful economies in the world by the 1980s.[283] China returned to its pre-war industrial production by 1952.[284]
World map of colonisation in 1945. With the end of the war, the wars of national liberation ensued, leading to the creation of Israel, together with the decolonisation of Asia and Africa.
The Supreme Commanders on 5 June 1945 in Berlin:
Estimates for the total casualties of the war vary, because many deaths went unrecorded. Most suggest that some 60 million people died in the war, including about 20 million soldiers and 40 million civilians.[285][286][287] Many civilians died because of disease, starvation, massacres, bombing and deliberate genocide. The Soviet Union lost around 27 million people during the war,[288] including 8.7 million military and 19 million civilian deaths. The largest portion of military dead were ethnic Russians (5,756,000), followed by ethnic Ukrainians (1,377,400).[289] One of every four Soviet citizens was killed or wounded in that war.[290] Germany sustained 5.3 million military losses, mostly on the Eastern Front and during the final battles in Germany.[291]
Of the total deaths in World War II, approximately 85 percentmostly Soviet and Chinesewere on the Allied side and 15 percent on the Axis side. Many of these deaths were caused by war crimes committed by German and Japanese forces in occupied territories. An estimated 11 million[292] to 17[293] civilians died as a direct or indirect result of Nazi ideological policies, including the systematic genocide of around six million Jews during The Holocaust along with a further five million Roma, homosexuals as well as Slavs and other ethnic and minority groups.[294]
Roughly 7.5 million civilians died in China under Japanese occupation.[295] Hundreds of thousands (varying estimates) of ethnic Serbs, along with gypsies and Jews, were murdered by the Axis-aligned Croatian Ustae in Yugoslavia,[296] with retribution-related killings of Croatian civilians just after the war ended.
The best-known Japanese atrocity was the Nanking Massacre, in which several hundred thousand Chinese civilians were raped and murdered.[297] Between 3 million to more than 10 million civilians, mostly Chinese, were killed by the Japanese occupation forces.[298] Mitsuyoshi Himeta reported 2.7 million casualties occurred during the Sank Sakusen. General Yasuji Okamura implemented the policy in Heipei and Shantung.[299]
The Axis forces employed limited biological and chemical weapons. The Italians used mustard gas during their conquest of Abyssinia,[300] while the Imperial Japanese Army used a variety of such weapons during their invasion and occupation of China (see Unit 731)[301][302] and in early conflicts against the Soviets.[303] Both the Germans and Japanese tested such weapons against civilians[304] and, in some cases, on prisoners of war.[305]
While many of the Axis's acts were brought to trial in the world's first international tribunals,[306] incidents caused by the Allies were not. Examples of such Allied actions include population transfers in the Soviet Union and Japanese American internment in the United States; the Operation Keelhaul,[307] expulsion of Germans after World War II, rape during the occupation of Germany; the Soviet Union's Katyn massacre, for which Germans faced counter-accusations of responsibility. Large numbers of famine deaths can also be partially attributed to the war, such as the Bengal famine of 1943 and the Vietnamese famine of 194445.[308]
It has been suggested by some historians, e.g. Jrg Friedrich, that the mass-bombing of civilian areas in enemy territory, including Tokyo and most notably the German cities of Dresden, Hamburg and Cologne by Western Allies, which resulted in the destruction of more than 160 cities and the deaths of more than 600,000 German civilians be considered as war crimes.[309]
The Nazis were responsible for The Holocaust, the killing of approximately six million Jews (overwhelmingly Ashkenazim), as well as two million ethnic Poles and four million others who were deemed "unworthy of life" (including the disabled and mentally ill, Soviet prisoners of war, homosexuals, Freemasons, Jehovah's Witnesses, and Romani) as part of a programme of deliberate extermination. About 12 million, most of whom were Eastern Europeans, were employed in the German war economy as forced labourers.[310]
In addition to Nazi concentration camps, the Soviet gulags (labour camps) led to the death of citizens of occupied countries such as Poland, Lithuania, Latvia, and Estonia, as well as German prisoners of war (POWs) and even Soviet citizens who had been or were thought to be supporters of the Nazis.[311] Sixty percent of Soviet POWs of the Germans died during the war.[312] Richard Overy gives the number of 5.7 million Soviet POWs. Of those, 57 percent died or were killed, a total of 3.6million.[313] Soviet ex-POWs and repatriated civilians were treated with great suspicion as potential Nazi collaborators, and some of them were sent to the Gulag upon being checked by the NKVD.[314]
Japanese prisoner-of-war camps, many of which were used as labour camps, also had high death rates. The International Military Tribunal for the Far East found the death rate of Western prisoners was 27.1 percent (for American POWs, 37 percent),[315] seven times that of POWs under the Germans and Italians.[316] While 37,583 prisoners from the UK, 28,500 from the Netherlands, and 14,473 from United States were released after the surrender of Japan, the number for the Chinese was only 56.[317]
According to historian Zhifen Ju, at least five million Chinese civilians from northern China and Manchukuo were enslaved between 1935 and 1941 by the East Asia Development Board, or Kain, for work in mines and war industries. After 1942, the number reached 10 million.[318] The U.S. Library of Congress estimates that in Java, between 4 and 10 million romusha (Japanese: "manual laborers"), were forced to work by the Japanese military. About 270,000 of these Javanese laborers were sent to other Japanese-held areas in South East Asia, and only 52,000 were repatriated to Java.[319]
On 19 February 1942, Roosevelt signed Executive Order 9066, interning thousands of Japanese, Italians, German Americans, and some emigrants from Hawaii who fled after the bombing of Pearl Harbor for the duration of the war. The U.S. and Canadian governments interned 150,000 Japanese-Americans,[320][321] In addition, 14,000 German and Italian residents of the U.S. who had been assessed as being security risks were also interned.[322]
In accordance with the Allied agreement made at the Yalta Conference millions of POWs and civilians were used as forced labor by the Soviet Union.[323] In Hungary's case, Hungarians were forced to work for the Soviet Union until 1955.[324]
In Europe, before the outbreak of the war, the Allies had significant advantages in both population and economics. In 1938, the Western Allies (United Kingdom, France, Poland and British Dominions) had a 30 percent larger population and a 30 percent higher gross domestic product than the European Axis (Germany and Italy); if colonies are included, it then gives the Allies more than a 5:1 advantage in population and nearly 2:1 advantage in GDP.[325] In Asia at the same time, China had roughly six times the population of Japan, but only an 89 percent higher GDP; this is reduced to three times the population and only a 38 percent higher GDP if Japanese colonies are included.[325]
Though the Allies' economic and population advantages were largely mitigated during the initial rapid blitzkrieg attacks of Germany and Japan, they became the decisive factor by 1942, after the United States and Soviet Union joined the Allies, as the war largely settled into one of attrition.[326] While the Allies' ability to out-produce the Axis is often attributed to the Allies having more access to natural resources, other factors, such as Germany and Japan's reluctance to employ women in the labour force,[327][328] Allied strategic bombing,[329][330] and Germany's late shift to a war economy[331] contributed significantly. Additionally, neither Germany nor Japan planned to fight a protracted war, and were not equipped to do so.[332][333] To improve their production, Germany and Japan used millions of slave labourers;[334] Germany used about 12 million people, mostly from Eastern Europe,[310] while Japan pressed more than 18 million people in Far East Asia.[318][319]
In Europe, occupation came under two very different forms. In Western, Northern and Central Europe (France, Norway, Denmark, the Low Countries, and the annexed portions of Czechoslovakia) Germany established economic policies through which it collected roughly 69.5 billion reichmarks (27.8 billion US Dollars) by the end of the war; this figure does not include the sizable plunder of industrial products, military equipment, raw materials and other goods.[335] Thus, the income from occupied nations was over 40 percent of the income Germany collected from taxation, a figure which increased to nearly 40 percent of total German income as the war went on.[336]
In the East, the much hoped for bounties of Lebensraum were never attained as fluctuating front-lines and Soviet scorched earth policies denied resources to the German invaders.[337] Unlike in the West, the Nazi racial policy encouraged excessive brutality against what it considered to be the "inferior people" of Slavic descent; most German advances were thus followed by mass executions.[338] Although resistance groups did form in most occupied territories, they did not significantly hamper German operations in either the East[339] or the West[340] until late 1943.
In Asia, Japan termed nations under its occupation as being part of the Greater East Asia Co-Prosperity Sphere, essentially a Japanese hegemony which it claimed was for purposes of liberating colonised peoples.[341] Although Japanese forces were originally welcomed as liberators from European domination in many territories, their excessive brutality turned local public opinions against them within weeks.[342] During Japan's initial conquest it captured 4,000,000 barrels (640,000m3) of oil (~5.5105 tonnes) left behind by retreating Allied forces, and by 1943 was able to get production in the Dutch East Indies up to 50 million barrels (~6.810^6t), 76 percent of its 1940 output rate.[342]
Aircraft were used for reconnaissance, as fighters, bombers and ground-support, and each role was advanced considerably. Innovation included airlift (the capability to quickly move limited high-priority supplies, equipment and personnel);[343] and of strategic bombing (the bombing of civilian areas to destroy industry and morale).[344] Anti-aircraft weaponry also advanced, including defences such as radar and surface-to-air artillery, such as the German 88 mm gun. The use of the jet aircraft was pioneered, and though late introduction meant it had little impact, it led to jets becoming standard in worldwide air forces.[345]
Advances were made in nearly every aspect of naval warfare, most notably with aircraft carriers and submarines. Although at the start of the war aeronautical warfare had relatively little success, actions at Taranto, Pearl Harbor, the South China Sea and the Coral Sea established the carrier as the dominant capital ship in place of the battleship.[346][347][348]
In the Atlantic, escort carriers proved to be a vital part of Allied convoys, increasing the effective protection radius and helping to close the Mid-Atlantic gap.[349] Carriers were also more economical than battleships due to the relatively low cost of aircraft[350] and their not requiring to be as heavily armoured.[351] Submarines, which had proved to be an effective weapon during the First World War[352] were anticipated by all sides to be important in the second. The British focused development on anti-submarine weaponry and tactics, such as sonar and convoys, while Germany focused on improving its offensive capability, with designs such as the Type VII submarine and wolfpack tactics.[353] Gradually, improving Allied technologies such as the Leigh light, hedgehog, squid, and homing torpedoes proved victorious.
Land warfare changed from the static front lines of World WarI to increased mobility and combined arms. The tank, which had been used predominantly for infantry support in the First World War, had evolved into the primary weapon.[354] In the late 1930s, tank design was considerably more advanced than it had been during World War I,[355] and advances continued throughout the war in increasing speed, armour and firepower.
At the start of the war, most commanders thought enemy tanks should be met by tanks with superior specifications.[356] This idea was challenged by the poor performance of the relatively light early tank guns against armour, and German doctrine of avoiding tank-versus-tank combat. This, along with Germany's use of combined arms, were among the key elements of their highly successful blitzkrieg tactics across Poland and France.[354] Many means of destroying tanks, including indirect artillery, anti-tank guns (both towed and self-propelled), mines, short-ranged infantry antitank weapons, and other tanks were utilised.[356] Even with large-scale mechanisation, infantry remained the backbone of all forces,[357] and throughout the war, most infantry were equipped similarly to World War I.[358]
The portable machine gun spread, a notable example being the German MG42, and various submachine guns which were suited to close combat in urban and jungle settings.[358] The assault rifle, a late war development incorporating many features of the rifle and submachine gun, became the standard postwar infantry weapon for most armed forces.[359][360]
Most major belligerents attempted to solve the problems of complexity and security presented by using large codebooks for cryptography with the use of ciphering machines, the most well known being the German Enigma machine.[361] SIGINT (signals intelligence) was the countering process of decryption, with the notable examples being the Allied breaking of Japanese naval codes[362] and British Ultra, which was derived from methodology given to Britain by the Polish Cipher Bureau, which had been decoding Enigma for seven years before the war.[363] Another aspect of military intelligence was the use of deception, which the Allies used to great effect, such as in operations Mincemeat and Bodyguard.[362][364] Other technological and engineering feats achieved during, or as a result of, the war include the world's first programmable computers (Z3, Colossus, and ENIAC), guided missiles and modern rockets, the Manhattan Project's development of nuclear weapons, operations research and the development of artificial harbours and oil pipelines under the English Channel.[365]
American Boeing B-17E. The Allies lost 160,000 airmen and 33,700 planes during the air war over Europe.[366]
German U-995 Type VIIC. Between 1939 and 1945, 3,500 Allied merchant ships were sunk at a cost of 783 German U-boats.
Soviet T-34, the most-produced tank of the war. Over 57,000 were built by 1945.
                      
Dissolution of the Third Reich
Creation of the United Nations
Emergence of the United States and the Soviet Union as superpowers
Beginning of the Cold War (more...)
Campaigns
Countries
Equipment
Lists
Outline
Timeline
Portal
Category
v
t
e
1 Chronology
2 Background
3 Pre-war events

3.1 Italian invasion of Ethiopia (1935)
3.2 Spanish Civil War (1936-39)
3.3 Japanese invasion of China (1937)
3.4 Japanese invasion of the Soviet Union and Mongolia (1938)
3.5 European occupations and agreements


3.1 Italian invasion of Ethiopia (1935)
3.2 Spanish Civil War (1936-39)
3.3 Japanese invasion of China (1937)
3.4 Japanese invasion of the Soviet Union and Mongolia (1938)
3.5 European occupations and agreements
4 Course of the war

4.1 War breaks out in Europe (1939)
4.2 Axis advances
4.3 War becomes global (1941)
4.4 Axis advance stalls (1942)
4.5 Allies gain momentum (1943)
4.6 Allies close in (1944)
4.7 Axis collapse, Allied victory (1945)


4.1 War breaks out in Europe (1939)
4.2 Axis advances
4.3 War becomes global (1941)
4.4 Axis advance stalls (1942)
4.5 Allies gain momentum (1943)
4.6 Allies close in (1944)
4.7 Axis collapse, Allied victory (1945)
5 Aftermath
6 Impact

6.1 Casualties and war crimes
6.2 Concentration camps and slave work
6.3 Home fronts and production
6.4 Occupation
6.5 Advances in technology and warfare


6.1 Casualties and war crimes
6.2 Concentration camps and slave work
6.3 Home fronts and production
6.4 Occupation
6.5 Advances in technology and warfare
7 See also
8 Footnotes
9 Citations
10 References
11 External links
3.1 Italian invasion of Ethiopia (1935)
3.2 Spanish Civil War (1936-39)
3.3 Japanese invasion of China (1937)
3.4 Japanese invasion of the Soviet Union and Mongolia (1938)
3.5 European occupations and agreements
4.1 War breaks out in Europe (1939)
4.2 Axis advances
4.3 War becomes global (1941)
4.4 Axis advance stalls (1942)
4.5 Allies gain momentum (1943)
4.6 Allies close in (1944)
4.7 Axis collapse, Allied victory (1945)
6.1 Casualties and war crimes
6.2 Concentration camps and slave work
6.3 Home fronts and production
6.4 Occupation
6.5 Advances in technology and warfare






American and Soviet troops meet in April 1945, east of the Elbe River.









A devastated Berlin street in the city centre post Battle of Berlin, taken 3 July 1945.









Atomic explosion at Nagasaki, 9 August 1945.









World map of colonisation in 1945. With the end of the war, the wars of national liberation ensued, leading to the creation of Israel, together with the decolonisation of Asia and Africa.









The Supreme Commanders on 5 June 1945 in Berlin:









American Boeing B-17E. The Allies lost 160,000 airmen and 33,700 planes during the air war over Europe.[366]









German U-995 Type VIIC. Between 1939 and 1945, 3,500 Allied merchant ships were sunk at a cost of 783 German U-boats.









Soviet T-34, the most-produced tank of the war. Over 57,000 were built by 1945.



Book: World War II
List of World War II battles
List of World War II military operations
World War II in popular culture
Death rates in the 20th century
Apocalypse: The Second World War (2009), a six-part French documentary by Daniel Costelle and Isabelle Clarke about World War II
Battlefield (TV series) documentary series initially issued in 19945 that explores many important World War II battles
BBC History of World WarII, a television series, initially issued from 1989 to 2005.
The World at War (1974), a 26-part Thames Television series that covers most aspects of World WarII from many points of view. It includes interviews with many key figures including Karl Dnitz, Albert Speer, and Anthony Eden.
Adamthwaite, Anthony P (1992). The Making of the Second World War. New York: Routledge. ISBN0-415-90716-0.
Brody, J Kenneth (1999). The Avoidable War: Pierre Laval and the Politics of Reality, 19351936. New Brunswick, NJ: Transaction Publishers. p.4. ISBN0-7658-0622-3.
Bullock, A. (1962). Hitler: A Study in Tyranny. London: Penguin Books. ISBN0-14-013564-2.
Busky, Donald F (2002). Communism in History and Theory: Asia, Africa, and the Americas. Westport, CT: Praeger Publishers. ISBN0-275-97733-1.
Davies, Norman (2008). No Simple Victory: World War II in Europe, 19391945. New York: Penguin Group. ISBN0-14-311409-3.
Glantz, David M. (2001). "The Soviet-German War 194145 Myths and Realities: A Survey Essay". Archived from the original on 17 June 2011. http://web.archive.org/web/20110617094931/http://www.strom.clemson.edu/publications/sg-war41-45.pdf.
Graham, Helen (2005). The Spanish Civil War: A Very Short Introduction. Oxford and New York: Oxford University Press. ISBN0-19-280377-8.
Holland, J (2006). Together We Stand, Turning the Tide in the West:North Africa, 194243. London: Harper Collins.
Hsiung, James Chieh (1992). China's Bitter Victory: The War with Japan, 19371945. M.E. Sharpe. ISBN1-56324-246-X.
Jowett, Philip S.; Andrew, Stephen (2002). The Japanese Army, 193145. Osprey Publishing. ISBN1-84176-353-5.
Kantowicz, Edward R (1999). The rage of nations. Wm. B. Eerdmans Publishing. ISBN0-8028-4455-3.
Kershaw, Ian (2001). Hitler, 19361945: Nemesis. W. W. Norton & Company. ISBN0-393-32252-1.
Kitson, Alison (2001). Germany 18581990: Hope, Terror, and Revival. Oxford: Oxford University Press. ISBN978-0-19-913417-5.
Mandelbaum, Michael (1988). The Fate of Nations: The Search for National Security in the Nineteenth and Twentieth Centuries. Cambridge University Press. p.96. ISBN0-521-35790-X.
Murray, Williamson; Millett, Allan Reed (2001). A War to Be Won: Fighting the Second World War. Harvard University Press. ISBN0-674-00680-1.
Myers, Ramon; Peattie, Mark (1987). The Japanese Colonial Empire, 18951945. Princeton University Press. ISBN0-691-10222-8.
Preston, Peter (1998). 'Pacific Asia in the global system: an introduction, Wiley-Blackwell. Oxford: Blackwell. p.104. ISBN0-631-20238-2.
Record, Jeffery (2005) (PDF). Appeasement Reconsidered: Investigating the Mythology of the 1930s. DIANE Publishing. p.50. ISBN1-58487-216-0. http://www.strategicstudiesinstitute.army.mil/pdffiles/PUB622.pdf. Retrieved 15 November 2009.
Shaw, Anthony (2000). World War II Day by Day. MBI Publishing Company. ISBN0-7603-0939-6.
Smith, Winston; Steadman, Ralph (2004). All Riot on the Western Front, Volume 3. Last Gasp. ISBN0-86719-616-5.
Weinberg, Gerhard L. (1995). A World at Arms: A Global History of World War II. Cambridge University Press. ISBN0-521-55879-4.
Weinberg, Gerhard L. (2005). A World at Arms: A Global History of World War II (Second ed.). Cambridge: Cambridge University Press. ISBN0-521-85316-8.
Zalampas, Michael (1989). Adolf Hitler and the Third Reich in American magazines, 19231939. Bowling Green University Popular Press. ISBN0-87972-462-5.
West Point Maps of the European War
West Point Maps of the Asian-Pacific War
Radio News From 1938 to 1945
World War II Propaganda Leaflet Archive
The Art of War Online Exhibition at the UK National Archive
Deutsche Welle special section on World War II created by a German public broadcaster on both the war and the world 60 years after.
Atlas of the World Battle Fronts (July 1943 to August 1945)
.
#(`*Civil war*`)#.
A civil war is a war between organized groups within the same nation state or republic,[1] or, less commonly, between two countries created from a formerly united nation state.[2] The aim of one side may be to take control of the country or a region, to achieve independence for a region, or to change government policies.[1] The term is a calque of the Latin bellum civile which was used to refer to the various civil wars of the Roman Republic in the 1st century BC.
A civil war is a high-intensity conflict, often involving regular armed forces, that is sustained, organized and large-scale. Civil wars may result in large numbers of casualties and the consumption of significant resources.[3]
Civil wars since the end of World War II have lasted on average just over four years, a dramatic rise from the one-and-a-half year average of the 1900-1944 period. While the rate of emergence of new civil wars has been relatively steady since the mid-19th century, the increasing length of those wars resulted in increasing numbers of wars ongoing at any one time. For example, there were no more than five civil wars underway simultaneously in the first half of the 20th century, while over 20 concurrent civil wars were occurring at the end of the Cold War, before a significant decrease as conflicts strongly associated with the superpower rivalry came to an end. Since 1945, civil wars have resulted in the deaths of over 25 million people, as well as the forced displacement of millions more. Civil wars have further resulted in economic collapse; Burma (Myanmar), Uganda and Angola are examples of nations that were considered to have promising futures before being engulfed in civil wars.[4]
James Fearon, a scholar of civil wars at Stanford University, defines a civil war as "a violent conflict within a country fought by organized groups that aim to take power at the center or in a region, or to change government policies".[1] Ann Hironaka further specifies that one side of a civil war is the state.[3] The intensity at which a civil disturbance becomes a civil war is contested by academics. Some political scientists define a civil war as having more than 1000 casualties,[1] while others further specify that at least 100 must come from each side.[5] The Correlates of War, a dataset widely used by scholars of conflict, classifies civil wars as having over 1000 war-related casualties per year of conflict. This rate is a small fraction of the millions killed in the Second Sudanese Civil War and Cambodian Civil War, for example, but excludes several highly publicized conflicts, such as The Troubles of Northern Ireland and the struggle of the African National Congress in Apartheid-era South Africa.[3]
Based on the 1000 casualties per year criterion, there were 213 civil wars from 1816 to 1997, 104 of which occurred from 1944 to 1997.[3] If one uses the less-stringent 1000 casualties total criterion, there were over 90 civil wars between 1945 and 2007, with 20 ongoing civil wars as of 2007.[1]
The Geneva Conventions do not specifically define the term "civil war", nevertheless they do outline the responsibilities of parties in "armed conflict not of an international character". This includes civil wars, however no specific definition of civil war is provided in the text of the Conventions.
Nevertheless the International Committee of the Red Cross has sought to provide some clarification through its commentaries on the Geneva Conventions, noting that the Conventions are "so general, so vague, that many of the delegations feared that it might be taken to cover any act committed by force of arms". Accordingly the commentaries provide for different 'conditions' on which the application of the Geneva Convention would depend, the commentary however points out that these should not be interpreted as rigid conditions. The conditions listed by the ICRC in its commentary are as follows:[6][7]
(1) That the Party in revolt against the de jure Government possesses an organized military force, an authority responsible for its acts, acting within a determinate territory and having the means of respecting and ensuring respect for the Convention.
(2) That the legal Government is obliged to have recourse to the regular military forces against insurgents organized as military and in possession of a part of the national territory.
(3) (a) That the de jure Government has recognized the insurgents as belligerents; or (b) That it has claimed for itself the rights of a belligerent; or (c) That it has accorded the insurgents recognition as belligerents for the purposes only of the present Convention; or (d) That the dispute has been admitted to the agenda of the Security Council or the General Assembly of the United Nations as being a threat to international peace, a breach of the peace, or an act of aggression.
(4) (a) That the insurgents have an organization purporting to have the characteristics of a State. (b) That the insurgent civil authority exercises de facto authority over the population within a determinate portion of the national territory. (c) That the armed forces act under the direction of an organized authority and are prepared to observe the ordinary laws of war. (d) That the insurgent civil authority agrees to be bound by the provisions of the Convention.
Scholars investigating the cause of civil war are attracted by two opposing theories, greed versus grievance. Roughly stated: are conflicts caused by who people are, whether that be defined in terms of ethnicity, religion or other social affiliation, or do conflicts begin because it is in the economic best interests of individuals and groups to start them? Scholarly analysis supports the conclusion that economic and structural factors are more important than those of identity in predicting occurrences of civil war.[8]
A comprehensive studies of civil war was carried out by a team from the World Bank in the early 21st century. The study framework, which came to be called the Collier-Hoeffler Model, examined 78 five-year increments when civil war occurred from 1960 to 1999, as well as 1,167 five-year increments of "no civil war" for comparison, and subjected the data set to regression analysis to see the effect of various factors. The factors that were shown to have a statistically significant effect on the chance that a civil war would occur in any given five-year period were:[9]
A high proportion of primary commodities in national exports significantly increases the risk of a conflict. A country at "peak danger", with commodities comprising 32% of gross domestic product, has a 22% risk of falling into civil war in a given five-year period, while a country with no primary commodity exports has a 1% risk. When disaggregated, only petroleum and non-petroleum groupings showed different results: a country with relatively low levels of dependence on petroleum exports is at slightly less risk, while a high-level of dependence on oil as an export results in slightly more risk of a civil war than national dependence on another primary commodity. The authors of the study interpreted this as being the result of the ease by which primary commodities may be extorted or captured compared to other forms of wealth, for example, it is easy to capture and control the output of a gold mine or oil field compared to a sector of garment manufacturing or hospitality services.[10]
A second source of finance is national diasporas, which can fund rebellions and insurgencies from abroad. The study found that statistically switching the size of a country's diaspora from the smallest found in the study to the largest resulted in a sixfold increase in the chance of a civil war.[10]
Higher male secondary school enrollment, per capita income and economic growth rate all had significant effects on reducing the chance of civil war. Specifically, a male secondary school enrollment 10% above the average reduced the chance of a conflict by about 3%, while a growth rate 1% higher than the study average resulted in a decline in the chance of a civil war of about 1%. The study interpreted these three factors as proxies for earnings foregone by rebellion, and therefore that lower foregone earnings encourages rebellion.[10] Phrased another way: young males (who make up the vast majority of combatants in civil wars) are less likely to join a rebellion if they are getting an education and/or have a comfortable salary, and can reasonably assume that they will prosper in the future.
Low per capita income has been proposed as a cause for grievance, prompting armed rebellion. However, for this to be true, one would expect economic inequality to also be a significant factor in rebellions, which it is not. The study therefore concluded that the economic model of opportunity cost better explained the findings.[9]
High levels of population dispersion and, to a lesser extent, the presence of mountainous terrain increased the chance of conflict. Both of these factors favor rebels, as a population dispersed outward toward the borders is harder to control than one concentrated in a central region, while mountains offer terrain where rebels can seek sanctuary.[10]
Most proxies for "grievance" - the theory that civil wars begin because of issues of identity, rather than economics - were statistically insignificant, including economic equality, political rights, ethnic polarization and religious fractionalization. Only ethnic dominance, the case where the largest ethnic group comprises a majority of the population, increased the risk of civil war. A country characterized by ethnic dominance has nearly twice the chance of a civil war. However, the combined effects of ethnic and religious fractionalization, i.e. the more chance that any two randomly chosen people will be from separate ethnic or religious groups the less chance of a civil war, were also significant and positive, as long as the country avoided ethnic dominance. The study interpreted this as stating that minority groups are more likely to rebel if they feel that they are being dominated, but that rebellions are more likely to occur the more homogeneous the population and thus more cohesive the rebels. These two factors may thus be seen as mitigating each other in many cases.[11]
The various factors contributing to the risk of civil war rise increase with population size. The risk of a civil war rises approximately proportionately with the size of a country's population.[9]
The more time that has elapsed since the last civil war, the less likely it is that a conflict will recur. The study had two possible explanations for this: one opportunity-based and the other grievance-based. The elapsed time may represent the depreciation of whatever capital the rebellion was fought over and thus increase the opportunity cost of restarting the conflict. Alternatively, elapsed time may represent the gradual process of healing of old hatreds. The study found that the presence of diaspora substantially reduced the positive effect of time, as the funding from diasporas offsets the depreciation of rebellion-specific capital.[11]
Evolutionary psychologist Satoshi Kanazawa has argued that an important cause of intergroup conflict may be the relative availability of women of reproductive age. He found that polygyny greatly increased the frequency of civil wars but not interstate wars.[12] Gleditsch et al. did not find a relationship between ethnic groups with polygyny and increased frequency of civil wars but nations having legal polygamy may have more civil wars. They argued that misogyny is a better explanation than polygyny. They found that increased women's rights were are associated with less civil wars and that legal polygamy had no effect after womens rights were controlled for.[13]
Ann Hironaka, author of Neverending Wars, divides the modern history of civil wars into the pre-19th century, 19th century to early 20th century, and late 20th century. In 19th century Europe, the length of civil wars fell significantly, largely due to the nature of the conflicts as battles for the power center of the state, the strength of centralized governments, and the normally quick and decisive intervention by other states to support the government. Following World War II the duration of civil wars grew past the norm of the pre-19th century, largely due to weakness of the many postcolonial states and the intervention by major powers on both sides of conflict. The most obvious commonality to civil wars are that they occur in fragile states.[14]
Civil wars through the 19th century to early 20th century tended to be short; the average length of a civil war between 1900 and 1944 was one and half years.[15] The state itself was the obvious center of authority in the majority of cases, and the civil wars were thus fought for control of the state. This meant that whoever had control of the capital and the military could normally crush resistance. If a rebellion failed to quickly seize the capital and control of the military for itself, it was normally doomed to a quick destruction. For example, the fighting associated with the 1871 Paris Commune occurred almost entirely in Paris, and ended quickly once the military sided with the government.[16]
The power of non-state actors resulted in a lower value placed on sovereignty in the 18th and 19th centuries, which further reduced the number of civil wars. For example, the pirates of the Barbary Coast were recognized as de facto states because of their military power. The Barbary pirates thus had no need to rebel against the Ottoman Empire, who were their nominal state government, to gain recognition for their sovereignty. Conversely, states such as Virginia and Massachusetts in the United States of America did not have sovereign status, but had significant political and economic independence coupled with weak federal control, reducing the incentive to secede.[17]
The two major global ideologies, monarchism and democracy, led to several civil wars. However, a bi-polar world, divided between the two ideologies, did not develop, largely due the dominance of monarchists through most of the period. The monarchists would thus normally intervene in other countries to stop democratic movements taking control and forming democratic governments, which were seen by monarchists as being both dangerous and unpredictable. The Great Powers, defined in the 1815 Congress of Vienna as the United Kingdom, Habsburg Austria, Prussia, France, and Russia, would frequently coordinate interventions in other nations' civil wars, nearly always on the side of the incumbent government. Given the military strength of the Great Powers, these interventions were nearly always decisive and quickly ended the civil wars.[18]
There were several exceptions from the general rule of quick civil wars during this period. The American Civil War (18611865) was unusual for at least two reasons: it was fought around regional identities, rather than political ideologies, and it was ended through a war of attrition, rather than over a decisive battle over control of the capital, as was the norm. The Spanish Civil War (19361939) was exceptional because both sides of the war received support from intervening great powers: Germany, Italy, and Portugal supported opposition leader Francisco Franco, while France and Russia supported the government[19] (see proxy war).
In the 1990s, about twenty civil wars were occurring concurrently during an average year, a rate about ten times the historical average since the 19th century. However, the rate of new civil wars had not increased appreciably; the drastic rise in the number of ongoing wars after World War II was a result of the tripling of the average duration of civil wars to over four years.[20] This increase was a result of the increased number of states, the fragility of states formed after 1945, the decline in interstate war, and the Cold War rivalry.[21]
Following World War II, the major European powers divested themselves of their colonies at an increasing rate: the number of ex-colonial states jumped from about 30 to almost 120 after the war. The rate of state formation leveled off in the 1980s, at which point few colonies remained.[22] More states also meant more states in which to have long civil wars. Hironaka statistically measures the impact of the increased number of ex-colonial states as increasing the post-WWII incidence of civil wars by +165% over the pre-1945 number.[23]
While the new ex-colonial states appeared to follow the blueprint of the idealized state - centralized government, territory enclosed by defined borders, and citizenry with defined rights -, as well as accessories such as a national flag, an anthem, a seat at the United Nations and an official economic policy, they were in actuality far weaker than the Western states they were modeled after.[24] In Western states, the structure of governments closely matched states' actual capabilities, which had been arduously developed over centuries. The development of strong administrative structures, in particular those related to extraction of taxes, is closely associated with the intense warfare between predatory European states in the 17th and 18th centuries, or in Charles Tilly's famous formulation: "War made the state and the state made war".[25] For example, the formation of the modern states of Germany and Italy in the 19th century is closely associated with the wars of expansion and consolidation led by Prussia and Sardinia, respectively.[25] The Western process of forming effective and impersonal bureaucracies, developing efficient tax systems, and integrating national territory continued into the 20th century. Nevertheless, Western states that survived into the latter half of the 20th century were considered "strong" by simple reason that they had managed to develop the institutional structures and military capability required to survive predation by their fellow states.
In sharp contrast, decolonization was an entirely different process of state formation. Most imperial powers had not foreseen a need to prepare their colonies for independence; for example, Britain had given limited self-rule to India and Sri Lanka, while treating British Somaliland as little more than a trading post, while all major decisions for French colonies were made in Paris and Belgium prohibited any self-government up until it suddenly granted independence to its colonies in 1960. Like Western states of previous centuries, the new ex-colonies lacked autonomous bureaucracies, which would make decisions based on the benefit to society as a whole, rather than respond to corruption and nepotism to favor a particular interest group. In such a situation, factions manipulate the state to benefit themselves or, alternatively, state leaders use the bureaucracy to further their own self-interest. The lack of credible governance was compounded by the fact that most colonies were economic loss-makers at independence, lacking both a productive economic base and a taxation system to effectively extract resources from economic activity. Among the rare states profitable at decolonization was India, to which scholars credibly argue that Uganda, Malaysia and Angola may be included. Neither did imperial powers make territorial integration a priority, and may have discouraged nascent nationalism as a danger to their rule. Many newly independent states thus found themselves impoverished, with minimal administrative capacity in a fragmented society, while faced with the expectation of immediately meeting the demands of a modern state.[26] Such states are considered "weak" or "fragile". The "strong"-"weak" categorization is not the same as "Western"-"non-Western", as some Latin American states like Argentina and Brazil and Middle Eastern states like Egypt and Israel are considered to have "strong" administrative structures and economic infrastructure.[27]
Historically, the international community would have targeted weak states for territorial absorption or colonial domination or, alternatively, such states would fragment into pieces small enough to be effectively administered and secured by a local power. However, international norms towards sovereignty changed in the wake of WWII in ways that support and maintain the existence of weak states. Weak states are given de jure sovereignty equal to that of other states, even when they do not have de facto sovereignty or control of their own territory, including the privileges of international diplomatic recognition and an equal vote in the United Nations. Further, the international community offers development aid to weak states, which helps maintain the facade of a functioning modern state by giving the appearance that the state is capable of fulfilling its implied responsibilities of control and order.[28] The formation of a strong international law regime and norms against territorial aggression is strongly associated with the dramatic drop in the number of interstate wars, though it has also been attributed to the effect of the Cold War or to the changing nature of economic development. Consequently, military aggression that results in territorial annexation became increasingly likely to prompt international condemnation, diplomatic censure, a reduction in international aid or the introduction of economic sanction, or, as in the case of 1990 invasion of Kuwait by Iraq, international military intervention to reverse the territorial aggression.[29] Similarly, the international community has largely refused to recognize secessionist regions, while keeping some states such as Cyprus and Taiwan in diplomatic recognition limbo. While there is not a large body of academic work examining the relationship, Hironaka's statistical study found a correlation that suggests that every major international anti-secessionist declaration increased the number of ongoing civil wars by +10%, or a total +114% from 1945 to 1997.[30] The diplomatic and legal protection given by the international community, as well as economic support to weak governments and discouragement of secession, thus had the unintended effect of encouraging civil wars.
There has been an enormous amount of international intervention in civil wars since 1945 that served to extend wars. While intervention has been practiced since the international system has existed, its nature changed substantially. It became common for both the state and opposition group to receive foreign support, allowing wars to continue well past the point when domestic resources had been exhausted. Superpowers, such as the European great powers, had always felt no compunction in intervening in civil wars that affected their interests, while distant regional powers such as the United States could declare the interventionist Monroe Doctrine of 1821 for events in its Central American "backyard". However, the large population of weak states after 1945 allowed intervention by former colonial powers, regional powers and neighboring states who themselves often had scarce resources. On average, a civil war with interstate intervention was 300% longer than those without. When disaggregated, a civil war with intervention on only one side is 156% longer, while intervention on both sides lengthens the average civil war by an addition 92%. If one of the intervening states was a superpower, a civil war is extended a further 72%; a conflict such as the Angolan Civil War, in which there is two-sided foreign intervention, including by a superpower (actually, two superpowers in the case of Angola), would be 538% longer on average than a civil war without any international intervention.[31]
The Cold War (19451989) provided a global network of material and ideological support that perpetuated civil wars, which were mainly fought in weak ex-colonial states, rather than the relatively strong states that were aligned with the Warsaw Pact and North Atlantic Treaty Organization. In some cases, superpowers would superimpose Cold War ideology onto local conflicts, while in others local actors using Cold War ideology would attract the attention of a superpower to obtain support. Using a separate statistical evaluation than used above for interventions, civil wars that included pro- or anti-communist forces lasted 141% longer than the average non-Cold War conflict, while a Cold War civil war that attracted superpower intervention resulted in wars typically lasting over three times as long as other civil wars. Conversely, the end of the Cold War marked by the fall of the Berlin Wall in 1989 resulted in a reduction in the duration of Cold War civil wars of 92% or, phrased another way, a roughly ten-fold increase in the rate of resolution of Cold War civil wars. Lengthy Cold War-associated civil conflicts that ground to a halt include the wars of Guatemala (19601996), El Salvador (19791991) and Nicaragua (19701990).[32]
Prehistoric
Ancient
Medieval
Gunpowder
Industrial
Modern
First
Second
Third
Fourth
Air
Information
Land
Sea
Space
Armor
Artillery
Biological
Cavalry
Conventional
Chemical
Electronic
Infantry
Nuclear
Psychological
Unconventional
Aerial
Battle
Cavalry
Charge
Cover
Counter-insurgency
Foxhole
Guerrilla warfare
Morale
Siege
Tactical objective
Blitzkrieg
Deep battle
Maneuver warfare
Operational manoeuvre group
Attrition
Deception
Defensive
Offensive
Goal
Naval
Containment
Economic warfare
Military science
Philosophy of war
Strategic studies
Total war
Command and control
Doctrine
Education and training
Engineers
Intelligence
Ranks
Staff
Technology and equipment
Materiel
Supply chain management
Asymmetric warfare
Cold war
Mercenary
Military operation
Operations research
Principles of War
Proxy war
Trench warfare
War crimes
Battles
Commanders
Operations
Sieges
Wars
War crimes
Weapons
Writers
v
t
e
1 Definition

1.1 Further definitions


1.1 Further definitions
2 Causes of civil war in the Collier-Hoeffler Model
3 Other causes
4 Duration of civil wars

4.1 Civil wars in the 19th and early 20th centuries
4.2 Civil wars since 1945
4.3 Effect of the Cold War


4.1 Civil wars in the 19th and early 20th centuries
4.2 Civil wars since 1945
4.3 Effect of the Cold War
5 See also
6 References
7 Bibliography
8 External links
1.1 Further definitions
4.1 Civil wars in the 19th and early 20th centuries
4.2 Civil wars since 1945
4.3 Effect of the Cold War
The Logic of Violence in Civil War
War of Independence (disambiguation)
Wars of national liberation
Ali, Taisier Mohamed Ahmed and Robert O. Matthews, eds. Civil Wars in Africa: roots and resolution (1999), 322 pages
Mats Berdal and David M. Malone, Greed and Grievance: Economic Agendas in Civil Wars (Lynne Rienner, 2000).
Paul Collier, Breaking the Conflict Trap: civil war and development policy World Bank (2003) - 320 pages
Collier, Paul; Sambanis, Nicholas, eds. (2005). Understanding Civil War:Evidence and Analysis. 1: Africa. Washington, DC: The World Bank. ISBN978-0-8213-6047-7. http://books.google.com/books?id=OnGQQVuIBjgC&pg=PP1&dq=Understanding+Civil+War:Evidence+and+Analysis&cd=1#v=onepage&q=&f=false.
Collier, Paul; Sambanis, Nicholas, eds. (2005). Understanding Civil War:Evidence and Analysis. 2: Europe, Central Asia, and Other Regions. Washington, DC: The World Bank. ISBN978-0-8213-6049-1. http://books.google.com/books?id=yNTQ-BDLPxIC&pg=PP1&dq=Understanding+Civil+War:Evidence+and+Analysis&cd=2#v=onepage&q=&f=false.
Stathis Kalyvas, "'New' and 'Old' Civil Wars: A Valid Distinction?" World Politics 54, no. 1 (2001): 99-118.
David Lake and Donald Rothchild, eds. The International Spread of Ethnic Conflict: Fear, Diffusion, and Escalation (Princeton University Press, 1996).
Roy Licklider, "The Consequences of Negotiated Settlements in Civil Wars, 1945--1993," American Political Science Review 89, no. 3 (summer 1995): pp 681690.
Andrew Mack, "Civil War: Academic Research and the Policy Community," Journal of Peace Research 39, no. 5 (2002): pp.515525.
David T. Mason and Patrick 3. Fett, "How Civil Wars End: A Rational Choice Approach," Journal of Conflict Resolution 40, no. 4 (fall 1996): 546-568.
Patrick M. Regan. Civil Wars and Foreign Powers: Outside Intervention in Intrastate Conflict (2000) 172 pages
Stephen John and others., eds. Ending Civil Wars: The Implementation of Peace Agreements (2002), 729 pages
Monica Duffy Toft, The Geography of Ethnic Violence: Identity, Interests, and the Indivisibility of Territory (Princeton NJ: Princeton University Press, 2003). ISBN 0-691-12383-7.
Barbara F. Walter, Committing to Peace: The Successful Settlement of Civil Wars (Princeton University Press, 2002),
Elisabeth Jean Wood; "Civil Wars: What We Don't Know," Global Governance, Vol. 9, 2003 pp 247+ online version
Royal Air Force Doctrine - The Nature of War and Armed Conflict
"What makes a civil war?", BBC News, 20 April 2006
.
#(`*Nuclear weapon*`)#.
A nuclear weapon is an explosive device that derives its destructive force from nuclear reactions, either fission or a combination of fission and fusion. Both reactions release vast quantities of energy from relatively small amounts of matter. The first fission ("atomic") bomb test released the same amount of energy as approximately 20,000 tons of TNT. The first thermonuclear ("hydrogen") bomb test released the same amount of energy as approximately 10,000,000 tons of TNT.[1]
A modern thermonuclear weapon weighing little more than 2,400 pounds (1,100kg) can produce an explosive force comparable to the detonation of more than 1.2 million tons (1.1 million tonnes) of TNT.[2] Thus, even a small nuclear device no larger than traditional bombs can devastate an entire city by blast, fire and radiation. Nuclear weapons are considered weapons of mass destruction, and their use and control have been a major focus of international relations policy since their debut.
Only two nuclear weapons have been used in the course of warfare, both by the United States near the end of World War II. On 6 August 1945, a uranium gun-type fission bomb code-named "Little Boy" was detonated over the Japanese city of Hiroshima. Three days later, on 9 August, a plutonium implosion-type fission bomb code-named "Fat Man" was exploded over Nagasaki, Japan. These two bombings resulted in the deaths of approximately 200,000 Japanese peoplemostly civiliansfrom acute injuries sustained from the explosions.[3] The role of the bombings in Japan's surrender, and their ethical status, remain the subject of scholarly and popular debate.
Since the bombings of Hiroshima and Nagasaki, nuclear weapons have been detonated on over two thousand occasions for testing purposes and demonstrations. Only a few nations possess such weapons or are suspected of seeking them. The only countries known to have detonated nuclear weaponsand that acknowledge possessing such weaponsare (chronologically by date of first test) the United States, the Soviet Union (succeeded as a nuclear power by Russia), the United Kingdom, France, the People's Republic of China, India, Pakistan, and North Korea. In addition, Israel is also widely believed to possess nuclear weapons, though it does not acknowledge having them.[4][5][6] One state, South Africa, fabricated nuclear weapons in the past, but as its apartheid regime was coming to an end it disassembled its arsenal, acceded to the NPT and accepted full-scope international safeguards.[7]
The Federation of American Scientists estimates there are more than 19,000 nuclear warheads in the world as of 2012, with around 4,400 of them kept in "operational" status, ready for use.[4]
There are two basic types of nuclear weapons: those that derive the majority of their energy from nuclear fission reactions alone, and those that use fission reactions to begin nuclear fusion reactions that produce a large amount of the total energy output.
All existing nuclear weapons derive some of their explosive energy from nuclear fission reactions. Weapons whose explosive output is exclusively from fission reactions are commonly referred to as atomic bombs or atom bombs (abbreviated as A-bombs). This has long been noted as something of a misnomer, as their energy comes from the nucleus of the atom.
In fission weapons, a mass of fissile material (enriched uranium or plutonium) is assembled into a supercritical massthe amount of material needed to start an exponentially growing nuclear chain reactioneither by shooting one piece of sub-critical material into another (the "gun" method) or by compressing a sub-critical sphere of material using chemical explosives to many times its original density (the "implosion" method). The latter approach is considered more sophisticated than the former and only the latter approach can be used if the fissile material is plutonium.
A major challenge in all nuclear weapon designs is to ensure that a significant fraction of the fuel is consumed before the weapon destroys itself. The amount of energy released by fission bombs can range from the equivalent of less than a ton of TNT upwards of 500,000 tons (500 kilotons) of TNT.[8]
All fission reactions necessarily generate fission products, the radioactive remains of the atomic nuclei split by the fission reactions. Many fission products are either highly radioactive (but short-lived) or moderately radioactive (but long-lived), and as such are a serious form of radioactive contamination if not fully contained. Fission products are the principal radioactive component of nuclear fallout.
The most commonly used fissile materials for nuclear weapons applications have been uranium-235 and plutonium-239. Less commonly used has been uranium-233. Neptunium-237 and a number of isotopes of americium may be usable for nuclear explosives as well, but it is not clear that this has ever been implemented, and even their plausible use in nuclear weapons is a matter of scientific dispute.[9]
The other basic type of nuclear weapon produces a large proportion of its energy in nuclear fusion reactions. Such fusion weapons are generally referred to as thermonuclear weapons or more colloquially as hydrogen bombs (abbreviated as H-bombs), as they rely on fusion reactions between isotopes of hydrogen (deuterium and tritium). All such weapons derive a significant portion, and sometimes a majority, of their energy from fission. This is because a fission weapon is required as a "trigger" for the fusion reactions, and the fusion reactions can themselves trigger additional fission reactions.[10]
Only six countriesUnited States, Russia, United Kingdom, People's Republic of China, France and Indiahave conducted thermonuclear weapon tests. (Whether India has detonated a "true", multi-staged thermonuclear weapon is controversial.)[11] All thermonuclear weapons are considered much more difficult to successfully design and execute than primitive fission weapons. Almost all of the nuclear weapons deployed today use the thermonuclear design because it is more efficient.
Thermonuclear bombs work by using the energy of a fission bomb to compress and heat fusion fuel. In the Teller-Ulam design, which accounts for all multi-megaton yield hydrogen bombs, this is accomplished by placing a fission bomb and fusion fuel (tritium, deuterium, or lithium deuteride) in proximity within a special, radiation-reflecting container. When the fission bomb is detonated, gamma rays and X-rays emitted first compress the fusion fuel, then heat it to thermonuclear temperatures. The ensuing fusion reaction creates enormous numbers of high-speed neutrons, which can then induce fission in materials not normally prone to it, such as depleted uranium. Each of these components is known as a "stage", with the fission bomb as the "primary" and the fusion capsule as the "secondary". In large, megaton-range hydrogen bombs, about half of the yield comes from the final fissioning of depleted uranium.[8]
Virtually all thermonuclear weapons deployed today use the "two-stage" design described above, but it is possible to add additional fusion stageseach stage igniting a larger amount of fusion fuel in the next stage. This technique can result in thermonuclear weapons of arbitrarily large yield, in contrast to fission bombs, which are limited in their explosive force. The largest nuclear weapon ever detonatedthe Tsar Bomba of the USSR, which released an energy equivalent of over 50 million tons (50 megatons) of TNTwas a three-stage weapon. Most thermonuclear weapons are considerably smaller than this, due to practical constraints from missile warhead space and weight requirements.[12]
Fusion reactions do not create fission products, and thus contribute far less to the creation of nuclear fallout than fission reactions, but because all thermonuclear weapons contain at least one fission stage, and many high-yield thermonuclear devices have a final fission stage, thermonuclear weapons can generate at least as much nuclear fallout as fission-only weapons.
There are other types of nuclear weapons as well. For example, a boosted fission weapon is a fission bomb that increases its explosive yield through a small amount of fusion reactions, but it is not a fusion bomb. In the boosted bomb, the neutrons produced by the fusion reactions serve primarily to increase the efficiency of the fission bomb.
Some weapons are designed for special purposes; a neutron bomb is a thermonuclear weapon that yields a relatively small explosion but a relatively large amount of neutron radiation; such a device could theoretically be used to cause massive casualties while leaving infrastructure mostly intact and creating a minimal amount of fallout. The detonation of any nuclear weapon is accompanied by a blast of neutron radiation. Surrounding a nuclear weapon with suitable materials (such as cobalt or gold) creates a weapon known as a salted bomb. This device can produce exceptionally large quantities of radioactive contamination.
Research has been done into the possibility of pure fusion bombs: nuclear weapons that consist of fusion reactions without requiring a fission bomb to initiate them. Such a device might provide a simpler path to thermonuclear weapons than one that required development of fission weapons first, and pure fusion weapons would create significantly less nuclear fallout than other thermonuclear weapons, since they would not disperse fission products. In 1998, the United States Department of Energy divulged that the United States had, "...made a substantial investment" in the past to develop pure fusion weapons, but that, "The U.S. does not have and is not developing a pure fusion weapon," and that, "No credible design for a pure fusion weapon resulted from the DOE investment."[13]
Most variation in nuclear weapon design is for the purpose of achieving different yields for different situations, and in manipulating design elements to attempt to minimize weapon size.[8]
Nuclear weapons deliverythe technology and systems used to bring a nuclear weapon to its targetis an important aspect of nuclear weapons relating both to nuclear weapon design and nuclear strategy. Additionally, development and maintenance of delivery options is among the most resource-intensive aspects of a nuclear weapons program: according to one estimate, deployment costs accounted for 57% of the total financial resources spent by the United States in relation to nuclear weapons since 1940.[14]
Historically the first method of delivery, and the method used in the two nuclear weapons used in warfare, was as a gravity bomb, dropped from bomber aircraft. This is usually the first method that countries developed, as it does not place many restrictions on the size of the weapon and weapon miniaturization requires considerable weapons design knowledge. It does, however, limit attack range, response time to an impending attack, and the number of weapons that a country can field at the same time.
With the advent of miniaturization, nuclear bombs can be delivered by both strategic bombers and tactical fighter-bombers, allowing an air force to use its current fleet with little or no modification. This method may still be considered the primary means of nuclear weapons delivery; the majority of U.S. nuclear warheads, for example, are free-fall gravity bombs, namely the B61.[8]
More preferable from a strategic point of view is a nuclear weapon mounted onto a missile, which can use a ballistic trajectory to deliver the warhead over the horizon. While even short range missiles allow for a faster and less vulnerable attack, the development of long-range intercontinental ballistic missiles (ICBMs) and submarine-launched ballistic missiles (SLBMs) has given some nations the ability to plausibly deliver missiles anywhere on the globe with a high likelihood of success.
More advanced systems, such as multiple independently targetable reentry vehicles (MIRVs), can launch multiple warheads at different targets from one missile, reducing the chance of a successful missile defense. Today, missiles are most common among systems designed for delivery of nuclear weapons. Making a warhead small enough to fit onto a missile, though, can be difficult.[8]
Tactical weapons have involved the most variety of delivery types, including not only gravity bombs and missiles but also artillery shells, land mines, and nuclear depth charges and torpedoes for anti-submarine warfare. An atomic mortar was also tested at one time by the United States. Small, two-man portable tactical weapons (somewhat misleadingly referred to as suitcase bombs), such as the Special Atomic Demolition Munition, have been developed, although the difficulty of combining sufficient yield with portability limits their military utility.[8]
Nuclear warfare strategy is a set of policies that deal with preventing or fighting a nuclear war. The policy of trying to prevent an attack by a nuclear weapon from another country by threatening nuclear retaliation is known as the strategy of nuclear deterrence. The goal in deterrence is to always maintain a second strike capability (the ability of a country to respond to a nuclear attack with one of its own) and potentially to strive for first strike status (the ability to completely destroy an enemy's nuclear forces before they could retaliate). During the Cold War, policy and military theorists in nuclear-enabled countries worked out models of what sorts of policies could prevent one from ever being attacked by a nuclear weapon.
Different forms of nuclear weapons delivery (see above) allow for different types of nuclear strategies. The goals of any strategy are generally to make it difficult for an enemy to launch a pre-emptive strike against the weapon system and difficult to defend against the delivery of the weapon during a potential conflict. Sometimes this has meant keeping the weapon locations hidden, such as deploying them on submarines or rail cars whose locations are very hard for an enemy to track and other times this means protecting them by burying them in hardened bunkers.
Other components of nuclear strategies have included using missile defense (to destroy the missiles before they land) or implementation of civil defense measures (using early-warning systems to evacuate citizens to safe areas before an attack).
Note that weapons designed to threaten large populations, or to generally deter attacks are known as strategic weapons. Weapons designed for use on a battlefield in military situations are called tactical weapons.
There are critics of the very idea of nuclear strategy for waging nuclear war who have suggested that a nuclear war between two nuclear powers would result in mutual annihilation. From this point of view, the significance of nuclear weapons is purely to deter war because any nuclear war would immediately escalate out of mutual distrust and fear, resulting in mutually assured destruction. This threat of national, if not global, destruction has been a strong motivation for anti-nuclear weapons activism.
Critics from the peace movement and within the military establishment have questioned the usefulness of such weapons in the current military climate. According to an advisory opinion issued by the International Court of Justice in 1996, the use of (or threat of use of) such weapons would generally be contrary to the rules of international law applicable in armed conflict, but the court did not reach an opinion as to whether or not the threat or use would be lawful in specific extreme circumstances such as if the survival of the state were at stake.
Perhaps the most controversial idea in nuclear strategy is that nuclear proliferation would be desirable. This view argues that, unlike conventional weapons, nuclear weapons successfully deter all-out war between states, and they are said to have done this during the Cold War between the U.S. and the Soviet Union.[15] Political scientist Kenneth Waltz is the most prominent advocate of this argument.[16][17]
The threat of potentially suicidal terrorists possessing nuclear weapons (a form of nuclear terrorism) complicates the decision process. The prospect of mutually assured destruction may not deter an enemy who expects to die in the confrontation. Further, if the initial act is from a stateless terrorist instead of a sovereign nation, there is no fixed nation or fixed military targets to retaliate against. It has been argued, especially after the September 11, 2001 attacks, that this complication is the sign of the next age of nuclear strategy, distinct from the relative stability of the Cold War.[18] In 1996, the United States adopted a policy of allowing the targeting of its nuclear weapons at terrorists armed with weapons of mass destruction.[19]
Because of the immense military power they can confer, the political control of nuclear weapons has been a key issue for as long as they have existed; in most countries the use of nuclear force can only be authorized by the head of government or head of state.[20]
In the late 1940s, lack of mutual trust was preventing the United States and the Soviet Union from making ground towards international arms control agreements, but by the 1960s steps were being taken to limit both the proliferation of nuclear weapons to other countries and the environmental effects of nuclear testing. The Partial Test Ban Treaty (1963) restricted all nuclear testing to underground nuclear testing, to prevent contamination from nuclear fallout, while the Nuclear Non-Proliferation Treaty (1968) attempted to place restrictions on the types of activities signatories could participate in, with the goal of allowing the transference of non-military nuclear technology to member countries without fear of proliferation.
In 1957, the International Atomic Energy Agency (IAEA) was established under the mandate of the United Nations to encourage development of peaceful applications for nuclear technology, provide international safeguards against its misuse, and facilitate the application of safety measures in its use. In 1996, many nations signed the Comprehensive Test Ban Treaty,[21] which prohibits all testing of nuclear weapons. A testing ban imposes a significant hindrance to nuclear arms development by any complying country.[22] The Treaty requires the ratification by 44 specific states before it can go into force; as of 2012, the ratification of eight of these states is still required.[21]
Additional treaties and agreements have governed nuclear weapons stockpiles between the countries with the two largest stockpiles, the United States and the Soviet Union, and later between the United States and Russia. These include treaties such as SALT II (never ratified), START I (expired), INF, START II (never ratified), SORT, and New START, as well as non-binding agreements such as SALT I and the Presidential Nuclear Initiatives[23] of 1991. Even when they did not enter into force, these agreements helped limit and later reduce the numbers and types of nuclear weapons between the United States and the Soviet Union/Russia.
Nuclear weapons have also been opposed by agreements between countries. Many nations have been declared Nuclear-Weapon-Free Zones, areas where nuclear weapons production and deployment are prohibited, through the use of treaties. The Treaty of Tlatelolco (1967) prohibited any production or deployment of nuclear weapons in Latin America and the Caribbean, and the Treaty of Pelindaba (1964) prohibits nuclear weapons in many African countries. As recently as 2006 a Central Asian Nuclear Weapon Free Zone was established amongst the former Soviet republics of Central Asia prohibiting nuclear weapons.
In the middle of 1996, the International Court of Justice, the highest court of the United Nations, issued an Advisory Opinion concerned with the "Legality of the Threat or Use of Nuclear Weapons". The court ruled that the use or threat of use of nuclear weapons would violate various articles of international law, including the Geneva Conventions, the Hague Conventions, the UN Charter, and the Universal Declaration of Human Rights. In view of the unique, destructive characteristics of nuclear weapons, the International Committee of the Red Cross calls on States to ensure that these weapons are never used, irrespective of whether they consider them lawful or not.[24]
Additionally, there have been other, specific actions meant to discourage countries from developing nuclear arms. In the wake of the tests by India and Pakistan in 1998, economic sanctions were (temporarily) levied against both countries, though neither were signatories with the Nuclear Non-Proliferation Treaty. One of the stated casus belli for the initiation of the 2003 Iraq War was an accusation by the United States that Iraq was actively pursuing nuclear arms (though this was soon discovered not to be the case as the program had been discontinued). In 1981, Israel had bombed a nuclear reactor being constructed in Osirak, Iraq, in what it called an attempt to halt Iraq's previous nuclear arms ambitions; in 2007, Israel bombed another reactor being constructed in Syria.
Nuclear disarmament refers to both the act of reducing or eliminating nuclear weapons and to the end state of a nuclear-free world, in which nuclear weapons are completely eliminated.
Beginning with the 1963 Partial Test Ban Treaty and continuing through the 1996 Comprehensive Test Ban Treaty, there have been many treaties to limit or reduce nuclear weapons testing and stockpiles. The 1968 Nuclear Non-Proliferation Treaty has as one of its explicit conditions that all signatories must "pursue negotiations in good faith" towards the long-term goal of "complete disarmament". The nuclear weapon states have largely treated that aspect of the agreement as "decorative" and without force.[25]
Only one countrySouth Africahas ever fully renounced nuclear weapons they had independently developed. The former Soviet republics of Belarus, Kazakhstan, and Ukraine returned Soviet nuclear arms stationed in their countries to Russia after the collapse of the USSR.
Proponents of nuclear disarmament say that it would lessen the probability of nuclear war occurring, especially accidentally. Critics of nuclear disarmament say that it would undermine deterrence and could lead to increased global instability. Various American government officials, who were in office during the Cold War period, have recently been advocating the elimination of nuclear weapons. These officials include Henry Kissinger, George Shultz, Sam Nunn, and William Perry. In January 2010, Lawrence M. Krauss stated that "no issue carries more importance to the long-term health and security of humanity than the effort to reduce, and perhaps one day, rid the world of nuclear weapons".[26]
In the years after the end of the Cold War, there have been numerous campaigns to urge the abolition of nuclear weapons, such as that organized by the Global Zero movement, and the goal of a "world without nuclear weapons" was advocated by United States President Barack Obama in an April 2009 speech in Prague.[27] A CNN poll from April 2010 indicated that the American public was nearly evenly split on the issue.[28]
Even before the first nuclear weapons had been developed, scientists involved with the Manhattan Project were divided over the use of the weapon. The role of the two atomic bombings of the country in Japan's surrender and the U.S.'s ethical justification for them has been the subject of scholarly and popular debate for decades. The question of whether nations should have nuclear weapons, or test them, has been continually and nearly universally controversial.
Radioactive fallout from nuclear weapons testing was first drawn to public attention in 1954 when the Castle Bravo hydrogen bomb test at the Pacific Proving Grounds contaminated the crew and catch of the Japanese fishing boat Lucky Dragon.[29] One of the fishermen died in Japan seven months later, and the fear of contaminated tuna led to a temporary boycotting of the popular staple in Japan. The incident caused widespread concern around the world, especially regarding the effects of nuclear fallout and atmospheric nuclear testing, and "provided a decisive impetus for the emergence of the anti-nuclear weapons movement in many countries".[29]
Peace movements emerged in Japan and in 1954 they converged to form a unified "Japanese Council Against Atomic and Hydrogen Bombs". Japanese opposition to nuclear weapons tests in the Pacific Ocean was widespread, and "an estimated 35 million signatures were collected on petitions calling for bans on nuclear weapons".[30]
In the United Kingdom, the first Aldermaston March organised by the Campaign for Nuclear Disarmament took place at Easter 1958, when several thousand people marched for four days from Trafalgar Square, London, to the Atomic Weapons Research Establishment close to Aldermaston in Berkshire, England, to demonstrate their opposition to nuclear weapons.[31][32] The Aldermaston marches continued into the late 1960s when tens of thousands of people took part in the four-day marches.[30]
In 1959, a letter in the Bulletin of Atomic Scientists was the start of a successful campaign to stop the Atomic Energy Commission dumping radioactive waste in the sea 19 kilometres from Boston.[33] In 1962, Linus Pauling won the Nobel Peace Prize for his work to stop the atmospheric testing of nuclear weapons, and the "Ban the Bomb" movement spread.[34]
In 1963, many countries ratified the Partial Test Ban Treaty prohibiting atmospheric nuclear testing. Radioactive fallout became less of an issue and the anti-nuclear weapons movement went into decline for some years.[29][35] A resurgence of interest occurred amid European and American fears of nuclear war in the 1980s.[36]
Between 1940 and 1996, the U.S. spent at least $8.63trillion in present day terms[37] on nuclear weapons development. Over half was spent on building delivery mechanisms for the weapon. $541billion in present day terms was spent on nuclear waste management and environmental remediation.[38]
Apart from their use as weapons, nuclear explosives have been tested and used for various non-military uses, and proposed, but not used for large-scale earth moving. When long term health and clean-up costs were included, there was no economic advantage over conventional explosives.[39]
Synthetic elements, such as einsteinium and fermium, created by neutron bombardment of uranium and plutonium during thermonuclear explosions, were discovered in the aftermath of the first thermonuclear bomb test. In 2008 the worldwide presence of new isotopes from atmospheric testing beginning in the 1950s was developed into a reliable way of detecting art forgeries, as all paintings created after that period may contain traces of cesium-137 and strontium-90, isotopes that did not exist in nature before 1945.[40]
Nuclear explosives have also been seriously studied as potential propulsion mechanisms for space travel (see Project Orion) and for asteroid deflection.
  
History
Warfare
Arms race
Design
Testing
Effects
Delivery
Espionage
Proliferation
Arsenals
Terrorism
Opposition
United States
Russia
United Kingdom
France
China
Israel
India
Pakistan
North Korea
South Africa (former)
v
t
e
1 Types

1.1 Fission weapons
1.2 Fusion weapons
1.3 Other types


1.1 Fission weapons
1.2 Fusion weapons
1.3 Other types
2 Weapons delivery
3 Nuclear strategy
4 Governance, control, and law

4.1 Disarmament


4.1 Disarmament
5 Controversy
6 Non-weapons uses
7 See also

7.1 Aftermath
7.2 History
7.3 More technical details
7.4 Popular culture
7.5 Proliferation and politics


7.1 Aftermath
7.2 History
7.3 More technical details
7.4 Popular culture
7.5 Proliferation and politics
8 References

8.1 Notes
8.2 Bibliography


8.1 Notes
8.2 Bibliography
9 External links

9.1 General
9.2 Historical


9.1 General
9.2 Historical
1.1 Fission weapons
1.2 Fusion weapons
1.3 Other types
4.1 Disarmament
7.1 Aftermath
7.2 History
7.3 More technical details
7.4 Popular culture
7.5 Proliferation and politics
8.1 Notes
8.2 Bibliography
9.1 General
9.2 Historical
Biological
Chemical
Nuclear
Radiological
Albania
Algeria
Argentina
Australia
Brazil
Bulgaria
Burma
Canada
China

PRC
ROC


PRC
ROC
France
Germany
India
Iran
Iraq
Israel
Japan
Libya
Mexico
Netherlands
North Korea
Pakistan
Poland
Romania
Russia
Saudi Arabia
South Africa
Sweden
Syria
Ukraine
United Kingdom
United States
PRC
ROC
Chemical
Nuclear
Missiles
List of treaties
 Book
 Category
v
t
e
Nuclear darkness
Nuclear summer
Nuclear winter
History of nuclear weapons

Atomic spies
German nuclear energy project
Japanese atomic program
Manhattan Project
Soviet atomic bomb project


Atomic spies
German nuclear energy project
Japanese atomic program
Manhattan Project
Soviet atomic bomb project
Los Alamos National Laboratory
Lawrence Livermore National Laboratory
Lists of nuclear disasters and radioactive incidents
Nuclear and radiation accidents, including nuclear weapons accidents
Nuclear testing

Nevada Test Site
Project Gnome


Nevada Test Site
Project Gnome
Military strategy

Civil Defense
Fractional Orbital Bombardment System
Mutual Assured Destruction


Civil Defense
Fractional Orbital Bombardment System
Mutual Assured Destruction
Weapon of mass destruction

Nuclear strategy
Nuclear warfare


Nuclear strategy
Nuclear warfare
Atomic spies
German nuclear energy project
Japanese atomic program
Manhattan Project
Soviet atomic bomb project
Nevada Test Site
Project Gnome
Civil Defense
Fractional Orbital Bombardment System
Mutual Assured Destruction
Nuclear strategy
Nuclear warfare
Effects of nuclear explosions
Intercontinental ballistic missile
Neutron bomb
Nuclear bombs and health
Nuclear weapon design
Nuclear weapon yield
Nuclear weapons in popular culture
The Butter Battle Book
Agency for the Prohibition of Nuclear Weapons in Latin America and the Caribbean
Comprehensive Test Ban Treaty
International Court of Justice advisory opinion on legality of nuclear weapons
List of states with nuclear weapons
List of nuclear weapons
Nth Country Experiment
Nuclear disarmament
Nuclear explosive
Nuclear Non-Proliferation Treaty
Nuclear peace
Nuclear proliferation
Nuclear weapons and the United Kingdom
The Letters of last resort (United Kingdom)
Nuclear weapons and Russia
Nuclear weapons and the United States
Paranuclear
Strategic Arms Limitation Talks
Three Non-Nuclear Principles, of Japan
Bethe, Hans Albrecht. The Road from Los Alamos. New York: Simon and Schuster, 1991. ISBN 0-671-74012-1
DeVolpi, Alexander, Minkov, Vladimir E., Simonenko, Vadim A., and Stanford, George S. Nuclear Shadowboxing: Contemporary Threats from Cold War Weaponry. Fidlar Doubleday, 2004 (Two volumes, both accessible on Google Book Search) (Content of both volumes is now available in the 2009 trilogy by Alexander DeVolpi: Nuclear Insights: The Cold War Legacy available on [1].
Glasstone, Samuel and Dolan, Philip J. The Effects of Nuclear Weapons (third edition). Washington, D.C.: U.S. Government Printing Office, 1977. Available online (PDF).
NATO Handbook on the Medical Aspects of NBC Defensive Operations (Part I  Nuclear). Departments of the Army, Navy, and Air Force: Washington, D.C., 1996
Hansen, Chuck. U.S. Nuclear Weapons: The Secret History. Arlington, TX: Aerofax, 1988
Hansen, Chuck. The Swords of Armageddon: U.S. nuclear weapons development since 1945. Sunnyvale, CA: Chukelea Publications, 1995. [2]
Holloway, David. Stalin and the Bomb. New Haven: Yale University Press, 1994. ISBN 0-300-06056-4
The Manhattan Engineer District, "The Atomic Bombings of Hiroshima and Nagasaki" (1946)
Smyth, Henry DeWolf. Atomic Energy for Military Purposes. Princeton, NJ: Princeton University Press, 1945. (Smyth Report the first declassified report by the US government on nuclear weapons)
The Effects of Nuclear War. Office of Technology Assessment, May 1979.
Rhodes, Richard. Dark Sun: The Making of the Hydrogen Bomb. New York: Simon and Schuster, 1995. ISBN 0-684-82414-0
Rhodes, Richard. The Making of the Atomic Bomb. New York: Simon and Schuster, 1986 ISBN 0-684-81378-5
Weart, Spencer R. Nuclear Fear: A History of Images. Cambridge, MA: Harvard University Press, 1988. ISBN 0-674-62836-5
Weart, Spencer R. The Rise of Nuclear Fear. Cambridge, MA: Harvard University Press, 2012. ISBN 0-674-05233-1
Current World Nuclear Arsenals has estimates of nuclear arsenals in the respective countries.
Nuclear Weapon Archive from Carey Sublette is a reliable source of information and has links to other sources and an informative FAQ.
The Federation of American Scientists provide solid information on weapons of mass destruction, including nuclear weapons and their effects
Alsos Digital Library for Nuclear Issuescontains many resources related to nuclear weapons, including a historical and technical overview and searchable bibliography of web and print resources.
Everything you wanted to know about nuclear technologyProvided by New Scientist.
Congressional Research Service (CRS) Reports regarding Nuclear weapons
Video archive of US, Soviet, UK, Chinese and French Nuclear Weapon Testing at sonicbomb.com
The National Museum of Nuclear Science & History (United States)located in Albuquerque, New Mexico; a Smithsonian Affiliate Museum
Nuclear Emergency and Radiation Resources
The Manhattan Project: Making the Atomic Bomb at AtomicArchive.com
Los Alamos National Laboratory: History (U.S. nuclear history)
Race for the Superbomb, PBS website on the history of the H-bomb
U.S. nuclear test photographs from the DOE Nevada Site Office
U.S. nuclear test film clips from the DOE Nevada Site Office
Recordings of recollections of the victims of Hiroshima and Nagasaki
The Woodrow Wilson Center's Nuclear Proliferation International History Project or NPIHP is a global network of individuals and institutions engaged in the study of international nuclear history through archival documents, oral history interviews and other empirical sources.
.
#(`*The Holocaust*`)#.
The Holocaust (from the Greek  holkaustos: hlos, "whole" and kausts, "burnt")[2] also known as the Shoah (Hebrew: , HaShoah, "catastrophe"; Yiddish: , Churben or Hurban, from the Hebrew for "destruction"), was the mass murder or genocide of approximately six million Jews during World War II, a programme of systematic state-sponsored murder by Nazi Germany, led by Adolf Hitler and the Nazi Party, throughout German-occupied territory.[3][4] Of the nine million Jews who had resided in Europe before the Holocaust, approximately two-thirds were killed.[5] Over one million Jewish children were killed in the Holocaust, as were approximately two million Jewish women and three million Jewish men.[6][7]
Some scholars argue that the mass murder of the Romani and people with disabilities should be included in the definition,[8][9] and some use the common noun "holocaust" to describe other Nazi mass murders, including those of Soviet prisoners of war, Polish and Soviet civilians, and homosexuals.[10][11] Recent estimates based on figures obtained since the fall of the Soviet Union indicates some ten to 11 million civilians and prisoners of war were intentionally murdered by the Nazi regime.[12][13]
The persecution and genocide were carried out in stages. Various laws to remove the Jews from civil society, most prominently the Nuremberg Laws, were enacted in Germany years before the outbreak of World War II. Concentration camps were established in which inmates were subjected to slave labor until they died of exhaustion or disease. Where Germany conquered new territory in eastern Europe, specialized units called Einsatzgruppen murdered Jews and political opponents in mass shootings. The occupiers required Jews and Romani to be confined in overcrowded ghettos before being transported by freight train to extermination camps where, if they survived the journey, most were systematically killed in gas chambers. Every arm of Germany's bureaucracy was involved in the logistics that led to the genocides, turning the Third Reich into what one Holocaust scholar has called "a genocidal state".[14]
The term holocaust comes from the Greek word holkauston, referring to an animal sacrifice offered to a god in which the whole (olos) animal is completely burnt (kaustos).[15] For hundreds of years, the word "holocaust" was used in English to denote great massacres, but since the 1960s, the term has come to be used by scholars and popular writers to refer to the Nazi genocide of Jews.[16] The television mini-series Holocaust is credited with introducing the term into common parlance after 1978.[17]
The biblical word Shoah () (also spelled Sho'ah and Shoa), meaning "calamity", became the standard Hebrew term for the Holocaust as early as the 1940s, especially in Europe and Israel.[18] Shoah is preferred by many Jews for a number of reasons, including the theologically offensive nature of the word "holocaust", which they take to refer to the Greek pagan custom.[19]
The Nazis used a euphemistic phrase, the "Final Solution to the Jewish Question" (German: Endlsung der Judenfrage), and the phrase "Final Solution" has been widely used as a term for the genocide of the Jews. Nazis used the phrase "lebensunwertes Leben" (Life unworthy of life) in reference to their victims in an attempt to justify the killings.
Michael Berenbaum writes that Germany became a "genocidal state."[14] "Every arm of the country's sophisticated bureaucracy was involved in the killing process. Parish churches and the Interior Ministry supplied birth records showing who was Jewish; the Post Office delivered the deportation and denaturalization orders; the Finance Ministry confiscated Jewish property; German firms fired Jewish workers and disenfranchised Jewish stockholders." The universities refused to admit Jews, denied degrees to those already studying, and fired Jewish academics; government transport offices arranged the trains for deportation to the camps; German pharmaceutical companies tested drugs on camp prisoners; companies bid for the contracts to build the crematoria; detailed lists of victims were drawn up using the Dehomag (IBM Germany) company's punch card machines, producing meticulous records of the killings. As prisoners entered the death camps, they were made to surrender all personal property, which was catalogued and tagged before being sent to Germany to be reused or recycled. Berenbaum writes that the Final Solution of the Jewish question was "in the eyes of the perpetrators ... Germany's greatest achievement."[20] Through a concealed account, the German national bank helped launder valuables stolen from the victims.
Saul Friedlnder writes that: "Not one social group, not one religious community, not one scholarly institution or professional association in Germany and throughout Europe declared its solidarity with the Jews."[21] He writes that some Christian churches declared that converted Jews should be regarded as part of the flock, but even then only up to a point. Friedlnder argues that this makes the Holocaust distinctive because antisemitic policies were able to unfold without the interference of countervailing forces of the kind normally found in advanced societies, such as industry, small businesses, churches, and other vested interests and lobby groups.[21]
In other genocides, pragmatic considerations such as control of territory and resources were central to the genocide policy. Yehuda Bauer argues that:
The basic motivation [of the Holocaust] was purely ideological, rooted in an illusionary world of Nazi imagination, where an international Jewish conspiracy to control the world was opposed to a parallel Aryan quest. No genocide to date had been based so completely on myths, on hallucinations, on abstract, nonpragmatic ideologywhich was then executed by very rational, pragmatic means.[22]
German historian Eberhard Jckel wrote in 1986 that one distinctive feature of the Holocaust was that
never before had a state with the authority of its responsible leader decided and announced that a specific human group, including its aged, its women and its children and infants, would be killed as quickly as possible, and then carried through this resolution using every possible means of state power.[23]
The killings were systematically conducted in virtually all areas of Nazi-occupied territory in what are now 35 separate European countries.[24] It was at its most severe in Central and Eastern Europe, which had more than seven million Jews in 1939. About five million Jews were killed there, including three million in occupied Poland and over one million in the Soviet Union. Hundreds of thousands also died in the Netherlands, France, Belgium, Yugoslavia, and Greece. The Wannsee Protocol makes it clear that the Nazis intended to carry their "final solution of the Jewish question" to Britain and all the other neutral states in Europe, such as Ireland, Switzerland, Turkey, Sweden, Portugal, and Spain.[25]
Anyone with three or four Jewish grandparents was to be exterminated without exception. In other genocides, people were able to escape death by converting to another religion or in some other way assimilating. This option was not available to the Jews of occupied Europe,[26] unless their grandparents had converted before 18 January 1871. All persons of recent Jewish ancestry were to be exterminated in lands controlled by Germany.[27]
The use of camps equipped with gas chambers for the purpose of systematic mass extermination of peoples was a unique feature of the Holocaust and unprecedented in history. Never before had there existed places with the express purpose of killing people en masse. These were established at Auschwitz, Belzec, Chemno, Jasenovac, Majdanek, Maly Trostenets, Sobibor, and Treblinka.
A distinctive feature of Nazi genocide was the extensive use of human subjects in "medical" experiments. According to Raul Hilberg, "German physicians were highly Nazified, compared to other professionals, in terms of party membership,"[28] and some carried out experiments at Auschwitz, Dachau, Buchenwald, Ravensbrck, Sachsenhausen, and Natzweiler concentration camps.[29]
The most notorious of these physicians was Dr. Josef Mengele, who worked in Auschwitz. His experiments included placing subjects in pressure chambers, testing drugs on them, freezing them, attempting to change eye color by injecting chemicals into children's eyes, and various amputations and other surgeries.[29] The full extent of his work will never be known because the truckload of records he sent to Dr. Otmar von Verschuer at the Kaiser Wilhelm Institute was destroyed by von Verschuer.[30] Subjects who survived Mengele's experiments were almost always killed and dissected shortly afterwards.
He worked extensively with Romani children. He would bring them sweets and toys, and personally take them to the gas chamber. They would call him "Onkel (Uncle) Mengele".[31] Vera Alexander was a Jewish inmate at Auschwitz who looked after 50 sets of Romani twins:
I remember one set of twins in particular: Guido and Ina, aged about four. One day, Mengele took them away. When they returned, they were in a terrible state: they had been sewn together, back to back, like Siamese twins. Their wounds were infected and oozing pus. They screamed day and night. Then their parentsI remember the mother's name was Stellamanaged to get some morphine and they killed the children in order to end their suffering.[32]
Yehuda Bauer, Raul Hilberg, and Lucy Dawidowicz maintained that from the Middle Ages onward, German society and culture were suffused with antisemitism, and that there was a direct ideological link from medieval pogroms to the Nazi death camps.[34][35][36]
The second half of the 19th century saw the emergence in Germany and Austria-Hungary of the Vlkisch movement, developed by such thinkers as Houston Stewart Chamberlain and Paul de Lagarde. The movement presented a pseudoscientific, biologically based racism that viewed Jews as a race locked in mortal combat with the Aryan race for world domination.[37] Vlkisch antisemitism drew upon stereotypes from Christian antisemitism, but differed in that Jews were considered to be a race rather than a religion.[38] In a speech before the Reichstag in 1895, vlkisch leader Hermann Ahlwardt called Jews "predators" and "cholera bacilli" who should be "exterminated" for the good of the German people.[39] In his best-selling 1912 book Wenn ich der Kaiser wr (If I were the Kaiser), Heinrich Class, leader of the vlkisch group Alldeutscher Verband, urged that all German Jews be stripped of their German citizenship and be reduced to Fremdenrecht (alien status).[40] Class also urged that Jews be excluded from all aspects of German life, forbidden to own land, hold public office, or participate in journalism, banking, and the liberal professions.[40] Class defined a Jew as anyone who was a member of the Jewish religion on the day the German Empire was proclaimed in 1871, or anyone with at least one Jewish grandparent.[40] During the German Empire, vlkisch notions and pseudoscientific racism had become common and accepted throughout Germany,[41] with the educated professional classes of the country, in particular, adopting an ideology of human inequality.[42] Though the vlkisch parties were defeated in the 1912 Reichstag elections, being all but wiped out, antisemitism was incorporated into the platforms of the mainstream political parties.[41] The National Socialist German Workers' Party (Nazi Party; NSDAP) was founded in 1920 as an offshoot of the vlkisch movement, and adopted their antisemitism.[43] In a 1986 essay, German historian Hans Mommsen wrote about the situation in postWorld War I Germany that:
If one emphasizes the indisputably important connection in isolation, one should not then force a connection with Hitler's weltanschauung [worldview], which was in no ways original itself, in order to deprive from it the existence of Auschwitz... Thoughts about the extermination of the Jews had long been current, and not only for Hitler and his satraps. Many of these found their way to the NSDAP from the Deutschvlkisch Schutz-und Trutzbund [German Racial Union for Protection and Defiance], which itself had been called into life by the Pan-German Union.[44]
Tremendous scientific and technological changes in Germany during the late 19th and early 20th centuries, together with the growth of the welfare state, created widespread hopes that utopia was at hand and that soon all social problems could be solved.[45] At the same time a racist, social Darwinist, and eugenicist world-view which declared some people to be more biologically valuable than others was common.[46] Historian Detlev Peukert states that the Shoah did not result solely from antisemitism, but was a product of the "cumulative radicalization" in which "numerous smaller currents" fed into the "broad current" that led to genocide.[47] After the First World War, the pre-war mood of optimism gave way to disillusionment as German bureaucrats found social problems to be more insoluble than previously thought, which in turn led them to place increasing emphasis on saving the biologically "fit" while the biologically "unfit" were to be written off.[48] The economic strains of the Great Depression led to many in the German medical establishment to advocate the idea of euthanisation of the "incurable" mentally and physically disabled as a cost-saving measure to free up money to care for the curable.[49] By the time the Nazis came to power in 1933, a tendency already existed in German social policy to save the racially "valuable" while seeking to rid society of the racially "undesirable".[50]
Hitler was open about his hatred of Jews. In his book Mein Kampf, he gave warning of his intention to drive them from Germany's political, intellectual, and cultural life. He did not write that he would attempt to exterminate them, but he is reported to have been more explicit in private. As early as 1922, he allegedly told Major Joseph Hell, at the time a journalist:
Once I really am in power, my first and foremost task will be the annihilation of the Jews. As soon as I have the power to do so, I will have gallows built in rowsat the Marienplatz in Munich, for exampleas many as traffic allows. Then the Jews will be hanged indiscriminately, and they will remain hanging until they stink; they will hang there as long as the principles of hygiene permit. As soon as they have been untied, the next batch will be strung up, and so on down the line, until the last Jew in Munich has been exterminated. Other cities will follow suit, precisely in this fashion, until all Germany has been completely cleansed of Jews.[51]
The German historian Hans Mommsen claimed that there were three types of antisemitism in Germany:
One should differentiate between the cultural antisemitism symptomatic of the German conservatives  found especially in the German officer corps and the high civil administration  and mainly directed against the Eastern Jews on the one hand, and vlkisch antisemitism on the other. The conservative variety functions, as Shulamit Volkov has pointed out, as something of a "cultural code." This variety of German antisemitism later on played a significant role insofar as it prevented the functional elite from distancing itself from the repercussions of racial antisemitism. Thus, there was almost no relevant protest against the Jewish persecution on the part of the generals or the leading groups within the Reich government. This is especially true with respect to Hitler's proclamation of the "racial annihilation war" against the Soviet Union.

Besides conservative antisemitism, there existed in Germany a rather silent anti-Judaism within the Catholic Church, which had a certain impact on immunising the Catholic population against the escalating persecution. The famous protest of the Catholic Church against the euthanasia program was, therefore, not accompanied by any protest against the Holocaust.

The third and most vitriolic variety of antisemitism in Germany (and elsewhere) is the so-called vlkisch antisemitism or racism, and this is the foremost advocate of using violence. Anyhow, one has to be aware that even Hitler until 1938 and possibly 1939 still relied on enforced emigration to get rid of German Jewry; and there did not yet exist any clear-cut concept of killing them. This, however, does not mean that the Nazis elsewhere on all levels did not hesitate to use violent methods, and the inroads against Jews, Jewish shops, and institutions show that very clearly. But there did not exist any formal annihilation program until the second year of the war. It came into being after the "reservation" projects had failed. That, however, does not mean that those methods did not include a lethal component.[52]
Right from the establishment of the Third Reich, Nazi leaders proclaimed the existence of a Volksgemeinschaft ("people's community"). Nazi policies divided the population into two categories, the Volksgenossen ("national comrades"), who belonged to the Volksgemeinschaft, and the Gemeinschaftsfremde ("community aliens"), who did not. Nazi policies about repression divided people into three types of enemies, the "racial" enemies such as the Jews and the Gypsies who were viewed as enemies because of their "blood"; political opponents such as Marxists, liberals, Christians and the "reactionaries" who were viewed as wayward "National Comrades"; and moral opponents such as homosexuals, the "work-shy" and habitual criminals, also seen as wayward "National Comrades".[53] The last two groups were to be sent to concentration camps for "re-education", with the aim of eventual absorption into the Volksgemeinschaft, though some of the moral opponents were to be sterilized, as they were regarded as "genetically inferior".[53] "Racial" enemies such as the Jews could, by definition, never belong to the Volksgemeinschaft; they were to be totally removed from society.[53] German historian Detlev Peukert wrote that the National Socialists' "goal was an utopian Volksgemeinschaft, totally under police surveillance, in which any attempt at nonconformist behaviour, or even any hint or intention of such behaviour, would be visited with terror".[54] Peukert quotes policy documents on the "Treatment of Community Aliens" from 1944, which (though never implemented) showed the full intentions of Nazi social policy: "persons who ... show themselves [to be] unable to comply by their own efforts with the minimum requirements of the national community" were to be placed under police supervision, and if this did not reform them, they were to be taken to a concentration camp.[55]
Leading up to the March 1933 Reichstag elections, the Nazis intensified their campaign of violence against the opposition. With the co-operation of local authorities, they set up concentration camps for extrajudicial imprisonment of their opponents. One of the first, at Dachau, opened on 9 March 1933.[56] Initially the camp contained primarily communists and Social Democrats.[57] Other early prisonsfor example, in basements and storehouses run by the Sturmabteilung (SA) and less commonly by the Schutzstaffel (SS)were consolidated by mid-1934 into purpose-built camps outside the cities, run exclusively by the SS. The initial purpose of the camps was to serve as a deterrent by terrorizing those Germans did not conform to the Volksgemeinschaft.[58] Those sent to the camps included the "educable", whose wills could be broken into becoming "National Comrades", and the "biologically depraved", who were to be sterilized, were to be held permanently, and over time were increasingly subject to extermination through labor, i.e. being worked to death.[58]
Throughout the 1930s, the legal, economic, and social rights of Jews were steadily restricted. The Israeli historian Saul Friedlnder writes that, for the Nazis, Germany drew its strength "from the purity of its blood and from its rootedness in the sacred German earth."[59] On 1 April 1933, there occurred a boycott of Jewish businesses, which was the first national antisemitic campaign, initially planned for a week, but called off after one day owing to lack of popular support. In 1933, a series of laws were passed which contained Aryan paragraphs to exclude Jews from key areas: the Law for the Restoration of the Professional Civil Service, the first antisemitic law passed in the Third Reich; the Physicians' Law; and the Farm Law, forbidding Jews from owning farms or taking part in agriculture. Jewish lawyers were disbarred, and in Dresden, Jewish lawyers and judges were dragged out of their offices and courtrooms and beaten.[60] At the insistence of then president Paul von Hindenburg, Hitler added an exemption allowing Jewish civil servants who were veterans of the First World War, or whose fathers or sons had served, to remain in office. Hitler revoked this exemption in 1937. Jews were excluded from schools and universities (the Law to Prevent Overcrowding in Schools), from belonging to the Journalists' Association, and from being owners or editors of newspapers.[59] The Deutsche Allgemeine Zeitung of 27 April 1933 wrote:
A self-respecting nation cannot, on a scale accepted up to now, leave its higher activities in the hands of people of racially foreign origin ... Allowing the presence of too high a percentage of people of foreign origin in relation to their percentage in the general population could be interpreted as an acceptance of the superiority of other races, something decidedly to be rejected.[61]
In July 1933, the Law for the Prevention of Hereditarily Diseased Offspring calling for compulsory sterilization of the "inferior" was passed. This major eugenic policy led to over 200 Hereditary Health Courts (Erbgesundheitsgerichte) being set up, under whose rulings over 400,000 people were sterilized against their will during the Nazi period.[62]
In 1935, Hitler introduced the Nuremberg Laws, which: prohibited Jews from marrying or having sex with "Aryans" (the Law for the Protection of German Blood and German Honor), stripped German Jews of their citizenship and deprived them of all civil rights. Hitler described the "Blood Law" in particular "the attempt at a legal regulation of a problem, which in the event of further failure would then have through law to be transferred to the final solution of the National Socialist Part."Hitler said that if the "Jewish problem" cannot be solved by these laws, it "must then be handed over by law to the National-Socialist Party for a final solution."[63] The "final solution", or "Endlsung", became the standard Nazi euphemism for the extermination of the Jews. In January 1939, he said in a public speech: "If international-finance Jewry inside and outside Europe should succeed once more in plunging the nations into yet another world war, the consequences will not be the Bolshevization of the earth and thereby the victory of Jewry, but the annihilation (vernichtung) of the Jewish race in Europe."[64] Footage from this speech was used to conclude the 1940 Nazi propaganda movie The Eternal Jew (Der ewige Jude), whose purpose was to provide a rationale and blueprint for eliminating the Jews from Europe.[65]
Jewish intellectuals were among the first to leave. The philosopher Walter Benjamin left for Paris on 18 March 1933. Novelist Leon Feuchtwanger went to Switzerland. The conductor Bruno Walter fled after being told that the hall of the Berlin Philharmonic would be burned down if he conducted a concert there: the Frankfurter Zeitung explained on 6 April that Walter and fellow conductor Otto Klemperer had been forced to flee because the government was unable to protect them against the mood of the German public, which had been provoked by "Jewish artistic liquidators."[66] Albert Einstein was visiting the U.S. on 30 January 1933. He returned to Ostende in Belgium, never to set foot in Germany again, and calling events there a "psychic illness of the masses"; he was expelled from the Kaiser Wilhelm Society and the Prussian Academy of Sciences, and his citizenship was rescinded.[67] When Germany annexed Austria in 1938, Sigmund Freud and his family fled from Vienna to England. Saul Friedlnder writes that when Max Liebermann, honorary president of the Prussian Academy of Arts, resigned his position, not one of his colleagues expressed a word of sympathy, and he was still ostracized at his death two years later. When the police arrived in 1943 with a stretcher to deport his 85-year-old bedridden widow, she committed suicide with an overdose of barbiturates rather than be taken.[67]
On 7 November 1938, Jewish minor Herschel Grnspan assassinated Nazi German diplomat Ernst vom Rath in Paris.[68] This incident was used by the Nazis as a pretext to go beyond legal repression to large-scale physical violence against Jewish Germans. What the Nazis claimed to be spontaneous "public outrage" was in fact a wave of pogroms instigated by the Nazi party, and carried out by SA members and affiliates throughout Nazi Germany, at the time consisting of Germany proper, Austria and Sudetenland.[68] These pogroms became known as Reichskristallnacht ("the Night of Broken Glass", literally "Crystal Night"), or November pogroms. Jews were attacked and Jewish property was vandalized, over 7,000 Jewish shops and 1,668 synagogues (almost every synagogue in Germany) were damaged or destroyed. The death toll is assumed to be much higher than the official number of 91 dead.[68] 30,000 were sent to concentration camps, including Dachau, Sachsenhausen, Buchenwald, and Oranienburg concentration camp,[69][70] where they were kept for several weeks, and released when they could either prove that they were about to emigrate in the near future, or transferred their property to the Nazis.[71] Coinciding with Kristallnacht was the 11 November 1938 passage of Regulations Against Jews' Possession of Weapons, which made it illegal for Jews to possess firearms or other weapons (see The 1938 German Weapons Act).[72] German Jewry was collectively made responsible for restitution of the material damage of the pogroms, amounting to several hundred thousand Reichsmarks, and furthermore had to pay an "atonement tax" of more than a billion Reichsmarks.[68]
After these pogroms, Jewish emigration from Germany accelerated, while public Jewish life in Germany ceased to exist.[68]
Before the war, the Nazis considered mass exportation of German (and subsequently the European) Jewry from Europe. Hitler's agreement to the 19389 Schacht Plan, and the continued flight of thousands of Jews from Hitler's clutches for an extended period when the Schacht Plan came to nothing, indicate that the preference for a concerted genocide of the type that came later did not yet exist.[73]
Plans to reclaim former German colonies such as Tanganyika and South West Africa for Jewish resettlement were halted by Hitler, who argued that no place where "so much blood of heroic Germans had been spilled" should be made available as a residence for the "worst enemies of the Germans".[74] Diplomatic efforts were undertaken to convince the other former colonial powers, primarily the United Kingdom and France, to accept expelled Jews in their colonies.[75] Areas considered for possible resettlement included British Palestine,[76] Italian Abyssinia,[76] British Rhodesia,[77] French Madagascar,[76] and Australia.[78]
Of these areas, Madagascar was the most seriously discussed. Heydrich called the Madagascar Plan a "territorial final solution"; it was a remote location, and the island's unfavorable conditions would hasten deaths.[79] Approved by Hitler in 1938, the resettlement planning was carried out by Eichmann's office, only being abandoned once the mass killing of Jews began in 1941. In retrospect, although futile, this plan did constitute an important psychological step on the path to the Holocaust.[80] The end of the Madagascar Plan was announced on 10 February 1942. The German Foreign Office was given the official explanation that, due to the war with the Soviet Union, Jews were to be "sent to the east".[81]
Nazi bureaucrats also developed plans to deport Europe's Jews to Siberia.[82] Palestine was the only location to which any Nazi relocation plan succeeded in producing significant results, by means of an agreement begun in 1933 between the Zionist Federation of Germany (die Zionistische Vereinigung fr Deutschland) and the Nazi government, the Haavara Agreement. This agreement resulted in the transfer of about 60,000 German Jews and $100 million from Germany to Palestine, up until the outbreak of World War II.[83][84]
Germany's invasion of Poland in September 1939 increased the urgency of the "Jewish Question". Poland, was home to about two million Jews (nearly nine percent of the population), in centuries-old communities.
Himmler's right-hand man, Reinhard Heydrich, recommended concentrating all the Polish Jews in ghettos in major cities, where they would be put to work for the German war industry. The ghettos would be in cities located on railway junctions in order to furnish, in Heydrich's words, "a better possibility of control and later deportation."[85] During his interrogation in 1961, Adolf Eichmann recalled that this "later deportation" actually meant "physical extermination."[86]
I ask nothing of the Jews except that they should disappear.
Hans Frank, Nazi governor of Poland.[87]
In September, Himmler appointed Heydrich head of the Reich Main Security Office (Reichssicherheitshauptamt or RSHA, not to be confused with the RuSHA). This body was to oversee the work of the SS, the Security Police (SD), and the Gestapo in occupied Poland, and carry out the policy towards the Jews described in Heydrich's report. The first organized murders of Jews by German forces occurred during Operation Tannenberg and through Selbstschutz units. Later, the Jews were herded into ghettos, mostly in the General Government area of central Poland, where they were put to work under the Reich Labor Office headed by Fritz Sauckel. Here many thousands died from maltreatment, disease, starvation, and exhaustion, but there was still no program of systematic killing. There is no doubt, however, that the Nazis saw forced labor as a form of extermination. The expression Vernichtung durch Arbeit ("destruction through work") was frequently used.
Although it was clear by 1941 that the SS hierarchy was determined to embark on a policy of killing all the Jews under German control, there was still opposition to this policy within the Nazi regime, although the motive was economic, not humanitarian. Hermann Gring, who had overall control of the German war industry, and the German army's Economics Department, argued that the enormous Jewish labor force assembled in the General Government area (more than a million able-bodied workers) was an asset too valuable to waste while Germany was preparing to invade the Soviet Union.
When Germany occupied Norway, the Netherlands, Luxembourg, Belgium, and France in 1940, and Yugoslavia and Greece in 1941, antisemitic measures were also introduced into these countries, although the pace and severity varied greatly from country to country according to local political circumstances. Jews were removed from economic and cultural life and were subject to various restrictive laws, but physical deportation did not occur in most places before 1942. The Vichy regime in occupied France actively collaborated in persecuting French Jews. Germany's allies Italy, Hungary, Romania, Bulgaria and Finland were pressured to introduce antisemitic measures, but for the most part they did not comply until compelled to do so. During the course of the war some 900 Jews and 300 Roma passed through the concentration camp Banjica in Belgrade, intended primarily for Serbian communists, royalists and other patriots who resisted occupation. The German puppet regime in Croatia, on the other hand, began actively persecuting Jews on its own initiative, so the Legal Decree on the Nationalization of the Property of Jews and Jewish Companies was declared on 10 October 1941 in the Independent State of Croatia.
On 28 September 1939, Germany gained control over the Lublin area through the German-Soviet agreement in exchange for Lithuania.[88] According to the Nisko Plan, they set up the Lublin-Lipowa Reservation in the area. The reservation was designated by Adolf Eichmann, who was assigned the task of removing all Jews from Germany, Austria and the Protectorate of Bohemia and Moravia.[89] They shipped the first Jews to Lublin less than three weeks later on 18 October 1939. The first train loads consisted of Jews deported from Austria and the Protectorate of Bohemia and Moravia.[90] By 30 January 1940, a total of 78,000 Jews had been deported to Lublin from Germany, Austria and Czechoslovakia.[91] On 12 and 13 February 1940, the Pomeranian Jews were deported to the Lublin reservation, resulting in Pomeranian Gauleiter Franz Schwede-Coburg to be the first to declare his Gau "judenrein" ("free of Jews").[92] On 24 March 1940 Hermann Gring put the Nisko Plan on hold, and abandoned it entirely by the end of April.[93] By the time the Nisko Plan was stopped, the total number of Jews who had been transported to Nisko had reached 95,000, many of whom had died from starvation.[94]
In July 1940, due to the difficulties of supporting the increased population in the General Government, Hitler had the deportations temporarily halted.[95]
In October 1940, Gauleiters Josef Brckel and Robert Heinrich Wagner oversaw Operation Brckel, the expulsion of the Jews into unoccupied France from their Gaues and the parts of Alsace-Lorraine that had been annexed that summer to the Reich.[96] Only those Jews in mixed marriages were not expelled.[96] The 6,500 Jews affected by Operation Brckel were given at most two hours warning on the night of 2223 October 1940, before being rounded up. The nine trains carrying the deported Jews crossed over into France "without any warning to the French authorities", who were not happy to receive them.[96] The deportees had not been allowed to take any of their possessions with them, these being confiscated by the German authorities.[96] The German Foreign Minister Joachim von Ribbentrop treated the ensuing complaints by the Vichy government over the expulsions in a "most dilatory fashion".[96] As a result, the Jews expelled in Operation Brckel were interned in harsh conditions by the Vichy authorities at the camps in Gurs, Rivesaltes and Les Milles while awaiting a chance to return them to Germany.[96]
During 1940 and 1941, murder of large numbers of Jews in German-occupied Poland continued, and the deportation of Jews to the General Government was undertaken. The deportation of Jews from Germany, particularly Berlin, was not officially completed until 1943. (Many Berlin Jews were able to survive in hiding.) By December 1939, 3.5 million Jews were crowded into the General Government area.
From the beginning of the Third Reich concentration camps were founded, initially as places of incarceration. Though the death rate in the concentration camps was high, with a mortality rate of 50%, they were not designed to be killing centres. (By 1942, six large extermination camps had been established in Nazi-occupied Poland, which were built solely for mass killings.) After 1939, the camps increasingly became places where Jews and POWs were either killed or made to work as slave laborers, undernourished and tortured.[97] It is estimated that the Germans established 15,000 camps and subcamps in the occupied countries, mostly in eastern Europe.[98][99] New camps were founded in areas with large Jewish, Polish intelligentsia, communist, or Roma and Sinti populations, including inside Germany. The transportation of prisoners was often carried out under horrifying conditions using rail freight cars, in which many died before reaching their destination.
Extermination through labour was a policy of systematic extermination  camp inmates would literally be worked to death, or worked to physical exhaustion, when they would be gassed or shot. Slave labour was used in war production, for example producing V-2 rockets at Mittelbau-Dora, and various armaments around the Mauthausen-Gusen concentration camp complex.
Upon admission, some camps tattooed prisoners with a prisoner ID.[100] Those fit for work were dispatched for 12 to 14 hour shifts. Before and after, there were roll calls that could sometimes last for hours, with prisoners regularly dying of exposure.[101]
After the invasion of Poland, the Nazis established ghettos in which Jews and some Romani were confined until they were eventually shipped to extermination camps. The first order for the establishment of the councils came in a letter dated 29 September 1939 from Heydrich to the heads of the Einsatzgruppen.[102] Each ghetto was run by a Judenrat (Jewish council) of German-appointed Jewish community leaders, who were responsible for the day-to-day running of the ghetto, including the distribution of food, water, heat, medicine, and shelter. The basic strategy adopted by the councils was one of trying to minimise losses, largely by cooperating with Nazi authorities (or their surrogates), accepting the increasingly terrible treatment, and petitioning for better conditions and clemency.[103]
Councils were also expected to make arrangements for deportations to extermination camps,[104] thus the defining moment that tested the courage and character of each Judenrat came when they were asked to provide a list of names of the next group to be deported. The Judenrat members went through the tried and tested methods of delay, bribery, stonewalling, pleading, and argumentation, until finally a decision had to be made. Some, like Chaim Rumkowski, argued that their responsibility was to save the Jews who could be saved, and that therefore others had to be sacrificed; others argued, following Maimonides, that not a single individual should be handed over who had not committed a capital crime. Judenrat leaders such as Dr. Joseph Parnas in Lviv, who refused to compile a list, were shot. On 14 October 1942, the entire Judenrat of Byaroza committed suicide rather than cooperate with the deportations.[105]
The importance of the councils in facilitating the persecution and murder of ghetto inhabitants was not lost on the Germans: one official was emphatic that "the authority of the Jewish council be upheld and strengthened under all circumstances",[106] another that "Jews who disobey instructions of the Jewish council are to be treated as saboteurs."[104] When such cooperation crumbled, as happened in the Warsaw ghetto after the Jewish Combat Organisation displaced the council's authority, the Germans lost control.[107]
The Warsaw Ghetto was the largest, with 380,000 people, and the d Ghetto the second largest, holding 160,000. They were, in effect, immensely crowded prisons, described by Michael Berenbaum as instruments of "slow, passive murder."[108] Though the Warsaw Ghetto contained 400,000 people30% of the population of Warsawit occupied only 2.4% of the city's area, averaging 9.2 people per room.[109]
From 1940 through 1942, starvation and disease, especially typhoid, killed hundreds of thousands. Over 43,000 residents of the Warsaw ghetto died there in 1941,[109] more than one in ten; in Theresienstadt, more than half the residents died in 1942.[108]
The Germans came, the police, and they started banging houses: "Raus, raus, raus, Juden raus." ... [O]ne baby started to cry ... The other baby started crying. So the mother urinated in her hand and gave the baby a drink to keep quiet ... [When the police had gone], I told the mothers to come out. And one baby was dead ... from fear, the mother [had] choked her own baby.
 Abraham Malik, describing his experience in the Kovno Ghetto.[110]
Himmler ordered the start of the deportations on 19 July 1942, and three days later, on 22 July, the deportations from the Warsaw Ghetto began; over the next 52 days, until 12 September, 300,000 people from Warsaw alone were transported in freight trains to the Treblinka extermination camp. Many other ghettos were completely depopulated.
The first ghetto uprising occurred in September 1942 in the small town of achwa in south-east Poland. Though there were armed resistance attempts in the larger ghettos in 1943, such as the Warsaw Ghetto Uprising and the Biaystok Ghetto Uprising, in every case they failed against the overwhelming Nazi military force, and the remaining Jews were either killed or deported to the death camps.[111]
A number of deadly pogroms by local populations occurred during the Second World War, some with Nazi encouragement, and some spontaneously. This included the Iai pogrom in Romania on 30 June 1941, in which as many as 14,000 Jews were killed by Romanian residents and police, and the Jedwabne pogrom, in which between 380 and 1,600 Jews were killed by local Poles in July 1941.[112]
The German invasion of the Soviet Union in June 1941 opened a new phase. The Holocaust intensified after the Nazis occupied Lithuania, where close to 80% of the country's 220,000 Jews were exterminated before the end of the year.[113][114] The Soviet territories occupied by early 1942, including all of Belarus, Estonia, Latvia, Lithuania, Ukraine, and Moldova and most Russian territory west of the line Leningrad-Moscow-Rostov, contained about three million Jews, including hundreds of thousands who had fled Poland in 1939.
Members of the local populations in certain occupied Soviet territories participated actively in the killings of Jews and others.[115] In Lithuania, Latvia and western Ukraine, locals were deeply involved in the murder of Jews from the very beginning of the German occupation.[115] The Latvian Arajs Kommando was an example of an auxiliary unit involved in these killings.[115] To the south, Ukrainians killed approximately 24,000 Jews.[115] In addition, Latvian and Lithuanian units left their own countries, and committed murders of Jews in Belarus, and Ukrainians served as concentration and death camp guards in Poland.[115] Ustae militia in Croatian areas also carried out acts of persecution and murder. Ultimately it was the Germans who organized and channelled these local participants in the Holocaust.[115]
Many of the mass killings were carried out in public, a change from previous practice.[115] German witnesses to these killings emphasized the participation of the locals.[115] The massacres committed by the Einsatzgruppen were usually justified under the grounds of anti-partisan or anti-bandit operations, but the German historian Andreas Hillgruber wrote that this was merely an excuse for the German Army's considerable involvement in the Holocaust in Russia and the terms "war crimes" and "crimes against humanity" were indeed correct labels for what happened.[116] Hillgruber maintained that the slaughter of about 2.2 million defenceless men, women and children for the reasons of racist ideology cannot possibly be justified for any reason, and that those German generals who claimed that the Einsatzgruppen were a necessary anti-partisan response were lying.[117]
Army co-operation with the SS in anti-partisan and anti-Jewish operations was close and intensive.[118] In the summer of 1941, the SS Cavalry Brigade commanded by Hermann Fegelein during the course of "anti-partisan" operations in the Pripyat Marshes killed 699 Red Army soldiers, 1,100 partisans and 14,178 Jews.[118] Before the operation, Fegelein had been ordered to shoot all adult Jews while driving the women and children into the marshes. After the operation, General Max von Schenckendorff, who commanded the rear areas of Army Group Centre ordered on 10 August 1941 that all Wehrmacht security divisions when on anti-partisan duty to emulate Fegelein's example, and organized in Mogilev between 2426 September 1941 a joint SS-Wehrmacht seminar on how best to murder Jews.[118] The seminar ended with the 7th Company of Police Battalion 322 shooting 32 Jews at village called Knjashizy before the assembled officers as an example of how to "screen" the population for partisans.[119] As the war diary of the Battalion 322 read:
The action, first scheduled as a training exercise was carried out under real-life conditions (ernstfallmssig) in the village itself. Strangers, especially partisans could not be found. The screening of the population, however resulted in 13 Jews, 27 Jewish women and 11 Jewish children, of which 13 Jews and 19 Jewish women were shot in co-operation with the Security Service[119]
Based on what they had learned during the Mogilev seminar, one Wehrmacht officer told his men "Where the partisan is, there is the Jew and where the Jew is, there is the partisan".[119] In Order #24 November 24, 1941, the commander of the 707th division declared:
Jews and Gypsies:...As already has been ordered, the Jews have to vanish from the flat country and the Gypsies have to be annihilated too. The carrying out of larger Jewish actions is not the task of the divisional units. They are carried out by civilian or police authorities, if necessary ordered by the commandant of White Ruthenia, if he has special units at his disposal, or for security reasons and in the case of collective punishments. When smaller or larger groups of Jews are met in the flat country, they can be liquidated by divisional units or be massed in the ghettos near bigger villages designated for that purpose, where they can be handed over to the civilian authority or the SD.[120]
The German historian Jrgen Frster, a leading expert on the subject of Wehrmacht war crimes argued the Wehrmacht played a key role in the Holocaust, and it is wrong to describe the Shoah as solely the work of the SS with the Wehrmacht as a passive and disapproving bystander.[121]
Raul Hilberg writes that the German Einsatzgruppen commanders were ordinary citizens: the great majority were professionals, most were intellectuals, and they brought to bear all their skills and training, becoming efficient killers.[122]
The large-scale killings of Jews in the occupied Soviet territories was assigned to SS formations called Einsatzgruppen ("task groups"), under the overall command of Heydrich. These had been used on a limited scale in Poland in 1939, but were now organized on a much larger scale. Einsatzgruppe A was assigned to the Baltic area, Einsatzgruppe B to Belarus, Einsatzgruppe C to north and central Ukraine, and Einsatzgruppe D to Moldova, south Ukraine, Crimea, and, during 1942, the north Caucasus.[123]
According to Otto Ohlendorf at his trial, "the Einsatzgruppen had the mission to protect the rear of the troops by killing the Jews, Gypsies, Communist functionaries, active Communists, and all persons who would endanger the security." In practice, their victims were nearly all defenseless Jewish civilians (not a single Einsatzgruppe member was killed in action during these operations). By December 1941, the four Einsatzgruppen listed above had killed, respectively, 125,000, 45,000, 75,000, and 55,000 peoplea total of 300,000 peoplemainly by shooting or with hand grenades at mass killing sites outside the major towns.
The United States Holocaust Memorial Museum tells the story of one survivor of the Einsatzgruppen in Piryatin, Ukraine, when they killed 1,600 Jews on 6 April 1942, the second day of Passover:
I saw them do the killing. At 5:00 p.m. they gave the command, "Fill in the pits." Screams and groans were coming from the pits. Suddenly I saw my neighbor Ruderman rise from under the soil ... His eyes were bloody and he was screaming: "Finish me off!" ... A murdered woman lay at my feet. A boy of five years crawled out from under her body and began to scream desperately. "Mommy!" That was all I saw, since I fell unconscious.[124]
The most notorious massacre of Jews in the Soviet Union was at a ravine called Babi Yar outside Kiev, where 33,771 Jews were killed in a single operation on 2930 September 1941.[125] The killing of all the Jews in Kiev was decided on by the military governor (Major-General Friedrich Eberhardt), the Police Commander for Army Group South (SS-Obergruppenfhrer Friedrich Jeckeln) and the Einsatzgruppe C Commander Otto Rasch. The killings were carried out by a mixture of SS, SD and Security Police, assisted by Ukrainian police. In addition, men of the 6th Army through they not did participate in the killings, played a key role in rounding up the Jews of Kiev and transporting them to be shot at Babi Yar.[126]
On Monday, the Jews of Kiev gathered by the cemetery, expecting to be loaded onto trains. The crowd was large enough that most of the men, women, and children could not have known what was happening until it was too late; by the time they heard the machine gun fire, there was no chance to escape. All were driven down a corridor of soldiers, in groups of ten, and then shot. A truck driver described the scene, as
one after the other, they had to remove their luggage, then their coats, shoes, and outer garments and also underwear ... Once undressed, they were led into the ravine which was about 150 meters long and 30 meters wide and a good 15 meters deep ... When they reached the bottom of the ravine they were seized by members of the Schutzpolizei and made to lie down on top of Jews who had already been shot ... The corpses were literally in layers. A police marksman came along and shot each Jew in the neck with a submachine gun ... I saw these marksmen stand on layers of corpses and shoot one after the other ... The marksman would walk across the bodies of the executed Jews to the next Jew, who had meanwhile lain down, and shoot him.[127]
In August 1941 Himmler travelled to Minsk, where he personally witnessed 100 Jews being shot in a ditch outside the town, an event described by Karl Wolff in his diary. "Himmler's face was green. He took out his handkerchief and wiped his cheek where a piece of brain had squirted up onto it. Then he vomited." After recovering his composure, he lectured the SS men on the need to follow the "highest moral law of the Party" in carrying out their tasks.
Starting in December 1939, the Nazis introduced new methods of mass murder by using gas.[129] First, experimental gas vans equipped with gas cylinders and a sealed trunk compartment, were used to kill mental care clients of sanatoria in Pomerania, East Prussia, and occupied Poland, as part of an operation termed Action T4.[129] In the Sachsenhausen concentration camp, larger vans holding up to 100 people were used from November 1941, using the engine's exhaust rather than a cylinder.[129] These vans were introduced to the Chemno extermination camp in December 1941, and another 15 of them were used by the Einsatzgruppen in the occupied Soviet Union.[129] These gas vans were developed and run under supervision of the Reich Main Security Office, and were used to kill about 500,000 people, primarily Jews, but also Romani and others.[129] The vans were carefully monitored and after a month of observation a report stated that "ninety seven thousand have been processed using three vans, without any defects showing up in the machines".[130]
A need for new mass murder techniques was also expressed by Hans Frank, governor of the General Government, who noted that this many people could not be simply shot. "We shall have to take steps, however, designed in some way to eliminate them." It was this problem which led the SS to experiment with large-scale killings using poison gas. Finally, Christian Wirth seems to have been the inventor of the gas chamber.
The Wannsee Conference was convened by Reinhard Heydrich on 20 January 1942 in the Berlin suburb of Wannsee and brought together some 15 Nazi leaders which included a number of state secretaries, senior officials, party leaders, SS officers and other leaders of government departments who were responsible for policies which were linked to Jewish issues. The initial purpose of the meeting was to discuss plans for a comprehensive solution to the "Jewish question in Europe." Heydrich intended to "outline the mass murders in the various occupied territories ... as part of a solution to the European Jewish question ordered by Hitler ... to ensure that they, and especially the ministerial bureaucracy, would share both knowledge and responsibility for this policy."{{sfn|Longerich|2010|p=305]}
A copy of the minutes which were drawn up by Eichmann has survived, but on Heydrich's instructions, they were written up in "euphemistic language." Thus the exact words used at the meeting are not known.[134] However, Heydrich addressed the meeting indicating the policy of emigration was superseded by a policy of evacuating Jews to the east. This was seen to be only a temporary solution leading up to a final solution which would involve some 11 million Jews living not only in territories controlled then by the Germans, but to major countries in the rest of the world including the UK, and the US.[135] There was little doubt what the solution was: "Heydrich also made it clear what was understood by the phrase 'Final Solution': the Jews were to be annihilated by a combination of forced labour and mass murder."[136]
The officials were told there were 2.3 million Jews in the General Government, 850,000 in Hungary, 1.1 million in the other occupied countries, and up to 5 million in the USSR, although 2 million of these were in areas still under Soviet control  a total of about 6.5 million. These would all be transported by train to extermination camps (Vernichtungslager) in Poland, where almost all of them would be gassed at once. In some camps, such as Auschwitz, those fit for work would be kept alive for a while, but eventually all would be killed. Gring's representative, Dr. Erich Neumann, gained a limited exemption for some classes of industrial workers.[137]
In his 1983 book, Popular Opinion and Political Dissent in the Third Reich, Ian Kershaw examined the Alltagsgeschichte (history of everyday life) in Bavaria during the Nazi period.[138] Describing the attitudes of most Bavarians, Kershaw argued that the most common viewpoint was indifference towards what was happening to the Jews.[139] Kershaw argued that most Bavarians were vaguely aware of the Shoah, but were vastly more concerned about the war than about the "Final Solution to the Jewish Question".[139] Kershaw made the analogy that "the road to Auschwitz was built by hate, but paved with indifference".[140][141]
Kershaw's assessment that most Bavarians, and by implication most Germans, were indifferent to the Shoah faced criticism from the Israeli historian Otto Dov Kulka, an expert on public opinion in Nazi Germany, and the Canadian historian Michael Kater. Kater contended that Kershaw downplayed the extent of popular antisemitism, and that though admitting that most of the "spontaneous" antisemitic actions of Nazi Germany were staged, argued that because these actions involved substantial numbers of Germans, it is wrong to see the extreme antisemitism of the Nazis as coming solely from above.[142] Kulka argued that most Germans were more antisemitic than Kershaw portrayed them in Popular Opinion and Political Dissent, and that rather than "indifference", "passive complicity" would be a better term to describe the reaction of the German people.[143]
In a study focusing only on the views about Jews of Germans opposed to the Nazi regime, the German historian Christof Dipper in his 1983 essay "Der Deutsche Widerstand und die Juden" (translated into English as "The German Resistance and the Jews" in Yad Vashem Studies, Volume 16, 1984) argued that the majority of the anti-Nazi national-conservatives were antisemitic.[142] Dipper wrote that for the majority of the national-conservatives "the bureaucratic, pseudo-legal deprivation of the Jews practiced until 1938 was still considered acceptable".[142] Though Dipper noted no one in the German resistance supported the Holocaust, he also commented that the national-conservatives did not intend to restore civil rights to the Jews after the planned overthrow of Hitler.[142] Dipper went on to argue that, based on such views held by opponents of the regime, "a large part of the German people...believed that a "Jewish Question" existed and had to be solved...".[142]
Robert Gellately has argued that the German civilian population were, by and large, aware of what was happening. According to Gellately, the government openly announced the conspiracy through the media and civilians were aware of its every aspect except for the use of gas chambers.[144] In contrast, some historical evidence indicates that the vast majority of Holocaust victims, prior to their deportation to concentration camps, were either unaware of the fate that awaited them or were in denial; they honestly believed that they were to be resettled.[145][146][147]
In his 1965 essay "Command and Compliance", which originated in his work as an expert witness for the prosecution at the Frankfurt Auschwitz Trials, the German historian Hans Buchheim wrote there was no coercion to murder Jews and others, and all who committed such actions did so out of free will.[148] Buchheim wrote that chances to avoid executing criminal orders "...were both more numerous and more real than those concerned are generally prepared to admit...",[148] and that he found no evidence that SS men who refused to carry out criminal orders were sent to concentration camps or executed.[149] Moreover, SS rules prohibited acts of gratuitous sadism, as Himmler wished for his men to remain "decent", and that acts of sadism were taken on the individual initiative of those who were either especially cruel or who wished to prove themselves ardent National Socialists.[148] Finally, he argued that those of a non-criminal bent who committed crimes did so because they wished to conform to the values of the group they had joined and were afraid of being branded "weak" by their colleagues if they refused.[150]
In his 1992 book Ordinary Men: Reserve Police Battalion 101 and the Final Solution in Poland, Christopher Browning examined the German Ordnungspolizei Reserve Battalion 101, used to massacre and round up Jews for deportation to the Nazi death camps. The men of the battalion were middle-aged men of working-class background from Hamburg, who were unfit for military duty and were given no special training for genocide. The commander of the unit gave his men the choice of opting out of direct participation if they found it too unpleasant (for example, by being part of a passive cordon round the area of the killing). The majority chose not to exercise that option fewer than 15 men out of a battalion of 500 did so.[151] Influenced by the work of Stanley Milgram, Browning argued that the men of the battalion killed out of obedience to authority and peer pressure, not blood-lust or hatred. The general implication of the book is that when placed in a cohesive group setting, most people will obey commands given by an authority-figure seen as legitimate, even if they find them morally reprehensible a hypothesis studied in the Milgram Experiment.
The Russian historian Sergei Kudryashov studied the guards trained at the Trawniki concentration camp, who provided the bulk of personnel for the Operation Reinhard death camps. Some Trawniki guards were Red Army POWs who volunteered to join the SS in order to get out of the POW camps.[152] The majority of the Trawniki men were Ukrainians or Volksdeutche, though there were also Russians, Poles, Latvians, Lithuanians, Tartars, Georgians, Armenians and Azerbaijanis amongst them.[153] Kudryashov reported that he found there was little sign of antisemitism or any attraction to National Socialism among the Trawniki men, many of whom prior to their capture had been Communists.[154] Despite the generally apathetic views of the Trawniki guards, the vast majority faithfully carried out the SS's expectations of how to mistreat Jews; the mistreatment of Jews by the Trawniki guards was "systematic and without any particular cause".[154] Many, though not all of the Trawniki men executed Jews, and almost all of them while working as guards in the Operation Reinhard camps personally killed dozens of Jews.[155] Following Christopher Browning, Kudryashov argued that the Trawniki men were examples of ordinary people becoming willing killers out of peer pressure and obedience to authority.[156]
During 1942, in addition to Auschwitz, five other camps were designated as extermination camps (Vernichtungslager) for the carrying out of the Reinhard plan.[175][176] Two of these, Chemno[177] and Majdanek were already functioning as labor camps: these now had extermination facilities added to them. Three new camps were built for the sole purpose of killing large numbers of Jews as quickly as possible, at Belzec, Sobibor and Treblinka. A seventh camp, at Maly Trostinets in Belarus, was also used for this purpose. Jasenovac was an extermination camp where mostly ethnic Serbs were killed.
Extermination camps are frequently confused with concentration camps such as Dachau and Belsen, which were mostly located in Germany and intended as places of incarceration and forced labor for a variety of enemies of the Nazi regime (such as Communists and homosexuals). They should also be distinguished from slave labor camps, which were set up in all German-occupied countries to exploit the labor of prisoners of various kinds, including prisoners of war. In all Nazi camps there were very high death rates as a result of starvation, disease and exhaustion, but only the extermination camps were designed specifically for mass killing.
There was a place called the ramp where the trains with the Jews were coming in. They were coming in day and night, and sometimes one per day and sometimes five per day ... Constantly, people from the heart of Europe were disappearing, and they were arriving to the same place with the same ignorance of the fate of the previous transport. And the people in this mass ... I knew that within a couple of hours ... ninety percent would be gassed.
 Rudolf Vrba, who worked on the Judenrampe in Auschwitz from August 18, 1942 to June 7, 1943.[108]
The extermination camps were run by SS officers, but most of the guards were Ukrainian or Baltic auxiliaries. Regular German soldiers were kept well away.
At the extermination camps with gas chambers all the prisoners arrived by train. Sometimes entire trainloads were sent straight to the gas chambers, but usually the camp doctor on duty subjected individuals to selections, where a small percentage were deemed fit to work in the slave labor camps; the majority were taken directly from the platforms to a reception area where all their clothes and other possessions were seized by the Nazis to help fund the war. They were then herded naked into the gas chambers. Usually they were told these were showers or delousing chambers, and there were signs outside saying "baths" and "sauna." They were sometimes given a small piece of soap and a towel so as to avoid panic, and were told to remember where they had put their belongings for the same reason. When they asked for water because they were thirsty after the long journey in the cattle trains, they were told to hurry up, because coffee was waiting for them in the camp, and it was getting cold.[178]
According to Rudolf H, commandant of Auschwitz, bunker 1 held 800 people, and bunker 2 held 1,200.[179] Once the chamber was full, the doors were screwed shut and solid pellets of Zyklon-B were dropped into the chambers through vents in the side walls, releasing toxic HCN, or hydrogen cyanide. Those inside died within 20 minutes; the speed of death depended on how close the inmate was standing to a gas vent, according to H, who estimated that about one third of the victims died immediately.[180] Joann Kremer, an SS doctor who oversaw the gassings, testified that: "Shouting and screaming of the victims could be heard through the opening and it was clear that they fought for their lives."[181] When they were removed, if the chamber had been very congested, as they often were, the victims were found half-squatting, their skin colored pink with red and green spots, some foaming at the mouth or bleeding from the ears.[180]
The gas was then pumped out, the bodies were removed (which would take up to four hours), gold fillings in their teeth were extracted with pliers by dentist prisoners, and women's hair was cut.[181][182] The floor of the gas chamber was cleaned, and the walls whitewashed.[181] The work was done by the Sonderkommando, which were work units of Jewish prisoners. In crematoria 1 and 2, the Sonderkommando lived in an attic above the crematoria; in crematoria 3 and 4, they lived inside the gas chambers.[183] When the Sonderkommando had finished with the bodies, the SS conducted spot checks to make sure all the gold had been removed from the victims' mouths. If a check revealed that gold had been missed, the Sonderkommando prisoner responsible was thrown into the furnace alive as punishment.[184]
At first, the bodies were buried in deep pits and covered with lime, but between September and November 1942, on the orders of Himmler, they were dug up and burned. In the spring of 1943, new gas chambers and crematoria were built to accommodate the numbers.[185]
Another improvement we made over Treblinka was that we built our gas chambers to accommodate 2,000 people at one time, whereas at Treblinka their 10 gas chambers only accommodated 200 people each. The way we selected our victims was as follows: we had two SS doctors on duty at Auschwitz to examine the incoming transports of prisoners. The prisoners would be marched by one of the doctors who would make spot decisions as they walked by. Those who were fit for work were sent into the Camp. Others were sent immediately to the extermination plants. Children of tender years were invariably exterminated, since by reason of their youth they were unable to work. Still another improvement we made over Treblinka was that at Treblinka the victims almost always knew that they were to be exterminated and at Auschwitz we endeavored to fool the victims into thinking that they were to go through a delousing process. Of course, frequently they realized our true intentions and we sometimes had riots and difficulties due to that fact. Very frequently women would hide their children under the clothes but of course when we found them we would send the children in to be exterminated. We were required to carry out these exterminations in secrecy but of course the foul and nauseating stench from the continuous burning of bodies permeated the entire area and all of the people living in the surrounding communities knew that exterminations were going on at Auschwitz.
 Rudolf H, Auschwitz camp commandant, Nuremberg testimony.[186]
In his seminal study of the Holocaust, The Destruction of the European Jews, the world's pre-eminent Holocaust scholar, Raul Hilberg, noted:
The reaction pattern of the Jews is characterized by almost complete lack of resistance. In marked contrast to German propaganda, the documentary evidence of Jewish resistance, overt or submerged, is very slight. On a European-wide scale the Jews had no resistance organization, no blueprint for armed action, no plan even for psychological warfare. They were completely unprepared. ... Measured in German casualties, Jewish armed opposition shrinks into insignificance. ... A large component of the entire [destruction] process depended on Jewish participation, from the simple acts of individuals to the organized activity in councils. ... Jewish resistance organizations attempting to reverse the mass inertia spoke the words: "Do not be led like sheep to slaughter." Franz Stangl, who had commanded two death camps, was asked in a West German prison about his reaction to the Jewish victims. He said that only recently he had read a book about lemmings. It reminded him of Treblinka.[187]
In his important study, Peter Longerich observes likewise: "On the Jewish side there was practically no resistance."[188] Hilberg accounts for this compliant attitude by evoking the history of Jewish persecution: as had been the case so many times before down through the centuries, simply appealing to their oppressors, and complying with orders, would hopefully avoid inflaming the situation and so mitigate the damage done to the Jews until the onslaught abated. "There were many casualties in these times of stress, but always the Jewish community emerged once again like a rock from a receding tidal wave. The Jews had never disappeared from the earth." They were "caught in the straitjacket of their history", and the realisation that this time was different came too late.[189]
Discussing the case of Warsaw, Timothy Snyder notes in a similar vein that it was only during the three months after the massive deportations of JulySeptember 1942 that general agreement on the need for armed resistance was reached, and lays the passivity emanating from the conservative center of Jewish politics at the door of the overall success the Jewish community had enjoyed by engaging in a quid pro quo with the pre-war Polish government.[190] By the time of the biggest act of armed resistance, the Warsaw Ghetto Uprising of spring 1943, only a small minority of Polish Jews were still alive.[188]
Yehuda Bauer and other historians argue that resistance consisted not only of physical opposition, but of any activity that gave the Jews dignity and humanity in humiliating and inhumane conditions.[191]
In every ghetto, in every deportation train, in every labor camp, even in the death camps, the will to resist was strong, and took many forms. Fighting with the few weapons that would be found, individual acts of defiance and protest, the courage of obtaining food and water under the threat of death, the superiority of refusing to allow the Germans their final wish to gloat over panic and despair. Even passivity was a form of resistance. To die with dignity was a form of resistance. To resist the demoralizing, brutalizing force of evil, to refuse to be reduced to the level of animals, to live through the torment, to outlive the tormentors, these too were acts of resistance. Merely to give a witness of these events in testimony was, in the end, a contribution to victory. Simply to survive was a victory of the human spirit.
 Martin Gilbert. The Holocaust: The Jewish Tragedy.[192]
Hilberg argued against overstating the extent of Jewish resistance, or using all-encompassing definitions of it like that deployed by Gilbert. "When relatively isolated or episodic acts of resistance are represented as typical, a basic characteristic of the German measures is obscured", namely that the merciless slaughter of peaceable innocent people is turned into some kind of battle. "The inflation of resistance has another consequence which has been of concern to those Jews who have regarded themselves as the actual resisters. If heroism is an attribute that should be assigned to every member of the European Jewish community, it will diminish the accomplishment of the few who took action." Finally, the blending of the passive majority with the active few was "not merely a form of dilution, which blurred the multitudinous problems of organizing a defense in a cautious, reluctant Jewish community; it was also a way of shutting off a great many questions about that community, its reasoning and survival strategy." Without posing these questions, Jewish history could not be written.[193]
The most famous example of Jewish armed resistance was the Warsaw Ghetto Uprising of January 1943, when thousands of poorly armed Jewish fighters held the SS at bay for four weeks before being crushed by overwhelmingly superior forces. According to Jewish accounts, several hundred Germans were killed, while the Germans claimed to have lost 17 dead and 93 wounded. 13,000 Jews were killed during the uprising, and 57,885 were deported and gassed according to German figures. This uprising was followed by the uprising in the Treblinka extermination camp in May 1943, when about 200 inmates escaped from the camp after overpowering the guards. They killed a number of German guards and set the camp buildings ablaze, but 900 inmates were also killed, and out of the 600 who successfully escaped, only 40 survived the war. Two weeks later, there was an uprising in the Biaystok Ghetto. In September, there was a short-lived uprising in the Vilna Ghetto. In October, 600 Jewish prisoners, including Jewish Soviet prisoners of war, attempted an escape at the Sobibor death camp. The prisoners killed 11 German SS officers and a number of camp guards. However, the killings were discovered, and the inmates were forced to run for their lives under heavy fire. 300 of the prisoners were killed during the escape. Most of the survivors either died in the minefields surrounding the camp or were recaptured and executed. About 60 survived and joined the Soviet partisans. On 7 October 1944, 250 Jewish Sonderkommandos (laborers) at Auschwitz attacked their guards and blew up Crematorium IV with explosives that female prisoners had smuggled in from a nearby factory. Three German guards were killed during the uprising, one of whom was stuffed into an oven. The Sonderkommandos attempted a mass breakout, but all 250 were killed soon after.
An estimated 20,000 to 30,000 Jewish partisans (see the list at the top of this section) actively fought the Nazis and their collaborators in Eastern Europe.[194][195] They engaged in guerilla warfare and sabotage against the Nazis, instigated Ghetto uprisings, and freed prisoners. In Lithuania alone, they killed approximately 3,000 German soldiers. As many as 1.4 million Jewish soldiers fought in the Allied armies.[196] Of these, approximately 40% served in the Red Army.[197] About 200,000 Jewish soldiers serving in the Red Army died in the war.[198] The Jewish Brigade, a unit of 5,000 Jewish volunteers from the British Mandate of Palestine, fought in the British Army. German-speaking Jewish volunteers from the Special Interrogation Group performed commando and sabotage operations against the Nazis behind front lines in the Western Desert Campaign.
In occupied Poland and Soviet territories, thousands of Jews fled into the swamps or forests and joined the partisans, although the partisan movements did not always welcome them. In Lithuania and Belarus, an area with a heavy concentration of Jews, and also an area which suited partisan operations, Jewish partisan groups saved thousands of Jewish civilians from extermination. No such opportunities existed for the Jewish populations of cities such as Budapest. However in Amsterdam, and other parts of the Netherlands, many Jews were active in the Dutch Resistance.[199] Timothy Snyder wrote that "Other combatants in the Warsaw Uprising were veterans of the ghetto uprising of 1943. Most of these Jews joined the Home Army; others found the People's Army, or even the antisemitic National Armed Forces. Some Jews (or Poles of Jewish origin) were already enlisted in the Home Army and the People's Army. Almost certainly, more Jews fought in the Warsaw Uprising of August 1944 than in the Warsaw Ghetto Uprising of April 1943."[200] Joining the partisans was an option only for the young and the fit who were willing to leave their families. Many Jewish families preferred to die together rather than be separated.
French Jews were also highly active in the French Resistance, which conducted a guerilla campaign against the Nazis and Vichy French authorities, assisted the Allies in their sweep across France, and supported Allied including Free French forces in the liberation of many occupied French cities. Although Jews made up only one percent of the French population, they made up fifteen to twenty percent of the French Resistance.[201] The Jewish youth movement EEIF, which had originally shown support for the Vichy regime, was banned in 1943, and many of its older members formed armed resistance units. Zionist Jews also formed the Armee Juive (Jewish Army), which participated in armed resistance under a Zionist flag, and smuggled Jews out of the country. Both organizations merged in 1944, and participated in the liberation of Paris, Lyon, Toulouse, Grenoble, and Nice.[202]
Many people think the Jews went to their deaths like sheep to the slaughter, and that's not trueit's absolutely not true. I worked closely with many Jewish people in the Resistance, and I can tell you, they took much greater risks than I did.
 Pieter Meerburg[203]
For the great majority of Jews resistance could take only the passive forms of delay, evasion, negotiation, bargaining and, where possible, bribery of German officials. The Nazis encouraged this by forcing the Jewish communities to police themselves, through bodies such as the Reich Association of Jews (Reichsvereinigung der Juden) in Germany and the Jewish Councils (Judenrte) in the urban ghettos in occupied Poland. They held out the promise of concessions in exchange for each surrender, enmeshing the Jewish leadership so deeply in well-intentioned compromise that a decision to stand and fight was never possible. Holocaust survivor Alexander Kimel wrote: "The youth in the Ghettos dreamed about fighting. I believe that although there were many factors that inhibited our responses, the most important factors were isolation and historical conditioning to accepting martyrdom."[citation needed]
The historical conditioning of the Jewish communities of Europe to accept persecution and avert disaster through compromise and negotiation was the most important factor in the failure to resist until the very end. The Warsaw Ghetto uprising took place only when the Jewish population had been reduced from 500,000 to 100,000, and it was obvious that no further compromise was possible. Paul Johnson writes: "The Jews had been persecuted for a millennium and a half and had learned from long experience that resistance cost lives rather than saved them. Their history, their theology, their folklore, their social structure, even their vocabulary trained them to negotiate, to pay, to plead, to protest, not to fight."[204]
The Jewish communities were also systematically deceived about German intentions, and were cut off from most sources of news from the outside world. The Germans told the Jews that they were being deported to work camps euphemistically calling it "resettlement in the East" and maintained this illusion through elaborate deceptions all the way to the gas chamber doors (which were marked with labels stating that the chambers were for removal of lice) to avoid uprisings. As photographs testify, Jews disembarked at the railway stations at Auschwitz and other extermination camps carrying sacks and suitcases, clearly having no idea of the fate that awaited them. Rumours of the reality of the extermination camps filtered back only slowly to the ghettos, and were usually not believed, just as they were not believed when couriers such as Jan Karski, the Polish resistance fighter, conveyed them to the western Allies.[205]
Heydrich was assassinated in Prague in June 1942. He was succeeded as head of the RSHA by Ernst Kaltenbrunner. Kaltenbrunner and Eichmann, under Himmler's close supervision, oversaw the climax of the Final Solution. During 1943 and 1944, the extermination camps worked at a furious rate to kill the hundreds of thousands of people shipped to them by rail from almost every country within the German sphere of influence. By the spring of 1944, up to 8,000 people were being gassed every day at Auschwitz.[206]
Despite the high productivity of the war industries based in the Jewish ghettos in the General Government, during 1943 they were liquidated, and their populations shipped to the camps for extermination. The largest of these operations, the deportation of 100,000 people from the Warsaw Ghetto in early 1943, provoked the Warsaw Ghetto Uprising, which was suppressed with great brutality. Approximately 42,000 Jews were shot during the Operation Harvest Festival on 34 November 1943.[207] At the same time, rail shipments arrived regularly from western and southern Europe. Few Jews were shipped from the occupied Soviet territories to the camps: the killing of Jews in this zone was left in the hands of the SS, aided by locally recruited auxiliaries. In any case, by the end of 1943 the Germans had been driven from most Soviet territory.
Shipments of Jews to the camps had priority on the German railways, and continued even in the face of the increasingly dire military situation after the Battle of Stalingrad at the end of 1942 and the escalating Allied air attacks on German industry and transport. Army leaders and economic managers complained at this diversion of resources and at the killing of irreplaceable skilled Jewish workers. By 1944, moreover, it was evident to most Germans not blinded by Nazi fanaticism that Germany was losing the war. Many senior officials began to fear the retribution that might await Germany and them personally for the crimes being committed in their name. But the power of Himmler and the SS within the German Reich was too great to resist, and Himmler could always evoke Hitler's authority for his demands.
In October 1943, Himmler gave a speech to senior Nazi Party officials gathered in Posen (now Pozna in western Poland). Here he came closer than ever before to stating explicitly that he was intent on exterminating the Jews of Europe:
I may here in this closest of circles allude to a question which you, my party comrades, have all taken for granted, but which has become for me the most difficult question of my life, the Jewish question ... I ask of you that what I say in this circle you really only hear and never speak of ... We come to the question: how is it with the women and children? I have resolved even here on a completely clear solution. I do not consider myself justified in eradicating the menso to speak killing them or ordering them to be killedand allowing the avengers in the shape of the children to grow up ... The difficult decision had to be taken, to cause this people to disappear from the earth.
The audience for this speech included Admiral Karl Dnitz and Armaments Minister Albert Speer. Dnitz successfully claimed at the Nuremberg trials that he had had no knowledge of the Final Solution. Speer, however, declared at the trial and in a subsequent interview that "If I didn't see it, then it was because I didn't want to see it."[208] The text of this speech was not known at the time of their trials.
The scale of extermination slackened somewhat at the beginning of 1944 once the ghettos in occupied Poland were emptied, but on 19 March 1944, Hitler ordered the military occupation of Hungary, and Eichmann was dispatched to Budapest to supervise the deportation of Hungary's 800,000 Jews. Hitler had personally complained to the Hungarian regent Admiral Mikls Horthy on the previous day, 18 March 1944, that:
Hungary did nothing in the matter of the Jewish problem, and was not prepared to settle accounts with the large Jewish population in Hungary.[209]
More than half of them were shipped to Auschwitz in the course of the year. The commandant, Rudolf H, said at his trial that he killed 400,000 Hungarian Jews in three months.
The operation to kill Hungarian Jews met strong opposition within the Nazi hierarchy, and there were some suggestions that Hitler should offer the Allies a deal where they would be spared in exchange for a favorable peace settlement. There were unofficial negotiations in Istanbul between Himmler's agents, British agents, and representatives of Jewish organizations; at one point an attempt by Eichmann to exchange one million Jews for 10,000 trucksthe so-called "blood for goods" proposalbut there was no real possibility of such a deal being struck on this scale.
Escapes from the camps were few, but not unknown. In 1940, the Auschwitz commandant reported that "the local population is fanatically Polish and ... prepared to take any action against the hated SS camp personnel. Every prisoner who managed to escape can count on help the moment he reaches the wall of a first Polish farmstead."[211] According to Ruth Linn, however, escapees, particularly Jewish ones, could not rely on help from the local population or the Polish underground.[212]
In February 1942, an escaped inmate from the Chemno extermination camp, Jacob Grojanowski, reached the Warsaw Ghetto, where he gave detailed information about the Chemno camp to the Oneg Shabbat group. His report, which became known as the Grojanowski Report, was smuggled out of the ghetto through the channels of the Polish underground to the Delegatura, and reached London by June 1942. It is unclear what was done with the report at that point.[163][213][214][215] In the meantime, by 1 February, the United States Office of War Information had decided not to release information about the extermination of the Jews because it was felt that it would mislead the public into thinking the war was simply a Jewish problem.[216]
By at least 9 October 1942, British radio had broadcast news of gassing of Jews to the Netherlands.[217] In December 1942, the western Allies released the Joint Declaration by Members of the United Nations, that described how "Hitler's oft-repeated intention to exterminate the Jewish people in Europe" was being carried out and which declared that they "condemn in the strongest possible terms this bestial policy of cold-blooded extermination."[218]
In 1942, Jan Karski reported to the Polish, British and U.S. governments on the situation in Poland, especially the destruction of the Warsaw Ghetto and the Holocaust of the Jews. He met with Polish politicians in exile including the prime minister, as well as members of political parties such as the Socialist Party, National Party, Labor Party, People's Party, Jewish Bund and Poalei Zion. He also spoke to Anthony Eden, the British foreign secretary, and included a detailed statement on what he had seen in Warsaw and Beec.[219] In 1943 in London he met the then-well-known journalist Arthur Koestler. He then traveled to the United States and reported to President Franklin D. Roosevelt. His report was a major factor in informing the West.
In July 1943, Karski again personally reported to Roosevelt about the situation in Poland. During their meeting Roosevelt suddenly interrupted his report and asked about the condition of horses in occupied Poland.[220][221][222] He also met with many other government and civic leaders in the United States, including Felix Frankfurter, Cordell Hull, William Joseph Donovan, and Stephen Wise. Karski also presented his report to media, bishops of various denominations (including Cardinal Samuel Stritch), members of the Hollywood film industry and artists, but without success. Many of those he spoke to did not believe him, or supposed that his testimony was much exaggerated or was propaganda from the Polish government in exile.[223]
News about gassing Jews was also published in illegal newspapers of the Dutch resistance, like in the issue of Het Parool of 27 September 1943. However, the news was so unbelievable that many assumed it was merely war propaganda. The publications were halted because they were counter-productive for the Dutch resistance. Nevertheless, many Jews were warned that they would be murdered, but as escape was impossible for most of them, they preferred to believe that the warnings were false.[224][225]
In September 1940, Captain Witold Pilecki, a member of the Polish underground and a soldier of the Polish Home Army, worked out a plan to enter Auschwitz and volunteered to be sent there, the only known person to volunteer to be imprisoned at Auschwitz. He organized an underground network Zwizek Organizacji Wojskowej (translation: "Union of Military Organizations") that was ready to initiate an uprising but it was decided that the probability of success was too low for the uprising to succeed. UMO's numerous and detailed reports became later a principal source of intelligence on Auschwitz for the Western Allies. Pilecki escaped from Auschwitz with information that became the basis of a two-part report in August 1943 that was sent to the Office of Strategic Services (OSS) in London. The report included details about the gas chambers, about "selection", and about the sterilization experiments. It stated that there were three crematoria in Birkenau able to burn 10,000 people daily, and that 30,000 people had been gassed in one day. The author wrote: "History knows no parallel of such destruction of human life."[226] When Pilecki returned to Poland after the war the communist authorities arrested and accused him of spying for the Polish government in exile. He was sentenced to death in a show trial and was executed on 25 May 1948.
Before Pilecki escaped from Auschwitz the most spectacular escape took place on 20 June 1942, when Ukrainian Eugeniusz Bendera and three Poles, Kazimierz Piechowski, Stanisaw Gustaw Jaster and Jzef Lempart made a daring escape.[227] The escapees were dressed as members of the SS-Totenkopfverbnde, fully armed and in an SS staff car. They drove out the main gate in a stolen Rudolf Hoss automobile Steyr 220 with a smuggled first report from Witold Pilecki to Polish resistance about the Holocaust. The Germans never recaptured any of them.[228]
Rudolf Vrba and Alfred Wetzler, Jewish inmates, escaped from Auschwitz in April 1944, eventually reaching Slovakia. The 32-page document they dictated to Jewish officials about the mass murder at Auschwitz became known as the Vrba-Wetzler report. Vrba had an eidetic memory and had worked on the Judenrampe, where Jews disembarked from the trains to be "selected" either for the gas chamber or slave labor. The level of detail with which he described the transports allowed Slovakian officials to compare his account with their own deportation records, and the corroboration convinced the Allies to take the report seriously.[229]
Two other Auschwitz inmates, Arnost Rosin and Czesaw Mordowicz escaped on 27 May 1944, arriving in Slovakia on 6 June, the day of the Normandy landing (D-Day). Hearing about Normandy, they believed the war was over and got drunk to celebrate, using dollars they'd smuggled out of the camp. They were arrested for violating currency laws, and spent eight days in prison, before the Judenrat paid their fines. The additional information they offered the Judenrat was added to Vrba and Wetzler's report and became known as the Auschwitz Protocols. They reported that, between 15 and 27 May 1944, 100,000 Hungarian Jews had arrived at Birkenau, and had been killed at an unprecedented rate, with human fat being used to accelerate the burning.[230]
The BBC and The New York Times published material from the Vrba-Wetzler report on 15 June[231] 20 June 3 July[232] and 6 July[233] 1944. The subsequent pressure from world leaders persuaded Mikls Horthy to bring the mass deportations of Jews from Hungary to Auschwitz to a halt on 9 July, saving up to 200,000 Jews from the extermination camps.[230]
On 14 November 2001, in the 150th anniversary issue, The New York Times ran an article by former editor Max Frankel reporting that before and during World War II, the Times had maintained a strict policy in their news reporting and editorials to minimize reports on the Holocaust.[234] The Times accepted the detailed analysis and findings of journalism professor Laurel Leff, who had published an article the year before in the Harvard International Journal of the Press and Politics, that The New York Times had deliberately suppressed news of the Third Reich's persecution and murder of Jews.[235] Leff concluded that New York Times reporting and editorial policies made it virtually impossible for American Jews to impress Congress, church or government leaders with the importance of helping Europe's Jews.[236]
By mid 1944, the Final Solution had largely run its course. Those Jewish communities within easy reach of the Nazi regime had been largely exterminated, in proportions ranging from about 25 percent in France to more than 90 percent in Poland. On 5 May, Himmler claimed in a speech that "The Jewish question has in general been solved in Germany and in the countries occupied by Germany."[237] During 1944, in any case, the task became steadily more difficult. German armies were evicted from the Soviet Union, the Balkans and Italy, and German forcesas well as forces aligned with themwere either defeated or were switching sides to the Allies. In June, the western Allies landed in France. Allied air attacks and the operations of partisans made rail transport increasingly difficult, and the objections of the military to the diversion of rail transport for carrying Jews to Poland more urgent and harder to ignore.
At this time, as the Soviet armed forces approached, the camps in eastern Poland were closed down, any surviving inmates being shipped west to camps closer to Germany, first to Auschwitz and later to Gross Rosen in Silesia. Auschwitz itself was closed as the Soviets advanced through Poland. The last 13 prisoners, all women, were killed in Auschwitz II on 25 November 1944; records show they were "unmittelbar gettet" ("killed outright"), leaving open whether they were gassed or otherwise disposed of.[238]
Despite the desperate military situation, great efforts were made to conceal evidence of what had happened in the camps. The gas chambers were dismantled, the crematoria dynamited, mass graves dug up and the corpses cremated, and Polish farmers were induced to plant crops on the sites to give the impression that they had never existed. Local commanders continued to kill Jews, and to shuttle them from camp to camp by forced "death marches" until the last weeks of the war.[239]
Already sick after months or years of violence and starvation, prisoners were forced to march for tens of miles in the snow to train stations; then transported for days at a time without food or shelter in freight trains with open carriages; and forced to march again at the other end to the new camp. Those who lagged behind or fell were shot. Around 250,000 Jews died during these marches.[240]
The largest and best-known of the death marches took place in January 1945, when the Soviet army advanced on Poland. Nine days before the Soviets arrived at Auschwitz, the SS marched 60,000 prisoners out of the camp toward Wodzislaw, 56km (35mi) away, where they were put on freight trains to other camps. Around 15,000 died on the way. Elie Wiesel and his father, Shlomo, were among the marchers:
An icy wind blew in violent gusts. But we marched without faltering. ...
Pitch darkness. Every now and then, an explosion in the night. They had orders to fire on any who could not keep up. Their fingers on the triggers, they did not deprive themselves of this pleasure. If one of us had stopped for a second, a sharp shot finished off another filthy son of a bitch. ...
Near me, men were collapsing in the dirty snow. Shots.[241]
The first major camp, Majdanek, was discovered by the advancing Soviets on 23 July 1944. Chemno was liberated by the Soviets on 20 January 1945. Auschwitz was liberated, also by the Soviets, on 27 January 1945;[242] Buchenwald by the Americans on 11 April;[243] Bergen-Belsen by the British on 15 April;[244] Dachau by the Americans on 29 April;[245] Ravensbrck by the Soviets on the same day; Mauthausen by the Americans on 5 May;[246] and Theresienstadt by the Soviets on 8 May.[247] Treblinka, Sobibor, and Beec were never liberated, but were destroyed by the Nazis in 1943. Colonel William W. Quinn of the U.S. 7th Army said of Dachau: "There our troops found sights, sounds, and stenches horrible beyond belief, cruelties so enormous as to be incomprehensible to the normal mind."[248][249]
In most of the camps discovered by the Soviets, almost all the prisoners had already been removed, leaving only a few thousand alive7,600 inmates were found in Auschwitz,[250] including 180 children who had been experimented on by doctors. Some 60,000 prisoners were discovered at Bergen-Belsen by the British 11th Armoured Division,[251] 13,000 corpses lay unburied, and another 10,000 died from typhus or malnutrition over the following weeks.[252] The British forced the remaining SS guards to gather up the corpses and place them in mass graves.[253]
The BBC's Richard Dimbleby described the scenes that greeted him and the British Army at Belsen:
Here over an acre of ground lay dead and dying people. You could not see which was which ... The living lay with their heads against the corpses and around them moved the awful, ghostly procession of emaciated, aimless people, with nothing to do and with no hope of life, unable to move out of your way, unable to look at the terrible sights around them ... Babies had been born here, tiny wizened things that could not live ... A mother, driven mad, screamed at a British sentry to give her milk for her child, and thrust the tiny mite into his arms ... He opened the bundle and found the baby had been dead for days. This day at Belsen was the most horrible of my life.[254]
The number of victims depends on which definition of "the Holocaust" is used. Donald Niewyk and Francis Nicosia write in The Columbia Guide to the Holocaust that the term is commonly defined as the mass murder of more than 5 million European Jews. They further state that 'Not everyone finds this a fully satisfactory definition.'[267] According to Martin Gilbert the total number of victims is just under six millionaround 78 percent of the 7.3 million Jews in occupied Europe at the time.[268]
Broader definitions include approximately 2 to 3 million Soviet POWs, 2 million ethnic Poles, up to 1,500,000 Romani, 200,000 handicapped, political and religious dissenters, 15,000 homosexuals and 5,000 Jehovah's Witnesses, bringing the death toll to around 11 million. The broadest definition would include 6 million Soviet civilians, raising the death toll to 17 million.[267] R.J. Rummel estimates the total democide death toll of Nazi Germany to be 21 million. Other estimates put total casualties of Soviet Union's citizens alone to about 26 million.[269]
Since 1945, the most commonly cited figure for the total number of Jews killed has been six million. The Yad Vashem Holocaust Martyrs' and Heroes' Remembrance Authority in Jerusalem, writes that there is no precise figure for the number of Jews killed. The figure most commonly used is the six million attributed to Adolf Eichmann, a senior SS official.[270] Early calculations range from 5.1 million from Raul Hilberg, to 5.95 million from Jacob Leschinsky. Yisrael Gutman and Robert Rozett in the Encyclopedia of the Holocaust estimate 5.595.86 million.[271] A study led by Wolfgang Benz of the Technical University of Berlin suggests 5.296.2 million.[272][273] Yad Vashem writes that the main sources for these statistics are comparisons of prewar and postwar censuses and population estimates, and Nazi documentation on deportations and murders.[272] Its Central Database of Shoah Victims' Names currently holds close to 3 million names of Holocaust victims, all accessible online. Yad Vashem continues its project of collecting names of Jewish victims from historical documents and individual memories.[274]
Hilberg's estimate of 5.1 million, in the third edition of The Destruction of the European Jews, includes over 800,000 who died from "ghettoization and general privation"; 1,400,000 killed in open-air shootings; and up to 2,900,000 who perished in camps. Hilberg estimates the death toll of Jews in Poland as up to 3,000,000.[275] Hilberg's numbers are generally considered to be a conservative estimate, as they typically include only those deaths for which records are available, avoiding statistical adjustment.[276]
British historian Martin Gilbert arrived at a "minimum estimate" of over 5.75 million Jewish victims.[277] Lucy S. Dawidowicz used pre-war census figures to estimate that 5.934 million Jews died (see table below).[278]
There were about 8 to 10 million Jews in the territories controlled directly or indirectly by Germany (the uncertainty arises from the lack of knowledge about how many Jews there were in the Soviet Union). The six million killed in the Holocaust thus represent 60 to 75 percent of these Jews. Of Poland's 3.3 million Jews, over 90 percent were killed. The same proportion were killed in Latvia and Lithuania, but most of Estonia's Jews were evacuated in time. Of the 750,000 Jews in Germany and Austria in 1933, only about a quarter survived. Although many German Jews emigrated before 1939, the majority of these fled to Czechoslovakia, France or the Netherlands, from where they were later deported to their deaths. In Czechoslovakia, Greece, the Netherlands, and Yugoslavia, over 70 percent were killed. 50 to 70 percent were killed in Romania, Belgium and Hungary. It is likely that a similar proportion were killed in Belarus and Ukraine, but these figures are less certain. Countries with notably lower proportions of deaths include Bulgaria, Denmark, France, Italy, and Norway. Albania was the only country occupied by Germany that had a significantly larger Jewish population in 1945 than in 1939. About two hundred native Jews and over a thousand refugees were provided with false documents, hidden when necessary, and generally treated as honored guests in a country whose population was roughly 60% Muslim.[279] Additionally, Japan, as an Axis member, had its own unique response to German policies regarding Jews; see Shanghai Ghetto.
This gives a total of over 3.8 million; of these, 8090% were estimated to be Jews. These seven camps thus accounted for half the total number of Jews killed in the entire Nazi Holocaust. Virtually the entire Jewish population of Poland died in these camps.[256]
In addition to those who died in the above extermination camps, at least half a million Jews died in other camps, including the major concentration camps in Germany. These were not extermination camps, but had large numbers of Jewish prisoners at various times, particularly in the last year of the war as the Nazis withdrew from Poland. About a million people died in these camps, and although the proportion of Jews is not known with certainty, it was estimated to be at least 50 percent.[citation needed] Another 800,000 to one million Jews were killed by the Einsatzgruppen in the occupied Soviet territories (an approximate figure, since the Einsatzgruppen killings were frequently undocumented).[283] Many more died through execution or of disease and malnutrition in the ghettos of Poland before they could be deported.
In the 1990s, the opening of government archives in Eastern Europe resulted in the adjustment of the death tolls published in the pioneering work by Hilberg, Dawidowicz and Gilbert (e.g. compare Gilbert's estimation of 2 million deaths in Auschwitz-Birkenau with the updated figure of 1 million in the Extermination Camp data box). As pointed out above, Wolfgang Benz has been carrying out work on the more recent data. He concluded in 1999:
As the significant majority of the Jewish victims of the Holocaust were speakers of Yiddish, the Holocaust had a profound and permanent effect on the fate of Yiddish language and culture (see Yiddish Renaissance). On the eve of World War II, there were 11 to 13 million Yiddish speakers in the world.[285] The Holocaust led to a dramatic, sudden decline in the use of Yiddish, as the extensive Jewish communities, both secular and religious, that used it in their day-to-day life were largely destroyed. Around 5 million (85%) of the victims of the Holocaust were speakers of Yiddish.[286] Of the remaining non-Yiddish population, the Ladino speaking populations of Greece and the Balkans were also destroyed, which contributed to the extinction of this Judaeo-Spanish language.
Himmler's Generalplan Ost (General Plan East), which was enthusiastically agreed to by Hitler in the summer of 1942, involved exterminating, expelling, or enslaving most or all Slavs from their native lands so as to make living space for German settlers, something that would be carried out over a period of 2030 years.[287][288]
William W. Hagen wrote:
Generalplan Ost ... forecast the diminution of the targeted east European peoples' populations by the following measures: Poles  85 percent; Belarusians  75 percent; Ukrainians  65 percent; Czechs  50 percent. These enormous reductions would result from "extermination through labor" or decimation through malnutrition, disease, and controls on reproduction. ... The Russian people, once subjugated in war, would join the four Slavic-speaking nations whose fate Generalplan Ost foreshadowed.[289]
It is a question of existence, thus it will be a racial struggle of pitiless severity, in the course of which 20 to 30 million Slavs and Jews will perish through military actions and crises of food supply.
 Heinrich Himmler spoke about Operation Barbarossa, June 1941[290]
German planners had in November 1939 called for "the complete destruction" of all Poles.[291] "All Poles", Heinrich Himmler swore, "will disappear from the world".[292] The Polish state under German occupation was to be cleared of ethnic Poles and settled by German colonists.[293] Of the Poles, by 1952 only about 34 million of them were to be left in the former Poland, and only to serve as slaves for German settlers. They were to be forbidden to marry, the existing ban on any medical help to Poles in Germany would be extended, and eventually Poles would cease to exist. On 22 August 1939, just over a week before the onset of war, Hitler declared that "the object of the war is ... physically to destroy the enemy. That is why I have prepared, for the moment only in the East, my 'Death's Head' formations with orders to kill without pity or mercy all men, women, and children of Polish descent or language. Only in this way can we obtain the living space we need."[294] Nazi planners decided against a genocide of ethnic Poles on the same scale as against ethnic Jews; it could not proceed in the short run since "such a solution to the Polish question would represent a burden to the German people into the distant future, and everywhere rob us of all understanding, not least in that neighbouring peoples would have to reckon at some appropriate time, with a similar fate".[295]
The actions taken against ethnic Poles were not on the scale of the genocide of the Jews. Most Polish Jews (perhaps 90% of their pre-war population) perished during the Holocaust, while most Christian Poles survived the brutal German occupation.[296] Between 1.8 and 2.1 million non-Jewish Polish citizens perished in German hands during the course of the war, about four-fifths of whom were ethnic Poles with the remaining fifth being ethnic minorities of Ukrainians and Belarusians, the vast majority of them civilians.[258][259] At least 200,000 of these victims died in concentration camps with about 146,000 being killed in Auschwitz. Many others died as a result of general massacres such as in the Warsaw Uprising where between 120,000 and 200,000 civilians were killed.[297][298] The policy of the Germans in Poland included diminishing food rations, conscious lowering of the state of hygiene and depriving the population of medical services. The general mortality rate rose from 13 to 18 per thousand.[299] Overall, about 5.6 million of the victims of World War II were Polish citizens,[259] both Jewish and non-Jewish, and over the course of the war Poland lost 16 percent of its pre-war population; approximately 3.1 million of the 3.3 million Polish Jews and approximately 2 million of the 31.7 million non-Jewish Polish citizens died at German hands during the war.[300] According to recent (2009) estimates by IPN, over 2.5 million non-Jewish Polish citizens died as a result of the German occupation.[301] Over 90 percent of the death toll came through non-military losses, as most of the civilians were targeted by various deliberate actions by Nazi Germany and the Soviet Union.[297]
A few days before the invasion of Poland, on 22 August 1939, Adolf Hitler said to his generals:
Genghis Khan led millions of women and children to slaughterwith premeditation and a happy heart. History sees in him solely the founder of a state. ... Our war aim does not consist in reaching certain lines, but in the physical destruction of the enemy. Accordingly, I have placed my death-head formations in readinessfor the present only in the Eastwith orders to them to send to death mercilessly and without compassion, men, women, and children of Polish derivation and language. Only thus shall we gain the living space (Lebensraum) which we need. Who, after all, speaks today of the annihilation of the Armenians? ... Poland will be depopulated and settled with Germans. ... As for the rest, gentlemen, the fate of Russia will be exactly the same as I am now going through with in the case of Poland.[302][303]
In the Balkans, up to 581,000 Yugoslavs were killed by the Nazis and their Croatian fascist allies in Yugoslavia.[304][305] German forces, under express orders from Hitler, fought with a special vengeance against the Serbs, who were considered Untermensch.[306] The Ustae collaborators conducted a systematic extermination of large numbers of people for political, religious or racial reasons. The most numerous victims were Serbs.
Bosniaks, Croats and others were also victims of the Jasenovac concentration camp. According to the U.S. Holocaust Museum:
"The Ustaa authorities established numerous concentration camps in Croatia between 1941 and 1945. These camps were used to isolate and murder Serbs, Jews, Roma, Muslims [Bosniaks], and other non-Catholic minorities, as well as Croatian political and religious opponents of the regime."
The USHMM and Jewish Virtual Library report between 56,000 and 97,000 persons were killed at the Jasenovac concentration camp.[307][308][309] Yad Vashem reports an overall number of over 500,000 murders of Serbs "in horribly sadistic ways" at the hands of the Ustaa.[310]
As per the most recent study, Bosnjaci u Jasenovackom logoru ("Bosniaks in Jasenovac concentration camp") by the author Nihad Halilbegovic, at least 103,000 Bosniaks (Bosnian Muslim Slavs) perished during Holocaust at the hands of the Nazi regime and Croatian Ustae. According to the study "unknown is the full number of Bosniaks who were murdered under Serb or Croat alias or national name" and "large numbers of Bosniaks were killed and listed under Roma populations", therefore in advance sentenced to death and extermination.[311][312]
Excluding Slovenes under Italian rule, between 20,000 and 25,000 Slovenes were killed by Nazis or fascists (counting only civilian victims).[313]
The Nazi and Albanian cooperation entity was followed by extensive persecution of non-Albanians (mostly Serbs) by Albanian fascists. Most of the war crimes were perpetrated by the Skenderbeg SS Division and the Balli Kombtar. Albanian Fascists killed 40,000 to 60,000 Serbs and another 200,000 were driven out.[314][315]
Soviet civilian populations in the occupied areas were also heavily persecuted (in addition to the barbarity of the Eastern Front frontline warfare manifesting itself in episodes such as the siege of Leningrad in which more than 1 million civilians died).[316] Thousands of peasant villages across Russia, Belarus and Ukraine were annihilated by German troops. Bohdan Wytwycky has estimated that as many as one quarter of all Soviet civilian deaths at the hands of the Nazis and their allies were racially motivated.[267] The Russian Academy of Sciences in 1995 reported civilian victims in the USSR at German hands, including Jews, totaled 13.7 million dead, 20% of the 68 million persons in the occupied USSR. This included 7.4 million victims of Nazi genocide and reprisals.[317]
In Belarus, Nazi Germany imposed a regime in the country that was responsible for burning down some 9,000 villages, deporting some 380,000 people for slave labour, and killing hundreds of thousands of civilians. More than 600 villages, like Khatyn, were burned along with their entire population and at least 5,295 Belarusian settlements were destroyed by the Nazis and some or all of their inhabitants killed. Tim Snyder states: "Of the nine million people who were on the territory of Soviet Belarus in 1941, some 1.6 million were killed by the Germans in actions away from battlefields, including about 700,000 prisoners of war, 500,000 Jews, and 320,000 people counted as partisans (the vast majority of whom were unarmed civilians."[318]
The German racists assigned the Slavs to the lowest rank of human life, from which the Jews were altogether excluded. The Germans thus looked upon Slavs as people not fit to be educated, not able to govern themselves, worthy only as slaves whose existence would be justified because they served their German masters. Hitler's racial policy with regard to the Slavs, to the extent that it was formulated, was "depopulation." The Slavs were to be prevented from procreating, except to provide the necessary continuing supply of slave laborers.
 Lucy Dawidowicz, The Holocaust and the historians[319]
According to Michael Berenbaum, between two and three million Soviet prisoners-of-waror around 57 percent of all Soviet POWsdied of starvation, mistreatment, or executions between June 1941 and May 1945, and most those during their first year of captivity. According to other estimates by Daniel Goldhagen, an estimated 2.8 million Soviet POWs died in eight months in 194142, with a total of 3.5 million by mid-1944.[320] The USHMM has estimated that 3.3 million of the 5.7 million Soviet POWs died in German custodycompared to 8,300 of 231,000 British and American prisoners.[321] The death rates decreased as the POWs were needed to work as slaves to help the German war effort; by 1943, half a million of them had been deployed as slave labor.[257]
[T]hey wish to toss into the Ghetto everything that is characteristically dirty, shabby, bizarre, of which one ought to be frightened and which anyway had to be destroyed.
Emmanuel Ringelblum on the Roma.[322]
Because the Roma and Sinti are traditionally a secretive people with a culture based on oral history, less is known about their experience of the genocide than about that of any other group.[323] Yehuda Bauer writes that the lack of information can be attributed to the Roma's distrust and suspicion, and to their humiliation, because some of the basic taboos of Romani culture regarding hygiene and sexual contact were violated at Auschwitz. Bauer writes that "most [Roma] could not relate their stories involving these tortures; as a result, most kept silent and thus increased the effects of the massive trauma they had undergone."[324]
The treatment of Romanis was not consistent in the different areas that Nazi Germany conquered. In some areas (e.g. Luxembourg and the Baltic countries), the Nazis killed virtually the entire Romani population. In other areas (e.g. Denmark, Greece), there is no record of Romanis being subjected to mass killings.[325]
Donald Niewyk and Frances Nicosia write that the death toll was at least 130,000 of the nearly one million Roma and Sinti in Nazi-controlled Europe.[323] Michael Berenbaum writes that serious scholarly estimates lie between 90,000 and 220,000.[326] A study by Sybil Milton, senior historian at the U.S. Holocaust Memorial Museum, calculated a death toll of at least 220,000 and possibly closer to 500,000, but this study explicitly excluded the Independent State of Croatia where the genocide of Romanies was intense.[260][327] Martin Gilbert estimates a total of more than 220,000 of the 700,000 Romani in Europe.[328] Ian Hancock, Director of the Program of Romani Studies and the Romani Archives and Documentation Center at the University of Texas at Austin, has argued in favour of a higher figure of between 500,000 and 1,500,000.[261] Hancock writes that, proportionately, the death toll equaled "and almost certainly exceed[ed], that of Jewish victims."[329]
Before being sent to the camps, the victims were herded into ghettos, including several hundred into the Warsaw Ghetto.[109] Further east, teams of Einsatzgruppen tracked down Romani encampments and murdered the inhabitants on the spot, leaving no records of the victims. They were also targeted by the puppet regimes that cooperated with the Nazis, e.g. the Ustae regime in Croatia, where a large number of Romani were killed in the Jasenovac concentration camp. The genocide analyst Helen Fein has stated that the Ustashe killed virtually every Romani in Croatia.[330]
In May 1942, the Romani were placed under the same labor and social laws as the Jews. On 16 December 1942, Heinrich Himmler, Commander of the SS and regarded as the "architect" of the Nazi genocide,[331] issued a decree that "Gypsy Mischlinge (mixed breeds), Romani, and members of the clans of Balkan origins who are not of German blood" should be sent to Auschwitz, unless they had served in the Wehrmacht.[332] On 29 January 1943, another decree ordered the deportation of all German Romani to Auschwitz.
This was adjusted on 15 November 1943, when Himmler ordered that, in the occupied Soviet areas, "sedentary Gypsies and part-Gypsies (Mischlinge) are to be treated as citizens of the country. Nomadic Gypsies and part-Gypsies are to be placed on the same level as Jews and placed in concentration camps."[333] Bauer argues that this adjustment reflected Nazi ideology that the Roma, originally an Aryan population, had been "spoiled" by non-Romani blood.[334]
The number of black people in Germany when the Nazis came to power is variously estimated at 5,00025,000.[335] It is not clear whether these figures included Asians. According to the United States Holocaust Memorial Museum, Washington, D.C., "The fate of black people from 1933 to 1945 in Nazi Germany and in German-occupied territories ranged from isolation to persecution, sterilization, medical experimentation, incarceration, brutality, and murder. However, there was no systematic program for their elimination as there was for Jews and other groups."[336] Meanwhile, Afrikaaners, Berbers, Iranians and Pre-Partition Indians were classified as Aryans, so not persecuted (see main article).
Our starting-point is not the individual, and we do not subscribe to the view that one should feed the hungry, give drink to the thirsty or clothe the nakedthose are not our objectives. Our objectives are entirely different. They can be put most crisply in the sentence: we must have a healthy people in order to prevail in the world.
Joseph Goebbels, 1938.[339]
Action T4 was a program established in 1939 to maintain the genetic "purity" of the German population by killing or sterilizing German and Austrian citizens who were judged to be disabled or suffering from mental disorder.[340]
Between 1939 and 1941, 80,000 to 100,000 mentally ill adults in institutions were killed; 5,000 children in institutions; and 1,000 Jews in institutions.[341] Outside the mental health institutions, the figures are estimated as 20,000 (according to Dr. Georg Renno, the deputy director of Schloss Hartheim, one of the euthanasia centers) or 400,000 (according to Frank Zeireis, the commandant of Mauthausen concentration camp).[341] Another 300,000 were forcibly sterilized.[342] Overall it has been estimated that over 200,000 individuals with mental disorders of all kinds were put to death, although their mass murder has received relatively little historical attention. Along with physically disabled, people suffering from dwarfism were persecuted as well. Many were put on display in cages and experimented on by the Nazis.[343] Despite not being formally ordered to take part, psychiatrists and psychiatric institutions were at the center of justifying, planning and carrying out the atrocities at every stage, and "constituted the connection" to the later annihilation of Jews and other "undesirables" in the Holocaust.[344] After strong protests by the German Catholic and Protestant churches on 24 August 1941 Hitler ordered the cancellation of the T4 program.[345]
The program was named after Tiergartenstrae 4, the address of a villa in the Berlin borough of Tiergarten, the headquarters of the General Foundation for Welfare and Institutional Care,[346] led by Philipp Bouhler, head of Hitler's private chancellery (Kanzlei des Fhrer der NSDAP) and Karl Brandt, Hitler's personal physician.
Brandt was tried in December 1946 at Nuremberg, along with 22 others, in a case known as United States of America vs. Karl Brandt et al., also known as the Doctors' Trial. He was hanged at Landsberg Prison on 2 June 1948.
Between 5,000 and 15,000 homosexuals of German nationality are estimated to have been sent to concentration camps.[265] James D. Steakley writes that what mattered in Germany was criminal intent or character, rather than criminal acts, and the "gesundes Volksempfinden" ("healthy sensibility of the people") became the leading normative legal principle.[347] In 1936, Himmler created the Reich Central Office for the Combating of Homosexuality and Abortion.[348] Homosexuality was declared contrary to "wholesome popular sentiment,"[265] and homosexuals were consequently regarded as "defilers of German blood." The Gestapo raided gay bars, tracked individuals using the address books of those they arrested, used the subscription lists of gay magazines to find others, and encouraged people to report suspected homosexual behavior and to scrutinize the behavior of their neighbours.[265][347]
Tens of thousands were convicted between 1933 and 1944 and sent to camps for "rehabilitation", where they were identified by yellow armbands[349] and later pink triangles worn on the left side of the jacket and the right trouser leg, which singled them out for sexual abuse.[347] Hundreds were castrated by court order.[350] They were humiliated, tortured, used in hormone experiments conducted by SS doctors, and killed.[265] Steakley writes that the full extent of gay suffering was slow to emerge after the war. Many victims kept their stories to themselves because homosexuality remained criminalized in postwar Germany. Around two percent of German homosexuals were persecuted by Nazis.[347]
German communists, socialists and trade unionists were among the earliest domestic opponents of Nazism[351] and were also among the first to be sent to concentration camps. Hitler claimed that communism was a Jewish ideology which the Nazis termed "Judeo-Bolshevism". Fear of communist agitation was used as justification for the Enabling Act of 1933, the law which gave Hitler his original dictatorial powers. Hermann Gring later testified at the Nuremberg Trials that the Nazis' willingness to repress German communists prompted President Paul von Hindenburg and the German elite to cooperate with the Nazis.[citation needed] MI6 assisted the Gestapo via "the exchange of information about Communism", and, as late as October 1937, the head of the British agency's Berlin station, Frank Foley, described his relationship with Heinrich Mller's so-called communism expert as "cordial".[352]
Hitler and the Nazis also hated German leftists because of their resistance to the party's racism. Many leaders of German leftist groups were Jews, and Jews were especially prominent among the leaders of the Spartacist uprising in 1919. Hitler already referred to Marxism and "Bolshevism" as a means of "the international Jew" to undermine "racial purity" and survival of the Nordics or Aryans, as well to stir up socioeconomic class tension and labor unions against the government or state-owned businesses. Within the concentration camps such as Buchenwald, German communists were privileged in comparison to Jews because of their "racial purity".[353]
Whenever the Nazis occupied a new territory, members of communist, socialist, or anarchist groups were normally to be the first persons detained or executed. Evidence of this is found in Hitler's infamous Commissar Order, in which he ordered the summary execution of all political commissars captured among Soviet soldiers, as well as the execution of all Communist Party members in German held territory.[354][355] Einsatzgruppen carried out these executions in the east.
Nacht und Nebel (German for "Night and Fog") was a directive (German: Erlass) of Hitler on 7 December 1941 signed and implemented by Chief of Staff of the Armed Forces Wilhelm Keitel, resulting in kidnapping and disappearance of many political activists throughout Nazi Germany's occupied territories.
In Mein Kampf, Hitler wrote that Freemasonry had "succumbed" to the Jews: "The general pacifistic paralysis of the national instinct of self-preservation begun by Freemasonry is then transmitted to the masses of society by the Jewish press."[356] Within the Reich, however, the "threat" posed by Freemasons was not considered serious from the mid-1930s onwards.[357] Heydrich even established a Freemasonry museumat which Eichmann spent some time early in his SD career[358]for what he regarded as a "disappeared cult".[359] Similarly, Hitler was happy to issue a proclamation on 27 April 1938 whose third point lifted restrictions on Party membership for former Freemasons, "provided the applicants had not served with the Lodge as highdegree members."[360] The Fhrer still maintained Freemasonry within his conspiratorial outlook,[361] but its adherents were not persecuted in a systematic fashion like groups such as the Jews.[357] Those Freemasons who were sent to concentration camps as political prisoners were forced to wear an inverted red triangle.[362]
The United States Holocaust Memorial Museum believes that, "because many of the Freemasons who were arrested were also Jews and/or members of the political opposition, it is not known how many individuals were placed in Nazi concentration camps and/or were targeted only because they were Freemasons."[363] However, the Grand Lodge of Scotland estimates the number of Freemasons executed between 80,000 and 200,000.[263]
Refusing to pledge allegiance to the Nazi party or to serve in the military, roughly 12,000 Jehovah's Witnesses were forced to wear a purple triangle and were placed in camps where they were given the option of renouncing their faith and submitting to the state's authority. Between 2,500 and 5,000 were killed.[266] Historian Detlef Garbe, director at the Neuengamme (Hamburg) Memorial, writes that "no other religious movement resisted the pressure to conform to National Socialism with comparable unanimity and steadfastness."[364]
Dr. Shimon Samuels, director for International Liaison of the Simon Wiesenthal Centre, describes the acrimonious debate that exists between "specifists" and "universalists". The former fear debasement of the Holocaust by invidious comparisons, while the latter places the Holocaust alongside non-Jewish experiences of mass extermination as part and parcel of the global context of genocide. Dr. Samuels considers the debate, ipso facto, to dishonour the memory of the respective victims of each genocide. In his words, "Each case is specific as a threshold phenomenon, while each also adds its unique memory as signposts along an incremental continuum of horror."[365] Peter Novick argued: "A moment's reflection makes clear that the notion of uniqueness is quite vacuous ... [and], in practice, deeply offensive. What else call all of this possibly mean except 'your catastrophe, unlike ours, is ordinary'".[366] Adam Jones, professor at the University of British Columbia Okanagan, believes that claims of uniqueness for the Holocaust have become less common since the 1994 Rwandan genocide.[367] In 1997, the publication of The Black Book of Communism led to further debate on the comparison between Soviet and Nazi crimes; the book argued that Nazi crimes were not very different from the Soviet ones, and that Nazi methods were to a significant extent adopted from Soviet methods;[368] in the course of the debate, the term "Red Holocaust" appeared in discourse.[369][370] In The Holocaust Industry, Norman Finkelstein writes that the uniqueness theory first appeared in public discourse in 1967, but that it does not figure in scholarship of the Nazi Holocaust.[371] Steven Katz of Boston University has argued that the Holocaust is the only genocide that has occurred in history, and defines "Holocaust" to include only "the travail of European Jewry" and not other victims of the Nazis.[372]
   
Racial policy
Nazi eugenics
Nuremberg Laws
Haavara Agreement
Madagascar Plan
Forced euthanasia
Jews in Europe
Jews in Germany
Romani people (Gypsies)
Poles
Soviet POWs
Slavs in Eastern Europe
Homosexuals
People with disabilities
Freemasons
Jehovah's Witnesses
Biaystok
Budapest
Kaunas
Krakw
d
Lublin
Lww
Minsk
Riga
Warsaw
Vilnius
Jewish ghettos in German-occupied Poland
List of selected ghettos
Jewish partisans
Bricha
Joint Declaration by Members of the United Nations
Auschwitz bombing debate
Nuremberg Trials
Denazification
Bricha
Displaced persons
Central Committee of the Liberated Jews
German Reparations
Holocaust survivors
Deportations of French Jews to death camps
Survivors of Sobibor
Timeline of Treblinka
Victims of Nazism
Rescuers of Jews
Bibliography of The Holocaust
The Destruction of the European Jews
Functionalism versus intentionalism
Auschwitz Protocols
Vrba-Wetzler report
Days of remembrance
Memorials and museums
v
t
e
v
t
e
1 Etymology and use of the term
2 Distinctive features

2.1 Institutional collaboration
2.2 Ideology and scale
2.3 Extermination camps
2.4 Medical experiments


2.1 Institutional collaboration
2.2 Ideology and scale
2.3 Extermination camps
2.4 Medical experiments
3 Development and execution

3.1 Origins
3.2 Legal repression and emigration
3.3 Kristallnacht (1938)
3.4 Resettlement and deportation
3.5 Early measures

3.5.1 In German occupied Poland
3.5.2 In other occupied countries
3.5.3 General Government and Lublin reservation (Nisko plan)


3.6 Concentration and labor camps (19331945)
3.7 Ghettos (19401945)
3.8 Pogroms (19391942)
3.9 Death squads (19411943)
3.10 New methods of mass murder
3.11 Wannsee Conference and the Final Solution (19421945)
3.12 Reaction

3.12.1 German public
3.12.2 International


3.13 Motivation
3.14 Extermination camps

3.14.1 Gas chambers


3.15 Jewish resistance
3.16 Climax

3.16.1 "Blood for Goods"


3.17 Escapes, publication of existence (AprilJune 1944)
3.18 Death marches (19441945)
3.19 Liberation


3.1 Origins
3.2 Legal repression and emigration
3.3 Kristallnacht (1938)
3.4 Resettlement and deportation
3.5 Early measures

3.5.1 In German occupied Poland
3.5.2 In other occupied countries
3.5.3 General Government and Lublin reservation (Nisko plan)


3.5.1 In German occupied Poland
3.5.2 In other occupied countries
3.5.3 General Government and Lublin reservation (Nisko plan)
3.6 Concentration and labor camps (19331945)
3.7 Ghettos (19401945)
3.8 Pogroms (19391942)
3.9 Death squads (19411943)
3.10 New methods of mass murder
3.11 Wannsee Conference and the Final Solution (19421945)
3.12 Reaction

3.12.1 German public
3.12.2 International


3.12.1 German public
3.12.2 International
3.13 Motivation
3.14 Extermination camps

3.14.1 Gas chambers


3.14.1 Gas chambers
3.15 Jewish resistance
3.16 Climax

3.16.1 "Blood for Goods"


3.16.1 "Blood for Goods"
3.17 Escapes, publication of existence (AprilJune 1944)
3.18 Death marches (19441945)
3.19 Liberation
4 Victims and death toll

4.1 Jewish

4.1.1 By country

4.1.1.1 Effect on Yiddish language




4.2 Non Jewish

4.2.1 Slavs

4.2.1.1 Ethnic Poles
4.2.1.2 Ethnic Serbs and other South Slavs
4.2.1.3 East Slavs
4.2.1.4 Soviet POWs


4.2.2 Romani people
4.2.3 Persons of color
4.2.4 Disabled and mentally ill
4.2.5 Homosexuals
4.2.6 The political left
4.2.7 Freemasons
4.2.8 Jehovah's Witnesses




4.1 Jewish

4.1.1 By country

4.1.1.1 Effect on Yiddish language




4.1.1 By country

4.1.1.1 Effect on Yiddish language


4.1.1.1 Effect on Yiddish language
4.2 Non Jewish

4.2.1 Slavs

4.2.1.1 Ethnic Poles
4.2.1.2 Ethnic Serbs and other South Slavs
4.2.1.3 East Slavs
4.2.1.4 Soviet POWs


4.2.2 Romani people
4.2.3 Persons of color
4.2.4 Disabled and mentally ill
4.2.5 Homosexuals
4.2.6 The political left
4.2.7 Freemasons
4.2.8 Jehovah's Witnesses


4.2.1 Slavs

4.2.1.1 Ethnic Poles
4.2.1.2 Ethnic Serbs and other South Slavs
4.2.1.3 East Slavs
4.2.1.4 Soviet POWs


4.2.1.1 Ethnic Poles
4.2.1.2 Ethnic Serbs and other South Slavs
4.2.1.3 East Slavs
4.2.1.4 Soviet POWs
4.2.2 Romani people
4.2.3 Persons of color
4.2.4 Disabled and mentally ill
4.2.5 Homosexuals
4.2.6 The political left
4.2.7 Freemasons
4.2.8 Jehovah's Witnesses
5 Uniqueness
6 See also

6.1 Related articles by country
6.2 Major perpetrators
6.3 Involvement of other countries and nationals
6.4 Aftermath and historiography
6.5 Miscellaneous
6.6 Related links


6.1 Related articles by country
6.2 Major perpetrators
6.3 Involvement of other countries and nationals
6.4 Aftermath and historiography
6.5 Miscellaneous
6.6 Related links
7 Footnotes
8 Bibliography
9 External links
2.1 Institutional collaboration
2.2 Ideology and scale
2.3 Extermination camps
2.4 Medical experiments
3.1 Origins
3.2 Legal repression and emigration
3.3 Kristallnacht (1938)
3.4 Resettlement and deportation
3.5 Early measures

3.5.1 In German occupied Poland
3.5.2 In other occupied countries
3.5.3 General Government and Lublin reservation (Nisko plan)


3.5.1 In German occupied Poland
3.5.2 In other occupied countries
3.5.3 General Government and Lublin reservation (Nisko plan)
3.6 Concentration and labor camps (19331945)
3.7 Ghettos (19401945)
3.8 Pogroms (19391942)
3.9 Death squads (19411943)
3.10 New methods of mass murder
3.11 Wannsee Conference and the Final Solution (19421945)
3.12 Reaction

3.12.1 German public
3.12.2 International


3.12.1 German public
3.12.2 International
3.13 Motivation
3.14 Extermination camps

3.14.1 Gas chambers


3.14.1 Gas chambers
3.15 Jewish resistance
3.16 Climax

3.16.1 "Blood for Goods"


3.16.1 "Blood for Goods"
3.17 Escapes, publication of existence (AprilJune 1944)
3.18 Death marches (19441945)
3.19 Liberation
3.5.1 In German occupied Poland
3.5.2 In other occupied countries
3.5.3 General Government and Lublin reservation (Nisko plan)
3.12.1 German public
3.12.2 International
3.14.1 Gas chambers
3.16.1 "Blood for Goods"
4.1 Jewish

4.1.1 By country

4.1.1.1 Effect on Yiddish language




4.1.1 By country

4.1.1.1 Effect on Yiddish language


4.1.1.1 Effect on Yiddish language
4.2 Non Jewish

4.2.1 Slavs

4.2.1.1 Ethnic Poles
4.2.1.2 Ethnic Serbs and other South Slavs
4.2.1.3 East Slavs
4.2.1.4 Soviet POWs


4.2.2 Romani people
4.2.3 Persons of color
4.2.4 Disabled and mentally ill
4.2.5 Homosexuals
4.2.6 The political left
4.2.7 Freemasons
4.2.8 Jehovah's Witnesses


4.2.1 Slavs

4.2.1.1 Ethnic Poles
4.2.1.2 Ethnic Serbs and other South Slavs
4.2.1.3 East Slavs
4.2.1.4 Soviet POWs


4.2.1.1 Ethnic Poles
4.2.1.2 Ethnic Serbs and other South Slavs
4.2.1.3 East Slavs
4.2.1.4 Soviet POWs
4.2.2 Romani people
4.2.3 Persons of color
4.2.4 Disabled and mentally ill
4.2.5 Homosexuals
4.2.6 The political left
4.2.7 Freemasons
4.2.8 Jehovah's Witnesses
4.1.1 By country

4.1.1.1 Effect on Yiddish language


4.1.1.1 Effect on Yiddish language
4.1.1.1 Effect on Yiddish language
4.2.1 Slavs

4.2.1.1 Ethnic Poles
4.2.1.2 Ethnic Serbs and other South Slavs
4.2.1.3 East Slavs
4.2.1.4 Soviet POWs


4.2.1.1 Ethnic Poles
4.2.1.2 Ethnic Serbs and other South Slavs
4.2.1.3 East Slavs
4.2.1.4 Soviet POWs
4.2.2 Romani people
4.2.3 Persons of color
4.2.4 Disabled and mentally ill
4.2.5 Homosexuals
4.2.6 The political left
4.2.7 Freemasons
4.2.8 Jehovah's Witnesses
4.2.1.1 Ethnic Poles
4.2.1.2 Ethnic Serbs and other South Slavs
4.2.1.3 East Slavs
4.2.1.4 Soviet POWs
6.1 Related articles by country
6.2 Major perpetrators
6.3 Involvement of other countries and nationals
6.4 Aftermath and historiography
6.5 Miscellaneous
6.6 Related links
The Holocaust in Belarus
The Holocaust in Croatia
The Holocaust in Estonia
The Holocaust in France
The Holocaust in Latvia
The Holocaust in Lithuania
The Holocaust in Norway
The Holocaust in Poland
The Holocaust in Romania
The Holocaust in Russia
The Holocaust in Serbia
The Holocaust in Ukraine
The Holocaust in the USSR
Bermuda Conference
vian Conference
International response to the Holocaust
Struma
Voyage of the Damned
The response of individual states.
Arab rescue efforts during the Holocaust
List of people who assisted Jews during the Holocaust
List of Righteous Among the Nations by country
Rescue of the Danish Jews
Rescue of Jews by Poles during the Holocaust
Righteous Among the Nations
Zwizek Organizacji Wojskowej
egota
Albert Battel
ngel Sanz Briz
Aristides de Sousa Mendes
Chiune Sugihara
Corrie ten Boom
Dimitar Peshev
Folke Bernadotte
Henryk Slawik
Ho Feng Shan
Hugh O'Flaherty
Irena Sendler
Jan Karski
Jorge Pelasca
Luiz Martins de Souza Dantas
Oskar Schindler
Raoul Wallenberg
Witold Pilecki
Aftermath of the Holocaust
Aftermath of World War II
Denazification
Command responsibility
UN Resolution against genocide
Doctors' Trial
Dora Trial
German war crimes
Nuremberg Trials
Trial of Adolf Eichmann
War crimes of the Wehrmacht
List of victims of Nazism
List of victims and survivors of Auschwitz
List of famous Holocaust survivors
List of survivors of Sobibor
Sh'erit ha-Pletah
Wiedergutmachung
Holocaust memorials
Yom HaShoah
Yad Vashem
Holocaust research
Holocaust theology
The Holocaust in art and literature
Holocaust denial
Criticism of Holocaust denial
Days of Remembrance of the Victims of the Holocaust
Responsibility for the Holocaust
Command responsibility
List of major perpetrators of the Holocaust
Functionalism versus intentionalism and Historikerstreit.
Holocaust (resources)
Animal rights and the Holocaust
Antisemitism
Antiziganism
Aryanization
Bereavement in Judaism
Holocaust trivialization debate
Jews outside Europe under Nazi occupation
Armenian genocide
Bosnian genocide
Darfur genocide
Genocide
Herero and Namaqua Genocide
Holodomor
Japanese war crimes
Katyn Massacre
Maafa
Mass killings under Communist regimes
Rwandan genocide
The Killing Fields
Bauer, Yehuda. Forms of Jewish Resistance During the Holocaust. In The Nazi Holocaust: Historical Articles on the Destruction of European Jews. Vol. 7: Jewish Resistance to the Holocaust, edited by Michael R. Marrus, 3448. Westport, Connecticut: Meckler, 1989.
Bauer, Yehuda, They chose life: Jewish resistance in the Holocaust, New York, The American Jewish Committee, 1973.
Jewish Resistance During the Holocaust by Israel Gutman. Yad Vashem.
Resistance During the Holocaust U.S. Holocaust Memorial Museum
Jewish Resistance. A Working Bibliography. The Miles Lerman Center for the Study of Jewish Resistance. Center for Advanced Holocaust Studies. U.S. Holocaust Memorial Museum
Bauer, Yehuda (1982). A History of the Holocaust. New York: Franklin Watts.
Bauer, Yehuda (1989). "Rescue by negotiations? Jewish attempts to negotiate with the Nazis". In Michael R. Marrus. The Nazi Holocaust, Part 9: The End of the Holocaust. Walter de Gruyter. pp.321.
Bauer, Yehuda (1998) [1994]. "Gypsies". In Yisrael Gutman; Michael Berenbaum. Anatomy of the Auschwitz Death Camp. Bloomington,IN: Indiana University Press. pp.441455.
Bauer, Yehuda (2002). Rethinking the Holocaust. New Haven,CT: Yale University Press.
Benz, Wolfgang (2007). Die 101 wichtigsten Fragen- das dritte Reich (2nd ed.). C.H. Beck.
Berenbaum, Michael (2005). The World Must Know: The History of the Holocaust as Told in the United States Holocaust Memorial Museum. United States Holocaust Memorial Museum, Johns Hopkins University Press.
Berghahn, Volker R. (1999). "Germans and Poles, 18711945". Yearbook of European Studies 13: 1536.
Black, Edwin (2001). The Transfer Agreement: The Dramatic Story of the Pact Between the Third Reich and Jewish Palestine. New York,NY: Carroll & Graf Publishers.
Brechtken, Magnus (1998). Madagaskar fr die Juden: antisemitische Idee und politische Praxis 18851945 (2nd ed.). Munich: Oldenbourg Wissenschaftsverlag.
Breitman, Richard (1991). The Architect of Genocide: Himmler and the Final Solution. New York: Knopf.
Browning, Christopher (1992). Ordinary Men: Reserve Police Battalion 101 and the Final Solution in Poland. New York: HarperCollins.
Browning, Christopher (2004). The Origins of the Final Solution: The Evolution of Nazi Jewish Policy, September 1939  March 1942. Jerusalem: Yad Vashem.
Buchheim, Hans (1968). "Command and Compliance". In Helmut Krausnick; Hans Buchheim; Martin Broszat; Hans-Adolf Jacobsen. The Anatomy of the SS State. New York: Walker and Company. pp.303396.
Buchholz, Werner (1999). Pomern. Deutsche Geschicte im Osten Europas. Berlin: Siedler.
Burleigh, Michael (2000). "Psychiatry, Society and Nazi 'Euthanasia'". In Omer Bartov. The Holocaust: Origins, Implementation, Aftermath. London: Routledge. pp.4362.
Burleigh, Michael; Wippermann, Wolfgang (1991). The Racial State: Germany 19331945. Cambridge: Cambridge University Press.
Cesarani, David (2004). Holocaust: From the Persecution of the Jews to Mass Murder. London: Routledge.
Cesarani, David (2005). Eichmann: His Life and Crimes. London: Vintage.
Cooper, Robert (2010). The Red Triangle: The History of the Persecution of Freemasons. Bungay: Lewis Masonic.
Courtois, Stphane (1999). "Introduction: The Crimes of Communism". In Stphane Courtois; Nicolas Werth; Jean-Louis Pann; Andrzej Paczkowski; Karel Bartoek; Jean-Louis Margolin. The Black Book of Communism: Crimes, Terror, Repression. Cambridge,MA: Harvard University Press. pp.132.
Czech, Danuta (1989). Kalendarium der Ereignisse im Konzentrationslager Auschwitz-Birkenau 19391945. Rowohlt, Reinbek.
Dawidowicz, Lucy (1975). The War Against the Jews.
Dawidowicz, Lucy (1981). The Holocaust and the Historians. Cambridge,MA: Harvard University Press.
Dear, Ian; Foot, Richard D. (2001). The Oxford companion to World War II. Oxford: Oxford University Press.
Domarus, Max (2004). Hitler: Speeches and Proclamations (4 volumes). Wauconda,IL: Bolchazy-Carducci Publishers.
Evans, Richard J. (1989). In Hitler's Shadow. New York,NY: Pantheon.
Evans, Richard J. (2002). Lying About Hitler: The Holocaust, History and the David Irving Trial. London: Verso.
Evans, Richard J. (2008). The Third Reich at War. London: Allen Lane.
Farbstein, Esther (1998). "Diaries and Memoirs as a Historical Source: The Diary and Memoir of a Rabbi at the 'Konin House of Bondage'". Yad Vashem Studies 26: 87128. http://www1.yadvashem.org/odot_pdf/Microsoft%20Word%20-%203134.pdf.
Fest, Joachim (1999). Speer: The Final Verdict. San Diego,CA: Harcourt.
Finkelstein, Norman (2003) [2000]. The Holocaust Industry: Reflections on the Exploitation of Jewish Suffering. London & New York: Verso Books.
Fischer, Conan (2002). The Rise of the Nazis. Manchester: Manchester University Press.
Fitzgerald, Stephanie (2011). Children of the Holocaust. Mankato,MN: Compass Point Books.
Fleming, Gerald (1987). Hitler and the Final Solution. Berkeley & Los Angeles, CA: University of California Press.
Frster, Jrgen (1998). "Complicity or Entanglement?". In Michael Berenbaum; Abraham Peck. The Holocaust and History. Bloomington,IN: Indiana University Press. pp.266283.
Frank, Anne (2007) [1947]. The Diary of a Young Girl. London: Penguin Books.
Friedlander, Henry (1994). "Step by Step: The Expansion of Murder, 19391941". German Studies Review 17 (3): 495507. doi:10.2307/1431896. JSTOR1431896.
Friedlander, Henry (1995). The Origins of Nazi Genocide: From Euthanasia to the Final Solution. Chapel Hill,NC: University of North Carolina Press.
Friedlander, Henry (1997). "Registering the Handicapped in Nazi Germany: A Case Study". Jewish History 11 (2): 8998. doi:10.1007/BF02335679. JSTOR20101303.
Friedlnder, Saul (1997). The Years of Persecution: Nazi Germany and the Jews 19331939. London: Weidenfeld & Nicolson.
Friedlnder, Saul (2007). The Years of Extermination: Nazi Germany and the Jews 19391945. London: Weidenfeld & Nicolson.
Garbe, Detlef (2001). "Social Disinterest, Governmental Disinformation, Renewed Persecution, and Now Manipulation of History?". In Hans Hesse. Persecution and Resistance of Jehovah's Witnesses During the Nazi-Regime 19331945. Bremen: Edition Temmen. pp.251265.
Gellately, Robert (2001). Backing Hitler: Consent and Coercion in Nazi Germany. Oxford: Oxford University Press.
Gerwarth, Robert (2011). Hitler's Hangman: The Life of Heydrich. New Haven & London: Yale University Press.
Gilbert, Martin (1986). The Holocaust: The Jewish Tragedy. London: Collins.
Gilbert, Martin (1988). Atlas of the Holocaust.
Giles, Geoffrey J. (1992). "The Most Unkindest Cut of All: Castration, Homosexuality and Nazi Justice". Journal of Contemporary History 27 (1): 4161. doi:10.1177/002200949202700103. JSTOR260778.
Gramel, Hermann (1992). Antisemitism in the Third Reich. London: Blackwell.
Hagen, William W. (2012). German History in Modern Times: Four Lives of the Nation. Cambridge: Cambridge University Press.
Halbrook, Stephen P. (2000). "Nazi Firearms Law and the Disarming of the German Jews". Arizona Journal of International and Comparative Law 17 (3): 483535. http://www.stephenhalbrook.com/article-nazilaw.pdf.
Hancock, Ian (2004). "Romanies and the Holocaust: A Reevaluation and Overview". In Dan Stone. The Historiography of the Holocaust. New York,NY: Palgrave-Macmillan. pp.383396. http://www.radoc.net/radoc.php?doc=art_e_holocaust_porrajmos&lang=en&articles=true.
Harran, Marilyn J. (2000). The Holocaust Chronicles: A History in Words and Pictures. Lincolnwood,IL: Publications International. http://www.holocaustchronicle.org/index.html.
Hedgepeth, Sonja M.; Saidel, Rochelle G. (2010). Sexual Violence against Jewish Women During the Holocaust. Lebanon,NH: University Press of New England.
Hilberg, Raul (1995) [1992]. Perpetrators Victims Bystanders: The Jewish Catastrophe 19331945. London: Secker & Warburg.
Hilberg, Raul (1996). The Politics of Memory: The Journey of a Holocaust Historian. Chicago,IL: Ivan R. Dee.
Hilberg, Raul (2003) [1961]. The Destruction of the European Jews (3 volumes). New Haven,CT: Yale University Press.
Hildebrand, Klaus (2005) [1984]. The Third Reich. Routledge.
Hitchcock, William I. (2009). Liberation: The Bitter Road to Freedom, Europe 19441945. London: Faber and Faber.
Jacobs, Neil G. (2005). Yiddish: A Linguistic Introduction. Cambridge: Cambridge University Press.
Jeffery, Keith (2010). MI6: The History of the Secret Intelligence Service, 19091949. London: Bloomsbury.
Johnson, Paul (1988). A History of the Jews. Harper Perennial.
Jones, Adam, ed. (2010). Genocide: A Comprehensive Introduction (2nd ed.). Abingdon: Routledge.
Karski, Jan (2012). Story of a Secret State: My Report to the World. London: Penguin Classics.
Kats, Alfred (1970). Poland's Ghettos at War. New York,NY: Twayne Publishers.
Krn, Miroslav (1998) [1994]. "The Vrba and Wetzler Report". In Yisrael Gutman; Michael Berenbaum. Anatomy of the Auschwitz Death Camp. Bloomington,IN: Indiana University Press. pp.553568.
Kennedy, David M., ed. (2007). The Library of Congress World War II Companion. New York,NY: Simon & Schuster.
Kermish, Joseph, ed. (1968). "Emmanuel Ringelblum's Notes, Hitherto Unpublished". Yad Vashem Studies 7: 173183.
Kershaw, Ian (1998). Hitler 18891936: Hubris. London: Allen Lane.
Kershaw, Ian (2000). Hitler 19361945: Nemesis. London: Allen Lane.
Klempner, Mark (2006). The Heart Has Reasons: Holocaust Rescuers and Their Stories of Courage. Cleveland,OH: The Pilgrim Press.
Kogon, E.; Langbein; Rueckerl, A., eds. (1993). Nazi Mass Murder: A Documentary History of the Use of Poison Gas. New Haven,CT: Yale University Press.
Krakowski, Shmuel (1989). "The Death Marches in the Period of the Evacuation of the Camps". In Michael R. Marrus. The Nazi Holocaust, Part 9: The End of the Holocaust. Walter de Gruyter. pp.476490.
Krausnick, Helmut (1968). "The Persecution of the Jews". In Helmut Krausnick; Hans Buchheim; Martin Broszat; Hans-Adolf Jacobsen. The Anatomy of the SS State. New York,NY: Walker and Company. pp.1125.
Kudryashov, Sergei (2004). "Ordinary Collaborators: The Case of the Travniki Guards". In Mark Erickson; Ljubica Erickson. Russia: War, Peace and Diplomacy Essays in Honour of John Erickson. London: Weidenfeld & Nicolson. pp.226239.
Kwiet, Konrad (1998). "Rehearsing for Murder: The Beginning of the Final Solution in Lithuania in June 1941". Holocaust and Genocide Studies 12 (1): 326. doi:10.1093/hgs/12.1.3. http://defendinghistory.com/wp-content/uploads/2011/09/Konrad-Kwiets-Rehearsing-for-Murder-in-1941.pdf.
Lador-Lederer, Joseph (1980). "World War II: Jews as Prisoners of War". Israel Yearbook on Human Rights (Tel Aviv: Tel Aviv University) 10: 7089.
Leff, Laurel (2005). Buried by The Times: The Holocaust and American's Most Important Newspaper. New York,NY: Cambridge University Press.
Lemkin, Raphael (2005). Axis Rule In Occupied Europe: Laws of Occupation, Analysis of Government, Proposals for Redress. New York,NY: Lawbook Exchange.
Lewis, Jon E. (2002). The Mammoth Book of Heroes. London: Constable & Robinson.
Lifton, Robert J. (2000) [1986]. The Nazi Doctors: Medical Killing and the Psychology of Genocide. New York: Basic Books.
Linn, Ruth (2004). Escaping Auschwitz: A Culture of Forgetting. Ithaca,NY: Cornell University Press.
Longerich, Peter (2003) [2001]. The Unwritten Order: Hitler's Role in the Final Solution. Stroud: Tempus Publishing.
Longerich, Peter (2010). Holocaust: The Nazi Persecution and Murder of the Jews. Oxford: Oxford University Press.
Longerich, Peter (2012). Heinrich Himmler. Oxford: Oxford University Press.
Lower, Wendy (2006). "The 'reibungslose' Holocaust? The German Military and Civilian Implementation of the 'Final Solution' in Ukraine, 19411944". In Gerald D. Feldman; Wolfgang Seibel. Networks of Nazi Persecution: Bureaucracy, Business, and the Organization of the Holocaust. New York & Oxford: Berghahn Books. pp.236256.
Lusane, Clarence (2003). Hitler's Black Victims: The Historical Experience of Afro-Germans, European Blacks, Africans and African Americans in the Nazi Era. London; New York: Routledge.
Maier, Charles S. (1988). The Unmasterable Past. Cambridge,MA: Harvard University Press.
Mann, Michael (2005). The Dark Side of Democracy: Explaining Ethnic Cleansing. New York: Cambridge University Press.
Marrus, Michael R. (2000). The Holocaust in History. Toronto: KeyPorter.
Matthus, Jrgen (2004). "Operation Barbarossa and the Onset of the Holocaust, JuneDecember 1941". In Christopher Browning. The Origins of the Final Solution: The Evolution of Nazi Jewish Policy, September 1939  March 1942. Jerusalem: Yad Vashem. pp.244308.
Mazower, Mark (2008). Hitler's Empire: Nazi Rule in Occupied Europe. London: Allen Lane.
Michael, Robert; Doerr, Karin (2002). Nazi-Deutsch/Nazi-German: An English Lexicon of the Language of the Third Reich. Greenwood Press.
Milton, Sybil (1990). "The Context of the Holocaust". German Studies Review 13 (2): 269283. doi:10.2307/1430708. JSTOR1430708.
Montague, Patrick (2012). Chelmno and the Holocaust: A History of Hitler's First Death Camp. London: I.B.Tauris.
Mller, Horst (1999). Der rote Holocaust und die Deutschen. Die Debatte um das 'Schwarzbuch des Kommunismus'. Munich: Piper Verlag.
Mommsen, Hans (2003). "The New Historical Consciousness". In Ernst Piper. Forever In The Shadow of Hitler?. Humanities Press, Atlantic Highlands. pp.114124.
Mller, Rolf-Dieter; Ueberschr, Gerd R. (2002). Hitler's War in the East, 19411945: A Critical Assessment. New York & Oxford: Berghahn Books.
Mller-Hill, Benno (1998). Murderous Science: Elimination by Scientific Selection of Jews, Gypsies, and Others in Germany, 19331945. Plainview,NY: Cold Spring Harbor Laboratory Press.
Murray, Williamson; Millett, Allan R. (2000). A War To Be Won. Cambridge,MA: Harvard University Press.
Naimark, Norman M. (2001). Fires of Hatred. Cambridge,MA: Harvard University Press.
Neugebauer, Wolfgang (1998). "Racial Hygiene in Vienna 1938". Wiener Klinische Wochenschrift (Special Edition). http://www.doew.at/information/mitarbeiter/beitraege/rachyg.html.
Nicosia, Francis R. (2000). The Third Reich & the Palestine Question. New Brunswick,NJ: Transaction Publishers.
Niewyk, Donald L.; Nicosia, Francis R. (2000). The Columbia Guide to the Holocaust. New York,NY: Columbia University Press.
Niewyk, Donald L. (2012). "The Holocaust: Jews, Gypsies, and the Handicapped". In Parsons, Samuel; Totten, William S.. Centuries of Genocide: Essays and Eyewitness Accounts. New York,NY: Routledge. pp.191248.
Noakes, Jeremy; Pridham, Geoffrey (1983). Nazism: A History in Documents and Eyewitness Accounts, 19191945. Schocken Books.
Novick, Peter (1999). The Holocaust in American Life. New York,NY: Houghton Mifflin.
Pelt, Robert Jan van (2002). The Case for Auschwitz: Evidence from the Irving Trial. Bloomington,IN: Indiana University Press.
Petrie, Jon (2000). "The Secular Word 'HOLOCAUST': Scholarly Myths, History, and Twentieth Century Meanings". Journal of Genocide Research 2 (1): 3163. doi:10.1080/146235200112409. http://www.berkeleyinternet.com/holocaust.
Peukert, Detlev (1987). Inside Nazi Germany: Conformity, Opposition and Racism In Everyday Life. London: Batsford.
Peukert, Detlev (1994). "The Genesis of the 'Final Solution' from the Spirit of Science". In David F. Crew. Nazism and German Society, 19331945. London: Routledge. pp.274299.
Phayer, Michael (2000). The Catholic Church and the Holocaust, 19301965. Bloomington,IN: Indiana University Press.
Pinkus, Benjamin (1990). The Jews of the Soviet Union: The History of a National Minority. Cambridge: Cambridge University Press.
Pinkus, Oscar (2005). The War Aims and Strategies of Adolf Hitler. Jefferson,NC: McFarland & Company.
Piotrowski, Tadeusz (1998). Poland's Holocaust: Ethnic Strife, Collaboration With Occupying Forces and Genocide in the Second Republic, 19181947. Jefferson,NC: McFarland & Company.
Piper, Franciszek (1998) [1994]. "Gas chambers and Crematoria". In Yisrael Gutman; Michael Berenbaum. Anatomy of the Auschwitz Death Camp. Bloomington,IN: Indiana University Press. pp.157182.
Poprzeczny, Joseph (2004). Odilo Globocnik: Hitler's Man in the East. Jefferson,NC: McFarland & Company.
Porat, Dina (2002). "The Holocaust in Lithuania: Some Unique Aspects". In David Cesarani. The Final Solution: Origins and Implementation. London: Routledge. pp.159174.
Proctor, Robert (1988). Racial Hygiene: Medicine Under the Nazis. Cambridge,MA: Harvard University Press.
Rosefielde, Steven (2009). Red Holocaust. Oxford: Routledge.
Ryan, Donna F.; Schuchman, John S. (2002). Deaf People in Hitler's Europe. Washington,DC: Gallaudet University Press.
Samuels, Simon (2001). "Applying the Lessons of the Holocaust". In Alan S. Rosenbaum. Is the Holocaust Unique?. Boulder,CO: Westview Press. pp.472494.
Sereny, Gitta (1995) [1974]. Into That Darkness: From Mercy Killing to Mass Murder. London: Pimlico.
Snyder, Timothy (2010). Bloodlands: Europe Between Hitler and Stalin. London: The Bodley Head.
Steinweis, Alan E. (2001). "The Holocaust and American Culture: An Assessment of Recent Scholarship". Holocaust and Genocide Studies 15 (2): 296310. doi:10.1093/hgs/15.2.296.
Strous, Rael D. (2007). "Psychiatry during the Nazi era: ethical lessons for the modern professional". Annals of General Psychiatry 6 (8): 8. doi:10.1186/1744-859X-6-8. http://www.annals-general-psychiatry.com/content/6/1/8.
Suhl, Yuri (1987). They Fought Back. New York: Schocken. ISBN978-0-8052-0479-7.
Swiebocki, Henryk (1998) [1994]. "Prisoner Escapes". In Yisrael Gutman; Michael Berenbaum. Anatomy of the Auschwitz Death Camp. Bloomington,IN: Indiana University Press.
Tooze, Adam (2006). The Wages of Destruction: The Making and Breaking of the Nazi Economy. London: Allen Lane.
Trunk, Isaiah (1996) [1972]. Judenrat: The Jewish Councils in Eastern Europe under Nazi Occupation. Lincoln,NE: University of Nebraska Press.
Vrba, Rudolf (2006) [2002]. I Escaped from Auschwitz. London: Robson Books.
Wiesel, Elie (2002). After the Darkness: Reflections on the Holocaust. New York,NY: Schocken Books.
Wiesel, Elie (2012) [1960]. Night. London: Penguin Books.
Wood, Thomas E.; Jankowski, Stanisaw M. (1994). Karski: How One Man Tried to Stop the Holocaust.
Yahil, Leni (1991). The Holocaust: The Fate of European Jewry, 19321945. Oxford: Oxford University Press.
Zuccotti, Susan (1999). The Holocaust, the French, and the Jews. Lincoln,NE: University of Nebraska Press.
Online documents available from the Dwight D. Eisenhower Presidential Library
Guide to materials available at the Dwight D. Eisenhower Presidential Library
The Wiener Library for the Study of the Holocaust & Genocide  The World's Oldest Holocaust Memorial Institution
External links, references, and other resources are listed at Holocaust (resources).
.
#(`*College Board*`)#.
The College Board is a not-for-profit membership association in the United States that was formed in 1900 as the College Entrance Examination Board (CEEB). It is composed of more than 5,900 schools, colleges, universities and other educational organizations. It sells standardized tests used by academically oriented post-secondary education institutions to measure a student's ability. The College Board is headquartered in the Upper West Side of Manhattan, New York City.[1] David Coleman is the current president of College Board, who, in October 2012, replaced his predecessor Gaston Caperton, former Governor of West Virginia, who has held this position since 1999 [2][3]
In addition to managing tests for which it charges fees, the College Board works with programs that claim to increase achievement by poor and minority middle and high school students. Funded by grants from various foundations, such as the Bill and Melinda Gates Foundation, the College Board Schools operate autonomously within New York City public school buildings. A similar program named EXCELerator began a pilot program for the 20062007 school year at 11 schools in Washington, D.C., Jacksonville/Duval County, FL, and Chicago Public Schools.[4] Both of these school reform programs use the SpringBoard and CollegeEd materials as part of their programs.
The College Board maintains a numbered registry of countries, college majors, colleges, scholarship programs, test centers, and high schools. In the United States, in addition to the College Board's internal use this registry is borrowed by other institutions as a means of unambiguous identification; thus, a student might give his or her guidance department not only a college's name and address, but also its CEEB code, to ensure that his or her transcript is sent correctly. There exists a similar set of ACT codes for colleges and scholarships, centers, and high schools, however these codes are less widely used outside ACT, Inc.
The SAT Reasoning Test is a fee-based, standardized test for college admissions in the United States. The SAT is administered by the College Board corporation in the United States and is developed, published, and scored by the Educational Testing Service (ETS). SAT Subject Tests are said to measure student performance in specific areas, such as mathematics, science, and history. In the marketplace, the SAT competes with another organization's standardized college admission's test called the ACT.
The SAT is an aptitude test, meaning that it tests a person's ability to analyze and solve problems. It focuses on writing, reading, and mathematics. SAT scores range from 600 to 2,400, with each of the 3 sections being worth 200-800 points. This is a timed test that currently allots three hours and 45 minutes, and costs $50. Most students take the test during their junior or senior year of high school.
PSAT/NMSQT stands for Preliminary SAT/National Merit Scholarship Qualifying Test. It's a fee-based standardized test that provides first-hand practice for the SAT Reasoning Test. It also functions as a qualifying test for the National Merit Scholarship Corporation scholarship programs.
The College Board's Advanced Placement Program is an extensive program that offers high school students the chance to participate in what they describe as college level classes for a fee, reportedly broadening their intellectual horizons and preparing them for college work. It also plays a large part in the college admissions process, showing students' intellectual capacity and genuine interest in learning. The program allows many students to gain college credit for high performance on the AP exams, much in the same manner as the CLEP. Granting credit, however, is still at the discretion of the college. Critics of the Advanced Placement Program charge that courses and exams focus on breadth of content coverage instead of depth. There are 2,900 colleges that grant credit and/or advanced standing.
College Level Examination Program (CLEP) provides students of any age with the opportunity to demonstrate college-level achievement through a program of exams in undergraduate college courses.
The College Board's Accuplacer test is a computer-based placement test that assesses reading, writing and math skills.[5] The Accuplacer test includes reading comprehension, sentence skills, arithmetic, elementary algebra, college-level mathematics and the writing test, Writeplacer. The Accuplacer test is used primarily by more than 1,000 high schools and colleges [6] to determine a student's needed placement. Often community colleges have specific guidelines for students requiring the Accuplacer test. The Accuplacer Companion paper-and-pencil tests allows for students with disabilities to take the test through its braille, large print and audio tests. The biggest benefit of the Accuplacer and Accuplacer Companion tests are its ability to be scored immediately through an online scoring system and taken in remote locations. While there are normally no fees for taking the test, some institutions may charge a fee to retake the test.
Spring Board is a pre-Advanced Placement program created by the College Board to prepare students who intend to take AP courses or college-level courses in their scholastic career. Based on Wiggins and McTighe's Understanding by Design model, the SpringBoard program attempts to map knowledge into scholastic skill sets in preparation for Advanced Placement testing and college success. Units of instruction are titrated to students within and across all school grades, providing a vertically articulated curriculum framework that scaffolds learning skills and subject test knowledge. Implicit in the course curriculum, the program embeds pre-AP and AP teaching and learning strategies across grade school levels and classwork.
The curriculum is applicable to grades 6th - 12th. Teachers are provided with formative assessments, professional training, and a variety of teaching tools to track student progress. The instructional framework is integrated in the curriculum content and subject materials - SpringBoard also provides other Web 2.0 resources aimed at making the program more community oriented.
The College Board also offers the CSS/Financial Aid PROFILE, a financial aid application service that many institutions use in determining family contribution and financial assistance packages. This is a fee-based service to institutions and students also must pay a fee to submit it to a school.
Since at least the late 1970s, the College Board has been subject to criticism from students, educators, and consumer rights activists. College Board owns the most widely used college admissions exams, and many students must take SAT exams for admission to competitive colleges. Although the ACT is usually accepted as an alternative to the SAT, some colleges require students to take the SAT subject tests. Some colleges also require students submit a College Board "CSS/Financial Aid PROFILE" when applying for financial aid. As there are no broadly accepted alternative to the College Board's AP, SAT Subject Test, and CSS/Financial Aid products, the company is often criticized as exploiting its monopoly on these products.
Consumer rights organization Americans for Educational Testing Reform (AETR) has criticized College Board for violating its non-profit status through excessive profits and exorbitant executive compensation; twelve of its executives make more than $300,000 per year,[7] with CEO Gaston Caperton earning over $800,000.[8] AETR also claims that College Board is acting unethically by selling test preparation materials, directly lobbying legislators and government officials, and refusing to acknowledge test-taker rights.[9]
The SAT Reasoning Test costs $50 ($77 if late), the AP Tests cost US $92 (for the May 2012 administration),[10] and the SAT Subject Tests cost a baseline of $22 with additional tests costing $11.[11] Some feel the testing fees can be prohibitive for many individuals. Furthermore, there are numerous other services that can be added to the basic costs, including late registration, score verification services, and various answering services that are available. SAT score reports cost $11.00 per college for 1-2 week electronic delivery, or 2-4 week paper or disk delivery, depending on what method the school requires ($31.00 extra for 2-day processing). The College Board allows high school administrators to authorize fee waivers for some services to students from low-income families, generally those meeting National School Lunch Act criteria.[12] In addition, due to the competitive nature of the test, many students find it necessary to take preparatory courses or to have SAT tutoring, which can cost hundreds, sometimes thousands, of dollars.
Even the College Board's College Scholarship Service Profile (CSS), a college financial aid application meant to help students pay for college, requires a fee. For the 2008-09 school year, the price is $25 for the first report sent and an additional $16 for each additional college to receive the information.
In 2006, the College Board had $582.9 million of revenue but spent only $527.8 million, leaving a $55.1 million surplus.[13]
In 2005, MIT Writing Director Les Perelman plotted essay length versus essay score on the new SAT from released essays and found a high correlation between them. After studying 23 graded essays he found that the longer the essay was, the higher the score it was given. Perelman found that he could, with perfect accuracy, determine the score of an essay without even reading the essay. He also discovered that several of these essays were full of factual inaccuracies, although the College Board does not claim to grade for factual accuracy.
Perelman, along with the National Council of Teachers of English also criticized the 25-minute writing section of the test for damaging standards of writing taught in the classroom. They say that writing teachers training their students for the SAT will not focus on revision, depth, accuracy, but will instead produce long, formulaic, and wordy pieces.[14] "You're getting teachers to train students to be bad writers," concluded Perelman.[15]
Some teachers have criticized AP classes as restrictive in the nature of their curriculum and yet indisposable due to the importance of AP classes in the college admissions process. The College Board is effectively able to control every aspect of AP classes directly or indirectly. The $92 fee, which is noted in criticism above, results only in a score report with the test name and grade. No details are given on how this scoring was reached nor are individuals given access to this information from College Board.[16]
In March 2006, it was discovered that the College Board had misscored several thousand tests taken in October 2005. Although the Board was aware of the error as early as December, it waited months to respond and, in late March, schools still did not have correct details. Within days of the first announcement, the Board corrected upward the number of affected students.[17]
Many colleges use the SAT score to decide acceptance and scholarships. The late reporting of errors upset many high-profile colleges. The dean of admissions at Pomona College commented, "Everybody appears to be telling half-truths, and that erodes confidence in the College BoardIt looks like they hired the people who used to do the books for Enron."[17]
In spring 2012, various news agencies reported that the College Board released a "controversial guide for undocumented students" designed to help these individuals navigate through the college admissions process.[18][19] In 2009, the College Board released a report supporting the DREAM Act, a bill that would legalize illegal aliens in the United States up to age 35 provided they met certain criteria.[20] The Board of Trustees of the College Board voted unanimously in support of the DREAM Act.[21]The College Board works with the national United We DREAM coalition, helping to encourage U.S. legislators to cosponsor the federal DREAM Act.[22]
This article incorporates text from a publication now in the public domain:Gilman, D. C.; Thurston, H. T.; Moore, F., eds. (1905). "article name needed". New International Encyclopedia (1st ed.). New York: Dodd, Mead.
1 CEEB Code
2 College Board Tests

2.1 SAT
2.2 PSAT/NMSQT
2.3 Advanced Placement Program
2.4 College Level Examination Program
2.5 Accuplacer
2.6 SpringBoard


2.1 SAT
2.2 PSAT/NMSQT
2.3 Advanced Placement Program
2.4 College Level Examination Program
2.5 Accuplacer
2.6 SpringBoard
3 CSS/Financial Aid PROFILE
4 Criticism

4.1 Exam fees
4.2 MIT study
4.3 Advanced placement (AP) classes
4.4 Reporting errors


4.1 Exam fees
4.2 MIT study
4.3 Advanced placement (AP) classes
4.4 Reporting errors
5 Support for DREAM Act
6 See also
7 References
8 External links
2.1 SAT
2.2 PSAT/NMSQT
2.3 Advanced Placement Program
2.4 College Level Examination Program
2.5 Accuplacer
2.6 SpringBoard
4.1 Exam fees
4.2 MIT study
4.3 Advanced placement (AP) classes
4.4 Reporting errors
ACT, a test by ACT, Inc., the main competitor to the College Board's SAT
College admissions in the United States
IB Diploma Programme, a pre-university educational program administered by the International Baccalaureate, the main competitor to the College Board's AP Program
Official website
About the College Board
The College Board's tests
History of the College Board
.
#(`*SAT*`)#.
The SAT is a standardized test for college admissions in the United States. The SAT is owned, published, and developed by the College Board, a nonprofit organization in the United States. It was formerly developed, published, and scored by the Educational Testing Service[1] which still administers the exam. The test is intended to assess a student's readiness for college. It was first introduced in 1926, and its name and scoring have changed several times. It was first called the Scholastic Aptitude Test, then the Scholastic Assessment Test, but now SAT does not stand for anything, hence it is an empty acronym.
The current SAT Reasoning Test, introduced in 2005, takes three hours and forty-five minutes to finish, and costs $50 ($81 International), excluding late fees.[2] Possible scores range from 600 to 2400, combining test results from three 800-point sections (Mathematics, Critical Reading, and Writing).
Taking the SAT or its competitor, the ACT, is required for freshman entry to many, but not all, universities in the United States.[3]
The College Board states that SAT measures literacy and writing skills that are needed for academic success in college. They state that the SAT assesses how well the test takers analyze and solve problemsskills they learned in school that they will need in college. The SAT is typically taken by high school sophomores, juniors and seniors.[4] Specifically, the College Board states that use of the SAT in combination with high school grade point average (GPA) provides a better indicator of success in college than high school grades alone, as measured by college freshman GPA. Various studies conducted over the lifetime of the SAT show a statistically significant increase in correlation of high school grades and freshman grades when the SAT is factored in.[5]
There are substantial differences in funding, curricula, grading, and difficulty among U.S. secondary schools due to American federalism, local control, and the prevalence of private, distance, and home schooled students. SAT (and ACT) scores are intended to supplement the secondary school record and help admission officers put local datasuch as course work, grades, and class rankin a national perspective.[6]
Historically, the SAT has been more popular among colleges on the coasts and the ACT more popular in the Midwest and South. There are some colleges that require the ACT to be taken for college course placement, and a few schools that formerly did not accept the SAT at all. Nearly all colleges accept the test.[7]
Certain high IQ societies, like Mensa, the Prometheus Society and the Triple Nine Society, use scores from certain years as one of their admission tests. For instance, the Triple Nine Society accepts scores of 1450 on tests taken before April 1995, and scores of at least 1520 on tests taken between April 1995 and February 2005.[8]
The SAT is sometimes given to students younger than 13 by organizations such as the Study of Mathematically Precocious Youth, who use the results to select, study and mentor students of exceptional ability.
While the exact manner in which SAT scores will help to determine admission of a student at American institutions of higher learning is generally a matter decided by the individual institution, some foreign countries have made SAT (and ACT) scores a legal criterion in deciding whether holders of American high school diplomas will be admitted at their public universities.
SAT consists of three major sections: Critical Reading, Mathematics, and Writing. Each section receives a score on the scale of 200800. All scores are multiples of 10. Total scores are calculated by adding up scores of the three sections. Each major section is divided into three parts. There are 10 sub-sections, including an additional 25-minute experimental or "equating" section that may be in any of the three major sections. The experimental section is used to normalize questions for future administrations of the SAT and does not count toward the final score. The test contains 3 hours and 45 minutes of actual timed sections,[9] most administrations (after including orientation, distribution of materials, completion of biographical sections, and eleven minutes of timed breaks) run for about four and a half hours. The questions range from easy, medium, and hard depending on the scoring from the experimental sections. Easier questions typically appear closer to the beginning of the section while harder questions are towards the end in certain sections. This is not true for every section (the Critical Reading section is in chronological order) but it is the rule of thumb mainly for math and the 19 sentence completions on the test.
The Critical Reading (formerly Verbal) section of the SAT is made up of three scored sections: two 25-minute sections and one 20-minute section, with varying types of questions, including sentence completions and questions about short and long reading passages. Critical Reading sections normally begin with 5 to 8 sentence completion questions; the remainder of the questions are focused on the reading passages. Sentence completions generally test the student's vocabulary and understanding of sentence structure and organization by requiring the student to select one or two words that best complete a given sentence. The bulk of the Critical Reading section is made up of questions regarding reading passages, in which students read short excerpts on social sciences, humanities, physical sciences, or personal narratives and answer questions based on the passage. Certain sections contain passages asking the student to compare two related passages; generally, these consist of shorter reading passages. The number of questions about each passage is proportional to the length of the passage. Unlike in the Mathematics section, where questions go in the order of difficulty, questions in the Critical Reading section go in the order of the passage. Overall, question sets towards the beginning of the section are easier, and question sets towards the end of the section are harder.
The Mathematics section of the SAT is widely known as the Quantitative Section or Calculation Section. The mathematics section consists of three scored sections. There are two 25-minute sections and one 20-minute section, as follows:
The SAT has done away with quantitative comparison questions on the math section, leaving only questions with symbolic or numerical answers.
Four-function, scientific, and graphing calculators are permitted on the SAT math section; however, calculators are not permitted on either of the other sections. Calculators with QWERTY keyboards, cell phone calculators, portable computers, and personal organizers are not permitted. [10]
With the recent changes to the content of the SAT math section, the need to save time while maintaining accuracy of calculations has led some to use calculator programs during the test. These programs allow students to complete problems faster than would normally be possible when making calculations manually.
The use of a graphing calculator is sometimes preferred, especially for geometry problems and questions involving multiple calculations. According to research conducted by the CollegeBoard, performance on the math sections of the exam is associated with the extent of calculator use, with those using calculators on about a third to a half of the items averaging higher scores than those using calculators less frequently.[11] The use of a graphing calculator in mathematics courses, and also becoming familiar with the calculator outside of the classroom, is known to have a positive effect on the performance of students using a graphing calculator during the exam.
The writing section of the SAT, based on but not directly comparable to the old SAT II subject test in writing (which in turn was developed from the old TSWE), includes multiple choice questions and a brief essay. The essay subscore contributes about 28% towards the total writing score, with the multiple choice questions contributing 70%. This section was implemented in March 2005 following complaints from colleges about the lack of uniform examples of a student's writing ability and critical thinking.
The multiple choice questions include error identification questions, sentence improvement questions, and paragraph improvement questions. Error identification and sentence improvement questions test the student's knowledge of grammar, presenting an awkward or grammatically incorrect sentence; in the error identification section, the student must locate the word producing the source of the error or indicate that the sentence has no error, while the sentence improvement section requires the student to select an acceptable fix to the awkward sentence. The paragraph improvement questions test the student's understanding of logical organization of ideas, presenting a poorly written student essay and asking a series of questions as to what changes might be made to best improve it.
The essay section, which is always administered as the first section of the test, is 25 minutes long. All essays must be in response to a given prompt. The prompts are broad and often philosophical and are designed to be accessible to students regardless of their educational and social backgrounds. For instance, test takers may be asked to expand on such ideas as their opinion on the value of work in human life or whether technological change also carries negative consequences to those who benefit from it. No particular essay structure is required, and the College Board accepts examples "taken from [the student's] reading, studies, experience, or observations." Two trained readers assign each essay a score between 1 and 6, where a score of 0 is reserved for essays that are blank, off-topic, non-English, not written with a Number 2 pencil, or considered illegible after several attempts at reading. The scores are summed to produce a final score from 2 to 12 (or 0). If the two readers' scores differ by more than one point, then a senior third reader decides. The average time each reader/grader spends on each essay is less than 3 minutes.[12]
In March 2004, Dr. Les Perelman analyzed 15 scored sample essays contained in the College Board's ScoreWrite book along with 30 other training samples and found that in over 90% of cases, the essay's score could be predicted from simply counting the number of words in the essay.[12] Two years later, Dr. Perelman trained high school seniors to write essays that made little sense but contained infrequently used words such as "plethora" and "myriad." All of the students received scores of "10" or better, which placed the essays in the 92nd percentile or higher.[13]
Most of the questions on the SAT, except for the essay and the grid-in math responses, are multiple choice; all multiple-choice questions have five answer choices, one of which is correct. The questions of each section of the same type are generally ordered by difficulty. However, an important exception exists: Questions that follow the long and short reading passages are organized chronologically, rather than by difficulty. Ten of the questions in one of the math sub-sections are not multiple choice. They instead require the test taker to bubble in a number in a four-column grid.
The questions are weighted equally. For each correct answer, one raw point is added. For each incorrect answer one-fourth of a point is deducted.[14] No points are deducted for incorrect math grid-in questions. This ensures that a student's mathematically expected gain from guessing is zero. The final score is derived from the raw score; the precise conversion chart varies between test administrations.
The SAT therefore recommends only making educated guesses, that is, when the test taker can eliminate at least one answer he or she thinks is wrong. Without eliminating any answers one's probability of answering correctly is 20%. Eliminating one wrong answer increases this probability to 25% (and the expected gain to 1/16 of a point); two, a 33.3% probability (1/6 of a point); and three, a 50% probability (3/8 of a point).
The SAT is offered seven times a year in the United States; in October, November, December, January, March (or April, alternating), May, and June. The test is typically offered on the first Saturday of the month for the November, December, May, and June administrations. In other countries, the SAT is offered on the same dates as in the United States except for the first spring test date (i.e., March or April), which is not offered. In 2006, the test was taken 1,465,744 times.[15]
Candidates may take either the SAT Reasoning Test or up to three SAT Subject Tests on any given test date, except the first spring test date, when only the SAT Reasoning Test is offered. Candidates wishing to take the test may register online at the College Board's website, by mail, or by telephone, at least three weeks before the test date.
The SAT Subject Tests are all given in one large book on test day. Therefore, it is actually immaterial which tests, and how many, the student signs up for; with the possible exception of the language tests with listening, the student may change his or her mind and take any tests, regardless of his or her initial sign-ups. Students who choose to take more subject tests than they signed up for will later be billed by College Board for the additional tests and their scores will be withheld until the bill is paid. Students who choose to take fewer subject tests than they signed up for are not eligible for a refund.
The SAT Reasoning Test costs $49 ($78 International, $99 for India and Pakistan). For the Subject tests, students pay a $22 ($49 International, $73 for India and Pakistan) Basic Registration Fee and $11 per test (except for language tests with listening, which cost $21 each).[2] The College Board makes fee waivers available for low income students. Additional fees apply for late registration, standby testing, registration changes, scores by telephone, and extra score reports (beyond the four provided for free).
Candidates whose religious beliefs prevent them from taking the test on a Saturday may request to take the test on the following day, except for the October test date in which the Sunday test date is eight days after the main test offering. Such requests must be made at the time of registration and are subject to denial.
Students with verifiable disabilities, including physical and learning disabilities, are eligible to take the SAT with accommodations. The standard time increase for students requiring additional time due to learning disabilities is time + 50%; time + 100% is also offered.
Students receive their online score reports approximately three weeks after test administration (six weeks for mailed, paper scores), with each section graded on a scale of 200800 and two sub scores for the writing section: the essay score and the multiple choice sub score. In addition to their score, students receive their percentile (the percentage of other test takers with lower scores). The raw score, or the number of points gained from correct answers and lost from incorrect answers (ranges from just under 50 to just under 60, depending upon the test), is also included.[16] Students may also receive, for an additional fee, the Question and Answer Service, which provides the student's answer, the correct answer to each question, and online resources explaining each question.
The corresponding percentile of each scaled score varies from test to testfor example, in 2003, a scaled score of 800 in both sections of the SAT Reasoning Test corresponded to a percentile of 99.9, while a scaled score of 800 in the SAT Physics Test corresponded to the 94th percentile. The differences in what scores mean with regard to percentiles are due to the content of the exam and the caliber of students choosing to take each exam. Subject Tests are subject to intensive study (often in the form of an AP, which is relatively more difficult), and only those who know they will perform well tend to take these tests, creating a skewed distribution of scores.
The percentiles that various SAT scores for college-bound seniors correspond to are summarized in the following chart:[17][18]
The older SAT (before 1995) had a very high ceiling. In any given year, only seven of the million test-takers scored above 1580. A score above 1580 was equivalent to the 99.9995 percentile.[19]
Although there is no official conversion chart between the SAT and its biggest rival, the ACT, the College Board released an unofficial chart based on results from 103,525 test takers who took both tests between October 1994 and December 1996;[20] however, both tests have changed since then. Several colleges have also issued their own charts. The following is based on the University of California's conversion chart.[21]
Frey and Detterman (2003) analyzed the correlation of SAT scores with intelligence test scores.[22] They found SAT scores to be highly correlated with general mental ability, or g (r=.82 in their sample, .86 when corrected for non-linearity). The correlation between SAT scores and scores on the Raven's Advanced Progressive Matrices was .483 (.72 corrected for restricted range). They concluded that the SAT is primarily a test of g. Beaujean and colleagues (2006) have reached similar conclusions.[23]
Originally used mainly by colleges and universities in the northeastern United States, and developed by Carl Brigham, one of the psychologists who worked on the Army Alpha and Beta tests, the SAT was originally developed as a way to eliminate test bias between people from different socio-economic backgrounds.
The College Board began on June 17, 1901, when 973 students took its first test, across 67 locations in the United States, and two in Europe. Although those taking the test came from a variety of backgrounds, approximately one third were from New York, New Jersey, or Pennsylvania. The majority of those taking the test were from private schools, academies, or endowed schools. About 60% of those taking the test applied to Columbia University. The test contained sections on English, French, German, Latin, Greek, history, mathematics, chemistry, and physics. The test was not multiple choice, but instead was evaluated based on essay responses as "excellent", "good", "doubtful", "poor" or "very poor".[25]
The first administration of the SAT occurred on June 23, 1926, when it was known as the Scholastic Aptitude Test.[26][27] This test, prepared by a committee headed by Princeton psychologist Carl Campbell Brigham, had sections of definitions, arithmetic, classification, artificial language, antonyms, number series, analogies, logical inference, and paragraph reading. It was administered to over 8,000 students at over 300 test centers. Men composed 60% of the test-takers. Slightly over a quarter of males and females applied to Yale University and Smith College.[27] The test was paced rather quickly, test-takers being given only a little over 90 minutes to answer 315 questions.[26]
In 1928 the number of verbal sections was reduced to 7, and the time limit was increased to slightly under two hours. In 1929 the number of sections was again reduced, this time to 6. These changes in part loosened time constraints on test-takers. Math was eliminated entirely for these tests, instead focusing only on verbal ability.[26]
In 1930 the SAT was first split into the verbal and math sections, a structure that would continue through 2004. The verbal section of the 1930 test covered a more narrow range of content than its predecessors, examining only antonyms, double definitions (somewhat similar to sentence completions), and paragraph reading. In 1936, analogies were re-added. Between 1936 and 1946, students had between 80 and 115 minutes to answer 250 verbal questions (over a third of which were on antonyms). The mathematics test introduced in 1930 contained 100 free response questions to be answered in 80 minutes, and focused primarily on speed. From 1936 to 1941, like the 1928 and 1929 tests, the mathematics section was eliminated entirely. When the mathematics portion of the test was re-added in 1942, it consisted of multiple choice questions.[26]
Paragraph reading was eliminated from the verbal portion of the SAT in 1946, and replaced with reading comprehension, and "double definition" questions were replaced with sentence completions. Between 1946 and 1957 students were given 90 to 100 minutes to complete 107 to 170 verbal questions. Starting in 1958 time limits became more stable, and for 17 years, until 1975, students had 75 minutes to answer 90 questions. In 1959 questions on data sufficiency were introduced to the mathematics section, and then replaced with quantitative comparisons in 1974. In 1974 both verbal and math sections were reduced from 75 minutes to 60 minutes each, with changes in test composition compensating for the decreased time.[26]
The inclusion of the "Strivers" Score study was implemented. This study was introduced by The Educational Testing Service, which administers the SAT, and has been conducting research on how to make it easier for minorities and individuals who suffer from social and economic barriers. The original "Strivers" project, which was in the research phase from 19801994, awarded special "Striver" status to test-takers who scored 200 points higher than expected for their race, gender and income level. The belief was that this would give minorities a better chance at being accepted in to a college of higher standard, i.e. an Ivy League school. In 1992, the Strivers Project was leaked to the public; as a result the Strivers Project was terminated in 1993. After Federal Courts heard arguments from the ACLU, NAACP and the Educational Testing Service, the courts ordered the study to alter its data collection process, stating that only the age, race and zip code could be used to determine the test-takers eligibility for "Strivers" points. These changes were introduced to the SAT effective in 1994.
In 1994 the verbal section received a dramatic change in focus. Among these changes were the removal of antonym questions, and an increased focus on passage reading. The mathematics section also saw a dramatic change in 1994, thanks in part to pressure from the National Council of Teachers of Mathematics. For the first time since 1935, the SAT asked some non-multiple choice questions, instead requiring students to supply the answers. 1994 also saw the introduction of calculators into the mathematics section for the first time in the test's history. The mathematics section introduced concepts of probability, slope, elementary statistics, counting problems, median and mode.[26]
The average score on the 1994 modification of the SAT I was usually around 1000 (500 on the verbal, 500 on the math). The most selective schools in the United States (for example, those in the Ivy League) typically had SAT averages exceeding 1400 on the old test[citation needed].
The test scoring was initially scaled to make 500 the mean score on each section with a standard deviation of 100.[28] As the test grew more popular and more students from less rigorous schools began taking the test, the average dropped to about 428 Verbal and 478 Math. The SAT was "recentered" in 1995, and the average "new" score became again close to 500. Scores awarded after 1994 and before October 2001 are officially reported with an "R" (e.g. 1260R) to reflect this change. Old scores may be recentered to compare to 1995 to present scores by using official College Board tables,[29] which in the middle ranges add about 70 points to Verbal and 20 or 30 points to Math. In other words, current students have a 100 (70 plus 30) point advantage over their parents.
Certain educational organizations viewed the SAT re-centering initiative as an attempt to stave off international embarrassment in regards to continuously declining test scores, even among top students. As evidence, it was presented that the number of pupils who scored above 600 on the verbal portion of the test had fallen from a peak of 112,530 in 1972 to 73,080 in 1993, a 36% backslide, despite the fact that the total number of test-takers had risen over 500,000.[30]
In October 2002, the College Board dropped the Score Choice Option for SAT-II exams. Under this option, scores were not released to colleges until the student saw and approved of the score.[31] The College Board has since decided to re-implement Score Choice in the spring of 2009. It is described as optional, and it is not clear if the reports sent will indicate whether or not this student has opted-in or not. A number of highly selective colleges and universities, including Yale, the University of Pennsylvania, and Stanford, have announced they will require applicants to submit all scores. Stanford, however, only prohibits Score Choice for the traditional SAT.[32] Others, such as MIT and Harvard, have fully embraced Score Choice.
In 2005, the test was changed again, largely in response to criticism by the University of California system.[33] Because of issues concerning ambiguous questions, especially analogies, certain types of questions were eliminated (the analogies from the verbal and quantitative comparisons from the Math section). The test was made marginally harder, as a corrective to the rising number of perfect scores. A new writing section, with an essay, based on the former SAT II Writing Subject Test, was added,[34] in part to increase the chances of closing the opening gap between the highest and midrange scores. Other factors included the desire to test the writing ability of each student; hence the essay. The New SAT (known as the SAT Reasoning Test) was first offered on March 12, 2005, after the last administration of the "old" SAT in January 2005. The Mathematics section was expanded to cover three years of high school mathematics. The Verbal section's name was changed to the Critical Reading section.
In late 2008, a new variable came into play. Previously, applicants to most colleges were required to submit all scores, with some colleges that embraced Score Choice retaining the option of allowing their applicants not to have to submit all scores. However, in 2008, an initiative to make Score Choice universal had begun, with some opposition from colleges desiring to maintain score report practices. While students theoretically now have the choice to submit their best score (in theory one could send any score one wishes to send) to the college of their choice, some popular colleges and universities, such as Cornell, ask that students send all test scores.[35] This had led the College Board to display on their web site which colleges agree with or dislike Score Choice, with continued claims that students will still never have scores submitted against their will.[36] Regardless of whether a given college permits applicants to exercise Score Choice options, most colleges do not penalize students who report poor scores along with high ones; many universities, such as Columbia[citation needed] and Cornell,[citation needed] expressly promise to overlook those scores that may be undesirable to the student and/or to focus more on those scores that are most representative of the student's achievement and academic potential. College Board maintains a list of colleges and their respective score choice policies that is recent as of November 2011.[37]
Beginning in 2012, test takers became subject to a new requirement that entails uploading a digital portrait for enhanced identification purposes. Subsequent critical commentary raised substantive concerns of racial and other discrimination because provided photos are obligatorily submitted alongside test scores in the admissions process.[38]
The name originally stood for "Scholastic Aptitude Test".[39] But in 1990, because of uncertainty about the SAT's ability to function as an intelligence test, the name was changed to Scholastic Assessment Test. In 1993 the name was changed to SAT I: Reasoning Test (with the letters not standing for anything) to distinguish it from the SAT II: Subject Tests.[39] In 2004, the roman numerals on both tests were dropped, and the SAT I was renamed the SAT Reasoning Test.[39] The scoring categories are now the following: Critical Reading (comparable to some of the Verbal portions of the old SAT I), Mathematics, and Writing. The writing section now includes an essay, whose score is involved in computing the overall score for the Writing section, as well as grammar sections (also comparable to some Verbal portions of the previous SAT).
The test scoring was initially scaled to make 500 the mean score on each section with a standard deviation of 100.[28] The SAT was "recentered" in 1995, and the average "new" score became again close to 500. Scores awarded after 1994 and before October 2001 are officially reported with an "R" (e.g. 1260R) to reflect this change. Old scores may be recentered to compare to 1995 to present scores by using official College Board tables,[29] which in the middle ranges add about 70 points to Verbal and 20 or 30 points to Math.
In March 2006, it was announced that a small percentage of the SATs taken in October 2005 had been scored incorrectly due to the test papers being moist and not scanning properly, and that some students had received erroneous scores. The College Board announced they would change the scores for the students who were given a lower score than they earned, but at this point many of those students had already applied to colleges using their original scores. The College Board decided not to change the scores for the students who were given a higher score than they earned. A lawsuit was filed in 2005 by about 4,400 students who received an incorrect low score on the SAT. The class-action suit was settled in August 2007 when The College Board and another company that administers the college-admissions test announced they would pay $2.85 million to over 4,000 students. Under the agreement each student can either elect to receive $275 or submit a claim for more money if he or she feels the damage was even greater.[40] A similar scoring error occurred on a secondary school admission test in 2010-2011 when the ERB (Educational Records Bureau) announced after the admission process was over that an error had been made in the scoring of the tests of 2010 (17%) of the students who had taken the Independent School Entrance Examination for admission to private secondary schools for 2011. Commenting on the effect of the error on students' school applications in the New York Times, David Clune, President of the ERB stated "It is a lesson we all learn at some point  that life isnt fair."[41]
In 2002, Richard Rothstein (education scholar and columnist) wrote in The New York Times that the U.S. math averages on the SAT & ACT continued its decade-long rise over national verbal averages on the tests.[42]
For decades many critics have accused designers of the verbal SAT of cultural bias toward the white and wealthy. A famous example of this bias in the SAT I was the oarsmanregatta analogy question.[43] The object of the question was to find the pair of terms that have the relationship most similar to the relationship between "runner" and "marathon". The correct answer was "oarsman" and "regatta". The choice of the correct answer presupposed students' familiarity with crew, a sport popular with the wealthy, and so upon their knowledge of its structure and terminology. Fifty-three percent (53%) of white students correctly answered the question, while only 22% of black students also scored correctly.[44] However, according to Murray and Herrnstein, the black-white gap is smaller in culture-loaded questions like this one than in questions that appear to be culturally neutral.[45] Analogy questions have since been replaced by short reading passages.
Recent research has linked high family incomes to higher mean scores. Test score data from California has shown that test-takers with family incomes of less than $20,000 a year had a mean score of 1310 while test-takers with family incomes of over $200,000 had a mean score of 1715, a difference of 405 points. The estimates of correlation of SAT scores and household income range from 0.23 to 0.4 (explaining about 5-16% of the variation).[46] One calculation has shown a 40-point average score increase for every additional $20,000 in income.[47] There are conflicting opinions on the source of this correlation. Some think it is evidence of superior education and tutoring that is accessible to the more affluent adolescents. Still others propose it relates to wealthier families being exposed to a broader range of cultural ideas and experiences, because of travel and other means of wider exposure, and that "Cultural Literacy" can lead to enhancement of aptitude.[48]
A growing number of colleges have responded to this criticism by joining the SAT optional movement. These colleges do not require the SAT for admission.
In a 2001 speech to the American Council on Education, Richard C. Atkinson, the president of the University of California, urged dropping the SAT Reasoning Test as a college admissions requirement:
In response to threats by the University of California to drop the SAT as an admission requirement, the College Entrance Examination Board announced the restructuring of the SAT, to take effect in March 2005, as detailed above.
In the 1960s and 70's there was a movement to drop achievement scores. After a period of time, the countries, states and provinces that reintroduced them agreed that academic standards had dropped, students had studied less and had taken their studying less seriously. They reintroduced the tests after studies and research concluded that the high stakes tests produced benefits that out weighed the costs.[50]
In 2005, MIT Writing Director Les Perelman plotted essay length versus essay score on the new SAT from released essays and found a high correlation between them. After studying over 50 graded essays, he found that longer essays consistently produced higher scores. In fact, he argues that by simply gauging the length of an essay without reading it, the given score of an essay could likely be determined correctly over 90% of the time. He also discovered that several of these essays were full of factual errors; the College Board does not claim to grade for factual accuracy.
Perelman, along with the National Council of Teachers of English also criticized the 25-minute writing section of the test for damaging standards of writing teaching in the classroom. They say that writing teachers training their students for the SAT will not focus on revision, depth, accuracy, but will instead produce long, formulaic, and wordy pieces.[51] "You're getting teachers to train students to be bad writers", concluded Perelman.[52]
SAT preparation is a highly lucrative field.[53] Many companies and organizations offer test preparation in the form of books, classes, online courses, and tutoring.
Although the College Board maintains that the SAT is essentially uncoachable, some research suggests that tutoring courses result in an average increase of about 20 points on the math section and 10 points on the verbal section.[54]
 ATAR, STAT    Gao Kao    JUPAS(HKDSE or HKALE)    IITJEE, AIEEE, EAMCET    Concours    YGS-LYS    The Psychometry    National Center Test for University Admissions
 College Scholastic Ability Test (CSAT or Suneung)    GCE-O, GCE-A 
 Baccalaurat     Ylioppilastutkinto    Abitur    Leaving Certificate    Bacalaureat    Unified State Exam (EGE)    Selectividad
 Swedish Scholastic Aptitude Test (Hgskoleprovet)    Highers      A-Level 
1 Function
2 Structure

2.1 Critical Reading
2.2 Mathematics

2.2.1 Calculator use


2.3 Writing
2.4 Style of questions


2.1 Critical Reading
2.2 Mathematics

2.2.1 Calculator use


2.2.1 Calculator use
2.3 Writing
2.4 Style of questions
3 Taking the test
4 Raw scores, scaled scores, and percentiles
5 SAT-ACT score comparisons
6 Correlations with IQ
7 History

7.1 1901 test
7.2 1926 test
7.3 1928 and 1929 tests
7.4 1930 test and 1936 changes
7.5 1946 test and associated changes
7.6 1980 test and associated changes
7.7 1994 changes
7.8 1995 re-centering (raising mean score back to 500)
7.9 1995 re-centering controversy
7.10 2002 changes  Score Choice
7.11 2005 changes
7.12 2008 changes
7.13 2012 changes


7.1 1901 test
7.2 1926 test
7.3 1928 and 1929 tests
7.4 1930 test and 1936 changes
7.5 1946 test and associated changes
7.6 1980 test and associated changes
7.7 1994 changes
7.8 1995 re-centering (raising mean score back to 500)
7.9 1995 re-centering controversy
7.10 2002 changes  Score Choice
7.11 2005 changes
7.12 2008 changes
7.13 2012 changes
8 Name changes and recentered scores
9 Scoring problems of October 2005 tests
10 The math-verbal achievement gap
11 Criticism

11.1 Cultural bias
11.2 Test score disparity by income
11.3 Dropping SAT
11.4 MIT study
11.5 Test preparation


11.1 Cultural bias
11.2 Test score disparity by income
11.3 Dropping SAT
11.4 MIT study
11.5 Test preparation
12 See also
13 References
14 Further reading
15 External links
2.1 Critical Reading
2.2 Mathematics

2.2.1 Calculator use


2.2.1 Calculator use
2.3 Writing
2.4 Style of questions
2.2.1 Calculator use
7.1 1901 test
7.2 1926 test
7.3 1928 and 1929 tests
7.4 1930 test and 1936 changes
7.5 1946 test and associated changes
7.6 1980 test and associated changes
7.7 1994 changes
7.8 1995 re-centering (raising mean score back to 500)
7.9 1995 re-centering controversy
7.10 2002 changes  Score Choice
7.11 2005 changes
7.12 2008 changes
7.13 2012 changes
11.1 Cultural bias
11.2 Test score disparity by income
11.3 Dropping SAT
11.4 MIT study
11.5 Test preparation
One of the 25-minute sections is entirely multiple choice, with 20 questions.
The other 25-minute section contains 8 multiple choice questions and 10 grid-in questions. For grid-in questions, test-takers write the answer inside a grid on the answer sheet. Unlike multiple choice questions, there is no penalty for incorrect answers on grid-in questions because the test-taker is not limited to a few possible choices.
The 20-minute section is all multiple choice, with 16 questions.
New topics include Algebra II and scatter plots. These recent changes have resulted in a shorter, more quantitative exam requiring higher level mathematics courses relative to the previous exam.
ACT (test), a college entrance exam, competitor to the SAT
College admissions in the United States
List of admissions tests
PSAT/NMSQT
SAT calculator program
SAT Subject Tests
Coyle, T. R. & Pillow, D. R. (2008). "SAT and ACT predict college GPA after removing g". Intelligence 36 (6): 719729. doi:10.1016/j.intell.2008.05.001.
Coyle, T.; Snyder, A.; Pillow, D.; Kochunov, P. (2011). "SAT predicts GPA better for high ability subjects: Implications for Spearman's Law of Diminishing Returns". Personality and Individual Differences 50 (4): 470474. doi:10.1016/j.paid.2010.11.009.
Frey, M. C.; Detterman, D. K. (2003). "Scholastic Assessment or g? The Relationship Between the Scholastic Assessment Test and General Cognitive Ability". Psychological Science 15 (6): 373378. doi:10.1111/j.0956-7976.2004.00687.x. PMID15147489. http://www.missouri.edu/~aab2b3/Detterman.g.Psychological.Science.pdf.
Gould, Stephen Jay (1996). The Mismeasure of Man (Rev/Expd ed.). W. W. Norton & Company. ISBN0-393-31425-1.
Hoffman, Banesh (1962). The Tyranny of Testing. Orig. pub. Collier. ISBN0-486-43091-X. (and others)
Hubin, David R. (1988). The Scholastic Aptitude Test: Its Development and Introduction, 19001948. Ph.D. dissertation in American History at the University of Oregon. http://www.uoregon.edu/~hubin/.
Owen, David (1999). None of the Above: The Truth Behind the SATs (Revised ed.). Rowman & Littlefield. ISBN0-8476-9507-7.
Sacks, Peter (2001). Standardized Minds: The High Price of America's Testing Culture and What We Can Do to Change It. Perseus. ISBN0-7382-0433-1.
Zwick, Rebecca (2002). Fair Game? The Use of Standardized Admissions Tests in Higher Education. Falmer. ISBN0-415-92560-6.
Gladwell, Malcolm (December 17, 2001). "Examined Life: What Stanley H. Kaplan taught us about the S.A.T.". The New Yorker. http://www.newyorker.com/archive/2001/12/17/011217crat_atlarge.
Official SAT Test website
.
#(`*Advanced Placement*`)#.
The Advanced Placement (AP) is a program in the United States created by the College Board offering college-level curriculum and examinations to high school students. American colleges often grant placement and course credit to students who obtain high scores on the examinations. The AP curriculum for the various subjects is created for the College Board by a panel of experts and college-level educators in each subject. For a high school course to have the AP designation, the course must be audited by the College Board to ascertain it satisfies the AP curriculum. If the course is approved the school may use the AP designation and the course will be publicly listed on the AP Ledger.
The most taken AP exam in 2008 was AP United States History with 346,641 students, and the least taken was AP Italian Language and Culture with 1,930 students.
After World War II, the Ford Foundation created a fund that supported committees studying education.[1] The program was founded and pioneered at Kenyon College in Gambier Ohio, by the then college president Gordon Chalmers which was then referred to as the "Kenyon Plan."[2] The first study was conducted by three prep schoolsthe Lawrenceville School, Phillips Academy and Phillips Exeter Academyand three universitiesHarvard University, Princeton University and Yale University. In 1952 they issued the report General Education in School and College: A Committee Report which recommended allowing high school seniors to study college level material and to take achievement exams that allowed them to attain college credit for this work.[3] The second committee, the Committee on Admission with Advanced Standing, developed and implemented the plan to choose a curriculum. A pilot program was run in 1952 which covered eleven disciplines.
The College Board, a non-profit organization[4] based in New York City, has run the AP program since 1955.[5] From 1965 to 1989, Harlan Hanson was the director of the Advanced Placement Program.[6] It develops and maintains guidelines for the teaching of higher level courses in various subject areas. In addition, it supports teachers of AP courses, and supports universities.[7] These activities are funded through fees charged to students taking AP Exams.
In 2006, over one million students took over two million Advanced Placement examinations.[8] Many high schools in the United States offer AP courses,[9] though the College Board allows any student to take any examination, regardless of participation in its respective course.[10] Therefore, home-schooled students and students from schools that do not offer AP courses have an equal opportunity to take the examination.
As of the 2011 testing season, exams cost $87 each,[11] though the cost may be subsidized by local or state programs. Financial aid is available for students who qualify for it; the exam reduction is $22 per exam from College Board plus an additional $8 rebate per fee-reduced exam from the school. There may be further reductions depending on the state. Out of the $87, $8 goes directly to the school to pay for the administration of the test, which some schools will reduce to lower the cost to the student.
On April 3, 2008, the College Board announced that four AP courses  French Literature, Latin Literature, Computer Science AB, and Italian Language and Culture  would be discontinued after the 20082009 school year due to lack of funding.[12][13]
AP tests are scored on a 1 to 5 scale as follows:[14]
Grading the AP exam is a long and complicated process. The multiple choice component of the exam is scored by computer, while the free response and essay portions are scored by trained Readers at the AP Reading each June. The scores on various components are weighted and combined into a raw Composite Score. The Chief Reader for each exam then decides on the grade cutoffs for that year's exam, which determine how the Composite Scores are converted into the final grades. During the process a number of reviews and statistical analyses are performed to ensure that the grading is reliable. The overall goal is for the grades to reflect an absolute scale of performance which can be compared from year to year.[15]
Some colleges use AP test scores to exempt students from introductory coursework. Each college's policy is different (see link below), but most require a minimum score of 3 or 4 to receive college credit.[16] Typically this appears as a "CR" grade on the college transcript, although some colleges and universities will award an A grade for a 5 score.[17] Some foreign countries, such as Germany, that do not offer general admission to their universities and colleges for holders of an American high school diploma without lengthy preparatory courses will directly admit students that have completed a specific set of AP tests, depending on the subject they wish to study there.
In addition, completing AP courses help students qualify for various types of scholarships. According to the College Board, 31 percent of colleges and universities look at AP experience when making scholarship decisions.[18]
Beginning with the May 2011 AP Exam administration, the College Board changed the scoring of AP Exams.[19][20] Total scores on the multiple-choice section are now based on the number of questions answered correctly. Points are no longer deducted for incorrect answers and, as was the case before, no points are awarded for unanswered questions. However, scoring requirements have also been increased.
Recognizing that the cost could be an impediment to students of limited means, a number of states and municipalities independent of the College Board have partially or fully subsidized the cost. For example, the state of Florida reimburses schools districts for the exam costs of students enrolled in Advanced Placement courses. The Los Angeles Unified School District, the Montebello Unified School District, the Hawaii Department of Education, New York City Department of Education, and the state of Indiana subsidize all AP Examination fees in subjects of math and science, and the Edmonds School District in suburban Seattle currently subsidizes Advanced Placement fees of students who enroll in the free school lunch program. In addition some school districts offer free tests to all students enrolled in any Advanced Placement class.
There are currently 34 courses and exams available through the AP Program. A complete list can be found below:
Faculty at a number of universities have doubted the value of a passing AP score. Students who receive scores of 3 or sometimes 4, but not the perfect 5, are being given college credit at fewer universities. Academic departments cite the increasing number of students who take AP courses but are not ready for college-level work. [21]
Research conducted by Philip M. Sadler and published in AP: A critical examination of the Advanced Placement program found that students who took AP courses in the sciences but failed the AP exam performed no better in college science courses than students without any AP course at all. Referring to students who complete the course but fail the exam, Sadler stated in an interview that "research shows that they dont appear to have learned anything during the year, so there is probably a better course for them".[22]
Art History
Biology
Calculus (AB & BC)
Chemistry
Chinese Language and Culture
Comparative Government & Politics
Computer Science A
English Language & Composition
English Literature & Composition
Environmental Science
European History
French Language
German Language
Human Geography
Italian Language and Culture
Japanese Language and Culture
Latin
Macroeconomics
Microeconomics
Music Theory
Physics B
Physics C: Mechanics
Physics C: Electricity and Magnetism
Psychology
Russian Language and Culture
Spanish Language
Spanish Literature
Statistics
Studio Art (2-D, 3-D, & Drawing)
U.S. History
U.S. Government & Politics
World History
Computer Science AB
French Literature
Latin Literature
1 History
2 Scoring
3 Exam subsidies
4 Advanced Placement courses
5 Criticism
6 See also
7 References
8 Further reading
9 External links
5  Extremely well qualified
4  Well qualified
3  Qualified
2  Possibly qualified
1  No recommendation
Art History
Biology
Calculus AB
Calculus BC
Chemistry
Chinese Language and Culture
Computer Science A
English Language and Composition
English Literature and Composition
Environmental Science
European History
French Language
German Language
Government and Politics: Comparative
Government and Politics: United States
Human Geography
Italian Language and Culture
Japanese Language and Culture
Latin: Vergil
Macroeconomics
Microeconomics
Music Theory
Physics B
Physics C: Electricity and Magnetism
Physics C: Mechanics
Psychology
Spanish Language
Spanish Literature
Statistics
Studio Art: 2-D Design
Studio Art: 3-D Design
Studio Art: Drawing
United States History
World History
Advanced Placement Awards
GCE Advanced Level
Education in the United States
Education in Canada
International Baccalaureate
McCauley, David. 2007. The Impact of Advanced Placement and Dual Enrollment Program on College Graduation.
Applied Research Project. Texas State University. http://ecommons.txstate.edu/arp/206/
Schneider, Jack. 2008. Schools' Unrest Over the AP Test
The College Board's AP website for students and parents
.
#(`*ACT (test)*`)#.
The ACT (/e si ti/ ay-see-tee; originally an abbreviation of American College Testing)[1] is a standardized test for high school achievement and college admissions in the United States produced by ACT, Inc.[1] It was first administered in November 1959 by Everett Franklin Lindquist as a competitor to the College Board's Scholastic Aptitude Test, now the SAT Reasoning Test.[2] The ACT has historically consisted of four tests: English, Mathematics, Reading, and Science Reasoning. In February 2005, an optional Writing test was added to the ACT, mirroring changes to the SAT that took place later in March of the same year.
The ACT has seen an increase in the number of test takers recently; In 2011 the ACT surpassed the SAT as 1,666,017 students took the ACT and 1,664,479 students took the SAT. [3] All four-year colleges and universities in the U.S. accept the ACT,[4] but different institutions place different emphases on standardized tests such as the ACT, compared to other factors of evaluation such as class rank, GPA, and extracurricular activities. The main four tests are scored individually on a scale of 136, and a Composite score is provided which is the whole number average of the four scores.
ACT, Inc. says that the ACT assessment measures high school students' general educational development and their capability to complete college-level work with the multiple choice tests covering four skill areas: English, mathematics, reading, and science. The optional Writing Test measures skill in planning and writing a short essay.[5] Specifically, ACT states that its scores provide an indicator of "college readiness", and that scores in each of the subtests correspond to skills in entry-level college courses in English, algebra, social science, humanities, and biology.[6] According to a research study conducted by ACT, Inc., in 2003, a relationship was found between a student's ACT composite score and the possibility of he or she earning a college degree.[7]
To develop the test, ACT incorporates the objectives for instruction for middle and high schools throughout the United States, reviews approved textbooks for subjects taught in Grades 712, and surveys educators on which knowledge skills are relevant to success in postsecondary education. ACT publishes a technical manual that summarizes studies conducted of its validity in predicting freshman GPA, equating different high school GPAs, and measuring educational achievement.[8]
Colleges use the ACT and the SAT Reasoning Test because there are substantial differences in funding, curricula, grading, and difficulty among U.S. secondary schools due to American federalism, local control, and the prevalence of private, distance, home schooled students and, most importantly, lack of rigorous college entrance examination system like those used in other countries. ACT/SAT scores are used to supplement the secondary school record and help admission officers put local datasuch as course work, grades, and class rankin a national perspective.
Most colleges use ACT scores as only one factor in their admission process. A sampling of ACT admissions scores shows that the 75th percentile composite score was 24.1 at public four year institutions and 25.3 at private four year institutions. It is recommended that students check with their prospective institutions directly to understand ACT admissions requirements.
In addition, some states have used the ACT to assess the performance of schools, and require all high school students to take the ACT, regardless of whether they are college bound. Colorado and Illinois have incorporated the ACT as part of their mandatory testing program since 2001. Michigan has required the ACT since 2007, Kentucky and Tennessee require all high school juniors to take the ACT and Wyoming requires all high school juniors to take either the ACT or the ACT WorkKeys.[9]
While the exact manner in which ACT scores will help to determine admission of a student at American institutions of higher learning is generally a matter decided by the individual institution, some foreign countries have made ACT (and SAT) scores a legal criterion in deciding whether holders of American high school diplomas will be admitted at their public universities.
The ACT is more widely used in the Midwestern, Rocky Mountain, and Southern United States, while the SAT is more popular on the East and West coasts. Recently, however, the ACT is being used more on the East Coast.[10] Use of the ACT by colleges has risen as a result of various criticisms of the effectiveness and fairness of the SAT. American Mensa is a high IQ society that allows use of the ACT for membership admission if the test was taken prior to September 1989. A composite score of 29 or above is required.[11] The Triple Nine Society also accepts the old ACT test for admission, with a qualifying score of 32; after September 1989 the qualifying score is 34.[12]
The required portion of the ACT is divided into four multiple choice subject tests: English, mathematics, reading, and science reasoning. Subject test scores range from 1 to 36; all scores are integers. The English, mathematics, and reading tests also have subscores ranging from 1 to 18. (The subject score is not the sum of the subscores.) The composite score is the average of all four tests. In addition, students taking the writing test receive a writing score ranging from 2 to 12, a combined English/writing score ranging from 1 to 36 (based on the writing score and English score), and one to four comments on the essay from the essay scorers. The writing score does not affect the composite score.
On the ACT, each question correctly answered is worth one raw point. Unlike the SAT, there is no penalty for marking incorrect answers on the multiple-choice part of the test. Therefore, a student can answer all questions without suffering a decrease in their score for questions they answer incorrectly. This is parallel to several AP Tests eliminating the penalties for incorrect answers. To improve the result, students can retake the test: 55% of students who retake the ACT improve their scores, 22% score the same, and 23% see their scores decrease.[13]
The first section is the 45-minute English test covering usage/mechanics and rhetorical skills. The 75-question test consists of five passages with various sections underlined on one side of the page and options to correct the underlined portions on the other side of the page. More specifically, questions focus on usage and mechanics  issues such as commas, apostrophes, (misplaced/dangling) modifiers, the colons, and fragments and run-ons  as well as on rhetorical skills  style (clarity and brevity), strategy, transitions, and organization (sentences in a paragraph and paragraphs in a passage.)
The second section is the 60-minute, 60-question mathematics test with 14 covering pre-algebra, 10 elementary algebra, 9 intermediate algebra, 14 plane geometry, 9 coordinate geometry, and 4 elementary trigonometry.[14] Calculators are permitted in this section only. The calculator requirements are stricter than the SAT's in that computer algebra systems are not allowed; however, the ACT permits calculators with paper tapes, that make noise (but must be disabled), or that have power cords with certain "modifications" (i.e., disabling the mentioned features), which the SAT does not allow.[15] Also, this is the only section that has five instead of four answer choices.
The 35-minute, 40-question reading section measures reading comprehension in four passages (taken and edited from books and magazines) one representing prose fiction (short stories and novels), another representing social science (history, economics, psychology, political science, and anthropology), a third representing humanities (art, music, architecture, dance), and the last representing natural science (biology, chemistry, physics, and the physical sciences), in that order.[16]
The science reasoning test is a 35-minute, 40-question test. There are seven passages each followed by five to seven questions. There are three Data Representation passages with 5 questions following each passage, 3 Research Summary passage with six questions each, and one Conflicting Viewpoints passage with 7 questions.[17]
The optional writing section, which is always administered at the end of the test, is 30 minutes long. All essays must be in response to a given prompt. The prompts are about a social issue applicable to high school students. The essay can affect the score of the English section only. If a student were to score a 10 out of 12 on the writing, and the student scored an English composite score of 25 then the score would be affected, but would most likely stay the same. If a student were to score poorly on the writing section, then the score would be reduced from 25 to 23 at the most. No particular essay structure is required. Two trained readers assign each essay a score between 1 and 6, where a score of 0 is reserved for essays that are blank, off-topic, non-English, not written with no. 2 pencil, or considered illegible after several attempts at reading. The scores are summed to produce a final score from 2 to 12 (or 0). If the two readers' scores differ by more than one point, then a senior third reader decides.
Although the writing section is optional, several schools do require an essay score and will factor it in the admissions decision.[18]
For the original standardization groups, the mean composite score on the ACT was 18, and the standard deviation 6. These statistics vary from year to year for current populations of ACT takers.
The chart below summarizes each section and the average test score based on graduating high school seniors in 2009.[19][20]
The chart below summarizes how many students achieved a score of 36 on the ACT between the years of 1997 and 2011.[21]
At ACT.org, they provide the current ACT Assessment Student Report provides the typical ACT Composite averages for college and universities ranging in admission policies. They caution that, "because admission policies vary across colleges, the score ranges should be considered rough guidelines." Following is a list of the average composite scores that typically are accepted at colleges or universities. [22]
The ACT is offered four to six times a year, depending on the state, in the United States, in September, October, December, February, April and June and is always on a Saturday except for those with credible religious obligations. The test can also be taken in other countries, however availability is much less than in the United States.
"Some people believe that it is possible to get a higher score by testing on one national test date than on another. they think that on certain national test dates, easier forms of the ACT are routinely administered, thereby making it possible to get a higher score simply by choosing to test on one of those "easy" test dates. Likewise, they may think that there is an advantage to testing on one of the less popular national test dates, when fewer students take the ACT. These beliefs are not true. The ACT is designed, administered, and scored in such a way that there is no advantage to testing on one particular date or another." [23]
Candidates may choose either the ACT assessment ($34), or the ACT assessment plus writing ($49.50).
Students with verifiable disabilities, including physical and learning disabilities, are eligible to take the test with accommodations. The standard time increase for students requiring additional time due to learning disabilities is 50%.[24] Originally the score sheet was labeled that additional time was granted due to a learning disability, however this was dropped because it was deemed illegal under the Americans with Disabilities Act and could be seen as an unfair mark of disability.
Scores are sent to the student, his or her high school, and up to four colleges.[25]
Time is a major factor to consider in testing.
The ACT is generally regarded as being composed of somewhat easier questions (versus the SAT), but the time allotted to complete each section increases the overall difficulty (equalizing it to the SAT). The ACT allots:
Comparatively, the SAT is structured such that the test taker is allowed at least one minute per question, on generally shorter sections (25 or fewer questions).
Forty-five percent1,480,469 studentsof the 2009 high school graduating class took the ACT.[20][26] The average composite score was a 21.1 in 2009.[20] Of 2009 test-takers, 668,165 (or 45%) were males, 808,097 (or 55%) were females, and 4,207 (or 0.3%) did not report a gender.[20] Nationwide, 638 students who reported that they would graduate in 2009 received the highest ACT composite score of 36.[20]
The following is based on official ACT ACT-SAT concordance chart[27][28] ACT percentiles are calculated on the basis of the percent of test takers scoring the same score or a lower one, not (as is the case for many other assessments) only the percent scoring lower.
[29]
[29]
 ATAR, STAT    Gao Kao    JUPAS(HKDSE or HKALE)    IITJEE, AIEEE, EAMCET    Concours    YGS-LYS    The Psychometry    National Center Test for University Admissions
 College Scholastic Ability Test (CSAT or Suneung)    GCE-O, GCE-A 
 Baccalaurat     Ylioppilastutkinto    Abitur    Leaving Certificate    Bacalaureat    Unified State Exam (EGE)    Selectividad
 Swedish Scholastic Aptitude Test (Hgskoleprovet)    Highers      A-Level 
1 Function
2 Use
3 Format

3.1 English
3.2 Mathematics
3.3 Reading
3.4 Science reasoning
3.5 Writing
3.6 Averages
3.7 Highest score
3.8 College Admissions


3.1 English
3.2 Mathematics
3.3 Reading
3.4 Science reasoning
3.5 Writing
3.6 Averages
3.7 Highest score
3.8 College Admissions
4 Test availability
5 Test section durations
6 Score cumulative percentages and comparison with SAT
7 See also
8 References
9 External links
3.1 English
3.2 Mathematics
3.3 Reading
3.4 Science reasoning
3.5 Writing
3.6 Averages
3.7 Highest score
3.8 College Admissions
Highly selective (majority of accepted freshmen in top 10% of high school graduating class): scores 27-31
Selective (majority of accepted freshmen in top 25% of high school graduating class): scores 25-27
Traditional (majority of accepted freshmen in top 50% of high school graduating class): scores 22-24
Liberal (some freshmen from lower half of high school graduating class): scores 18-21
Open (all high school graduates accepted, to limit of capacity): scores 17-20
45 minutes for a 75-question English section
60 minutes for a 60-question Mathematics section
35 minutes for a 40-question Reading Comprehension section
35 minutes for a 40-question Science section
College admissions in the United States
Math-verbal achievement gap on standardized testing
SAT, the main competitor to ACT's examination
List of admission tests to colleges and universities
PLAN (test)
ACT Taker's Site
ACT Corporate Site
.
#(`*PSAT/NMSQT*`)#.
The Preliminary SAT/National Merit Scholarship Qualifying Test (PSAT/NMSQT) is a standardized test administered by the College Board and National Merit Scholarship Corporation (NMSC) in the United States. This test is offered by the College Board. Approximately 3.5 million students take the PSAT/NMSQT each year.[1] In 2008, 1.59 million high school sophomores and 1.52 million high school juniors took the PSAT.[2] Some freshmen and eighth and seventh graders also take the test. The scores from the PSAT/NMSQT are used (with the permission of the student) to determine eligibility and qualification for the National Merit Scholarship Program.
Prior to 1997, the PSAT was composed of only Math and Verbal sections. The Verbal section received a double weighting to allow a full composite score of 240 points [3] The Writing Skills section, introduced in 1997, was partially derived from the discontinued Test of Standard Written English (TSWE).[4]
Students cannot register for it online and have to register for it through the high schools which are members of the College Board. The test is composed of three sections: Math, Critical Reading, and Writing Skills, and takes two hours and ten minutes to complete. Each of the three sections is scored on a scale of 20 to 80 points, which add up to a maximum composite score of 240 points. This parallels the SAT, which is graded on a scale of 200 to 800 (the narrower range is to distinguish from which test a score comes and to denote less accuracy). However, unlike the new (2005) SAT, the new PSAT does not include higher-level mathematics (e.g., concepts from Algebra II) or an essay in its writing section (which was added to the SAT in 2005).
The test is mostly multiple-choice, but there are 10 open-response math questions that require takers to enter their responses on a grid. Students are allowed to use calculators on the math sections.[5]
The sum of the three scores is known as the Selection Index and is used, along with four general criteria for eligibility such as U.S. citizenship (or be a U.S. lawful permanent resident or have applied for permanent residence, the application for which has not been denied, and intend to become a U.S. citizen at the earliest opportunity allowed by law), for both preliminary and primary selection in the National Merit Scholarship Programs.
The minimum Selection Index for recognition as a Semifinalist is determined by selection unit (50 states, three other geographic units, and a number of boarding school regions) and is set by the NMSC in each state at whatever score yields about the 99th percentile. While many people object to this (particularly those who score well in states with high minimums), this is used instead of a national minimum to ensure an even geographical distribution of Semifinalists. Because it is dependent on selection unit, on the number of students taking the test in the selection unit, and how well the students in the selection unit do on the test, the minimum varies from year to year and from selection unit to selection unit. For example, for the 2007 competition (2005 PSAT), minimum scores required for Semifinalist recognition ranged from 207 in Mississippi to 224 in Massachusetts, with an unweighted mean of 215[citation needed].
Students not recognized as Semifinalists whose Selection Index is above a different limit are recognized as Commended Students and receive Letters of Commendation. This minimum is determined nationally and is set at whichever score yields the 96th percentile. It rose from 202 for the 2006 Program (2004 PSAT) to 203 for the 2007 Program (2005 PSAT)[citation needed]. It was 205 for the 2008 Program (2006 PSAT) and 209 for the 2009 Program (2007 PSAT).
After being confirmed as a Semifinalist (which occurs one year after taking the PSAT as a junior), students must complete an application to become a Finalist. Other factors besides the PSAT Selection Index score are taken into account, such as the student's Grade Point Average (GPA) and extra-curricular activities.
1 History
2 Format
3 Scoring

3.1 Selection index
3.2 Levels of recognition


3.1 Selection index
3.2 Levels of recognition
4 See also
5 References
6 External links
3.1 Selection index
3.2 Levels of recognition
SAT
PLAN (preliminary ACT test)
What's on the Test
.
#(`*FIRST Robotics Competition*`)#.
The FIRST Robotics Competition is an international high school robotics competition organized by FIRST (For Inspiration and Recognition of Science and Technology). Each year, teams of high school students compete to build robots weighing up to 120 pounds (54kg), not including battery and bumpers, that can complete a task, which changes every year. While teams are given a standard set of parts, they are also allowed a budget and encouraged to buy or make specialized parts as long as they conform to FIRST safety rules. Game details are revealed at the beginning of January and the teams are given six weeks to construct a competitive robot, that can operate autonomously as well as when guided by wireless controls, to accomplish the game's tasks.[4]
In 2010, the 19th year of competition, 1,808 high school teams with roughly 45,000 students from Australia, Brazil, Canada, Turkey, the Netherlands, Israel, the United States, the United Kingdom, and Mexico were involved.[5] In 2011, 2,075 teams participated in competitions in the United States, Canada, and Israel.[2]
FIRST was founded in 1989 by inventor and entrepreneur Dean Kamen, with inspiration and assistance from physicist and MIT professor emeritus Woodie Flowers. Kamen has stated that FIRST is the invention he feels most proud of for creating, and predicts that participants who have taken part in its contests so far will be responsible for some significant technological advances in years to come.[6] The first FRC season was in 1992 and had only one event at a high school gymnasium in New Hampshire.[7] That first competition was relatively small-scale, similar in size to today's FIRST Tech Challenge and Vex Robotics Competition games. In its first year, robots relied on a wired connection to receive data from drivers; in the following year, it quickly transitioned to a wireless system.[8][9] From 1996 to 1998, the FIRST Championship was covered by ESPN.[10] Live coverage is currently provided by NASA TV, which can be viewed on the internet, TVRO, DirecTV, and Dish Network; the sophistication of the broadcast of each event is dependent on the organizers of that event, and range from professionally called with color commentary, such as the 2011 Michigan State Championship, to single-camera setups with no commentary other than the on-field play caller, which is typical of most events.
Since 2005, the games have been played with two opposing "alliances" each composed of three teams. Prior to 2005, a variety of other formats were used, including 2-team alliances, 4-team alliances and 3 teams competing independently.
The FIRST Championship (and by extension, the FRC championships), which has been held at a large scale venue since 1995, has been held at Epcot Center in Orlando, Reliant Park in Houston, the Georgia Dome in Atlanta, and the Edward Jones Dome in St. Louis.
Over the years, FIRST has attracted the sponsorship of major companies such as Boeing, Microsoft, National Instruments, Google, Texas Instruments, and United Technologies. The main controller on FRC robots is a National Instruments CompactRIO.[11] The 2012 game featured the use of Microsoft Kinect systems.[12]
Every year, a different game is played. However, many game rules do not change from year to year and the competitive structure has not changed significantly since the competition started. Teams are randomly assigned alliances of 3 teams, which are paired into qualification matches, where they earn 'qualifying points', the calculation of which changes each year. The game changes every year, but for the most part, they involve some autonomous (computer controlled) robot operation for 1015 seconds at the beginning of a match, followed by a much longer period (usually 2 minutes) of remote control.[13][14][15][16][17] Teams use scoring objects on the field to get points, and they sometimes get bonus points for achieving tasks such as ending with the robot on a platform, behind a certain line, or even hanging from a field structure.
Members of the FRC community are generally very friendly and cooperative with one another; at competitions, it is commonplace for teams to aid each other in repairs and improvements, even if the involved teams are slated to compete against each other. Most regional competitions have systems set up to facilitate the lending of parts and tools between teams. A notable example of parts-sharing is the "One-Day Wonder". In 2004, at the Championship Event in Atlanta, GA, the Tottenville High School Pyrobots found themselves without a robot due to a shipping error. The Robotic Eagles (358), Adams Robotics (1340), the Killer Bees (33), and the Goodrich Martians (494), some of who would be on opposing teams in future matches, donated parts, tools, and assistance constructing a competition ready robot within a single day - a feat which the competition allows six weeks for.[18][19][20][21]
Some teams also choose to collaborate during the build season. The degree of collaboration can range from sharing part designs to each team building exactly the same robot. FIRST has encouraged this practice, as shown in an official Q&A response from 2006:
Q: Is collaboration between 2 teams acceptable and encouraged by FIRST?
A: Absolutely. Teams are encouraged to share their knowledge, experience, and innovations with each other on and off the play field, as well as before, during and after the competition season. Without inter-team collaborations, many of the central elements of the FIRST philosophy - such as distribution of technical innovations, team workshops, shared designs, software code-sharing, teams mentoring teams, team-run off-season events, etc. - would all be impossible. The whole concept of "coopertition" is based on the idea of teams helping each other to compete.
The competition is a yearly event. The most intense participation occurs between the months of January and April, but "mini-competitions" are hosted by many teams in school gymnasiums throughout the year.[22] In early January, FIRST announces the details of a game to all participating teams. The game changes very much from year to year, with only a few rules such as the approximate size of the robot staying the same.[23]
During the build season, the six weeks following the kickoff, the teams begin to design a robot that is able to play the game, essentially from scratch. Team members typically spend the time designing strategies to play the game, drawing up ideas for robot parts, working with size and weight constraints, and finally, building and assembling their robot. Other challenges include gaining driver experience, building the electronics for the robot, and software development. After the build season has ended (usually the 3rd full week of February), teams must stop work on their robot before transporting it to the location of their first competition. [24] This date is typically referred to as "ship date," as past competitions required the shipment of the robot to the first competition on this date. However, competitions are increasingly "bag and tag," where robots are sealed in a bag prior to competitions, and transported at the team's expense. Although the term "bag and tag date" has been proposed, "ship date" is still far more commonly used. Competitions for FIRST consist of dozens of regional competitions and one championship event.
The Championship event is held every year in April, often in a large stadium or convention center in the Southeastern United States. The championship event consists of four divisions of 8595 teams competing on one of four fields: Galileo, Newton, Archimedes and Curie.[25] The teams compete for the division championship title in the same way they would compete in a regional. The division champions then bring their robots over to the Einstein field to compete in an elimination tournament to determine the champion.
The FIRST Robotics Competition is most prominent in the United States, where FIRST was originally founded, though teams from Canada have been disproportionately successful. FIRST urges teams and sponsors to expand the reach of FRC by helping start new FIRST teams. International expansion has been most successful in North America where there are over 1,200 teams in the United States and over 70 in Canada (mostly in Ontario and Quebec). Although teams from British Columbia have competed in the past, the only Canadian teams west of Ontario remaining are 4334 (an Einstein participant and Archimedes champion in 2012) and 1482 from Calgary, Alberta. FIRST Robotics has recently reached Mexico, where 12 teams have been founded since 2007. There is a significant contingent of around 50 FRC teams in Brazil and Israel. Both countries have hosted regional competitions in the past; Israel continues to host one yearly, guaranteeing at least three Israeli or European teams in the Championship.[26]
The FIRST Robotics Competition involves teams of mentors (corporate employees, teachers, or college students) and high school students who collaborate to design and build a robot in six weeks. This robot is designed to play a game, which is designed by FIRST and changes from year to year. This game is announced at a nationally simulcast kickoff event in January.[27] Regional competitions take place around the United States as well as in Canada and Israel, but FIRST has a multinational following that further includes the United Kingdom, Brazil, Australia, and Germany.[28]
Teams are expected to solicit individuals, sponsors, and businesses for support in the form of donations of time, money, or skills. The average team has approximately 25 students, but participation has ranged from 4 to 100.[29]
A FRC team typically has approximately 25 students,[29] but can range anywhere from 10 to 100. Teams are also sponsored and aided by adult mentors, who can be professional engineers, teachers, parents, college students, or any other interested adults. The degree to which the mentors are involved varies significantly from team to team. FIRST's recommended season stretches the full year, starting with recruiting and fundraising in September to December, robot construction and competition in January through April, then returning to fundraising and community involvement events until August.[30]
Most FIRST regional events take place between Thursday and Saturday in a week in March.[31] Thursday is typically a practice day where matches take place but do not count towards final standings. All day Friday and on Saturday morning, teams participate in qualifying matches.[32] On Saturday afternoon, after the qualification matches have ended, the top eight ranked teams will pick partners from any team ranked below them, and the resulting alliances will compete to be regional winner. The top teams pick their partners starting with the top-ranked team, proceeding to the 8th ranked team, then back from the 8th team to the 1st team again. The alliances picked this way then proceed into elimination rounds, set up into quarterfinals, semifinals, and finals. Each quarterfinal, semifinal, and final is determined by a best-of-3 matchup between the two alliances.[33] All three teams in an alliance that wins a regional earns a reserved spot at the championship event, which in 2011 and 2012, was held in St. Louis, Missouri.[34][35][36] Hall of Fame teams automatically qualify for the Championships, regardless of regional statistics.
All events have the same tournament structure: qualification matches followed by elimination matches.
District events are smaller than regional events and are held at much smaller venues, usually high school gymnasiums. A typical district event has about 40 teams. Winning a district event does not necessarily qualify a team for the FIRST Championship. For 2009 through 2011, the US state of Michigan was the only region to hold district events. Beginning with 2012, a second district region was created, the Mid-Atlantic Robotics Region (MAR), comprising New Jersey, Delaware and eastern Pennsylvania.[37]
Regionals typically involve between 3065 teams, and regional events with more than 70 teams are usually split into two: for example, the Toronto (in 2011 only) and Seattle events have two regionals that share the same arena at the same time.[31] Teams that win a regional event automatically qualify for the FIRST Championship.[38]
Due to FIRST's mission to exposing students and the community at large to science and engineering, the three most prestigious awards they give out are awarded not to teams that have demonstrated prowess in the game, but to those teams and individuals who have done the most to realize FIRST's mission.
The Chairman's Award is the most prestigious award a team can win at a regional or at the championship, more so than even winning the competition itself.[39] Demonstrating the prestige of the award, a team that wins a Regional Chairman's Award receives a reserved spot at the championship event so that they may compete for the Championship Chairman's award, regardless of their on-field performance. The Chairman's award was created to recognize teams that demonstrate the greatest commitment to spreading passion about science and technology into their communities and schools. Submission involves writing an essay of no more than 10,000 characters documenting the team's efforts at spreading the message of FIRST, a five-minute presentation to a panel of judgesfollowed by a five-minute Q&A session for the judgesand a three-minute video.
The Woodie Flowers Award is awarded to a mentor within a team that the team believes has made a large contribution to them and deserves to be recognized. Criteria is based on how well the chosen mentor inspires the students towards better communication and engineering. Any regional WFA winner is eligible to be considered for the Championship WFA, though past regional WFA winners may not again win a regional WFA, to allow other mentors the chance to be recognized. The WFA trophy itself is a head-sized Mbius strip with bearings inside it.
The Dean's List Award is given to individual student team members. Much like the Woodie Flowers Award for mentors, the Dean's List Award is to recognize students for their technical knowledge, leadership skills, and their ability to inspire their team toward the mission of FIRST. Teams are allowed to recognize 2 students on their team as Dean's List Semi-Finalist, as semi-finalist they eligible to win the Dean's List Finalist Award at a regional. 2 Dean's List Finalist Awards are given out at every regional. At the annual FIRST Championship 10 finalists are then awarded the Dean's List Award during a special award ceremony.
At the start of the FIRST season all of the teams receive the Kit Of Parts and the game description. The kit includes the control system and a collection of parts to build a basic robot, as well as many parts donated by participating sponsors. Besides the control system, the kit includes items such as motors, structural components, speed controllers, pneumatic actuators, wheels, and gearboxes, as well as programming and design software. The control system and the provided software has a history of terrible documentation. As soon as the teams receive their kit of parts, the 6-week build season begins.[40] Teams are allowed to purchase additional off-the-shelf items where each individual item value may not exceed $400 USD with a total maximum project budget of $3,500.[41]
Another aspect of the competition is the dozens of companies that supply parts, tools, financial support and mentoring to the FIRST teams. These companies donate thousands of dollars worth of top-of-the-line parts to be included in the Kit of Parts. Items include bearings, tubing, slides, switches, tools, controls, software, and robot frames. Many of these sponsors also participate at the Championships by hosting booths in the Supplier Showcase.[42]
The PBS documentary "Gearing Up" followed four teams through the 2008 season.[43]
In the television series Dean of Invention, Dean Kamen made appeals promoting FIRST prior to commercial breaks.[44]
During the 2010 FIRST Robotics Competition season, FIRST team 3132, Thunder Down Under, was followed by a Macquarie University student film crew to document the first year of FRC in Australia. The crew produced a documentary film called I, Wombot.[45][46] The film premiered during the 2011 Dungog Film Festival.[47][48]
The CNN documentary "Don't Fail Me: Education in America", which aired on 15 May 2011, followed three FRC teams during the 2011 season.[49]
On August 14th, 2011, ABC aired a special on FIRST called "i.am FIRST: Science is Rock and Roll"[50] that featured many famous musical artists such as The Black Eyed Peas and Willow Smith. will.i.am himself was the executive producer of the special. The program placed a special focus on the FIRST Robotics competition, even though it included segments on the FIRST Tech Challenge, FIRST Lego League, and Junior FIRST Lego League.
The movie 'Drive Like A Girl' followed the Bronx High School of Science's all girls robot team the Fe Maidens
United States
Canada
Israel
Brazil
Mexico
United Kingdom
Chile
Germany
Turkey
Netherlands
Australia
Bosnia and Herzegovina
1 History
2 Competition

2.1 Community
2.2 Typical schedule


2.1 Community
2.2 Typical schedule
3 Teams

3.1 Concept and teams
3.2 Team organization


3.1 Concept and teams
3.2 Team organization
4 Competitive events

4.1 Tiers of competitive events

4.1.1 District events
4.1.2 Regional events
4.1.3 Championship


4.2 Awards

4.2.1 Chairman's Award
4.2.2 Woodie Flowers Award
4.2.3 Dean's List Award




4.1 Tiers of competitive events

4.1.1 District events
4.1.2 Regional events
4.1.3 Championship


4.1.1 District events
4.1.2 Regional events
4.1.3 Championship
4.2 Awards

4.2.1 Chairman's Award
4.2.2 Woodie Flowers Award
4.2.3 Dean's List Award


4.2.1 Chairman's Award
4.2.2 Woodie Flowers Award
4.2.3 Dean's List Award
5 Kit of Parts
6 Sponsors and media exposure

6.1 Movies and television


6.1 Movies and television
7 Games
8 Gallery
9 See also
10 Notes
11 References
12 External links

12.1 Regional
12.2 Community resources
12.3 Related or associated organizations and sponsors
12.4 Tutorial websites
12.5 Articles of interest


12.1 Regional
12.2 Community resources
12.3 Related or associated organizations and sponsors
12.4 Tutorial websites
12.5 Articles of interest
2.1 Community
2.2 Typical schedule
3.1 Concept and teams
3.2 Team organization
4.1 Tiers of competitive events

4.1.1 District events
4.1.2 Regional events
4.1.3 Championship


4.1.1 District events
4.1.2 Regional events
4.1.3 Championship
4.2 Awards

4.2.1 Chairman's Award
4.2.2 Woodie Flowers Award
4.2.3 Dean's List Award


4.2.1 Chairman's Award
4.2.2 Woodie Flowers Award
4.2.3 Dean's List Award
4.1.1 District events
4.1.2 Regional events
4.1.3 Championship
4.2.1 Chairman's Award
4.2.2 Woodie Flowers Award
4.2.3 Dean's List Award
6.1 Movies and television
12.1 Regional
12.2 Community resources
12.3 Related or associated organizations and sponsors
12.4 Tutorial websites
12.5 Articles of interest
2012: Rebound Rumble
2011: Logomotion
2010: Breakaway
2009: Lunacy
2008: FIRST Overdrive
2007: Rack 'n Roll
2006: Aim High
2005: Triple Play
2004: FIRST Frenzy: Raising the Bar
2003: Stack Attack
2002: Zone Zeal
2001: Diabolical Dynamics
2000: Co-Opertition FIRST
1999: Double Trouble
1998: Ladder Logic
1997: Toroid Terror
1996: Hexagon Havoc
1995: Ramp 'n Roll
1994: Tower Power
1993: Rug Rage
1992: Maize Craze
 FIRST at Wikipedia books
Walton, Marsha (2007-04-14). "Robotic trio wins 'Super Bowl of Smarts'". CNN. http://www.cnn.com/2007/TECH/04/14/robot.compete/index.html
Ante, Spencer E. (2007). "Building Robots Builds Scientists". Businessweek. http://www.businessweek.com/technology/content/apr2007/tc20070413_582820.htm?chan=technology_technology+index+page_top+stories
FIRST Robotics
FIRST Robotics Canada

Robotique FIRST Qubec


Robotique FIRST Qubec
FIRST Israel
FIRST do Brasil
Robotique FIRST Qubec
FIRST Robotics Wiki
FIRST Forge
ChiefDelphi Forums
FIRST Robotics Chat Room
Non-Engineering Mentor Organization
NASA FRC site
FRC Mastery - programming assistance
Michigan robotics champs off to world finals in St. Louis - Detroit Free Press
FIRST Robotics Competitions 2011 Regional Season Is Worth a Look - PCWorld
.
#(`*Wipeout (series)*`)#.
Wipeout (trademarked as WipEout)[1] is a series of futuristic anti-gravity racing games developed by SCE Studio Liverpool. The series is well known for its fast-paced gameplay and high-quality 3D visual design, along with running on the full resolution of the console the game belongs to; as well as its association with electronica and electronic dance music, as well as its continuous collaboration with certain artists (The Chemical Brothers, Optical, FSOL, Cold Storage, Kraftwerk, Orbital, Aphex Twin and others). The series identified itself with a strong graphical design, provided by The Designers Republic.
Wipeout is a racing series that features vehicles that hover over futuristic racetracks. Power-ups (or "pick-ups") are available and come in offensive ("weapon") and defensive ("item") varieties.
The Wipeout franchise began with the release of the original Wipeout in 1995. Three more Wipeout games were released by the end of the decade, all developed by Liverpool-based developer Psygnosis. In 1999, Sony Computer Entertainment Europe acquired Psygnosis and its intellectual property, including Wipeout. As part of the acquisition, the name "Psygnosis" was changed to "SCE Studio Liverpool". After the acquisition, the titles to follow experienced drastic changes in physics and gameplay - many core fans of the series believe the series hit near perfection with Wipeout XL / 2097. Five more Wipeout games were developed, one on the PlayStation 2, two for the PlayStation Portable, a downloadable title on the PlayStation 3 - Wipeout HD (which is also available on Blu-ray disc packaged with the Fury expansion), and a launch title for the PlayStation Vita - Wipeout 2048. Sony Liverpool had been working on a Wipeout game for the upcoming PlayStation 4 console when the studio was closed on 22 August 2012.[2]
A Wipeout "arcade" game was also featured in the 1995 movie "Hackers". The actual game footage was pre-rendered CGI and the actors merely pretended they were playing the game.
1 Gameplay
2 Games
3 History
4 References
5 External links
Wipeout Game - the official Wipeout series website.
Wipeout Zone
WipEout Future - multilingual website about the WipEout universe
Wipeout series at MobyGames
Cold Storage - Tim Wright, the creator of Wipeout music & sound effects.
.
#(`*The Simpsons*`)#.
The Simpsons is an American animated sitcom created by Matt Groening for the Fox Broadcasting Company. The series is a satirical parody of a middle class American lifestyle epitomized by its family of the same name, which consists of Homer, Marge, Bart, Lisa and Maggie. The show is set in the fictional town of Springfield and parodies American culture, society, television and many aspects of the human condition.
The family was conceived by Groening shortly before a solicitation for a series of animated shorts with the producer James L. Brooks. Groening created a dysfunctional family and named the characters after members of his own family, substituting Bart for his own name. The shorts became a part of The Tracey Ullman Show on April 19, 1987. After a three-season run, the sketch was developed into a half-hour prime time show and was an early hit for Fox, becoming the network's first series to land in the Top 30 ratings in a season (19891990).
Since its debut on December 17, 1989, the show has broadcast 515 episodes and the twenty-fourth season started airing on September 30, 2012. The Simpsons is the longest-running American sitcom, the longest-running American animated program, and in 2009 it surpassed Gunsmoke as the longest-running American primetime, scripted television series. The Simpsons Movie, a feature-length film, was released in theaters worldwide on July 26 and July 27, 2007, and grossed over $527 million.
The Simpsons has won dozens of awards since it debuted as a series, including 27 Primetime Emmy Awards, 30 Annie Awards and a Peabody Award. Time magazine's December 31, 1999 issue named it the 20th century's best television series, and on January 14, 2000 the Simpson family was awarded a star on the Hollywood Walk of Fame. Homer's exclamatory catchphrase "D'oh!" has been adopted into the English language, while The Simpsons has influenced many adult-oriented animated sitcoms.
When producer James L. Brooks was working on the television variety show The Tracey Ullman Show, he decided to include small animated sketches before and after the commercial breaks. Having seen one of cartoonist Matt Groening's Life in Hell comic strips, Brooks asked Groening to pitch an idea for a series of animated shorts, which Groening initially intended to present as his Life in Hell series.[1] Groening later realized that animating Life in Hell would require the rescinding of publication rights for his life's work. He therefore chose another approach while waiting in the lobby of Brooks's office for the pitch meeting, hurriedly formulating his version of a dysfunctional family that became the Simpsons.[1][2] He named the characters after his own family members, substituting "Bart" for his own name, adapting an anagram of the word "brat".[1]
The Simpson family first appeared as shorts in The Tracey Ullman Show on April 19, 1987.[3] Groening submitted only basic sketches to the animators and assumed that the figures would be cleaned up in production. However, the animators merely re-traced his drawings, which led to the crude appearance of the characters in the initial shorts.[1] The animation was produced domestically at Klasky Csupo,[4][5] with Wes Archer, David Silverman, and Bill Kopp being animators for the first season.[6] Colorist Gyorgyi Peluce was the person who decided to make the characters yellow.[6]
In 1989, a team of production companies adapted The Simpsons into a half-hour series for the Fox Broadcasting Company. The team included the Klasky Csupo animation house. Brooks negotiated a provision in the contract with the Fox network that prevented Fox from interfering with the show's content.[7] Groening said his goal in creating the show was to offer the audience an alternative to what he called "the mainstream trash" that they were watching.[8] The half-hour series premiered on December 17, 1989 with "Simpsons Roasting on an Open Fire", a Christmas special.[9] "Some Enchanted Evening" was the first full-length episode produced, but it did not broadcast until May 1990, as the last episode of the first season, because of animation problems.[10] In 1992, Tracey Ullman filed a lawsuit against Fox, claiming that her show was the source of the series' success. The suit said she should receive a share of the profits of The Simpsons[11]a claim rejected by the courts.[12]
List of showrunners throughout the series' run:
Matt Groening and James L. Brooks have served as executive producers during the show's entire history, and also function as creative consultants. Sam Simon, described by former Simpsons director Brad Bird as "the unsung hero" of the show,[13] served as creative supervisor for the first four seasons. He was constantly at odds with Groening, Brooks and Gracie Films and left in 1993.[14] Before leaving, he negotiated a deal that sees him receive a share of the profits every year, and an executive producer credit despite not having worked on the show since 1993.[14][15] A more involved position on the show is the showrunner, who acts as head writer and manages the show's production for an entire season.[6]
The first team of writers, assembled by Sam Simon, consisted of John Swartzwelder, Jon Vitti, George Meyer, Jeff Martin, Al Jean, Mike Reiss, Jay Kogen and Wallace Wolodarsky.[16] Newer Simpsons' writing teams typically consist of sixteen writers who propose episode ideas at the beginning of each December.[17] The main writer of each episode writes the first draft. Group rewriting sessions develop final scripts by adding or removing jokes, inserting scenes, and calling for re-readings of lines by the show's vocal performers.[18] Until 2004,[19] George Meyer, who had developed the show since the first season, was active in these sessions. According to long-time writer Jon Vitti, Meyer usually invented the best lines in a given episode, even though other writers may receive script credits.[18] Each episode takes six months to produce so the show rarely comments on current events.[20]
Credited with sixty episodes, John Swartzwelder is the most prolific writer on The Simpsons.[21] One of the best-known former writers is Conan O'Brien, who contributed to several episodes in the early 1990s before replacing David Letterman as host of the talk show Late Night.[22] English comedian Ricky Gervais wrote the episode "Homer Simpson, This Is Your Wife", becoming the first celebrity to both write and guest star in an episode.[23] Seth Rogen and Evan Goldberg, writers of the film Superbad, wrote the episode "Homer the Whopper", with Rogen voicing a character in it.[24]
At the end of 2007 the writers of The Simpsons went on strike together with the other members of the Writers Guild of America, East. The show's writers had joined the guild in 1998.[25]
The Simpsons has six main cast members: Dan Castellaneta, Julie Kavner, Nancy Cartwright, Yeardley Smith, Hank Azaria and Harry Shearer. Castellaneta performs Homer Simpson, Grampa Simpson, Krusty the Clown, Barney Gumble and other adult, male characters.[26] Julie Kavner speaks the voices of Marge Simpson and Patty and Selma, as well as several minor characters.[26] Castellaneta and Kavner had been a part of The Tracey Ullman Show cast and were given the parts so that new actors would not be needed.[27] Cartwright performs the voices of Bart Simpson, Nelson Muntz, Ralph Wiggum and other children.[26] Smith, the voice of Lisa Simpson, is the only cast member who regularly voices only one character, although she occasionally plays other episodic characters.[26] The producers decided to hold casting for the roles of Bart and Lisa. Smith had initially been asked to audition for the role of Bart, but casting director Bonita Pietila believed her voice was too high,[28] so she was given the role of Lisa instead.[29] Cartwright was originally brought in to voice Lisa, but upon arriving at the audition, she found that Lisa was simply described as the "middle child" and at the time did not have much personality. Cartwright became more interested in the role of Bart, who was described as "devious, underachieving, school-hating, irreverent, [and] clever".[30] Groening let her try out for the part instead, and upon hearing her read, gave her the job on the spot.[31] Cartwright is the only one of the six main Simpsons cast members who had been professionally trained in voice acting prior to working on the show.[32] Azaria and Shearer do not voice members of the title family, but play a majority of the male townspeople. Azaria, who has been a part of the Simpsons regular voice cast since the second season,[33] voices recurring characters such as Moe Szyslak, Chief Wiggum, Apu Nahasapeemapetilon and Professor Frink. Shearer provides voices for Mr. Burns, Waylon Smithers, Principal Skinner, Ned Flanders, Reverend Lovejoy and Dr. Hibbert.[26] With the exception of Shearer, every main cast member has won a Primetime Emmy Award for Outstanding Voice-Over Performance.[34] However, Shearer was nominated for the award in 2009.[35]
With one exception, episode credits list only the voice actors, and not the characters they voice. Both Fox and the production crew wanted to keep their identities secret during the early seasons and, therefore, closed most of the recording sessions while refusing to publish photos of the recording artists.[36] However, the network eventually revealed which roles each actor performed in the episode "Old Money", because the producers said the voice actors should receive credit for their work.[37] In 2003, the cast appeared in an episode of Inside the Actors Studio, doing live performances of their characters' voices.
Until 1998, the six main actors were paid $30,000 per episode. In 1998 they were involved in a pay dispute with Fox. The company threatened to replace them with new actors, even going as far as preparing for casting of new voices, but series creator Groening supported the actors in their action.[38] The issue was soon resolved and, from 1998 to 2004, they were paid $125,000 per episode. The show's revenue continued to rise through syndication and DVD sales, and in April 2004 the main cast stopped appearing for script readings, demanding they be paid $360,000 per episode.[39][40] The strike was resolved a month later[41] and their salaries were increased to something between $250,000[42] and $360,000 per episode.[43] In 2008, production for the twentieth season was put on hold due to new contract negotiations with the voice actors, who wanted a "healthy bump" in salary to an amount close to $500,000 per episode.[43] The negotiations were soon completed, and the actors' salary was raised to $400,000 per episode.[44] Three years later, with Fox threatening to cancel the series unless production costs were cut, the cast members accepted a 30 percent pay cut, down to just over $300,000 per episode.[45]
In addition to the main cast, Pamela Hayden, Tress MacNeille, Marcia Wallace, Maggie Roswell, and Russi Taylor voice supporting characters.[26] From 1999 to 2002, Roswell's characters were voiced by Marcia Mitzman Gaven. Karl Wiedergott has also appeared in minor roles, but does not voice any recurring characters.[46] Repeat "special guest" cast members include Albert Brooks, Phil Hartman, Jon Lovitz, Joe Mantegna, and Kelsey Grammer.[47] Following Hartman's death in 1998, the characters he voiced were retired.[2]
Episodes will quite often feature guest voices from a wide range of professions, including actors, athletes, authors, bands, musicians and scientists. In the earlier seasons, most of the guest stars voiced characters, but eventually more started appearing as themselves. Tony Bennett was the first guest star to appear as himself, appearing briefly in the season two episode "Dancin' Homer".[48] The Simpsons holds the world record for "Most Guest Stars Featured in a Television Series".[49]
The show has been dubbed into several other languages, including Japanese, German, Spanish, and Portuguese. It is also one of the few programs dubbed in both standard French and Quebec French.[50] The Simpsons has been broadcast in Arabic, but due to Islamic customs, numerous aspects of the show have been changed. For example, Homer drinks soda instead of beer and eats Egyptian beef sausages instead of hot dogs. Because of such changes, the Arabized version of the series met with a negative reaction from the lifelong Simpsons fans in the area.[51]
Several different U.S. and international studios animate The Simpsons. Throughout the run of the animated shorts on The Tracey Ullman Show, the animation was produced domestically at Klasky Csupo.[4] With the debut of the series, because of an increased workload, Fox subcontracted production to several international studios, located in South Korea.[4] These are AKOM,[52] Anivision,[53] Rough Draft Studios,[54] USAnimation,[55] and Toonzone Entertainment.[56] A subcontractor connection to the North Korean SEK studio has been suspected but not confirmed.[57] Artists at the U.S. animation studio, Film Roman, draw storyboards, design new characters, backgrounds, props and draw character and background layouts, which in turn become animatics to be screened for the writers at Gracie Films for any changes to be made before the work is shipped overseas. The overseas studios then draw the inbetweens, ink and paint, and render the animation to tape before it is shipped back to the United States to be delivered to Fox three to four months later.[58]
For the first three seasons, Klasky Csupo animated The Simpsons in the United States. In 1992, the show's production company, Gracie Films, switched domestic production to Film Roman,[59] who continue to animate the show as of 2012. In Season 14, production switched from traditional cel animation to digital ink and paint.[60] The first episode to experiment with digital coloring was "Radioactive Man" in 1995. Animators used digital ink and paint during production of the Season 12 episode "Tennis the Menace", but Gracie Films delayed the regular use of digital ink and paint until two seasons later. The already completed "Tennis the Menace" was broadcast as made.[61]
The series began high-definition production in Season 20; the first episode, "Take My Life, Please", aired February 15, 2009. The move to HDTV included a new opening sequence.[62] Matt Groening called it a complicated change because it affected the timing and composition of animation.[63]
The Simpsons are a typical family who live in a fictional "Middle American" town of Springfield.[64] Homer, the father, works as a safety inspector at the Springfield Nuclear Power Plant, a position at odds with his careless, buffoonish personality. He is married to Marge Simpson, a stereotypical American housewife and mother. They have three children: Bart, a ten-year-old troublemaker; Lisa, a precocious eight-year-old activist; and Maggie, the baby of the family who rarely speaks, but communicates by sucking on a pacifier. The family owns a dog, Santa's Little Helper, and a cat, Snowball V, renamed Snowball II in "I, (Annoyed Grunt)-Bot".[65] Both pets have had starring roles in several episodes. Despite the passing of yearly milestones such as holidays or birthdays, the Simpsons do not physically age and still appear just as they did at the end of the 1980s. Although the family is dysfunctional, many episodes examine their relationships and bonds with each other and they are often shown to care about one another.[66]
The show includes an array of quirky characters: co-workers, teachers, family friends, extended relatives, townspeople and local celebrities. The creators originally intended many of these characters as one-time jokesters or for fulfilling needed functions in the town. A number of them have gained expanded roles and subsequently starred in their own episodes. According to Matt Groening, the show adopted the concept of a large supporting cast from the comedy show SCTV.[67]
The Simpsons takes place in the fictional American town of Springfield in an unknown and impossible-to-determine U.S. state. The show is intentionally evasive in regard to Springfield's location.[68] Springfield's geography, and that of its surroundings, contains coastlines, deserts, vast farmland, tall mountains, or whatever the story or joke requires.[69] Groening has said that Springfield has much in common with Portland, Oregon, the city where he grew up.[70] The name "Springfield" is a common one in America and appears in 22 states.[71] Groening has said that he named it after Springfield, Oregon, and the fictitious Springfield which was the setting of the series Father Knows Best. He "figured out that Springfield was one of the most common names for a city in the U.S. In anticipation of the success of the show, I thought, 'This will be cool; everyone will think it's their Springfield.' And they do."[72]
The Simpsons uses the standard setup of a situational comedy, or sitcom, as its premise. The series centers on a family and their life in a typical American town,[64] serving as a satirical parody of a working and middle class American lifestyle.[73] However, because of its animated nature, The Simpsons' scope is larger than that of a regular sitcom. The town of Springfield acts as a complete universe in which characters can explore the issues faced by modern society. By having Homer work in a nuclear power plant, the show can comment on the state of the environment.[74] Through Bart and Lisa's days at Springfield Elementary School, the show's writers illustrate pressing or controversial issues in the field of education. The town features a vast array of media channelsfrom kids' television programming to local news, which enables the producers to make jokes about themselves and the entertainment industry.[75]
Some commentators say the show is political in nature and susceptible to a left-wing bias.[76] Al Jean admitted in an interview that "We [the show] are of liberal bent."[77] The writers often evince an appreciation for liberal ideals, but the show makes jokes across the political spectrum.[78] The show portrays government and large corporations as callous entities that take advantage of the common worker.[77] Thus, the writers often portray authority figures in an unflattering or negative light. In The Simpsons, politicians are corrupt, ministers such as Reverend Lovejoy are indifferent to churchgoers, and the local police force is incompetent.[79] Religion also figures as a recurring theme. In times of crisis, the family often turns to God, and the show has dealt with most of the major religions.[80]
The Simpsons' opening sequence is one of the show's most memorable hallmarks. Most episodes open with the camera zooming through the show's title towards the town of Springfield. The camera then follows the members of the family on their way home. Upon entering their house, the Simpsons settle down on their couch to watch television. The opening was created by David Silverman, the first task he did when production began on the show.[81] The series' distinctive theme song was composed by musician Danny Elfman in 1989, after Groening approached him requesting a retro style piece. This piece has been noted by Elfman as the most popular of his career.[82]
One of the most distinctive aspects of the opening is that three of the segments change from episode to episode: Bart writes different things on the school chalkboard,[81] Lisa plays different solos on her saxophone, and different gags accompany the family as they enter their living room to sit on the couch.[83] On February 15, 2009, a new opening credit sequence was introduced to accompany the switch to HDTV. The sequence had all of the features of the original opening, but added numerous details and characters.[84]
The special Halloween episode has become an annual tradition. "Treehouse of Horror" first broadcast in 1990 as part of season two and established the pattern of three separate, self-contained stories in each Halloween episode.[85] These pieces usually involve the family in some horror, science fiction, or supernatural setting and often parody or pay homage to a famous piece of work in those genres.[86] They always take place outside the normal continuity of the show. Although the Treehouse series is meant to be seen on Halloween, in recent years, new installments have premiered after Halloween due to Fox's current contract with Major League Baseball's World Series.[87]
The show's humor turns on cultural references that cover a wide spectrum of society so that viewers from all generations can enjoy the show.[88] Such references, for example, come from movies, television, music, literature, science, and history.[88] The animators also regularly add jokes or sight gags into the show's background via humorous or incongruous bits of text in signs, newspapers, and elsewhere.[89] The audience may often not notice the visual jokes in a single viewing. Some are so fleeting that they become apparent only by pausing a video recording of the show.[89] Kristin Thompson argues that The Simpsons uses a ". . . flurry of cultural references, intentionally inconsistent characterization, and considerable self-reflexivity about television conventions and the status of the programme as a television show."[90]
One of Bart's early hallmarks was his prank calls to Moe's Tavern owner Moe Szyslak in which Bart calls Moe and asks for a gag name. Moe tries to find that person in the bar, but soon realizes it is a prank call and angrily threatens Bart. These calls were based on a series of prank calls known as the Tube Bar recordings. Moe was based partly on Tube Bar owner Louis "Red" Deutsch, whose often profane responses inspired Moe's violent side.[91] As the series progressed, it became more difficult for the writers to come up with a fake name and to write Moe's angry response, and the pranks were dropped as a regular joke during the fourth season.[92][93] The Simpsons also often includes self-referential humor.[94] The most common form is jokes about Fox Broadcasting.[95] For example, the episode "She Used to Be My Girl" included a scene in which a Fox News Channel van drove down the street while displaying a large "Bush Cheney 2004" banner and playing Queen's "We Are the Champions", in reference to the 2004 presidential election.[96][97]
The show uses catchphrases, and most of the primary and secondary characters have at least one each.[98] Notable expressions include Homer's annoyed grunt "D'oh!", Mr. Burns' "Excellent..." and Nelson Muntz's "Ha-ha!". Some of Bart's catchphrases, such as "Ay, caramba!", "Don't have a cow, man!" and "Eat my shorts!" appeared on t-shirts in the show's early days.[99] However, Bart rarely used the latter two phrases until after they became popular through the merchandising. The use of many of these catchphrases has declined in recent seasons. The episode "Bart Gets Famous" mocks catchphrase-based humor, as Bart achieves fame on the Krusty the Clown Show solely for saying "I didn't do it."[100]
A number of neologisms that originated on The Simpsons have entered popular vernacular.[101][102] Mark Liberman, director of the Linguistic Data Consortium, remarked, "The Simpsons has apparently taken over from Shakespeare and the Bible as our culture's greatest source of idioms, catchphrases and sundry other textual allusions."[102] The most famous catchphrase is Homer's annoyed grunt: "D'oh!" So ubiquitous is the expression that it is now listed in the Oxford English Dictionary, but without the apostrophe.[103] Dan Castellaneta says he borrowed the phrase from James Finlayson, an actor in early Laurel and Hardy comedies, who pronounced it in a more elongated and whining tone. The staff of The Simpsons told Castellaneta to shorten the noise, and it went on to become the well-known exclamation in the television series.[104]
Groundskeeper Willie's description of the French as "cheese-eating surrender monkeys" was used by National Review columnist Jonah Goldberg in 2003, after France's opposition to the proposed invasion of Iraq. The phrase quickly spread to other journalists.[102][105] "Cromulent" and "Embiggen", words used in "Lisa the Iconoclast", have since appeared in the Dictionary.com's 21st Century Lexicon,[106] and scientific journals respectively.[102][107] "Kwyjibo", a fake Scrabble word invented by Bart in "Bart the Genius", was used as one of the aliases of the creator of the Melissa worm.[108] "I, for one, welcome our new insect overlords", was used by Kent Brockman in "Deep Space Homer" and has become a common variety of phrase.[109] Variants of Brockman's utterance are used to express mock submission.[110] It has been used in media, such as New Scientist magazine.[111] The dismissive term "Meh", believed to have been popularized by the show,[102][112][113] entered the Collins English Dictionary in 2008.[114] Other words credited as stemming from the show include "yoink" and "craptacular".[102]
The Oxford Dictionary of Modern Quotations includes several quotations from the show. As well as "cheese-eating surrender monkeys", Homer's lines, "Kids, you tried your best and you failed miserably. The lesson is never try", from "Burns' Heir" (season five, 1994) as well as "Kids are the best, Apu. You can teach them to hate the things you hate. And they practically raise themselves, what with the Internet and all", from "Eight Misbehavin'" (season 11, 1999), entered the dictionary in August 2007.[115]
The Simpsons was the first successful animated program in American prime time since Wait Till Your Father Gets Home in the 1970s.[116] During most of the 1980s, US pundits considered animated shows as appropriate only for children, and animating a show was too expensive to achieve a quality suitable for prime-time television. The Simpsons changed this perception.[4] The use of Korean animation studios for tweening, coloring, and filming made the episodes cheaper. The success of The Simpsons and the lower production cost prompted US television networks to take chances on other animated series.[4] This development led US producers to a 1990s boom in new, animated prime-time shows, such as South Park, Family Guy, King of the Hill, Futurama, and The Critic.[4] "The Simpsons created an audience for prime-time animation that had not been there for many, many years", said Family Guy creator Seth MacFarlane. "As far as I'm concerned, they basically re-invented the wheel. They created what is in many waysyou could classify it asa wholly new medium."[117] South Park later paid homage to The Simpsons with the episode "Simpsons Already Did It".[118] In Georgia, the animated television sitcom The Samsonadzes, launched in November 2009, has been noted for its very strong resemblance with The Simpsons, which its creator Shalva Ramishvili has acknowledged.[119][120][121]
The Simpsons has also influenced live-action shows like Malcolm in the Middle, which featured the use of sight gags and did not use a laugh track unlike most sitcoms.[122][123] Malcolm in the Middle debuted January 9, 2000 in the time slot after The Simpsons. Ricky Gervais has called The Simpsons a major influence on his British comedy The Office, which also dispenses with a laugh track.[124] Fellow British sitcom Spaced was, according to its director Edgar Wright, "an attempt to do a live-action The Simpsons."[125]
The Simpsons was the Fox network's first television series to rank among a season's top 30 highest-rated shows.[126] While later seasons would focus on Homer, Bart was the lead character in most of the first three seasons. In 1990, Bart quickly became one of the most popular characters on television in what was termed "Bartmania".[127][128][129][130] He became the most prevalent Simpsons character on memorabilia, such as T-shirts. In the early 1990s, millions of T-shirts featuring Bart were sold;[131] as many as one million were sold on some days.[132] Believing Bart to be a bad role model, several American public schools banned T-shirts featuring Bart next to captions such as "I'm Bart Simpson. Who the hell are you?" and "Underachiever ('And proud of it, man!')".[133][134][135] The Simpsons merchandise sold well and generated $2 billion in revenue during the first 14 months of sales.[133] Because of his popularity, Bart was often the most promoted member of the Simpson family in advertisements for the show, even for episodes in which he was not involved in the main plot.[136]
Due to the show's success, over the summer of 1990 the Fox Network decided to switch The Simpsons' time slot so that it would move from 8:00 p.m. ET on Sunday night to the same time on Thursday, where it would compete with The Cosby Show on NBC, the number one show at the time.[137][138] Through the summer, several news outlets published stories about the supposed "Bill vs. Bart" rivalry.[132][137] "Bart Gets an F" (season two, 1990) was the first episode to air against The Cosby Show, and it received a lower Nielsen ratings, tying for eighth behind The Cosby Show, which had an 18.5 rating. The rating is based on the number of household televisions that were tuned into the show, but Nielsen Media Research estimated that 33.6 million viewers watched the episode, making it the number one show in terms of actual viewers that week. At the time, it was the most watched episode in the history of the Fox Network,[139] and it is still the highest rated episode in the history of The Simpsons.[140] The show moved back to its Sunday slot in 1994 and has remained there ever since.[141]
The Simpsons has been praised by many critics, being described as "the most irreverent and unapologetic show on the air."[142] In a 1990 review of the show, Ken Tucker of Entertainment Weekly described it as "the American family at its most complicated, drawn as simple cartoons. It's this neat paradox that makes millions of people turn away from the three big networks on Sunday nights to concentrate on The Simpsons."[143] Tucker would also describe the show as a "pop-cultural phenomenon, a prime-time cartoon show that appeals to the entire family."[144]
On February 9, 1997, The Simpsons surpassed The Flintstones with the episode "The Itchy & Scratchy & Poochie Show" as the longest-running prime-time animated series in the United States.[145] In 2004, The Simpsons replaced The Adventures of Ozzie and Harriet (1952 to 1966) as the longest-running sitcom (animated or live action) in the United States.[146] In 2009, The Simpsons surpassed The Adventures of Ozzie and Harriet's record of 435 episodes and is now recognized by Guinness World Records as the world's longest running sitcom (in terms of episode count).[147][148] In October 2004, Scooby-Doo briefly overtook The Simpsons as the American animated show with the highest number of episodes.[149] However, network executives in April 2005 again cancelled Scooby-Doo, which finished with 371 episodes, and The Simpsons reclaimed the title with 378 episodes at the end of their seventeenth season.[150] In May 2007, The Simpsons reached their 400th episode at the end of the eighteenth season. While The Simpsons has the record for the number of episodes by an American animated show, other animated series have surpassed The Simpsons.[151] For example, the Japanese anime series Sazae-san has over 3,000 episodes to its credit.[151]
In 2009, Fox began a year-long celebration of the show titled "Best. 20 Years. Ever." to celebrate the 20th anniversary of the premiere of The Simpsons. One of the first parts of the celebration is the "Unleash Your Yellow" contest in which entrants must design a poster for the show.[152] The celebration ended on January 10, 2010 (almost twenty years after "Bart the Genius" aired on January 14, 1990) with The Simpsons 20th Anniversary Special  In 3-D! On Ice!, a documentary special by documentary filmmaker Morgan Spurlock that examines the "cultural phenomenon of The Simpsons".[153][154]
As of the twenty-first season (20092010), The Simpsons became the longest-running American primetime, scripted television series, having surpassed Gunsmoke. However, Gunsmoke's episode count of 635 episodes far surpasses The Simpsons, which would not reach that mark until its approximate 29th season, under normal programming schedules.[146][155] In October 2011, Fox announced that The Simpsons had been renewed for a 24th and 25th season, which means the show will reach at least 559 episodes.[156][157]
The Simpsons has won dozens of awards since it debuted as a series, including 27 Primetime Emmy Awards,[49] 30 Annie Awards[158] and a Peabody Award.[159] In a 1999 issue celebrating the 20th century's greatest achievements in arts and entertainment, Time magazine named The Simpsons the century's best television series.[160] In that same issue, Time included Bart Simpson in the Time 100, the publication's list of the century's 100 most influential people.[161] Bart was the only fictional character on the list. On January 14, 2000, the Simpsons were awarded a star on the Hollywood Walk of Fame.[162] Also in 2000, Entertainment Weekly magazine TV critic Ken Tucker named The Simpsons the greatest television show of the 1990s. Furthermore, viewers of the UK television channel Channel 4 have voted The Simpsons at the top of two polls: 2001's 100 Greatest Kids' TV shows,[163] and 2005's The 100 Greatest Cartoons,[164] with Homer Simpson voted into first place in 2001's 100 Greatest TV Characters.[165] Homer would also place ninth on Entertainment Weekly's list of the "50 Greatest TV icons".[166] In 2002, The Simpsons ranked #8 on TV Guide's 50 Greatest TV Shows of All Time,[167] and in 2007 it was included in Time's list of the "100 Best TV Shows of All Time".[168] In 2008 the show was placed in first on Entertainment Weekly's "Top 100 Shows of the Past 25 Years".[169] Empire named it the greatest TV show of all time.[170] In 2010, Entertainment Weekly named Homer "the greatest character of the last 20 years."[171]
Bart's rebellious nature, which frequently resulted in no punishment for his misbehavior, led some parents and conservatives to characterize him as a poor role model for children.[172][173] In schools, educators claimed that Bart was a "threat to learning" because of his "underachiever and proud of it" attitude and negative attitude regarding his education.[174] Others described him as "egotistical, aggressive and mean-spirited".[175] In a 1991 interview, Bill Cosby described Bart as a bad role model for children, calling him "angry, confused, frustrated". In response, Matt Groening said, "That sums up Bart, all right. Most people are in a struggle to be normal [and] he thinks normal is very boring, and does things that others just wished they dare do."[176] On January 27, 1992, then-President George H. W. Bush said, "We are going to keep on trying to strengthen the American family, to make American families a lot more like the Waltons and a lot less like the Simpsons."[133] The writers rushed out a tongue-in-cheek reply in the form of a short segment which aired three days later before a rerun of "Stark Raving Dad" in which Bart replied, "Hey, we're just like the Waltons. We're praying for an end to the Depression, too."[177][178]
Various episodes of the show have generated controversy. The Simpsons visit Australia in "Bart vs. Australia" (season six, 1995) and Brazil in "Blame It on Lisa" (season 13, 2002) and both episodes generated controversy and negative reaction in the visited countries.[179] In the latter case, Rio de Janeiro's tourist board who claimed that the city was portrayed as having rampant street crime, kidnappings, slums, and monkey and rat infestations went so far as to threaten Fox with legal action.[180] Groening was a fierce and vocal critic of the episode "A Star Is Burns" (season six, 1995) which featured a crossover with The Critic. He felt that it was just an advertisement for The Critic, and that people would incorrectly associate the show with him. When he was unsuccessful in getting the episode pulled, he had his name removed from the credits and went public with his concerns, openly criticizing James L. Brooks and saying the episode "violates the Simpsons' universe." In response, Brooks said "I am furious with Matt, [...] he's allowed his opinion, but airing this publicly in the press is going too far. [...] his behavior right now is rotten."[181][182] "The Principal and the Pauper" (season nine, 1997) is one of the most controversial episodes of The Simpsons. Many fans and critics reacted negatively to the revelation that Seymour Skinner, a recurring character since the first season, was an impostor. The episode has been criticized by Groening and by Harry Shearer, who provides the voice of Skinner. In a 2001 interview, Shearer recalled that after reading the script, he told the writers, "That's so wrong. You're taking something that an audience has built eight years or nine years of investment in and just tossed it in the trash can for no good reason, for a story we've done before with other characters. It's so arbitrary and gratuitous, and it's disrespectful to the audience."[183]
The show has reportedly been taken off the air in several countries. China banned it from prime-time television in August 2006, "in an effort to protect China's struggling animation studios."[184] In 2008, Venezuela barred the show from airing on morning television as it was "unsuitable for children".[185] The same year, several Russian Pentecostal churches demanded The Simpsons, South Park and some other Western cartoons to be put off air "for propaganda of various vices" and the broadcaster's license to be revoked. However, the court decision later dismissed this request.[186]
Critics' reviews of early Simpsons episodes praised the show for its wit, realism, and intelligence.[8][187] In the late 1990s, around the airing of season ten, the tone and emphasis of the show began to change. Some critics started calling the show "tired".[188] By 2000, some long-term fans had become disillusioned with the show and pointed to its shift from character-driven plots to what they perceived as an overemphasis on zany antics.[189][190][191] The BBC noted "the common consensus is that The Simpsons' golden era ended after season nine",[192] while Todd Leopold of CNN, in an article looking at its perceived decline, stated "for many fans [...] the glory days are long past."[191] Jim Schembri of the The Sydney Morning Herald called the show "a cultural touchstone for at least twopossibly threegenerations of couch potatoes", but claimed that the show has declined in quality. He attributed this decline in quality to an abandonment of character-driven storylines in favour of and overuse of celebrity cameo appearances and references to popular culture. Schembri wrote: "The central tragedy of The Simpsons is that it has gone from commanding attention to merely being attention seeking. It began by proving that cartoon characters don't have to be caricatures; they can be invested with real emotions. Now the show has in essence fermented into a limp parody of itself. Memorable story arcs have been sacrificed for the sake of celebrity walk-ons and punchline-hungry dialogue."[193]
Author Douglas Coupland described claims of declining quality in the series as "hogwash", saying "The Simpsons hasn't fumbled the ball in fourteen years, it's hardly likely to fumble it now."[194] Mike Scully, who was showrunner during seasons nine through twelve, has been the subject of criticism.[195][196] Chris Suellentrop of Slate wrote that "under Scully's tenure, The Simpsons became, well, a cartoon. [...] Episodes that once would have ended with Homer and Marge bicycling into the sunset now end with Homer blowing a tranquilizer dart into Marge's neck. The show's still funny, but it hasn't been touching in years."[195] When asked in 2007 how the series' longevity is sustained, Scully joked, "Lower your quality standards. Once you've done that you can go on forever."[197]
In 2003, to celebrate the show's 300th episode "Barting Over", USA Today published a pair of Simpsons related articles: a top-ten episodes list chosen by the webmaster of The Simpsons Archive fansite,[198] and a top-15 list by The Simpsons' own writers.[199] The most recent episode listed on the fan list was 1997's "Homer's Phobia"; the Simpsons' writers most recent choice was 2000's "Behind the Laughter". In 2004, Harry Shearer criticized what he perceived as the show's declining quality: "I rate the last three seasons as among the worst, so Season Four looks very good to me now."[200] In response, Dan Castellaneta stated "I don't agree, [...] I think Harry's issue is that the show isn't as grounded as it was in the first three or four seasons, that it's gotten crazy or a little more madcap. I think it organically changes to stay fresh."[201]
Despite the criticism, The Simpsons manages to maintain a large viewership and attract new fans. While the first season enjoyed an average of 13.4 million viewers per episode in the U.S.,[126] the twenty-first season had an average of 7.2 million viewers.[202] In an April 2006 interview, Matt Groening said, "I honestly don't see any end in sight. I think it's possible that the show will become too financially cumbersome... but right now, the show is creatively, I think, as good or better than it's ever been. The animation is incredibly detailed and imaginative, and the stories do things that we haven't done before. So creatively there's no reason to quit."[203]
Numerous Simpson-related comic books have been released over the years. So far, nine comic book series have been published by Bongo Comics since 1993.[204] The first comic strips based on The Simpsons appeared in 1991 in the magazine Simpsons Illustrated, which was a companion magazine to the show.[205] The comic strips were popular and a one-shot comic book titled Simpsons Comics and Stories, containing four different stories, was released in 1993 for the fans.[206] The book was a success and due to this, the creator of The Simpsons, Matt Groening, and his companions Bill Morrison, Mike Rote, Steve Vance and Cindy Vance created the publishing company Bongo Comics.[206] Issues of Simpsons Comics, Bart Simpson's Treehouse of Horror and Bart Simpson have been collected and reprinted in trade paperbacks in the United States by HarperCollins.[207][208][209]
20th Century Fox, Gracie Films, and Film Roman produced The Simpsons Movie, an animated film that was released on July 27, 2007.[210] The film was directed by long-time Simpsons producer David Silverman and written by a team of Simpsons writers comprising Matt Groening, James L. Brooks, Al Jean, George Meyer, Mike Reiss, John Swartzwelder, Jon Vitti, David Mirkin, Mike Scully, Matt Selman, and Ian Maxtone-Graham.[210] Production of the film occurred alongside continued writing of the series despite long-time claims by those involved in the show that a film would enter production only after the series had concluded.[210] There had been talk of a possible feature-length Simpsons film ever since the early seasons of the series. James L. Brooks originally thought that the story of the episode "Kamp Krusty" was suitable for a film, but he encountered difficulties in trying to expand the script to feature-length.[211] For a long time, difficulties such as lack of a suitable story and an already fully engaged crew of writers delayed the project.[203]
Collections of original music featured in the series have been released on the albums Songs in the Key of Springfield, Go Simpsonic with The Simpsons and The Simpsons: Testify.[212] Several songs have been recorded with the purpose of a single or album release and have not been featured on the show. The album The Simpsons Sing the Blues was released in September 1990 and was a success, peaking at #3 on the Billboard 200[213] and becoming certified 2 platinum by the Recording Industry Association of America.[214] The first single from the album was the pop rap song "Do the Bartman", performed by Nancy Cartwright and released on November 20, 1990. The song was written by Michael Jackson, although he did not receive any credit.[215] The Yellow Album was released in 1998, but received poor reception and did not chart in any country.
In 2007, it was officially announced that The Simpsons Ride, a simulator ride, would be implemented into the Universal Studios Orlando and Universal Studios Hollywood.[216] It officially opened May 15, 2008 in Florida[217] and May 19, 2008 in Hollywood.[218] In the ride, patrons are introduced to a cartoon theme park called Krustyland built by Krusty the Clown. However, Sideshow Bob is loose from prison to get revenge on Krusty and the Simpson family.[219] It features more than 24 regular characters from The Simpsons and features the voices of the regular cast members, as well as Pamela Hayden, Russi Taylor and Kelsey Grammer.[220] Harry Shearer decided not to participate in the ride, so none of his characters have vocal parts.[221]
Numerous video games based on the show have been produced. Some of the early games include Konami's arcade game The Simpsons (1991) and Acclaim Entertainment's The Simpsons: Bart vs. the Space Mutants (1991).[222][223] More modern games include The Simpsons: Road Rage (2001), The Simpsons: Hit & Run (2003) and The Simpsons Game (2007).[224][225][226] Electronic Arts, which produced The Simpsons Game, has owned the exclusive rights to create video games based on the show since 2005.[227] In 2010, they released a game called The Simpsons Arcade for iOS.[228] Electronic Arts released another iOS game, Tapped Out, in 2012.[229] Two Simpsons pinball machines have been produced: one that was available briefly after the first season, and another in 2007, both out of production.[230]
The popularity of The Simpsons has made it a billion-dollar merchandising industry.[133] The title family and supporting characters appear on everything from t-shirts to posters. The Simpsons has been used as a theme for special editions of well-known board games, including Clue, Scrabble, Monopoly, Operation, and The Game of Life, as well as the trivia games What Would Homer Do? and Simpsons Jeopardy!. Several card games such as trump cards and The Simpsons Trading Card Game have also been released. Many official or unofficial Simpsons books such as episode guides have been published. Many episodes of the show have been released on DVD and VHS over the years. When the first season DVD was released in 2001, it quickly became the best-selling television DVD in history, although it was later overtaken by the first season of Chappelle's Show.[231] In particular, seasons one through fourteen have been released on DVD in the U.S. (Region 1), Europe (Region 2) and Australia/New Zealand/Latin America (Region 4) with more seasons expected to be released in the future.[232]
In 2003, about 500 companies around the world were licensed to use Simpsons characters in their advertising.[233] As a promotion for The Simpsons Movie, twelve 7-Eleven stores were transformed into Kwik-E-Marts and sold The Simpsons related products. These included "Buzz Cola", "Krusty-O" cereal, pink doughnuts with sprinkles, and "Squishees".[234]
In 2008 consumers around the world spent $750 million on merchandise related to The Simpsons, with half of the amount originating from the United States. By 2009 20th Century Fox increased merchandising efforts.[235] On April 9, 2009, the United States Postal Service unveiled a series of five 44 cent stamps featuring Homer, Marge, Bart, Lisa and Maggie, to commemorate the show's twentieth anniversary.[236] The Simpsons is the first television series still in production to receive this recognition.[237][238] The stamps, designed by Matt Groening, were made available for purchase on May 7, 2009.[239] Approximately one billion were printed.[240]
        
1 Production

1.1 Development
1.2 Executive producers and showrunners
1.3 Writing
1.4 Voice actors
1.5 Animation


1.1 Development
1.2 Executive producers and showrunners
1.3 Writing
1.4 Voice actors
1.5 Animation
2 Characters
3 Setting
4 Themes
5 Hallmarks

5.1 Opening sequence
5.2 Halloween episodes
5.3 Humor


5.1 Opening sequence
5.2 Halloween episodes
5.3 Humor
6 Influence and legacy

6.1 Idioms
6.2 Television


6.1 Idioms
6.2 Television
7 Reception and achievements

7.1 Early success
7.2 Run length achievements
7.3 Awards
7.4 Criticism and controversy

7.4.1 Criticism of declining quality




7.1 Early success
7.2 Run length achievements
7.3 Awards
7.4 Criticism and controversy

7.4.1 Criticism of declining quality


7.4.1 Criticism of declining quality
8 Other media

8.1 Comic books
8.2 Film
8.3 Music
8.4 The Simpsons Ride
8.5 Video games


8.1 Comic books
8.2 Film
8.3 Music
8.4 The Simpsons Ride
8.5 Video games
9 Merchandise
10 Notes
11 References
12 Further reading
13 External links
1.1 Development
1.2 Executive producers and showrunners
1.3 Writing
1.4 Voice actors
1.5 Animation
5.1 Opening sequence
5.2 Halloween episodes
5.3 Humor
6.1 Idioms
6.2 Television
7.1 Early success
7.2 Run length achievements
7.3 Awards
7.4 Criticism and controversy

7.4.1 Criticism of declining quality


7.4.1 Criticism of declining quality
7.4.1 Criticism of declining quality
8.1 Comic books
8.2 Film
8.3 Music
8.4 The Simpsons Ride
8.5 Video games
Season 12: Matt Groening, James L. Brooks, & Sam Simon
Season 34: Al Jean & Mike Reiss
Season 56: David Mirkin
Season 78: Bill Oakley & Josh Weinstein
Season 912: Mike Scully
Season 13present: Al Jean
Alberti, John (ed.) (2003). Leaving Springfield: The Simpsons and the Possibility of Oppositional Culture. Detroit: Wayne State University Press. ISBN0-8143-2849-0.
Cartwright, Nancy (2000). My Life as a 10-Year-Old Boy. New York City: Hyperion Books. ISBN0-7868-8600-5.
Richmond, Ray; Antonia Coffman (1997). The Simpsons: A Complete Guide to Our Favorite Family. New York City: HarperCollins. ISBN0-00-638898-1.
Ortved, John (2009). The Simpsons: An Uncensored, Unauthorized History. Greystone Books. pp.248250. ISBN978-1-55365-503-9.
Turner, Chris (2004). Planet Simpson: How a Cartoon Masterpiece Documented an Era and Defined a Generation. Toronto: Random House Canada. ISBN0-679-31318-4.
Brown, Alan; Chris Logan (2006). The Psychology of The Simpsons. Dallas, Texas: Benbella Books. ISBN1-932100-70-9.
Gray, Jonathan (2006). Watching with The Simpsons: Television, Parody, and Intertextuality. Abingdon: Routledge. ISBN0-415-36202-4.
Irwin, William; Mark T. Conrad, Aeon Skoble (1999). The Simpsons and Philosophy: The D'oh! of Homer. Chicago: Open Court. ISBN0-8126-9433-3.
Keller, Beth L. (1992). The Gospel According to Bart: Examining the Religious Elements of The Simpsons. Regent University. ISBN0-8126-9433-3.
Keslowitz, Steven (2003). The Simpsons And Society: An Analysis Of Our Favorite Family And Its Influence In Contemporary Society. Hats Off Books. ISBN1-58736-253-8.
Pinsky, Mark I (2001). The Gospel According to The Simpsons: The Spiritual Life of the World's Most Animated Family. Louisville: Westminster John Knox Press. ISBN0-664-22419-9.
Pinsky, Mark I.; Samuel F. Parvin (2002). The Gospel According to the Simpsons: Leaders Guide for Group Study. Louisville, KY: Westminster John Knox Press. ISBN0-664-22590-X.
Book: The Simpsons
 The dictionary definition of Appendix:The Simpsons at Wiktionary
 Quotations related to The Simpsons at Wikiquote
 Media related to The Simpsons at Wikimedia Commons
Official website
The Simpsons at the Internet Movie Database
The Simpsons at TV.com
The Simpsons Archive
The Simpsons at the Encyclopedia of Television
.
#(`*Pixar*`)#.
Pixar Animation Studios, or simply Pixar (/pksr/, stylized PIXAR), is a computer animation film studio based in Emeryville, California. The studio is best known for its CGI-animated feature films created with PhotoRealistic RenderMan, its own implementation of the industry-standard RenderMan image-rendering application programming interface used to generate high-quality images. Pixar began in 1979 as the Graphics Group, part of the computer division of Lucasfilm before its spin-out as a corporation in 1986 with funding by Apple Inc. co-founder Steve Jobs, who became its majority shareholder.[1] The Walt Disney Company bought Pixar in 2006 at a valuation of $7.4billion, a transaction which made Jobs Disney's largest shareholder.
Pixar has produced thirteen feature films, beginning with Toy Story (1995). It was followed by A Bug's Life (1998), Toy Story 2 (1999), Monsters, Inc. (2001), Finding Nemo (2003), The Incredibles (2004), Cars (2006), Ratatouille (2007), WALL-E (2008), Up (2009), Toy Story 3 (2010), Cars 2 (2011), and Brave (2012). Twelve of the films have received both critical and financial success, with the notable exception being Cars 2, which, while commercially successful, has received substantially less praise than Pixar's other productions.[2] The studio has also produced several short films. As of February 2012, its feature films have made over $7billion worldwide,[3] with its $602million average gross by far the highest of any studio in the industry.[4] Three of Pixar's filmsFinding Nemo, Up, and Toy Story 3are among the 50 highest-grossing films of all time, and all of Pixar's films are among the 50 highest-grossing animated films, with Toy Story 3 being the all-time highest, grossing over $1billion worldwide.
The studio has earned 26 Academy Awards, seven Golden Globe Awards, and eleven Grammy Awards, among many other awards and acknowledgments. Since the award's inauguration in 2001, most of Pixar's films have been nominated for the Academy Award for Best Animated Feature, with six winning; Finding Nemo, The Incredibles, Ratatouille, WALL-E, Up, and Toy Story 3. Up and Toy Story 3 are two of only three animated films to be nominated for the Academy Award for Best Picture. On September 6, 2009, executives John Lasseter, Brad Bird, Pete Docter, Andrew Stanton, and Lee Unkrich were presented with the Golden Lion for Lifetime Achievement by the Biennale Venice Film Festival. The award was presented by Lucasfilm founder George Lucas.
Pixar was founded as The Graphics Group, which was one third of the Computer Division of Lucasfilm that was launched in 1979 with the hiring of Dr. Ed Catmull from the New York Institute of Technology (NYIT),[5] where he was in charge of the Computer Graphics Lab (CGL). At NYIT, the researchers pioneered many of the CG foundation techniquesin particular the invention of the "alpha channel" (by Catmull and Alvy Ray Smith);[citation needed] years later the CGL produced an experimental film called The Works. After moving to Lucasfilm, the team worked on creating the precursor to RenderMan, called REYES (for "renders everything you ever saw"); and developed a number of critical technologies for CGincluding "particle effects" and various animation tools.
In 1982, the team began working on film sequences with Industrial Light & Magic on special effects.[5] After years of research, and key milestones in films such as the Genesis Effect in Star Trek II: The Wrath of Khan and the Stained Glass Knight in Young Sherlock Holmes,[5] the group, which numbered 40 individuals back then,[1] was spun out as a corporation in February 1986 with investment by Steve Jobs shortly after he left Apple Computer.[1] Jobs paid $5million to George Lucas for technology rights and put them and $5million cash as capital into the company.[1] A factor contributing to Lucas' sale was an increase in cash flow difficulties following his 1983 divorce, which coincided with the sudden dropoff in revenues from Star Wars licenses following the release of Return of the Jedi. The newly independent company was headed by Dr. Edwin Catmull as President and Dr. Alvy Ray Smith as Executive Vice President. They were joined on the Board of Directors by Steve Jobs who was Chairman.[1]
Initially, while awaiting the consequences of Moore's law, that would reduce the cost of computing a film, Pixar was a high-end computer hardware company whose core product was the Pixar Image Computer, a system primarily sold to government agencies and the medical community. One of the buyers of Pixar Image Computers was Disney Studios, which was using the device as part of their secretive CAPS project, using the machine and custom software (written by Pixar) to migrate the laborious ink and paint part of the 2-D animation process to a more automated and thus efficient method. The Image Computer never sold well.[6] In a bid to drive sales of the system, Pixar employee John Lasseterwho had long been creating short demonstration animations, such as Luxo Jr., to show off the device's capabilitiespremiered his creations at SIGGRAPH, the computer graphics industry's largest convention, to great fanfare.[6]
As poor sales of Pixar's computers threatened to put the company out of business, Jobs invested more and more money and took more and more ownership away from the management and employees until after several years he owned essentially all the company for a total investment of $50million. Lasseter's animation department began producing computer-animated commercials for outside companies. Early successes included campaigns for Tropicana, Listerine, Life Savers and Terminator 2: Judgment Day.[7] In April 1990 Pixar sold its hardware division, including all proprietary hardware technology and imaging software, to Vicom Systems, and transferred 18 of Pixar's approximately 100 employees. The same year Pixar moved from San Rafael to Richmond, California.[8] During this period, Pixar continued its successful relationship with Walt Disney Feature Animation, a studio whose corporate parent would ultimately become its most important partner. In 1991, after a tough start of the year when about 30 employees in the company's computer department had to go (including the company's president, Chuck Kolstad),[9] which reduced the total number of employees to just 42, essentially its original number,[10] Pixar made a $26million deal with Disney to produce three computer-animated feature films, the first of which was Toy Story. At that point, the software programmers, who were doing RenderMan and CAPS, and Lasseters animation department, who made television commercials and a few shorts for Sesame Street, was all that was left of Pixar.[11]
Despite the total income of these products, the company was still losing money, and Jobs, still chairman of the board and now the full owner, often considered selling it. Even as late as 1994, Jobs contemplated selling Pixar to other companies, among them Microsoft. Only after learning from New York critics that Toy Story was probably going to be a success and confirming that Disney would distribute it for the 1995 Christmas season did he decide to give Pixar another chance.[12] He also began then for the first time to take an active direct leadership role in the company, making himself its CEO. The film went on to gross more than $361million worldwide.[13] Later that year, Pixar held its initial public offering on November 29, 1995, and the company's stock was priced at US$22 per share.[14]
Pixar built a new studio in Emeryville which opened in November 2000.
Some commentators have recognised distinctive features of the way management has been organised at Pixar. Features of management noted have included: Jobs' attitudes and behaviours, including his inspiring of employees and his confidence in them; Pixar personnel's deeper commitment to their work and creations and the impact of those, than their desire for money; a lack of strict management practices and of a rigid hierarchy; the quality and aspirations of employees hired; the creativity and imagination of all individuals working at the company; the design of the Emeryville campus of Pixar as a social workplace, and the encouragement of cooperation, teamwork and open conversation between all levels of employees.
Pixar and Disney had disagreements after the production of Toy Story 2. Originally intended as a straight-to-video release (and thus not part of Pixar's three-picture deal), the film was eventually upgraded to a theatrical release during production. Pixar demanded that the film then be counted toward the three-picture agreement, but Disney refused.[15] Though profitable for both, Pixar later complained that the arrangement was not equitable. Pixar was responsible for creation and production, while Disney handled marketing and distribution. Profits and production costs were split 50-50, but Disney exclusively owned all story and sequel rights and also collected a distribution fee. The lack of story and sequel rights was perhaps the most onerous aspect to Pixar and set the stage for a contentious relationship.[16]
The two companies attempted to reach a new agreement in early 2004. The new deal would be only for distribution, as Pixar intended to control production and own the resulting film properties themselves. The company also wanted to finance their films on their own and collect 100 percent of the profits, paying Disney only the 10 to 15 percent distribution fee.[17] More importantly, as part of any distribution agreement with Disney, Pixar demanded control over films already in production under their old agreement, including The Incredibles and Cars. Disney considered these conditions unacceptable, but Pixar would not concede.[17]
Disagreements between Steve Jobs and then Disney Chairman and CEO Michael Eisner made the negotiations more difficult than they otherwise might have been. They broke down completely in mid-2004, with Jobs declaring that Pixar was actively seeking partners other than Disney.[18] Pixar did not enter negotiations with other distributors. After a lengthy hiatus, negotiations between the two companies resumed following the departure of Eisner from Disney in September 2005. In preparation for potential fallout between Pixar and Disney, Jobs announced in late 2004 that Pixar would no longer release movies at the Disney-dictated November time frame, but during the more lucrative early summer months. This would also allow Pixar to release DVDs for their major releases during the Christmas shopping season. An added benefit of delaying Cars was to extend the time frame remaining on the Pixar-Disney contract to see how things would play out between the two companies.[19]
Pending the Disney acquisition of Pixar, the two companies created a distribution deal for the intended 2007 release of Ratatouille, in case the acquisition fell through, to ensure that this one film would still be released through Disney's distribution channels. (In contrast to the earlier Disney/Pixar deal, Ratatouille was to remain a Pixar property and Disney would have received only a distribution fee.) The completion of Disney's Pixar acquisition, however, nullified this distribution arrangement.[20]
Disney had agreed to buy Pixar for approximately $7.4billion in an all-stock deal.[21] Following Pixar shareholder approval, the acquisition was completed May 5, 2006. The transaction catapulted Steve Jobs, who was the majority shareholder of Pixar with 50.1%, to Disney's largest individual shareholder with 7% and a new seat on its board of directors.[22] Jobs' new Disney holdings exceeded holdings belonging to ex-CEO Michael Eisner, the previous top shareholder, who still held 1.7%; and Disney Director Emeritus Roy E. Disney, who held almost 1% of the corporation's shares.
Pixar shareholders received 2.3 shares of Disney common stock for each share of Pixar common stock redeemed.
As part of the deal, John Lasseter, by then Executive Vice President, became Chief Creative Officer (reporting to President and CEO Robert Iger and consulting with Disney Director Roy Disney) of both Pixar and Walt Disney Animation Studios, as well as the Principal Creative Adviser at Walt Disney Imagineering, which designs and builds the company's theme parks.[22] Catmull retained his position as President of Pixar, while also becoming President of Walt Disney Animation Studios, reporting to Bob Iger and Dick Cook, chairman of Walt Disney Studio Entertainment. Steve Jobs' position as Pixar's Chairman and Chief Executive Officer was also removed, and instead he took a place on the Disney board of directors.[23]
Lasseter and Catmull's oversight of both the Disney and Pixar studios did not mean that the two studios were merging, however. In fact, additional conditions were laid out as part of the deal to ensure that Pixar remained a separate entity, a concern that analysts had expressed about the Disney deal.[24] Some of those conditions were that Pixar HR policies would remain intact, including the lack of employment contracts. Also, the Pixar name was guaranteed to continue, and the studio would remain in its current Emeryville, California location with the "Pixar" sign. Finally, branding of films made post-merger would be "DisneyPixar" (beginning with Cars).[25]
Jim Morris, producer of WALL-E, became general manager of Pixar. In this new position, Morris took charge of the day-to-day running of the studio facilities and products.[26]
On April 20, 2010, Pixar Animation Studios opened a new studio in the downtown area of Vancouver, British Columbia, Canada.[27] The roughly 2,000 square meters studio is primarily producing shorts and TV specials based on characters from Pixar's feature films. The studio's first production was the Cars Toons episode, "Air Mater."
While some of Pixar's first animators were former cel animators, including John Lasseter, they also came from stop motion animation and/or computer animation or were fresh college graduates.[5] A large number of animators that make up the animation department at Pixar were hired around the time Pixar released A Bug's Life and Toy Story 2. Although Toy Story was a successful film, it was Pixar's only feature film at the time. The majority of the animation industry was, and is still located in Los Angeles, California, while Pixar is located 350 miles (560km) north in the San Francisco Bay Area. Also, traditional 2-D animation was still the dominant medium for feature animated films.
With the dearth of Los Angelesbased animators willing to move their families so far north, give up traditional animation, and try computer animation, Pixar's new-hires at this time either came directly from college, or had worked outside feature animation. For those who had traditional animation skills, the Pixar animation software (Marionette) is designed so that traditional animators would require a minimum amount of training before becoming productive.[5]
In an interview with PBS talk show host Tavis Smiley,[28] Lasseter said that Pixar films follow the same theme of self-improvement as the company itself has: with the help of friends or family, a character ventures out into the real world and learns to appreciate his friends and family. At the core, Lasseter said, "it's gotta be about the growth of the main character, and how he changes."[28]
Pixar has been criticized for its lack of female protagonists.[29][30] Brave, Pixar's thirteenth cinema release, is the studio's first with a female lead (voiced by Kelly Macdonald).
Toy Story 2 was commissioned by Disney as a direct-to-video, 60-minute film. Feeling the material was not very good, John Lasseter convinced the Pixar team to start from scratch and make that their third full-length feature film. Toy Story 3 was the second big-screen sequel when it was released on June 18, 2010. Cars 2, the studio's third theatrical sequel, was released on June 24, 2011. On June 27, 2011, Tom Hanks implied that a Toy Story 4 was in the works, but this has not been confirmed by the studio.[31][32]
Pixar states that they believe that sequels should only be made if they can come up with a story as good as the original. Following the release of Toy Story 2, Pixar and Disney had a gentlemen's agreement that Disney would not make any sequels without Pixar's involvement, despite their right to do so. In 2004, after Pixar announced they were unable to agree on a new deal, Disney announced that they would go ahead with sequels to Pixar's films with or without Pixar. Toy Story 3 was put into pre-production at the new CGI division of Walt Disney Feature Animation, Circle 7 Animation.
When Lasseter was placed in charge of all Disney and Pixar animation following the merger, he immediately put all sequels on hold; Disney stated that Toy Story 3 had been cancelled. However, in May 2006, it was announced that Toy Story 3 was back in pre-production and under Pixar's control when a new plot had been conceived.
Lasseter further fueled speculation on future sequels when he stated, "If we have a great story, we'll do a sequel."[33] Cars 2, Pixar's first sequel not based on Toy Story, was officially announced on April 8, 2008. Monsters University, the prequel to Monsters, Inc. and Pixar's first prequel, was announced on April 22, 2010, for release on November 2, 2012.[34] However, on April 5, 2011, it was announced that the film's release date had been pushed back to June 21, 2013 due to the success of Pixar films that are released in the summer, according to Disney distribution executive Chuck Viane.[35]
Toy Story was the first Pixar film to be adapted onto television, with the Buzz Lightyear of Star Command film and TV series. Cars was adapted to television via Cars Toons, a series of shorts (three to five minutes) running between regular Disney Channel shows and featuring Mater (the tow truck voiced by comedian Larry the Cable Guy).[36]
All Pixar films to date have been computer-animated features (WALL-E has so far been the only Pixar film not to be completely animated, featuring a small live-action element). 1906, the live action film by Brad Bird about the 1906 earthquake, is currently in development. Bird has stated that he was "interested in moving into the live action realm with some projects" while "staying at Pixar [because] it's a very comfortable environment for me to work in."
In 2008, Pixar announced Newt, a story about the last two blue-footed newts in existence destined to mate to save their species from extinction,[37] scheduled for release in June 2012. This project was to be followed by the fantasy film The Bear and the Bow, which was eventually retitled as Brave.[38]
In April 2010, Disney/Pixar announced that, instead, The Bear and the Bow would be released first, under the new name Brave, followed by a sequel to the 2001 Pixar feature Monsters, Inc. later that year.[39] Also, Newt was removed from the official Disney A to Z Encyclopedia supplement by chief archivist Dave Smith,[40] who confirmed that the film had been cancelled.[41][42] In May 2011, Pixar CEO John Lasseter implied that Newt had been cancelled due to having a similar plotline to Blue Sky Studios' film Rio.[43]
The upcoming Monsters University, a prequel to Monsters, Inc., will be followed by The Good Dinosaur, a film about a world in which dinosaurs never became extinct,[44] and The Inside Out, a film "about the inside of a girl's mind."[45][46]
In April 2012, Pixar announced their intention to create a film centered on the Mexican holiday Da de los Muertos[47] and to be directed by Lee Unkrich.[48]
Since December 2005, Pixar has held exhibitions celebrating the art and artists of Pixar, over their first twenty years in animation.[49]
Pixar celebrated 20 years in 2006 with the release of Pixar's seventh feature film, Cars, and held two exhibitions, from April to June 2010, at Science Centre Singapore, in Jurong East, Singapore, and the London Science Museum, London.[50] It was their first time holding an exhibition in Singapore.
The exhibition highlights consist of work-in-progress sketches from various Pixar productions, clay sculptures of their characters, and an autostereoscopic short showcasing a 3D version of the exhibition pieces which is projected through 4 projectors. Another highlight is the Zoetrope, where visitors of the exhibition are shown figurines of Toy Story characters "animated" in real-life through the zoetrope.[50]
Pixar celebrated 25 years of animation in 2011 with the release of its twelfth feature film, Cars 2. Pixar had celebrated its 20th anniversary with the first Cars. The Pixar: 25 Years of Animation exhibition was held at the Oakland Museum of California from July 2010 until January 2011.[51] The exhibition tour debuts in Hong Kong, and was held at the Hong Kong Heritage Museum in Sha Tin, between March 27 and July 11, 2011.[52][53]
Pixar: 25 Years of Animation includes all of the artwork from Pixar: 20 Years of Animation, plus art from Ratatouille, WALL-E, Up, and Toy Story 3.


Coordinates: 374958N 1221702W / 37.832639N 122.283789W / 37.832639; -122.283789

CGI animation
Motion pictures
1979(1979) as Graphics Group
February 3, 1986(February 3, 1986) as Pixar
Ed Catmull
Alvy Ray Smith
Steve Jobs (Head founder and investor, also co-founder of Apple Inc.)[1]
Ed Catmull (President)
John Lasseter (CCO)
Jim Morris (General Manager and Executive VP of Production)
1 History

1.1 Early history
1.2 Management and employee attitudes
1.3 Disney
1.4 Expansion


1.1 Early history
1.2 Management and employee attitudes
1.3 Disney
1.4 Expansion
2 Feature films and shorts

2.1 Traditions
2.2 Sequels and prequels
2.3 Adaptation to television
2.4 Animation and live-action
2.5 Product pipeline
2.6 Upcoming projects


2.1 Traditions
2.2 Sequels and prequels
2.3 Adaptation to television
2.4 Animation and live-action
2.5 Product pipeline
2.6 Upcoming projects
3 Exhibitions

3.1 Pixar: 20 Years of Animation
3.2 Pixar: 25 Years of Animation


3.1 Pixar: 20 Years of Animation
3.2 Pixar: 25 Years of Animation
4 See also
5 References
6 External links
1.1 Early history
1.2 Management and employee attitudes
1.3 Disney
1.4 Expansion
2.1 Traditions
2.2 Sequels and prequels
2.3 Adaptation to television
2.4 Animation and live-action
2.5 Product pipeline
2.6 Upcoming projects
3.1 Pixar: 20 Years of Animation
3.2 Pixar: 25 Years of Animation
List of Pixar films
List of Pixar staff
Organization of the artist
Production baby
Official website
Pixar's channel on YouTube
Pixar Animation Studios at the Internet Movie Database
Pixar Animation Studios at the Big Cartoon DataBase
List of the 40 founding employees of Pixar
.
#(`*The Walt Disney Company*`)#.
The Walt Disney Company (NYSE:DIS), commonly referred to as Disney, is an American diversified multinational mass media corporation headquartered in Walt Disney Studios, Burbank, California, United States. It is the largest media conglomerate in the world in terms of revenue.[3] Founded on October 16, 1923, by Walt and Roy Disney as the Disney Brothers Cartoon Studio, Walt Disney Productions established itself as a leader in the American animation industry before diversifying into live-action film production, television, and travel. Taking on its current name in 1986, The Walt Disney Company expanded its existing operations and also started divisions focused upon theatre, radio, music, publishing, and online media. In addition, it has created new divisions of the company in order to market more mature content than it typically associates with its flagship family-oriented brands.
The company is best known for the products of its film studio, the Walt Disney Studios, and today one of the largest and best-known studios in Hollywood. Disney also owns and operates the ABC broadcast television network; cable television networks such as Disney Channel, ESPN, A+E Networks, and ABC Family; publishing, merchandising, and theatre divisions; and owns and licenses 14 theme parks around the world. It also has a successful music division. The company has been a component of the Dow Jones Industrial Average since May 6, 1991. An early and well-known cartoon creation of the company, Mickey Mouse, is the official mascot of The Walt Disney Company.
In early 1923, Kansas City, Missouri animator Walt Disney created a short film entitled Alice's Wonderland, which featured child actress Virginia Davis interacting with animated characters. After the bankruptcy in 1923 of his previous firm, Laugh-O-Gram Films,[ChWDC 1] Disney moved to Hollywood to join his brother Roy O. Disney. Film distributor Margaret J. Winkler of M.J. Winkler Productions contacted Disney with plans to distribute a whole series of Alice Comedies purchased for $1,500 per reel with Disney as a production partner. Walt and his brother Roy Disney formed Disney Brothers Cartoon Studio that same year. More animated films followed after Alice.[4]
In January 1926 with the completion of the Disney studio on Hyperion Street, the Disney Brothers Studio's name is changed to the Walt Disney Studio.[ChWDC 2]
After the demise of the Alice comedies, Disney developed an all-cartoon series starring his first original character, Oswald the Lucky Rabbit,[4] which was distributed by Winkler Pictures through Universal Pictures.[ChWDC 2] The distributor had copyright Oswald, so Disney only made a few hundred dollars.[4] Disney only completed 26 Oswald shorts before losing the contract in February 1928, when Winkler's husband Charles Mintz took over their distribution company. After failing to take over the Disney Studio, Mintz hired away four of Disney's primary animators except Ub Iwerks to start his own animation studio, Snappy Comedies.[ChWDC 3]
In 1928, to recover from the loss of Oswald the Lucky Rabbit, Disney came up with idea of a mouse character named Mortimer while on a train headed to California drawing up a few simple drawings. The mouse was later renamed Mickey Mouse and starred in several Disney produced films. Ub Iwerks refined Disney's initial design of Mickey Mouse.[4] Disney's first sound film Steamboat Willie, a cartoon starring Mickey, was released on November 18, 1928[ChWDC 3] through Pat Powers' distribution company.[4] It was the first Mickey Mouse sound cartoon released, but the third to be created, behind Plane Crazy and The Gallopin' Gaucho.[ChWDC 3] Steamboat Willie was an immediate smash hit, and its initial success was attributed not just to Mickey's appeal as a character but to the fact that it was the first cartoon to feature synchronized sound.[4] Disney used Pat Powers' Cinephone system, created by Powers using Lee De Forest's Phonofilm system.[ChWDC 3] Steamboat Willie premiered at B. S. Moss's Colony Theater in New York City, now The Broadway Theatre.[5] Plane Crazy and The Galloping Gaucho were then retrofitted with synchronized sound tracks and re-released successfully in 1929.[ChWDC 3]
Disney continued to produce cartoons with Mickey Mouse and other characters,[4] and began the Silly Symphonies series with Columbia Pictures signing on as Symphonies distributor in August 1929. In September 1929, theater manager Harry Woodin requested permission to start a Mickey Mouse Club which Walt approves. In November, test comics strips are send to King Features, who request additional samples to show to the publisher, William Randolph Hearst. On December 16, the Walt Disney Studios partnership is reorganized as a corporation with the name of Walt Disney Productions, Limited with a merchandising division, Walt Disney Enterprises, and two subsidiaries, Disney Film Recording Company, Limited and Liled Realty and Investment Company for real estate holdings. Walt and his wife held 60% (6,000 shares) and Roy owned 40% of WD Productions. On December 30, King Features signed its first newspaper, New York Mirror, to publish the Mickey Mouse comic strip with Walt' permission.[ChWDC 4]
In 1932, Disney signed an exclusive contract with Technicolor (through the end of 1935) to produce cartoons in color, beginning with Flowers and Trees (1932). Disney released cartoons through Powers' Celebrity Pictures (19281930), Columbia Pictures (19301932), and United Artists (19321937).{cn|date=November 2012}} The popularity of the Mickey Mouse series allowed Disney to plan for his first feature-length animation.[4]
Deciding to push the boundaries of animation even further, Disney began production of his first feature-length animated film in 1934. Taking three years to complete, Snow White and the Seven Dwarfs, premiered in December 1937 and became highest-grossing film of that time by 1939.[6] Snow White was released through RKO Radio Pictures, which had assumed distribution of Disney's product in July 1937,[ChWDC 5] after United Artists attempted to attain future television rights to the Disney shorts.[7]
Using the profits from Snow White, Disney financed the construction of a new 51-acre (210,000m2) studio complex in Burbank, California. The new Walt Disney Studios, in which the company is headquartered to this day, was completed and open for business by the end of 1939.[ChWDC 6] The following year on April 2, Walt Disney Productions had its initial public offering.[ChWDC 7]
The studio continued releasing animated shorts and features, such as Pinocchio (1940), Fantasia (1940), Dumbo (1941), and Bambi (1942).[4] After World War II began, box-office profits declined. When the United States entered the war after the attack on Pearl Harbor, many of Disney's animators were drafted into the armed forces. The U.S. and Canadian governments commissioned the studio to produce training and propaganda films. By 1942 90% of its 550 employees were working on war-related films.[8] Films such as the feature Victory Through Air Power and the short Education for Death (both 1943) were meant to increase public support for the war effort. Even the studio's characters joined the effort, as Donald Duck appeared in a number of comical propaganda shorts, including the Academy Award-winning Der Fuehrer's Face (1943).
With limited staff and little operating capital during and after the war, Disney's feature films during much of the 1940s were "package films," or collections of shorts, such as The Three Caballeros (1944) and Melody Time (1948), which performed poorly at the box-office. At the same time, the studio began producing live-action films and documentaries. Song of the South (1946) and So Dear to My Heart (1948) featured animated segments, while the True-Life Adventures series, which included such films as Seal Island (1948) and The Vanishing Prairie (1954), were also popular and won numerous awards.
The release of Cinderella in 1950 proved that feature-length animation could still succeed in the marketplace. Other releases of the period included Alice in Wonderland (1951) and Peter Pan (1953), both in production before the war began, and Disney's first all-live action feature, Treasure Island (1950). Other early all-live-action Disney films included The Story of Robin Hood and His Merrie Men (1952), The Sword and the Rose (1953), and 20,000 Leagues Under the Sea (1954). Disney ended its distribution contract with RKO in 1953, forming its own distribution arm, Buena Vista Distribution.[9]
In December 1950, Walt Disney Productions and The Coca-Cola Company teamed up for Disney's first venture into television, the NBC television network special An Hour in Wonderland. In October 1954, the ABC network launched Disney's first regular television series, Disneyland, which would go on to become one of the longest-running primetime series of all time. Disneyland allowed Disney a platform to introduce new projects and broadcast older ones, and ABC became Disney's partner in the financing and development of Disney's next venture, located in the middle of an orange grove near Anaheim, California. It was the first phase of a long corporate relationship which, although no one could have anticipated it at the time, would culminate four decades later in the Disney company's acquisition of the ABC network, its owned and operated stations, and its numerous cable and publishing ventures.
In 1954, Walt Disney used his Disneyland series to unveil what would become Disneyland, an idea conceived out of a desire for a place where parents and children could both have fun at the same time. On July 18, 1955, Walt Disney opened Disneyland to the general public. On July 17, 1955, Disneyland was previewed with a live television broadcast hosted by Art Linkletter and Ronald Reagan. After a shaky start, Disneyland continued to grow and attract visitors from across the country and around the world. A major expansion in 1959 included the addition of America's first monorail system.
For the 1964 New York World's Fair, Disney prepared four separate attractions for various sponsors, each of which would find its way to Disneyland in one form or another. During this time, Walt Disney was also secretly scouting out new sites for a second Disney theme park. In November 1965, "Disney World" was announced, with plans for theme parks, hotels, and even a model city on thousands of acres of land purchased outside of Orlando, Florida.
Disney continued to focus its talents on television throughout the 1950s. Its weekday afternoon children's television program The Mickey Mouse Club, featuring its roster of young "Mouseketeers", premiered in 1955 to great success, as did the Davy Crockett miniseries, starring Fess Parker and broadcast on the Disneyland anthology show.[4] Two years later, the Zorro series would prove just as popular, running for two seasons on ABC, as well as separate episodes on the Disneyland series.[citation needed] Despite such success, Walt Disney Productions invested little into television ventures in the 1960s[citation needed], with the exception of the long-running anthology series, later known as The Wonderful World of Disney.[4]
Disney's film studios stayed busy as well, averaging five or six releases per year during this period. While the production of shorts slowed significantly during the 1950s and 1960s, the studio released a number of popular animated features, like Lady and the Tramp (1955), Sleeping Beauty (1959) and One Hundred and One Dalmatians (1961), which introduced a new xerography process to transfer the drawings to animation cels.[citation needed] Disney's live-action releases were spread across a number of genres, including historical fiction (Johnny Tremain, 1957), adaptations of children's books (Pollyanna, 1960) and modern-day comedies (The Shaggy Dog, 1959). Disney's most successful film of the 1960s was a live action/animated musical adaptation of Mary Poppins, which was one of the all time highest grossing movie[4] and received five Academy Awards, including Best Actress Julie Andrews.[citation needed]
On December 15, 1966, Walt Disney died of complications relating to lung cancer,[4] and Roy Disney took over as chairman, CEO, and president of the company. One of his first acts was to rename Disney World as "Walt Disney World" in honor of his brother and his vision.[citation needed]
In 1967, the last two films Walt actively supervised were released, the animated feature The Jungle Book[4] and the musical The Happiest Millionaire.[citation needed] The studio released a number of comedies in the late 1960s, including The Love Bug (1969's highest grossing film)[4] and The Computer Wore Tennis Shoes (1969), which starred another young Disney discovery, Kurt Russell. The 1970s opened with the release of Disney's first "post-Walt" animated feature, The Aristocats, followed by a return to fantasy musicals in 1971's Bedknobs and Broomsticks.[4] Blackbeard's Ghost was another successful film during this period.[4]
On October 1, 1971, Walt Disney World opened to the public, with Roy Disney dedicating the facility in person later that month. On December 20, 1971, Roy Disney died of a stroke, leaving the company under control of Donn Tatum, Card Walker, and Walt's son-in-law Ron Miller, each trained by Walt and Roy.
While Walt Disney Productions continued releasing family-friendly films throughout the 1970s, such as Escape to Witch Mountain (1975)[4] and Freaky Friday (1976), the films did not fare as well at the box office as earlier material. However, the animation studio saw success with Robin Hood (1973), The Rescuers (1977), and The Fox and the Hound (1981).
As head of the studio, Miller attempted make films to drive the profitable teenage market who generally passed on seeing Disney movies.[10] Inspired by the popularity of Star Wars, the Disney studio produced the science-fiction adventure The Black Hole in 1979 that cost $20 million to make, but was lost in Star Wars' wake.[4] The Black Hole was the first Disney production to carry a PG rating in the United States.[10][N 1] Disney dabbled in the horror genre with The Watcher in the Woods, and financed the boldly innovative Tron; both films were released to minimal success.[4]
Disney also hired outside producers for film projects, which had never been done before in the studio's history.[10] In 1979, Disney entered a joint venture with Paramount Pictures on the production of the 1980 film adaptation of Popeye and Dragonslayer (1981); the first time Disney collaborated with another studio. Paramount distributed Disney films in Canada at the time, and it was hoped that Disney's marketing prestige would help sell the two films.[10]
The 1983 release of Mickey's Christmas Carol began a string of successful movies, starting with Never Cry Wolf and the Ray Bradbury adaptation Something Wicked This Way Comes.[4] In 1984, Disney CEO Ron Miller created Touchstone Pictures as a brand for Disney to release more adult-oriented material. Touchstone's first release was the comedy Splash (1984), which was a box office success.[citation needed]
With The Wonderful World of Disney remaining a prime-time staple, Disney returned to television in the 1970s with syndicated programing such as the anthology series The Mouse Factory and a brief revival of the Mickey Mouse Club. In 1980, Disney launched Walt Disney Home Video to take advantage of the newly emerging videocassette market. On April 18, 1983, The Disney Channel debuted as a subscription-level channel on cable systems nationwide, featuring its large library of classic films and TV series, along with original programming and family-friendly third-party offerings.
Walt Disney World received much of the company's attention through the 1970s and into the 1980s. In 1978, Disney executives announced plans for the second Walt Disney World theme park, EPCOT Center, which would open in October 1982. Inspired by Walt Disney's dream of a futuristic model city, EPCOT Center was built as a "permanent World's Fair", complete with exhibits sponsored by major American corporations, as well as pavilions based on the cultures of other nations. In Japan, the Oriental Land Company partnered with Walt Disney Productions to build the first Disney theme park outside of the United States, Tokyo Disneyland, which opened in April 1983.
Despite the success of the Disney Channel and its new theme park creations, Walt Disney Productions was financially vulnerable. Its film library was valuable, but offered few current successes, and its leadership team was unable to keep up with other studios, particularly the works of Don Bluth, who defected from Disney in 1979.
By the early 1980s, the Parks generating 70% of Disney's income.[4]
In 1984, financier Saul Steinberg's Reliance Group Holdings launched a hostile takeover bid for Walt Disney Productions,[4] with the intent of dissolving the company and selling off its various assets.[citation needed] Disney bought out Reliance's 11.1% stake in the company. However, other shareholder filed suit claiming the deal devaluated Disney's stock and for Disney management to retain their positions. The shareholder lawsuit was settled in 1989 for $45 total from Disney and Reliance.[4]
With the Sid Bass family purchase of 18.7 percent of Disney, Bass and the board brought in Michael Eisner as CEO from Paramount Pictures and Frank Wells from Warner Bros. as president. Eisner emphasized Touchstone Films with Down and Out in Beverly Hills (1985) to start leading to increased output with Outrageous Fortune, Tin Men, Ruthless People, and additional hits. Eisner used expanding cable and home video markets to sign deals using Disney shows and films with a long-term deal with Showtime Networks for Disney/Touchstone releases through 1996 and entering television syndication and distribution for TV series as The Golden Girls. Disney began limited releases of its previous films on video tapes in the late 1980s. Eisner's Disney purchased KHJ, an independent Los Angeles TV station.[4]
Organized in 1985, Silver Screen Partners II, LP financed films for Walt Disney Company with $193 million. In January 1987, Silver Screen III began financing movies for Disney with $300 million raised, the largest amount raised for a film financing limited partnership by E.F. Hutton.[11] Silver Screen IV was also set up to finance Disney's studios.[12]
Beginning with Who Framed Roger Rabbit (1988), and later, The Little Mermaid (1989), its flagship animation studio enjoyed a series of commercial and critical successes. In addition, the company successfully entered the field of television animation with a number of lavishly budgeted and acclaimed series such as Adventures of the Gummi Bears, Duck Tales and Gargoyles.[citation needed] Disney moved to first place in box office receipts by 1988 and had increased revenues by 20% every year.[4]
Named the "Disney Decade" by the company, the executive talent attempted to move the company to new heights in the 1990s with huge changes and accomplishments.[4] In September 1990, The Disney Company arranged for financing up to $200 million by a unit of Nomura Securities for Interscope films made for Disney. On October 23, Disney formed Touchwood Pacific Partners I which would supplant the Silver Screen Partnership series as their movie studios' primary source of funding.[12]
In 1991, hotels, home video distribution, and Disney merchandising became 28 percent of total company revenues with international revenues contributed 22 percent of revenues. The company committed its studios in the first quarter of 1991 to produce 25 films in 1992. However, 1991 saw net income drop by 23% and had no growth for the year, but saw the release of Beauty and the Beast, winner of 2 Academy Awards and top grossing film in the genre. Disney next moved into publishing with Hyperion Press and adult music with Hollywood Records while Disney Imagineering was laying off 400 employees.[4]
Disney also broadened its adult offerings in film when then Disney Studio Chairman Jeffrey Katzenberg acquired Miramax Films in 1993.
Wells died in a helicopter crash in 1994.[4] Shortly thereafter, Katzenberg resigned and formed Dreamworks SKG because Eisner would not appoint Katzenberg to Wells' now-available post plus Katzenberg sued over the terms of his contract.[4] Instead, Eisner recruited his friend Michael Ovitz, one of the founders of the Creative Artists Agency, to be President, with minimal involvement from Disney's board of directors (which at the time included Oscar-winning actor Sidney Poitier, the CEO of Hilton Hotels Corporation Stephen Bollenbach, former U.S. Senator George Mitchell, Yale dean Robert A. M. Stern, and Eisner's predecessors Raymond Watson and Card Walker). Ovitz lasted only 14 months and left Disney in December 1996 via a "no fault termination" with a severance package of $38million in cash and 3 million stock options worth roughly $100million at the time of Ovitz's departure. The Ovitz episode engendered a long running derivative suit, which finally concluded in June 2006, almost 10 years later. Chancellor William B. Chandler, III of the Delaware Court of Chancery, despite describing Eisner's behavior as falling "far short of what shareholders expect and demand from those entrusted with a fiduciary position..." found in favor of Eisner and the rest of the Disney board because they hadn't violated the letter of the law (namely, the duty of care owed by a corporation's officers and board to its shareholders).[13]
Eisner attempt in 1994 to purchased NBC from GE, but the deal fail do to GE wanting to hold on to 51% ownership of the network. Disney acquired many other media sources during the decade, including a merger with Capital Cities/ABC in 1995 which brought broadcast network ABC and its assets, including the A&E Television Networks and ESPN networks, into the Disney fold.[4]
In 1998, Disney began a move into the internet field with the purchase of Starwave and 43 percent of Infoseek. In 1999, Disney purchased the remaining shares of Infoseek and launch the Go Network portal in January. Disney also launched its cruise line with the christening of Disney Magic and a sister ship, Disney Wonder. Two professional sports teams were acquired the Mighty Ducks of Anaheim and Anaheim Angels.[4]
As the Katzenberg case dragged on as his contract included a portion of the film revenue from ancillary markets forever. Katzenberg had offered $100 to settle the case but Eisner felt the original claim amount of about half a billion too much, but then the ancillary market clause was found. Disney lawyers tried to indicate a decline situation which reveal the some of the problems in the company. ABC had declining rating and increasing costs while the film segment had two film failures. While neither party revealed the settlement amount, it is estimated at $200 million.[4]
Eisner's controlling style inhibited efficiency and progress according to some critics, while other industry experts indicated that "age compression" theory led to a decline in the company's target market due to youth copying teenage behavior earlier.[4]
2000 brought an increase in revenue of 9% and net income of 39% with ABC and ESPN leading the way and Parks and Resorts marking its sixth consective year of growth. However the September 11 attacks led to a complete halt of vacation travel and led to a recession. The recession led to a decrease in ABC revenue. Plus, Eisner had the company make an expensive purchase of Fox Family Worldwide. 2001 was a year of cost cutting laying off 4,000 employees, Disney parks operations decreased, slashing annual live-action film investment, and minimizing Internet operations. While 2002 revenue had a small decrease from 2001 with the cost cutting, net income rose to $1.2 billion with two creative film releases. In 2003, the Studio became the first studio to record over $3 billion in worldwide box office receipts.[4]
Eisner wanted the board to not renominate Roy E. Disney, the son of Disney co-founder Roy O. Disney, as a board director citing his age of 72 as a required retirement age. Stanley Gold responded by resigning from the board and requesting the other board members oust Eisner.[4] In 2003, Disney resigned from his positions as the company's vice chairman and chairman of Walt Disney Feature Animation,[ChWDC 8] accusing Eisner of micromanagement, flops with the ABC television network, timidity in the theme park business, turning the Walt Disney Company into a "rapacious, soul-less" company, and refusing to establish a clear succession plan, as well as a string of box-office movie flops starting in the year 2000.
In 2004, Pixar Animation Studios began looking for another distributor after its 12 year contract with Disney with its relationship strained over issues of control and money with Eisner. Comcast Corporation made an unsolicited $54 billion bid to acquire the Company. A couple of high budget movies flopped at the box office. With these difficulties and some board directors dissatisfaction, Eisner ceded the board chairmanship.[4]
On March 3, 2004, at Disney's annual shareholders' meeting, a surprising and unprecedented 45% of Disney's shareholders, predominantly rallied by former board members Roy Disney and Stanley Gold, withheld their proxies to re-elect Eisner to the board. Disney's board then gave the chairmanship position to Mitchell. However, the board did not immediately remove Eisner as chief executive.[ChWDC 9]
On March 13, 2005, Robert Iger was announced as Eisner successor as CEO. On September 30, Eisner resigned both as an executive and as a member of the board of directors. [ChWDC 10]
On July 8, 2005, Walt Disney's nephew, Roy E. Disney returned to The Walt Disney Company as a consultant and with the new title of Non Voting Director, Emeritus. Walt Disney Parks and Resorts celebrated the 50th anniversary of Disneyland Park on July 17, and opened Hong Kong Disneyland on September 12. Walt Disney Feature Animation released Chicken Little, the company's first film using 3-D animation. On October 1, Bob Iger replaced Michael Eisner as CEO. Miramax co-founders Bob Weinstein and Harvey Weinstein also departed the company to form their own studio. On July 25, 2005, Disney announced that it was closing DisneyToon Studios Australia in October 2006, after 17 years of existence.[citation needed]
In 2006, Disney acquired Oswald the Lucky Rabbit, Disneys pre-Mickey silent animation star.[14] Aware that Disney's relationship with Pixar was wearing thin, President and CEO Robert Iger began negotiations with leadership of Pixar Animation Studios, Steve Jobs and Ed Catmull, regarding possible merger. On January 23, 2006, it was announced that Disney would purchase Pixar in an all-stock transaction worth $7.4billion. The deal was finalized on May 5; and among noteworthy results was the transition of Pixar's CEO and 50.1% shareholder, Steve Jobs, becoming Disney's largest individual shareholder at 7% and a member of Disney's Board of Directors.[15][16] Ed Catmull took over as President of Pixar Animation Studios. Former Executive Vice-President of Pixar, John Lasseter, became Chief Creative Officer of both Walt Disney Animation Studios and Pixar Animation Studios, as well assuming the role of Principal Creative Advisor at Walt Disney Imagineering.[16]
After a long time working in the company as a senior executive and large shareholder, Director Emeritus Roy E. Disney died from stomach cancer on December 16, 2009. At the time of his death, he owned roughly 1% of all of Disney which amounted to 16 million shares. He is seen to be the last member of the Disney family to be actively involved in the running of the company and working in the company altogether.[citation needed]
On August 31, 2009, Disney announced a deal to acquire Marvel Entertainment, Inc. for $4.24 billion.[17] The deal was finalized on December 31, 2009 in which Disney acquired ownership on the company.[18] Disney has stated that their acquisition of the company will not affect Marvel's products, neither will the nature of any Marvel characters be transformed.[19]
In October 2009, Disney Channel president Rich Ross, hired by Iger, replaced Dick Cook as chairman of the company and, in November, began restructuring the company to focus more on family friendly products. Later in January 2010, Disney decided to shut down Miramax after downsizing Touchstone, but one month later, they began selling the Miramax brand and its 700-title film library. On March 12, ImageMovers Digital, Robert Zemeckis's company which Disney had bought in 2007, was shut down. In April 2010, Lyric Street, Disney's country music label in Nashville, was shut down. In May 2010, the company sold the Power Rangers brand, as well as its 700-episode library, back to Haim Saban. In June, the company canceled Jerry Bruckheimer's film project Killing Rommel.[citation needed] In January 2011, Disney Interactive Studios was downsized.[20] In November, two ABC stations were sold.[21] With the release of Tangled in 2010, Ed Catmull said that the "princess" genre of films was taking a hiatus until "someone has a fresh take on it  but we don't have any other musicals or fairytales lined up."[22] He explained that they were looking to get away from the princess era due to the changes in audience composition and preference.[citation needed] However in the Facebook page, Ed Catmull stated that this was just a rumor.[23]
In April 2011, Disney broke ground on Shanghai Disney Resort. Costing $4.4 billion, the resort is slated to open in 2015.[24] Later, in August 2011, Bob Iger stated on a conference call that after the success of the Pixar and Marvel purchases, he and the Walt Disney Company are looking to "buy either new characters or businesses that are capable of creating great characters and great stories."[25] Later, in early February 2012, Disney completed its acquisition of UTV Software Communications, expanding their market further into India and Asia.[26]
On October 30, 2012, Disney announced plans to acquire Lucasfilm for $4.05 billion with plans to release Star Wars Episode VII in 2015.[27] On December 4, 2012, the Disney-Lucasfilm merger was approved by the Federal Trade Commission, allowing the acquisition to be finalized without dealing with antitrust problems.[28]
The Walt Disney Company operates as five primary units and segments: The Walt Disney Studios or Studio Entertainment, which includes the company's film, recording label, and theatrical divisions; Parks and Resorts, featuring the company's theme parks, cruise line, and other travel-related assets; Disney Consumer Products, which produces toys, clothing, and other merchandising based upon Disney-owned properties, Media Networks, which includes the company's television and Walt Disney Interactive Media Group, Internet, mobile, social media, virtual worlds and computer games operations.
Its main entertainment features and holdings include Walt Disney Studios, Disney Music Group, Disney Theatrical Group, Disney-ABC Television Group, Radio Disney, ESPN Inc., Disney Interactive Media Group, Disney Consumer Products, Disney India Ltd., The Muppets Studio, Pixar Animation Studios, Marvel Entertainment, UTV Software Communications, and Lucasfilm.
Its resorts and diversified holdings include Walt Disney Parks and Resorts, Disneyland Resort, Walt Disney World Resort, Tokyo Disney Resort, Disneyland Paris, Euro Disney S.C.A., Hong Kong Disneyland Resort, Disney Vacation Club and Disney Cruise Line.
Disney Media Networks is a reporting segment and primary unit of The Walt Disney Company that contains the company's various television networks, cable channels, associated production and distribution companies and owned and operated television stations. Media Networks also manages Disney's interest in its joint venture with Hearst Corporation, A+E Networks.
From 1945 to 1960 Walt and Roy Disney shared the role of Chairman of the Board. Walt dropped the Chairman title in 1960 so he could focus more on the creative aspects of the company. Roy O. Disney kept the Chairman and CEO's role.
Some of Disney's animated family films have drawn fire for being accused of having sexual references hidden in them, among them The Little Mermaid (1989), Aladdin (1992), and The Lion King (1994). Instances of sexual material hidden in some versions of The Rescuers (1977) and Who Framed Roger Rabbit (1988) resulted in recalls and modifications of the films to remove such content.[47]
Some religious welfare groups, such as the Catholic League, have opposed films including Priest (1994) and Dogma (1999).[48] A book called Growing Up Gay, published by Disney-owned Hyperion Press and similar publications, as well as the company's extension of benefits to same-sex domestic partners, spurred boycotts of Disney and its advertisers by the Catholic League, the Assemblies of God USA, the American Family Association, and other conservative groups.[48][49][50] The boycotts were discontinued by most of these organizations by 2005.[51] In addition to these social controversies, the company has been accused of human rights violations regarding the working conditions in factories that produce their merchandise.[52][53]
Polsson, Ken. "Chronology of the Walt Disney Company". KPolsson.com.

The Walt Disney Studios
Disney Media Networks
Walt Disney Parks and Resorts
Disney Interactive Media Group
Disney Consumer Products
Walt Disney Pictures
Walt Disney Animation Studios
Walt Disney Theatrical
Walt Disney India Ltd.
Pixar Animation Studios
Marvel Entertainment, LLC
Lucasfilm
The Muppets Studio
ABC Inc.
ESPN Inc. (80%)
A+E Networks (50%)
Radio Disney
Hulu (27%)
UTV Software Communications
1 Corporate history

1.1 19231928: The silent era
1.2 19281934: Mickey Mouse and Silly Symphonies
1.3 19341945: Snow White and the Seven Dwarfs and World War II
1.4 19461954: Post-war and television
1.5 19551965: Disneyland
1.6 19661971: The deaths of Walt and Roy Disney and the opening of Walt Disney World
1.7 19721984: Theatrical malaise and new leadership
1.8 19842004: The Eisner era

1.8.1 "Save Disney" campaign and Eisner's ouster


1.9 2005present: The Iger era


1.1 19231928: The silent era
1.2 19281934: Mickey Mouse and Silly Symphonies
1.3 19341945: Snow White and the Seven Dwarfs and World War II
1.4 19461954: Post-war and television
1.5 19551965: Disneyland
1.6 19661971: The deaths of Walt and Roy Disney and the opening of Walt Disney World
1.7 19721984: Theatrical malaise and new leadership
1.8 19842004: The Eisner era

1.8.1 "Save Disney" campaign and Eisner's ouster


1.8.1 "Save Disney" campaign and Eisner's ouster
1.9 2005present: The Iger era
2 Company divisions and subsidiaries

2.1 Disney Media Networks


2.1 Disney Media Networks
3 Executive management

3.1 Presidents
3.2 Chief Executive Officers
3.3 Chairmen of the Board
3.4 Vice Chairman of the Board
3.5 Chief Operating Officers


3.1 Presidents
3.2 Chief Executive Officers
3.3 Chairmen of the Board
3.4 Vice Chairman of the Board
3.5 Chief Operating Officers
4 Financial data

4.1 Revenues
4.2 Net income


4.1 Revenues
4.2 Net income
5 Criticism
6 See also
7 References
8 Further reading
9 External links
1.1 19231928: The silent era
1.2 19281934: Mickey Mouse and Silly Symphonies
1.3 19341945: Snow White and the Seven Dwarfs and World War II
1.4 19461954: Post-war and television
1.5 19551965: Disneyland
1.6 19661971: The deaths of Walt and Roy Disney and the opening of Walt Disney World
1.7 19721984: Theatrical malaise and new leadership
1.8 19842004: The Eisner era

1.8.1 "Save Disney" campaign and Eisner's ouster


1.8.1 "Save Disney" campaign and Eisner's ouster
1.9 2005present: The Iger era
1.8.1 "Save Disney" campaign and Eisner's ouster
2.1 Disney Media Networks
3.1 Presidents
3.2 Chief Executive Officers
3.3 Chairmen of the Board
3.4 Vice Chairman of the Board
3.5 Chief Operating Officers
4.1 Revenues
4.2 Net income
DisneyABC Television Group

Disney/ABC Television Group Digital Media
Walt Disney Television
Disney-ABC Domestic Television - formerly Buena Vista Television
Disney-ABC International Television - formerly Buena Vista International Television
ABC Television Network
ABC News


Disney/ABC Television Group Digital Media
Walt Disney Television
Disney-ABC Domestic Television - formerly Buena Vista Television
Disney-ABC International Television - formerly Buena Vista International Television
ABC Television Network
ABC News
ABC Entertainment Group

ABC Entertainment
ABC Studios - formerly Touchstone Television & ABC Television Studios
Times Square Studios (division)


ABC Entertainment
ABC Studios - formerly Touchstone Television & ABC Television Studios
Times Square Studios (division)
ABC Family

ABC Spark - with Corus Entertainment


ABC Spark - with Corus Entertainment
ABC Owned Television Stations Group

Live Well Network
ABC National Television Sales
ABC Regional Sports and Entertainment Sales[29]


Live Well Network
ABC National Television Sales
ABC Regional Sports and Entertainment Sales[29]
Disney Channel Worldwide

Disney Cinemagic
Disney Junior
Disney XD
Hungama
Radio Disney
Disney Television Animation


Disney Cinemagic
Disney Junior
Disney XD
Hungama
Radio Disney
Disney Television Animation
Hyperion Books

ABC Daytime Press
ESPN Books
VOICE


ABC Daytime Press
ESPN Books
VOICE
ESPN, Inc. (80%)
Disney/ABC Television Group Digital Media
Walt Disney Television
Disney-ABC Domestic Television - formerly Buena Vista Television
Disney-ABC International Television - formerly Buena Vista International Television
ABC Television Network
ABC News
ABC Entertainment
ABC Studios - formerly Touchstone Television & ABC Television Studios
Times Square Studios (division)
ABC Spark - with Corus Entertainment
Live Well Network
ABC National Television Sales
ABC Regional Sports and Entertainment Sales[29]
Disney Cinemagic
Disney Junior
Disney XD
Hungama
Radio Disney
Disney Television Animation
ABC Daytime Press
ESPN Books
VOICE
19231966: Walt Disney
19661971: Roy O. Disney
19681972: Donn Tatum
19711977: Card Walker
19801984: Ron W. Miller
19841994: Frank Wells
19951997: Michael Ovitz
20002012: Robert Iger
19291971: Roy O. Disney
19711976: Donn Tatum
19761983: Card Walker
19831984: Ron W. Miller
19842005: Michael Eisner
2005present: Robert Iger
19451960: Walt Disney
19451971: Roy O. Disney (Co-Chair 19451960)
19711980: Donn Tatum
19801983: Card Walker
19831984: Raymond Watson
19842004: Michael Eisner
20042006: George J. Mitchell
20072012: John E. Pepper, Jr.
2012present: Robert Iger
19842003: Roy E. Disney
19992000: Sanford Litvack (Co-Vice Chair)
19841994: Frank Wells
19971999: Sanford Litvack[30] (Acting Chief of Operations)
20002005: Robert Iger
Walt Disney
A Trip Through the Walt Disney Studios
Disney animated feature film source material
Disney Channel
Disney Junior
Disney University
Disney XD
Disneyfication
Disneynature
List of Walt Disney TV series
List of assets owned by Disney
List of channels owned by Disney
List of Disney animated characters
List of Disney animated shorts
List of Disney theatrical animated features
List of Disney theatrical feature films
List of Disney home entertainment
List of Disney live-action shorts
List of Disney video games by genre
Children's television series
Second screen apps from Disney
American Broadcasting Company
Building a Company: Roy O. Disney and the Creation of an Entertainment Empire, Bob Thomas, 1998
Building a Dream; The Art of Disney Architecture, Beth Dunlop, 1996, ISBN 0-8109-3142-7
Cult of the Mouse: Can We Stop Corporate Greed from Killing Innovation in America?, Henry M. Caroselli, 2004, Ten Speed Press
Disney: The Mouse Betrayed, Peter Schweizer
The Disney Touch: How a Daring Management Team Revived an Entertainment Empire, by Ron Grover (Richard D. Irwin, Inc., 1991), ISBN 1-55623-385-X
The Disney Version: The Life, Times, Art and Commerce of Walt Disney, Richard Schickel, 1968, revised 1997
Disneyana: Walt Disney Collectibles, Cecil Munsey, 1974
Disneyization of Society: Alan Bryman, 2004
DisneyWar, James B. Stewart, Simon & Schuster, 2005, ISBN 0-684-80993-1
Donald Duck Joins Up; the Walt Disney Studio During World War II, Richard Shale, 1982
How to Read Donald Duck: Imperialist Ideology in the Disney Comic ISBN 0-88477-023-0 (Marxist Critique) Ariel Dorfman, Armand Mattelart, David Kunzle (translator).
Inside the Dream: The Personal Story of Walt Disney, Katherine Greene & Richard Greene, 2001
The Keys to the Kingdom: How Michael Eisner Lost His Grip, Kim Masters (Morrow, 2000)
The Man Behind the Magic; the Story of Walt Disney, Katherine & Richard Greene, 1991, revised 1998, ISBN 0-7868-5350-6
Married to the Mouse, Richard E. Foglesorg, Yale University Press.
Mouse Tales: A Behind-the-Ears Look at Disneyland, David Koenig, 1994, revised 2005, ISBN 0-9640605-4-X
Mouse Tracks: The Story of Walt Disney Records, Tim Hollis and Greg Ehrbar, 2006, ISBN 1-57806-849-5
Storming the Magic Kingdom: Wall Street, the raiders, and the battle for Disney, John Taylor, 1987 New York Times
The Story of Walt Disney, Diane Disney Miller & Pete Martin, 1957
Team Rodent, Carl Hiaasen.
Walt Disney: An American Original, Bob Thomas, 1976, revised 1994, ISBN 0-671-22332-1
Work in Progress by Michael Eisner with Tony Schwartz (Random House, 1998), ISBN 978-0-375-50071-8
Official website
Official blog
Portrait of Walt Disney Company at independent data base mediadb.eu
.
#(`*General Electric*`)#.
General Electric Company, or GE (NYSE:GE), is an American multinational conglomerate corporation incorporated in Schenectady, New York and headquartered in Fairfield, Connecticut, United States.[1][4] The company operates through four segments: Energy, Technology Infrastructure, Capital Finance and Consumer & Industrial.[5][6]
In 2011, GE ranked among the Fortune 500 as the 6th largest firm in the U.S. by gross revenue,[7] as well as the 14th most profitable.[8] However, the company is currently listed the 3rd largest in the world among the Forbes Global 2000, further metrics being taken into account.[9] Other rankings for 2011/2012 include No. 7 company for leaders (Fortune), No. 5 best global brand (Interbrand), No. 63 green company (Newsweek), No. 15 most admired company (Fortune), and No. 19 most innovative company (Fast Company).[10]
By 1890, Thomas Edison had brought together several of his business interests under one corporation to form Edison General Electric. At about the same time, Charles Coffin, leading Thomson-Houston Electric Company, acquired a number of competitors and gained access to their key patents.
General Electric was formed by the 1892 merger of General Electric of Schenectady, New York and Thomson-Houston Electric Company of Lynn, Massachusetts. Both plants continue to operate under the GE banner to this day.[11] The company was incorporated in New York, with the Schenectady plant used as headquarters for many years thereafter. Around the same time, General Electric's Canadian counterpart, Canadian General Electric, was formed.
In 1896, General Electric was one of the original 12 companies listed on the newly formed Dow Jones Industrial Average. After 116 years, it is the only one of the original companies still listed on the Dow index, although it has not been on the DOW index continuously.[12]
In 1911 General Electric absorbed the National Electric Lamp Association (NELA) into its lighting business. GE established its lighting division headquarters at Nela Park in East Cleveland, Ohio. Nela Park is still the headquarters for GE's lighting business.
The Radio Corporation of America (RCA) was founded by GE in 1919 to further international radio. GE used RCA as its retail arm for radio sales from 1919, when GE began production, until separation in 1930.[13] RCA would quickly grow into an industrial giant of its own.
GE's long history of working with turbines in the power-generation field gave them the engineering know-how to move into the new field of aircraft turbosuperchargers. Led by Sanford Alexander Moss, GE introduced the first superchargers during World War I, and continued to develop them during the Interwar period. Superchargers became indispensable in the years immediately prior to World War II, and GE was the world leader in exhaust-driven supercharging when the war started. This experience, in turn, made GE a natural selection to develop the Whittle W.1 jet engine that was demonstrated in the United States in 1941. Although their early work with Whittle's designs was later handed to Allison Engine Company, GE Aviation emerged as one of the world's largest engine manufacturers, second only to the well-founded and older British company, Rolls-Royce plc, which led the way in the design and manufacture of innovative, reliable, efficient, high-performance, heavy-duty jet engines.
In 2002, GE acquired the windpower assets of Enron during its bankruptcy proceedings.[14] Enron Wind was the only surviving U.S. manufacturer of large wind turbines at the time, and GE increased engineering and supplies for the Wind Division and doubled the annual sales to $1.2billion in 2003.[15] It acquired ScanWind in 2009.[16]
Some consumers boycotted GE light bulbs, refrigerators and other products in the 1980s and 1990s to protest GEs role in nuclear weapons production.[17]
GE was one of the eight major computer companies during the 1960s with IBM, the largest, called "Snow White" followed by the "Seven Dwarfs": Burroughs, NCR, Control Data Corporation, Honeywell, RCA, UNIVAC and GE.
GE had an extensive line of general purpose and special purpose computers. Among them were the GE 200, GE 400, and GE 600 series general purpose computers, the GE 4010, GE 4020, and GE 4060 real time process control computers, the Datanet 30 and Datanet 355 message switching computers (Datanet 30 and 355 were also used as front end processors for GE mainframe computers). A Datanet 500 computer was designed, but never sold.
In 1962, GE started developing its GECOS (later renamed GCOS) operating system, originally for batch processing, but later extended to timesharing and transaction processing. Versions of GCOS are still in use today.
In 19641969, GE and Bell Laboratories (which soon dropped out) joined with MIT to develop the pioneering and influential Multics operating system on the GE 645 mainframe computer. The project took longer than expected and was not a major commercial success, but it demonstrated important concepts such as single level store, dynamic linking, hierarchical file system, and ring-oriented security. Active development of Multics continued until 1985.
It has been said that GE got into computer manufacturing because in the 1950s they were the largest user of computers outside of the United States federal government, aside from being the first business in the world to own a computer and its electronics manufacturing plant "Appliance Park" was the first non-governmental site to host one.[18]. However, in 1970, GE sold its computer division to Honeywell, exiting the computer manufacturing industry, though it retained its timesharing operations for some years afterwards. GE was a major provider of computer timesharing services, through General Electric Information Services (GEIS, now GXS), offering online computing services that included GEnie.
In 1986 GE reacquired RCA, primarily for the NBC television network (also parent of Telemundo Communications Group). The remainder was sold to various companies, including Bertelsmann (Bertelsmann acquired RCA Records) and Thomson SA which traces its roots to Thomson-Houston, one of the original components of GE.
Also in 1986, Kidder, Peabody & Co., a U.S.-based securities firm, was sold to GE and following heavy losses was subsequently sold to PaineWebber in 1994.[19]
In 2002, Francisco Partners and Norwest Venture Partners acquired a division of GE called GE Information Systems (GEIS). The new company, named GXS, is based in Gaithersburg, Maryland. GXS is a leading provider of B2B e-Commerce solutions. GE maintains a minority ownership position in GXS.
Also in 2002, GE Wind Energy was formed when GE bought the wind turbine manufacturing assets of Enron Wind after the Enron scandals.[15][20][21]
In 2004, GE bought 80% of Universal Pictures from Vivendi. Vivendi bought 20% of NBC forming the company NBCUniversal. GE then owned 80% of NBC Universal and Vivendi owned 20%. As of January 28, 2011 GE owns 49% and Comcast 51%.
In 2004, GE completed the spin-off of most of its mortgage and life insurance assets into an independent company, Genworth Financial, based in Richmond, Virginia.
Genpact formerly known as GE Capital International Services (GECIS) was established by GE in late 1997 as its captive India based BPO. GE sold 60% stake in Genpact to General Atlantic and Oak Hill Capital Partners in 2005 and hived off Genpact into an independent business. GE is still a major client to Genpact getting its services in customer service, finance, information technology and analytics.
GE Plastics was sold in 2007 to SABIC.
In May 2007, GE acquired Smiths Aerospace for $4.8billion.
In May 2008, GE announced it was exploring options for divesting the bulk of its Consumer and Industrial business.
General Electric's Schenectady, New York facilities (including GE's original headquarters) are assigned the ZIP code 12345. (All Schenectady ZIP codes begin with 123, but no others begin with 1234.)
On December 3, 2009, it was announced that NBCUniversal will become a joint venture between GE and cable television operator Comcast. The cable giant will hold a controlling interest in the company, while GE retains a 49% stake and will buy out shares currently owned by Vivendi.[22]
Vivendi will sell its 20% stake in NBCUniversal to GE for US$5.8billion. Vivendi will sell 7.66% of NBCUniversal to GE for US$2billion if the GE/Comcast deal is not completed by September 2010 and then sell the remaining 12.34% stake of NBCUniversal to GE for US$3.8billion when the deal is completed or to the public via an IPO if the deal is not completed.[23][24]
On March 1, 2010, General Electric (GE) announced that the company is planning to sell its 20.85% stake in Turkey-based Garanti Bank.[25]
In August 2010, GE Healthcare signed a strategic partnership to bring cardiovascular Computed Tomography (CT) technology from start-up Arineta Ltd. of Israel to the hospital market.[26]
In October 2010, General Electric acquired gas engines manufacture Dresser Inc. for a $3billion deal and also bought a $1.6billion portfolio of retail credit cards from Citigroup Inc. This is the first major deal since the start of the financial crisis.[27]
On October 14, 2010, GE announced acquisition of data migration & SCADA simulation specialists Opal Software.[28]
December 2010: For the second times of this year (after Dresser acquisition), General Electric Co. buy oil sector company British Wellstream Holding Plc. an oil drilling pipe maker for 800 million pounds ($1.3billion).[29]
February 2011: The company has agreed to buy the well-support division of John Wood Group Plc for about $2.8billion. It is another aggressive moves recently of GE Oil & Gas made GE's acquisition was the largest of oil-service unit world wide in 2010.[30]
March 2011: GE announced it has completed the acquisition of privately held Lineage Power Holdings, Inc., from The Gores Group, LLC. [31]
GE Capital sold its $2billion dollar Mexican assets to Santander for $162million and exit the business in Mexico. Santander will additionally assume the portfolio debts of GE Capital in the country. The transaction will be finished at first half of 2011. GE Capital will focus in the core business and will shed its non-core assets.[32] In June 2012,CEO and President of GE said that the company would invest 300 crores to accelerate its businesses in Karnataka.[33]
In October 2012, General Electric Company acquired $7 billion worth of bank deposits from Metlife Inc.[34]
GE is a multinational conglomerate headquartered in Fairfield, Connecticut. Its New York main offices are located at 30 Rockefeller Plaza in Rockefeller Center, known as the GE Building for the prominent GE logo on the roof. NBC's headquarters and main studios are also located in the building. Through its RCA subsidiary, it has been associated with the Center since its construction in the 1930s.
The company describes itself as composed of a number of primary business units or "businesses." Each unit is itself a vast enterprise, many of which would, even as a standalone company, rank in the Fortune 500[citation needed]. The list of GE businesses varies over time as the result of acquisitions, divestitures and reorganizations. GE's tax return is the largest return filed in the United States; the 2005 return was approximately 24,000 pages when printed out, and 237 megabytes when submitted electronically.[35] The company also "spends more on U.S. lobbying than any other company."[36]
In 2005 GE launched its "Ecomagination" initiative in an attempt to position itself as a "green" company. GE is currently one of the biggest players in the wind power industry, and it is also developing new environment-friendly products such as hybrid locomotives, desalination and water reuse solutions, and photovoltaic cells. The company "plans to build the largest solar-panel-making factory in the U.S.,"[36] and has set goals for its subsidiaries to lower their greenhouse gas emissions.[37]
On May 21, 2007, GE announced it would sell its GE Plastics division to petrochemicals manufacturer SABIC for net proceeds of $11.6billion. The transaction took place on August 31, 2007, and the company name changed to SABIC Innovative Plastics, with Brian Gladden as CEO.[38]
Jeffrey Immelt is the current chairman of the board and chief executive officer of GE. He was selected by GE's Board of Directors in 2000 to replace John Francis Welch Jr. (Jack Welch) following his retirement. Previously, Immelt had headed GE's Medical Systems division (now GE Healthcare) as its President and CEO.
His tenure as the Chairman and CEO started at a time of crisis he took over the role on September 7, 2001[39] four days before the terrorist attacks on the United States, which killed two employees and cost GE's insurance business $600million as well as having a direct effect on the company's Aircraft Engines sector. Immelt has also been selected as one of President Obama's financial advisors concerning the economic rescue plan.
GE's divisions include GE Capital, GE Energy, GE Technology Infrastructure, and GE Home & Business Solutions
Through these businesses, GE participates in a wide variety of markets including the generation, transmission and distribution of electricity (e.g. nuclear, gas and solar), lighting, industrial automation, medical imaging equipment, motors, railway locomotives, aircraft jet engines, and aviation services. It co-owns NBCUniversal with Comcast. Through GE Commercial Finance, GE Consumer Finance, GE Equipment Services, and GE Insurance it offers a range of financial services as well. It has a presence in over 100 countries.
GE also produces General Imaging digital cameras.[41] In 2010, General Imaging released the Bridge Camera GE X5 with 14MP and 15x optical zoom.[42] In 2011, it is replaced by 16MP GE X500 with optional red color in Japan besides traditional black or white color in world wide.[43]
Since over half of GE's revenue is derived from financial services, it is arguably a financial company with a manufacturing arm. It is also one of the largest lenders in countries other than the United States, such as Japan. Even though the first wave of conglomerates (such as ITT Corporation, Ling-Temco-Vought, Tenneco, etc.) fell by the wayside by the mid-1980s, in the late 1990s, another wave (consisting of Westinghouse, Tyco, and others) tried and failed to emulate GE's success.
It was announced on May 4, 2008 that GE would auction off its appliances business for an expected sale of $58billion.[44] However, this plan fell through as a result of the recession.
In 2011, Fortune ranked GE the 6th largest firm in the U.S.,[7] as well as the 14th most profitable.[8] Other rankings for 2011/2012 include the following:[10]
For 2010, GE's brand was valued at $42.8 billion.[45] CEO Jeffrey Immelt had a set of changes in the presentation of the brand commissioned in 2004, after he took the reins as chairman, to unify the diversified businesses of GE. The changes included a new corporate color palette, small modifications to the GE logo, a new customized font (GE Inspira), and a new slogan, "imagination at work" replacing the longtime slogan "We Bring Good Things to Life", composed by David Lucas. The standard requires many headlines to be lowercased and adds visual "white space" to documents and advertising to promote an open and approachable company. The changes were designed by Wolff Olins and are used extensively on GE's marketing, literature and website.
General Electric's formal organizational structure involves a hierarchal system as Max Weber, a German sociologist, describes as a form of bureaucracy that follows general rules of super and subordination.[46] Shareowners, the Corporate Executive Office, and the Board of Directors make up the top of this structure. The Chairman and Chief Executive Officer of the Company, Jeffrey R. Immelt, acts as the intermediary between the top of the structure and the presidents of the seven sectors.[47] Evidently, GE appears as a small organization in the shell of a large company. Maintaining a strict sense of hierarchal control, GE operates in large part like a small, uncluttered business in which employees have the ability to move the company forward without any bureaucratic roadblocks.
While limiting the hindering effects of a bureaucracy that require methodical provisions made for regular assignments and the businesss daily operations,[46] both Welch and Immelt have torn down the boundaries between management layers[48] that complicated simple matters and slowed down the company.
As of 2012, there are 191 senior executives who serve as "growth leaders" at head of GE's different sectors.[49] Unlike the term manager, leaders work to inspire and motivate others to perform their best. These leaders are expected to work and talk to their employees and promote the corporate culture of creating and bringing ideas to life. Establishing the corporate culture serves as the unifying force for GEs many businesses around the world. The corporate culture inspires people to move the world forward with GE.
Middle managers at GE serve as the facilitators between the executive leaders and the floor employees.[50] They have to be able to excite and praise people as well as push great ideas forward. In a company as large as GE, it is essential that middle managers not only prove excellence in production but also are good team players who listen and work with employees. As sociologist Frederick W. Taylor would note, this approach in which the initiative of the employees is coupled with the work of the managers exhibits scientific management,[51] which proves more efficient than the motives of the two to be independent.
John Welch wanted to make employees feel like they were an essential part of the business.[52] John Welch developed a program called Work-Out, which was designed to foster, capture, and implement good ideas, regardless of their origin and effectively empower workers by encouraging them to speak-up in town-hall meetings.[52] Evidently, by treating the workers as an integral part of the company, Welch was able to strengthen the GEs businesses. The task of increasing productivity was no longer left to managers but now people on the factory floors. Elton Mayo describes in a study in which factory managers interviewed workers for their input, how employees are thrilled to be listened to.[53] In the essence of the human relations school, the interviews allowed the workers to feel that their thoughts directly contributed to the success of the company. The system did not contradict the hierarchal system in place but rather increased involvement as employees were now more committed to their productivity.
In a company with hundreds of thousands of employees, the position of the CEO is to appoint the best people on the largest opportunities, providing them with the best resources and to get out of their way. Executive leaders are then responsible to meet with thousands of employees throughout the company to construct a plan early in the year and establish goals and tasks for the middle managers.[54] Such a system fully integrates the workings and culture of a large company from the executive leaders to the floor employees.
The design of the compensation program at GE is constructed to support the leadership development and long-term emphasis of the company.[54] The compensation system doesn't operate on a strict formula, but with a mix of cash and equity in which GE incentivizes innovation and progress at all levels.
GE has always taken a long-term view through its more than 130-year history and keeps the development of its leaders as one of the hallmarks of the Company.[55] It established a deeply rooted internal labor market in which thousands of people from every level of the company are trained at the John F. Welch Leadership Center.[49] As meritocracy is valued at GE, the company works to transform employees into leaders from within. GEs commitment to learning and progressing leaders allows the company to ingrain its values in its executives.
Performance measurement and compensation are aligned at GE. In performance evaluations, GE executives particularly focus on ones ability to balance risk and return and deliver long-term results for shareowners.[55] The corporate culture to think boldly is also evident in the way GE evaluates its own managers, as employees are evaluated on their ability to reflect the companys guiding principles. By not making it completely a numbers game, managers are encouraged to innovate and progress the business of GE in new ways.
General Electric buys dozens of firms every year. The acquisitions help GE to obtain new technologies and enter new markets.[56] The Company acts swiftly in integrating target aspects of acquired companies into the GE culture and releases parts it does not need. GE is a global company with 60% of its business and 54% of its employees operating overseas and uses local markets to fuel both its employment.[57] While it began in America, it is essential for GE to operate in the global market to remain competitive and achieve production efficiency.
The six reactors in the 2011 Fukushima I Nuclear Power Plant catastrophe had been designed by General Electric. Their design had been criticised as far back as 1972.[58]
In March 2011, The New York Times reported that, despite earning $14.2billion in worldwide profits, including more than $5billion from U.S. operations, General Electric did not owe taxes in 2010. General Electric had a tax benefit of $3.2billion. This same article also pointed out that GE has reduced its American workforce by one fifth since 2002.[59]
In December 2011, the non-partisan organization Public Campaign criticized General Electric for spending $84.35 million on lobbying and not paying any taxes during 20082010, instead getting $4.7 billion in tax rebates, despite making a profit of $10.4 billion, laying off 4,168 workers since 2008, and increasing executive pay by 27% to $75.9 million in 2010 for the top 5 executives.[60]
GE has a history of some of its activities giving rise to large-scale air and water pollution. Based on year 2000 data,[61] researchers at the Political Economy Research Institute listed the corporation as the fourth-largest corporate producer of air pollution in the United States, with more than 4.4million pounds per year (2,000 tons) of toxic chemicals released into the air.[62] GE has also been implicated in the creation of toxic waste. According to EPA documents, only the United States Government, Honeywell, and Chevron Corporation are responsible for producing more Superfund toxic waste sites.[63]
In 1983, New York State Attorney General Robert Abrams filed suit in the United States District Court for the Northern District of New York to compel GE to pay for the cleanup of what was claimed to be more than 100,000 tons of chemicals dumped (legally, at the time) from their plant in Waterford, New York.[64] In 1999, the company agreed to pay a $250million settlement in connection with claims it polluted the Housatonic River (Pittsfield, Massachusetts) and other sites with polychlorinated biphenyls (PCBs) and other hazardous substances.[65]
From approximately 1947 to 1977, GE discharged as much as 1.3million pounds of PCBs from its capacitor manufacturing plants at the Hudson Falls and Fort Edward upstate New York facilities into the Hudson River.[66] GE fought a media and political battle to avoid cleaning up the river and countered that dredging the river would actually stir up PCBs.[67] In 2002, GE was ordered to clean up a 40-mile (64km) stretch of the Hudson River it had contaminated.[68]
In 2003, acting on concerns that the plan proposed by GE did not "provide for adequate protection of public health and the environment," the United States Environmental Protection Agency issued a unilateral administrative order for the company to "address cleanup at the GE site" in Rome, Georgia, also contaminated with PCBs.[69]
The nuclear reactors involved in the 2011 crisis at Fukushima I in Japan are GE designs,[70] and the architectural designs were done by Ebasco,[71] formerly owned by GE. Concerns over the design and safety of these reactors were raised as early as 1975.[72]
On June 6, 2011, GE announced that it has licensed solar thermal technology from California-based eSolar for use in power plants that use both solar and natural gas.[73]
On May 26, 2011, GE unveiled its EV Solar Carport, a carport that incorporates solar panels on its roof, with electric vehicle charging stations under its cover.[74]
In May 2005 GE announced the launch of a program called "Ecomagination," intended, in the words of CEO Jeff Immelt "to develop your bowls tomorrow's solutions such as solar energy, hybrid locomotives, fuel cells, lower-emission aircraft engines, lighter and stronger durable materials, efficient lighting, and water purification technology.[75] The announcement prompted an op-ed piece in The New York Times to observe that, "while General Electric's increased emphasis on clean technology will probably result in improved products and benefit its bottom line, Mr. Immelt's credibility as a spokesman on national environmental policy is fatally flawed because of his company's intransigence in cleaning up its own toxic legacy."[76]
GE has said that it will invest $1.4billion in clean technology research and development in 2008 as part of its Ecomagination initiative. As of October 2008, the scheme had resulted in 70 green products being brought to market, ranging from halogen lamps to biogas engines. In 2007, GE raised the annual revenue target for its Ecomagination initiative from $20billion in 2010 to $25billion following positive market response to its new product lines.[77] In 2010, GE continued to raise its investment by adding $10billion into Ecomagination over the next five years.[78]
GE (General Electric) Energy's renewable energy business has expanded greatly, to keep up with growing U.S. and global demand for clean energy. Since entering the renewable energy industry in 2002, GE has invested more than $850million in renewable energy technology. In August 2008 it acquired Kelman Ltd,[79] a Northern Ireland company specializing in advanced monitoring and diagnostics technologies for transformers used in renewable energy generation, and announced an expansion of its business in Northern Ireland in May 2010.[80] In 2009, GE's renewable energy initiatives, which include solar power, wind power and GE Jenbacher gas engines using renewable and non-renewable methane-based gases,[81] employ more than 4,900 people globally and have created more than 10,000 supporting jobs.[82]
GE Energy and Orion New Zealand Limited (Orion) have announced implementation of the first phase of a GE network management system to help improve power reliability for customers. GE's ENMAC Distribution Management System is the foundation of Orion's initiative. The system of smart grid technologies will significantly improve the network company's ability to manage big network emergencies and help it to restore power faster when outages occur.
GE unveiled a 40W replacement Energy Smart LED bulb, to be available late 2010 or early 2011. The company claims that the new LED bulb will provide a 77% energy savings and produce nearly the same light output as a 40W incandescent bulb, while lasting more than 25 times as long.[83]
GE Healthcare is collaborating with The Wayne State University School of Medicine and the Medical University of South Carolina to offer an integrated radiology curriculum during their respective MD Programs led by investigators of the Advanced Diagnostic Ultrasound in Microgravity study.[84] GE has donated over one million dollars of Logiq E Ultrasound equipment to these two institutions.[85]
On August 4, 2009 the SEC fined General Electric $50million for breaking accounting rules in two separate cases, misleading investors into believing GE would meet or beat earnings expectations.[86]
GE has faced criminal action regarding its defense related operations. GE was convicted in 1990 of defrauding the US Department of Defense, and again in 1992 on charges of corrupt practices in the sale of jet engines to Israel.[87]
GE was the focus of a 1991 short subject Academy Award-winning documentary, Deadly Deception: General Electric, Nuclear Weapons, and Our Environment,[88] that juxtaposed GE's "We Bring Good Things To Life" commercials with the true stories of workers and neighbors whose lives have been affected by the company's activities involving nuclear weapons.[89]
In the early 1950s Kurt Vonnegut was a writer for General Electric. A number of his novels and stories (notably Cat's Cradle) refer to the fictional city of Ilium, which appears to be loosely based on Schenectady, New York.[citation needed] The Ilium Works is the setting for the short story "Deer in the Works".
The NBC sitcom 30 Rock is set at the NBC Studios in the GE Building at 30 Rockefeller Center (hence the title). All characters in the main cast are NBC (and therefore GE) employees, and one character, Jack Donaghy (Alec Baldwin) is portrayed as having risen through the ranks of GE management to become Vice President of NBC's East Coast operations through the company's microwave oven division.

1 History

1.1 Formation
1.2 Public company
1.3 RCA
1.4 Power generation
1.5 Computing
1.6 Acquisitions


1.1 Formation
1.2 Public company
1.3 RCA
1.4 Power generation
1.5 Computing
1.6 Acquisitions
2 Corporate affairs

2.1 CEO
2.2 Businesses
2.3 Corporate recognition and rankings


2.1 CEO
2.2 Businesses
2.3 Corporate recognition and rankings
3 Organizational structure and workforce management

3.1 Formal organization
3.2 Management strategies

3.2.1 Executive leaders
3.2.2 Middle managers
3.2.3 Floor employees


3.3 Methods of employee management

3.3.1 Responsibility and task management
3.3.2 Compensation and incentives
3.3.3 Promotion and training
3.3.4 Performance evaluations


3.4 General Electric as a conglomerate


3.1 Formal organization
3.2 Management strategies

3.2.1 Executive leaders
3.2.2 Middle managers
3.2.3 Floor employees


3.2.1 Executive leaders
3.2.2 Middle managers
3.2.3 Floor employees
3.3 Methods of employee management

3.3.1 Responsibility and task management
3.3.2 Compensation and incentives
3.3.3 Promotion and training
3.3.4 Performance evaluations


3.3.1 Responsibility and task management
3.3.2 Compensation and incentives
3.3.3 Promotion and training
3.3.4 Performance evaluations
3.4 General Electric as a conglomerate
4 Recent controversies
5 Environmental record

5.1 Pollution
5.2 Environmental initiatives


5.1 Pollution
5.2 Environmental initiatives
6 Educational initiatives
7 Legal issues
8 Notable appearances in media
9 See also
10 References
11 Further reading
12 External links

12.1 Video clips


12.1 Video clips
1.1 Formation
1.2 Public company
1.3 RCA
1.4 Power generation
1.5 Computing
1.6 Acquisitions
2.1 CEO
2.2 Businesses
2.3 Corporate recognition and rankings
3.1 Formal organization
3.2 Management strategies

3.2.1 Executive leaders
3.2.2 Middle managers
3.2.3 Floor employees


3.2.1 Executive leaders
3.2.2 Middle managers
3.2.3 Floor employees
3.3 Methods of employee management

3.3.1 Responsibility and task management
3.3.2 Compensation and incentives
3.3.3 Promotion and training
3.3.4 Performance evaluations


3.3.1 Responsibility and task management
3.3.2 Compensation and incentives
3.3.3 Promotion and training
3.3.4 Performance evaluations
3.4 General Electric as a conglomerate
3.2.1 Executive leaders
3.2.2 Middle managers
3.2.3 Floor employees
3.3.1 Responsibility and task management
3.3.2 Compensation and incentives
3.3.3 Promotion and training
3.3.4 Performance evaluations
5.1 Pollution
5.2 Environmental initiatives
12.1 Video clips
#7 company for leaders (Fortune)
#5 best global brand (Interbrand)
#82 green company (Newsweek)
#15 most admired company (Fortune)
#19 most innovative company (Fast Company).
Edison Engineering Development Program
GE Aviation
GE Building
GE Capital Finance
GE Energy
GE Home & Business Solutions
GE Global Research
GE Technology Infrastructure
Knolls Atomic Power Laboratory, Schenectady, NY
List of assets owned by General Electric
Phoebus cartel
Top 100 US Federal Contractors
Carlson, W. Bernard. Innovation as a Social Process: Elihu Thomson and the Rise of General Electric, 18701900 (Cambridge: Cambridge University Press, 1991).
Woodbury, David O. Elihu Thomson, Beloved Scientist (Boston: Museum of Science, 1944)
Haney, John L. The Elihu Thomson Collection American Philosophical Society Yearbook 1944.
Hammond, John W. Men and Volts: The Story of General Electric, published 1941, 436 pages.
Mill, John M. Men and Volts at War: The Story of General Electric in World War II, published 1947.
General Electric corporate page
Official site for GE Appliances, the corporation's leading consumer product
SEC filings including 10-k
Official site for SABIC Innovative Plastics.
Financial Times article on GE's effort to go green
GE energy
G.E. to Double Investments in Renewables to $4B
USA Today article about GE's success at creating CEOs at large companies
GE Reports YouTube channel
General Electric at Google Finance
General Electric at Yahoo! Finance
General Electric at Hoover's
General Electric at Reuters
General Electric SEC filings at EDGAR Online
General Electric SEC filings at the Securities and Exchange Commission
.
#(`*Peter Rost (doctor)*`)#.
Peter Rost, M.D. (born 1959 in Gothenburg, Sweden) is a former vice president at the pharmaceutical company Pfizer and most well known for testifying in the United States Congress against the business methods of the pharmaceutical industry. He is also an author of the insider books, The Whistleblower: Confessions of a Healthcare Hitman and Killer Drug. Rost provides services as a pharmaceutical marketing expert witness,[1] speaker[2] and writer.[3]
Rost is the author of The Whistleblower, Confessions of a Healthcare Hitman.[4] The Whistleblower is at once an unmasking of how corporations take care of malcontents and a gripping story of one man's fight to maintain his family and his sanity. Starting in 2003, the book details the illegal, even criminal business practices the author witnessed at his corporation, as well as his crusade to legalize the reimportation of drugs.
Rost first became well known in 2004 when he became the first drug company executive to speak out in favor of reimportation of drugs.
His fight for lower priced drugs was covered by radio and television broadcasts, among them 60 Minutes, and many newspaper articles, including The New York Times, The Wall Street Journal, The Washington Post and Los Angeles Times.
Rost is the author of a medical text book, Emergency Surgery, and has written numerous op-eds for The New York Times, Los Angeles Times and dozens of other major newspapers. He testified before the U.S. Senate, as well as many state congresses and conducted numerous press conferences with U.S. Senators, Members of U.S. Congress, and State Governors.
Rost was terminated[5] from his employment as Vice President for Pfizer in December 2005. After it became known that Rosts allegations had resulted in an ongoing criminal investigation of Pfizers marketing practices by the U.S. Department of Justice, Rost was nominated "Whiny Whistleblower of the Year 2005," by a drug company front organization; The American Council on Science and Health.[6] Rost competed for this title with two other consumer heroes; David Graham, FDA, and Eric Topol, the Cleveland Clinic. The American Council on Science and Health has accepted funding from Pfizer, Rost's prior employer.
Prior to his work for Pfizer, Rost was a Managing Director for Wyeth, responsible for the Nordic region in Europe. During this assignment he had four countries reporting to him and he doubled sales in the region in two years and moved the sales ranking of the Swedish affiliate from #19 to #7 by lowering drug prices. Later, working for Pharmacia, he tried to stop illegal promotion of the growth hormone Genotropin for off-label uses when the Pfizer acquisition occurred.[7]
Influential website The Huffington Post banned Peter Rost from blogging on this site in June 2006, after he discovered an anonymous heckler on his blog was actually the Post's technology manager. In his Huffington Post blog, Peter Rost exposed the identity of the heckler - known as a "troll" in blogging parlance - which prompted the Post to block his access.[8] The Huffington Post later changed its policy and disallowed employees to respond to blogs. The Post also removed "recommended replies," which Rost had shown could be abused.[9]
In September 2006, Rost's book, "The Whistleblower, Confessions of a Healthcare Hitman"[4] was published. The New York Post wrote, "A drug company executive is about to blow the lid off the pharmaceutical industry . . . revealing everything from sex in the corner office to private investigators spying on employees, company phone surveillance, FBI investigations and financial shenanigans resulting in million-dollar payouts." And the Library Journal wrote, "Here is a chilling tale from a former Pfizer exec about his decision to expose corporate activities, including takeovers and layoffs, physician payoffs, marketing to juvenile patients, and tax dodging. This blow-by-blow account amply supports his statement that the current U.S. healthcare system is certainly the best system for the drug companies."[4]
On April 2, 2007, the Department of Justice announced that "PHARMACIA & UPJOHN COMPANY, INC., a subsidiary of Pfizer, Inc., was charged today in federal court with offering a kickback in connection with its outsourcing contract for the administration and distribution of its human growth hormone product, Genotropin. The company has agreed to plead guilty to the charge and pay a criminal fine. Additionally, another Pfizer subsidiary, PHARMACIA & UPJOHN COMPANY LLC, has entered into a deferred prosecution agreement with the government for its illegal promotion of Genotropin for such off-label uses as anti-aging, cosmetic use and athletic performance enhancement. As a result of the criminal plea and deferred prosecution agreement, the companies will pay a total of $34.7 million."[10]
In July 2007 Rost published Killer Drug.[11] According to Amazon.com, "THE CIA on June 26, 2007 declassified secret documents that revealed "the Agency had relations with commercial drug manufacturers, whereby they passed on drugs rejected because of unfavorable side effects." KILLER DRUG is a thriller about one such drug company."[12]
In August 2007 Rost started to write a daily blog for BrandweekNRX[13] and a column for Realtid,[14] a Swedish online business newspaper. Later in 2007 Rost announced his new business venture, as a Pharmaceutical Marketing Expert Witness.[15]
Rost has been a resident of Short Hills, New Jersey.[16]
1 Background
2 See also
3 Notes
4 External links
Whistleblower
Pfizer
Wendell Potter
Peter Rost's personal blog
Interview at Guernica: a magazine of arts and politics
Video of Rost's speech to U.S. Congress
Letter from U.S. Congress on Rost to Pfizer
Press Release from U.S. Congress on Rost
Brandweek: Bad Medicine
BusinessWeek: Legal Tangle At The Fountain Of Youth
BioPharma: The perils of whistleblowing
Corporatewatch, Pfizer Inc. Corporate Crimes
.
#(`*Bill Gates*`)#.
William Henry "Bill" Gates III (born October 28, 1955)[4] is an American business magnate and philanthropist. Gates is the former chief executive and current chairman of Microsoft, the worlds largest personal-computer software company, which he co-founded with Paul Allen. He is consistently ranked among the world's wealthiest people[5] and was the wealthiest overall from 1995 to 2009, excluding 2008, when he was ranked third;[6] in 2011 he was the wealthiest American and the second wealthiest person.[7][8] During his career at Microsoft, Gates held the positions of CEO and chief software architect, and remains the largest individual shareholder, with 6.4 percent of the common stock.[9] He has also authored or co-authored several books.
Gates is one of the best-known entrepreneurs of the personal computer revolution. Gates has been criticized for his business tactics, which have been considered anti-competitive, an opinion which has in some cases been upheld by the courts.[10][11] In the later stages of his career, Gates has pursued a number of philanthropic endeavors, donating large amounts of money to various charitable organizations and scientific research programs through the Bill & Melinda Gates Foundation, established in 2000.[12]
Gates stepped down as chief executive officer of Microsoft in January 2000. He remained as chairman and created the position of chief software architect. In June 2006, Gates announced that he would be transitioning from full-time work at Microsoft to part-time work, and full-time work at the Bill & Melinda Gates Foundation. He gradually transferred his duties to Ray Ozzie, chief software architect, and Craig Mundie, chief research and strategy officer. Gates's last full-time day at Microsoft was June 27, 2008. He remains at Microsoft as non-executive chairman.
Gates was born in Seattle, Washington, to William H. Gates, Sr. and Mary Maxwell Gates. His ancestry includes English, German, and Scots-Irish.[13][14] His father was a prominent lawyer, and his mother served on the board of directors for First Interstate BancSystem and the United Way. Gates's maternal grandfather was J. W. Maxwell, a national bank president. Gates has one elder sister, Kristi (Kristianne), and one younger sister, Libby. He was the fourth of his name in his family, but was known as William Gates III or "Trey" because his father had the "II" suffix.[15] Early on in his life, Gates's parents had a law career in mind for him.[16] When Gates was young, his family regularly attended a Congregational church.[17][18][19]
At 13 he enrolled in the Lakeside School, an exclusive preparatory school.[20] When he was in the eighth grade, the Mothers Club at the school used proceeds from Lakeside School's rummage sale to buy a Teletype Model 33 ASR terminal and a block of computer time on a General Electric (GE) computer for the school's students.[21] Gates took an interest in programming the GE system in BASIC, and was excused from math classes to pursue his interest. He wrote his first computer program on this machine: an implementation of tic-tac-toe that allowed users to play games against the computer. Gates was fascinated by the machine and how it would always execute software code perfectly. When he reflected back on that moment, he said, "There was just something neat about the machine."[22] After the Mothers Club donation was exhausted, he and other students sought time on systems including DEC PDP minicomputers. One of these systems was a PDP-10 belonging to Computer Center Corporation (CCC), which banned four Lakeside studentsGates, Paul Allen, Ric Weiland, and Kent Evansfor the summer after it caught them exploiting bugs in the operating system to obtain free computer time.[23]
At the end of the ban, the four students offered to find bugs in CCC's software in exchange for computer time. Rather than use the system via Teletype, Gates went to CCC's offices and studied source code for various programs that ran on the system, including programs in FORTRAN, LISP, and machine language. The arrangement with CCC continued until 1970, when the company went out of business. The following year, Information Sciences, Inc. hired the four Lakeside students to write a payroll program in COBOL, providing them computer time and royalties. After his administrators became aware of his programming abilities, Gates wrote the school's computer program to schedule students in classes. He modified the code so that he was placed in classes with mostly female students. He later stated that "it was hard to tear myself away from a machine at which I could so unambiguously demonstrate success."[22] At age 17, Gates formed a venture with Allen, called Traf-O-Data, to make traffic counters based on the Intel 8008 processor.[24] In early 1973, Bill Gates served as a congressional page in the U.S. House of Representatives.[25]
Gates graduated from Lakeside School in 1973. He scored 1590 out of 1600 on the SAT[26] and enrolled at Harvard College in the autumn of 1973.[27] While at Harvard, he met Steve Ballmer, who later succeeded Gates as CEO of Microsoft.
In his sophomore year, Gates devised an algorithm for pancake sorting as a solution to one of a series of unsolved problems[28] presented in a combinatorics class by Harry Lewis, one of his professors. Gates's solution held the record as the fastest version for over thirty years;[28][29] its successor is faster by only one percent.[28] His solution was later formalized in a published paper in collaboration with Harvard computer scientist Christos Papadimitriou.[30]
Gates did not have a definite study plan while a student at Harvard[31] and spent a lot of time using the school's computers. Gates remained in contact with Paul Allen, and he joined him at Honeywell during the summer of 1974.[32] The following year saw the release of the MITS Altair 8800 based on the Intel 8080 CPU, and Gates and Allen saw this as the opportunity to start their own computer software company.[33] Gates dropped out of Harvard at this time.[34] He had talked this decision over with his parents, who were supportive of him after seeing how much Gates wanted to start a company.[31]
After reading the January 1975 issue of Popular Electronics that demonstrated the Altair 8800, Gates contacted Micro Instrumentation and Telemetry Systems (MITS), the creators of the new microcomputer, to inform them that he and others were working on a BASIC interpreter for the platform.[35] In reality, Gates and Allen did not have an Altair and had not written code for it; they merely wanted to gauge MITS's interest. MITS president Ed Roberts agreed to meet them for a demo, and over the course of a few weeks they developed an Altair emulator that ran on a minicomputer, and then the BASIC interpreter. The demonstration, held at MITS's offices in Albuquerque was a success and resulted in a deal with MITS to distribute the interpreter as Altair BASIC. Paul Allen was hired into MITS,[36] and Gates took a leave of absence from Harvard to work with Allen at MITS in Albuquerque in November 1975. They named their partnership "Micro-Soft" and had their first office located in Albuquerque.[36] Within a year, the hyphen was dropped, and on November 26, 1976, the trade name "Microsoft" was registered with the Office of the Secretary of the State of New Mexico.[36] Gates never returned to Harvard to complete his studies.
Microsoft's BASIC was popular with computer hobbyists, but Gates discovered that a pre-market copy had leaked into the community and was being widely copied and distributed. In February 1976, Gates wrote an Open Letter to Hobbyists in the MITS newsletter saying that MITS could not continue to produce, distribute, and maintain high-quality software without payment.[37] This letter was unpopular with many computer hobbyists, but Gates persisted in his belief that software developers should be able to demand payment. Microsoft became independent of MITS in late 1976, and it continued to develop programming language software for various systems.[36] The company moved from Albuquerque to its new home in Bellevue, Washington on January 1, 1979, after the former rejected his loan application.[35]
During Microsoft's early years, all employees had broad responsibility for the company's business. Gates oversaw the business details, but continued to write code as well. In the first five years, Gates personally reviewed every line of code the company shipped, and often rewrote parts of it as he saw fit.[38]
IBM approached Microsoft in July 1980 regarding its upcoming personal computer, the IBM PC.[39] The computer company first proposed that Microsoft write the BASIC interpreter. When IBM's representatives mentioned that they needed an operating system, Gates referred them to Digital Research (DRI), makers of the widely used CP/M operating system.[40] IBM's discussions with Digital Research went poorly, and they did not reach a licensing agreement. IBM representative Jack Sams mentioned the licensing difficulties during a subsequent meeting with Gates and told him to get an acceptable operating system. A few weeks later Gates proposed using 86-DOS (QDOS), an operating system similar to CP/M that Tim Paterson of Seattle Computer Products (SCP) had made for hardware similar to the PC. Microsoft made a deal with SCP to become the exclusive licensing agent, and later the full owner, of 86-DOS. After adapting the operating system for the PC, Microsoft delivered it to IBM as PC-DOS in exchange for a one-time fee of $50,000.[41]
Gates did not offer to transfer the copyright on the operating system, because he believed that other hardware vendors would clone IBM's system.[41] They did, and the sales of MS-DOS made Microsoft a major player in the industry.[42] Despite IBM's name on the operating system the press quickly identified Microsoft as being very influential on the new computer, with PC Magazine asking if Gates were "The Man Behind The Machine?"[39] He oversaw Microsoft's company restructuring on June 25, 1981, which re-incorporated the company in Washington state and made Gates President of Microsoft and the Chairman of the Board.[35]
Microsoft launched its first retail version of Microsoft Windows on November 20, 1985, and in August, the company struck a deal with IBM to develop a separate operating system called OS/2. Although the two companies successfully developed the first version of the new system, mounting creative differences caused the partnership to deteriorate. It ended in 1991, when Gates led Microsoft to develop a version of OS/2 independently from IBM.[43]
From Microsoft's founding in 1975 until 2006, Gates had primary responsibility for the company's product strategy. He aggressively broadened the company's range of products, and wherever Microsoft achieved a dominant position he vigorously defended it. He gained a reputation for being distant to others; as early as 1981 an industry executive complained in public that "Gates is notorious for not being reachable by phone and for not returning phone calls."[44] Another executive recalled that after he showed Gates a videogame and defeated him 35 of 37 times, when they met again a month later Gates "won or tied every game. He had studied the game until he solved it. That is a competitor."[45]
As an executive, Gates met regularly with Microsoft's senior managers and program managers. Firsthand accounts of these meetings describe him as verbally combative, berating managers for perceived holes in their business strategies or proposals that placed the company's long-term interests at risk.[46][47]
He often interrupted presentations with such comments as, "That's the stupidest thing I've ever heard!"[48] and, "Why don't you just give up your options and join the Peace Corps?"[49] The target of his outburst then had to defend the proposal in detail until, hopefully, Gates was fully convinced.[48] When subordinates appeared to be procrastinating, he was known to remark sarcastically, "I'll do it over the weekend."[50][51][52]
Gates's role at Microsoft for most of its history was primarily a management and executive role. However, he was an active software developer in the early years, particularly on the company's programming language products. He has not officially been on a development team since working on the TRS-80 Model 100,[53] but wrote code as late as 1989 that shipped in the company's products.[51] On June 15, 2006, Gates announced that he would transition out of his day-to-day role over the next two years to dedicate more time to philanthropy. He divided his responsibilities between two successors, placing Ray Ozzie in charge of day-to-day management and Craig Mundie in charge of long-term product strategy.[54]
Many decisions that led to antitrust litigation over Microsoft's business practices have had Gates's approval. In the 1998 United States v. Microsoft case, Gates gave deposition testimony that several journalists characterized as evasive. He argued with examiner David Boies over the contextual meaning of words such as, "compete", "concerned", and "we".[55] BusinessWeek reported:
Early rounds of his deposition show him offering obfuscatory answers and saying 'I don't recall,' so many times that even the presiding judge had to chuckle. Worse, many of the technology chief's denials and pleas of ignorance were directly refuted by prosecutors with snippets of e-mail that Gates both sent and received.[56]
Gates later said he had simply resisted attempts by Boies to mischaracterize his words and actions. As to his demeanor during the deposition, he said, "Did I fence with Boies? ... I plead guilty. Whatever that penalty is should be levied against me: rudeness to Boies in the first degree."[57] Despite Gates' denials, the judge ruled that Microsoft had committed monopolization and tying, and blocking competition, both in violation of the Sherman Antitrust Act.[57]
Gates appeared in a series of ads to promote Microsoft in 2008. The first commercial, co-starring Jerry Seinfeld, is a 90-second talk between strangers as Seinfeld walks up on a discount shoe store (Shoe Circus) in a mall and notices Gates buying shoes inside. The salesman is trying to sell Mr. Gates shoes that are a size too big. As Gates is buying the shoes, he holds up his discount card, which uses a slightly altered version of his own mugshot of his arrest in New Mexico in 1977 for a traffic violation.[58] As they are walking out of the mall, Seinfeld asks Gates if he has melded his mind to other developers, after getting a yes, he then asks if they are working on a way to make computers edible, again getting a yes. Some say that this is an homage to Seinfeld's own show about "nothing" (Seinfeld).[59] In a second commercial in the series, Gates and Seinfeld are at the home of an average family trying to fit in with normal people.
Since leaving day-to-day operations at Microsoft (where he remains Chairman[60]), Gates continues his philanthropy and, among other projects, purchased the video rights to the Messenger Lectures series called The Character of Physical Law, given at Cornell University by Richard Feynman in 1964 and recorded by the BBC. The videos are available online to the public at Microsoft's Project Tuva.[61][62]
In April 2010, Gates was invited to visit and speak at the Massachusetts Institute of Technology where he asked the students to take on the hard problems of the world in their futures.[63][64]
Gates married Melinda French on January 1, 1994. They have three children: daughters Jennifer Katharine (b. 1996) and Phoebe Adele (b. 2002) and son Rory John (b. 1999).
The family resides in the Gates's home, an earth-sheltered house in the side of a hill overlooking Lake Washington in Medina. According to King County public records, as of 2006 the total assessed value of the property (land and house) is $125million, and the annual property tax is $991,000.
His 66,000sqft (6,100m2) estate has a 60-foot (18m) swimming pool with an underwater music system, as well as a 2,500sqft (230m2) gym and a 1,000sqft (93m2) dining room.[65]
Also among Gates's private acquisitions is the Codex Leicester, a collection of writings by Leonardo da Vinci, which Gates bought for $30.8million at an auction in 1994.[66] Gates is also known as an avid reader, and the ceiling of his large home library is engraved with a quotation from The Great Gatsby.[67] He also enjoys playing bridge, tennis, and golf.[68][69]
Gates was number one on the Forbes 400 list from 1993 through to 2007 and number one on Forbes list of The World's Richest People from 1995 to 2007 and 2009. In 1999, his wealth briefly surpassed $101billion, causing the media to call Gates a "centibillionaire".[70] Despite his wealth and extensive business travel Gates usually flew coach until 1997, when he bought a private jet.[71] Since 2000, the nominal value of his Microsoft holdings has declined due to a fall in Microsoft's stock price after the dot-com bubble burst and the multi-billion dollar donations he has made to his charitable foundations. In a May 2006 interview, Gates commented that he wished that he were not the richest man in the world because he disliked the attention it brought.[72] Gates has several investments outside Microsoft, which in 2006 paid him a salary of $616,667 and $350,000 bonus totalling $966,667.[73] He founded Corbis, a digital imaging company, in 1989. In 2004 he became a director of Berkshire Hathaway, the investment company headed by long-time friend Warren Buffett.[74] In March 2010 Bill Gates was bumped down to the second wealthiest man behind Carlos Slim.
Gates began to appreciate the expectations others had of him when public opinion mounted suggesting that he could give more of his wealth to charity. Gates studied the work of Andrew Carnegie and John D. Rockefeller, and in 1994 sold some of his Microsoft stock to create the William H. Gates Foundation. In 2000, Gates and his wife combined three family foundations into one to create the charitable Bill & Melinda Gates Foundation, which is the largest transparently operated charitable foundation in the world.[75] The foundation allows benefactors access to information regarding how its money is being spent, unlike other major charitable organizations such as the Wellcome Trust.[76][77] The generosity and extensive philanthropy of David Rockefeller has been credited as a major influence. Gates and his father met with Rockefeller several times, and modeled their giving in part on the Rockefeller family's philanthropic focus, namely those global problems that are ignored by governments and other organizations.[78] As of 2007, Bill and Melinda Gates were the second-most generous philanthropists in America, having given over $28billion to charity.[79] They plan to eventually give 95% of their wealth to charity.[80]
The foundation was at the same time criticized because it invests assets that it has not yet distributed with the exclusive goal of maximizing return on investment. As a result, its investments include companies that have been charged with worsening poverty in the same developing countries where the Foundation is attempting to relieve poverty. These include companies that pollute heavily, and pharmaceutical companies that do not sell into the developing world.[81] In response to press criticism, the foundation announced in 2007 a review of its investments, to assess social responsibility.[82] It subsequently canceled the review and stood by its policy of investing for maximum return, while using voting rights to influence company practices.[83] The Gates Millennium Scholars program has been criticized for its exclusion of Caucasian students.[84][85]
Gates's wife urged people to learn a lesson from the philanthropic efforts of the Salwen family, which had sold its home and given away half of its value, as detailed in The Power of Half.[86] Gates and his wife invited Joan Salwen to Seattle to speak about what the family had done, and on December 9, 2010, Gates, investor Warren Buffett, and Mark Zuckerberg (Facebook's CEO) signed a promise they called the "Gates-Buffet Giving Pledge", in which they promised to donate to charity at least half of their wealth over the course of time.[87][88][89]
In 1987, Gates was listed as a billionaire in the pages of Forbes' 400 Richest People in America issue, just days before his 32nd birthday. As the world's youngest self-made billionaire, he was worth $1.25billion, over $900million more than he'd been worth the year before, when he'd debuted on the list.[90]
Time magazine named Gates one of the 100 people who most influenced the 20th century, as well as one of the 100 most influential people of 2004, 2005, and 2006. Time also collectively named Gates, his wife Melinda and U2's lead singer Bono as the 2005 Persons of the Year for their humanitarian efforts.[91] In 2006, he was voted eighth in the list of "Heroes of our time".[92] Gates was listed in the Sunday Times power list in 1999, named CEO of the year by Chief Executive Officers magazine in 1994, ranked number one in the "Top 50 Cyber Elite" by Time in 1998, ranked number two in the Upside Elite 100 in 1999 and was included in The Guardian as one of the "Top 100 influential people in media" in 2001.[93]
In 1994, he was honoured as the twentieth Distinguished Fellow of the British Computer Society. Gates has received honorary doctorates from Nyenrode Business Universiteit, Breukelen, The Netherlands, in 2000;[94] the Royal Institute of Technology, Stockholm, Sweden, in 2002;[95] Waseda University, Tokyo, Japan, in 2005; Tsinghua University, Beijing, China, in April 2007;[96] Harvard University in June 2007;[97] the Karolinska Institutet, Stockholm, in January 2008,[98] and Cambridge University in June 2009.[99] He was also made an honorary trustee of Peking University in 2007.[100] Gates was also made an honorary Knight Commander of the Order of the British Empire (KBE) by Queen Elizabeth II in 2005,[101] in addition to having entomologists name the Bill Gates flower fly, Eristalis gatesi, in his honor.[102]
In November 2006, he and his wife were awarded the Order of the Aztec Eagle for their philanthropic work around the world in the areas of health and education, particularly in Mexico, and specifically in the program "Un pas de lectores".[103] In October 2009, it was announced that Gates will be awarded the 2010 Bower Award for Business Leadership of The Franklin Institute for his achievements in business and for his philanthropic work. In 2010 he was honored with the Silver Buffalo Award by the Boy Scouts of America, its highest award for adults, for his service to youth.[104]
In 2011, Bill Gates was ranked as the fifth most powerful person in the world, according to rankings by Forbes magazine.[105]
To date, Bill Gates has authored two books. The Road Ahead, written with Microsoft executive Nathan Myhrvold and journalist Peter Rinearson, was published in November 1995, and it summarized the implications of the personal computing revolution and described a future profoundly changed by the arrival of a global information superhighway. Business @ the Speed of Thought was published in 1999, and discusses how business and technology are integrated, and shows how digital infrastructures and information networks can help getting an edge on the competition.
Gates has appeared in a number of documentaries, including the 2010 documentary film Waiting for "Superman",[106] and the BBC documentary series The Virtual Revolution.
Gates was prominently featured in Pirates of Silicon Valley, a 1999 film which chronicles the rise of Apple and Microsoft from the early 1970s to 1997. He was portrayed by Anthony Michael Hall.
1 Early life
2 Microsoft

2.1 BASIC
2.2 IBM partnership
2.3 Windows
2.4 Management style
2.5 Antitrust litigation
2.6 Appearance in ads


2.1 BASIC
2.2 IBM partnership
2.3 Windows
2.4 Management style
2.5 Antitrust litigation
2.6 Appearance in ads
3 Post-Microsoft
4 Personal life

4.1 Philanthropy
4.2 Recognition
4.3 Investments


4.1 Philanthropy
4.2 Recognition
4.3 Investments
5 Books and films
6 See also

6.1 Books


6.1 Books
7 References
8 Further reading
9 External links
2.1 BASIC
2.2 IBM partnership
2.3 Windows
2.4 Management style
2.5 Antitrust litigation
2.6 Appearance in ads
4.1 Philanthropy
4.2 Recognition
4.3 Investments
6.1 Books
Cascade Investments LLC, a private investment and holding company, incorporated in United States, is controlled by Bill Gates, and is headquartered in the city of Kirkland, Washington.
bgC3, a new think-tank company founded by Bill Gates.
Corbis, a digital image licensing and rights services company.
TerraPower, a nuclear reactor design company.
List of billionaires (also see List of college dropout billionaires and List of wealthiest non-inflated historical figures)
Paul Allen Microsoft's co-founder, friend, and fellow billionaire
Gary Kildall (October 25, 2004). "The Man Who Could Have Been Bill Gates". Bloomberg BusinessWeek. http://www.businessweek.com/magazine/content/04_43/b3905109_mz063.htm. Retrieved June 9, 2010.
Fridson, Martin (2001). How to Be a Billionaire: Proven Strategies from the Titans of Wealth. John Wiley & Sons. ISBN0-471-41617-7.
Gates, Bill (1996). The Road Ahead. Penguin Books. ISBN0-14-026040-4.
Lesinski, Jeanne M. (2006). Bill Gates (Biography (a & E)). A&E Television Networks. ISBN0-8225-7027-0.
Manes, Stephen (1994). Gates: How Microsoft's Mogul Reinvented an Industry and Made Himself The Richest Man in America. Touchstone Pictures. ISBN0-671-88074-8.
Wallace, James (1993). Hard Drive: Bill Gates and the Making of the Microsoft Empire. New York: HarperCollins Publishers. ISBN0-471-56886-4.
"The Meaning of Bill Gates: As his reign at Microsoft comes to an end, so does the era he dominated", The Economist, June 28, 2008.
Rivlin, Gary (1999). The plot to get Bill Gates: an irreverent investigation of the world's richest man...and the people who hate him. New York: Times Business. ISBN0-8129-3006-1.
"83 Reasons Why Bill Gates's Reign Is Over". Wired (Wired) 6 (12). December 1998. http://www.wired.com/wired/archive/6.12/microsoft.html.
Bank, David (2001). Breaking Windows: how Bill Gates fumbled the future of Microsoft. New York: Free Press. ISBN0-7432-0315-1.
Official website
Bill & Melinda Gates Foundation
Profile at Microsoft
Profile at Forbes
Appearances on C-SPAN
Bill Gates on Charlie Rose
Bill Gates at TED
Bill Gates at the Internet Movie Database
Bill Gates collected news and commentary at The Guardian
Bill Gates collected news and commentary at The New York Times
Bill Gates collected news and commentary at The Wall Street Journal
Bill Gates collected news and commentary at Bloomberg News
Works by or about Bill Gates in libraries (WorldCat catalog)
Works by Bill Gates on Open Library at the Internet Archive
Bill Gates at the Notable Names Database
How I Work: Bill Gates, Fortune, March 30, 2006
The Forbes 400
.
#(`*Steve Jobs*`)#.
Steven Paul "Steve" Jobs (/dbz/; February 24, 1955 October 5, 2011)[5][6] was an American entrepreneur.[7] He is best known as the co-founder, chairman, and CEO of Apple Inc. Through Apple, he was widely recognized as a charismatic pioneer of the personal computer revolution[8][9] and for his influential career in the computer and consumer electronics fields. Jobs also co-founded and served as chief executive of Pixar Animation Studios; he became a member of the board of directors of The Walt Disney Company in 2006, when Disney acquired Pixar.
In the late 1970s, Apple co-founder Steve Wozniak engineered one of the first commercially successful lines of personal computers, the Apple II series. Jobs was among the first to see the commercial potential of Xerox PARC's mouse-driven graphical user interface, which led to the creation of the Apple Lisa and, one year later, the Macintosh. He also played a role in introducing the LaserWriter, one of the first widely available laser printers, to the market.[10]
After a power struggle with the board of directors in 1985, Jobs left Apple and founded NeXT, a computer platform development company specializing in the higher-education and business markets. In 1986, he acquired the computer graphics division of Lucasfilm, which was spun off as Pixar.[11] He was credited in Toy Story (1995) as an executive producer. He served as CEO and majority shareholder until Disney's purchase of Pixar in 2006.[12] In 1996, after Apple had failed to deliver its operating system, Copland, Gil Amelio turned to NeXT Computer, and the NeXTSTEP platform became the foundation for the Mac OS X.[13] Jobs returned to Apple as an advisor, and took control of the company as an interim CEO. Jobs brought Apple from near bankruptcy to profitability by 1998.[14][15]
As the new CEO of the company, Jobs oversaw the development of the iMac, iTunes, iPod, iPhone, and iPad, and on the services side, the company's Apple Retail Stores, iTunes Store and the App Store.[16] The success of these products and services provided several years of stable financial returns, and propelled Apple to become the world's most valuable publicly traded company in 2011.[17] The reinvigoration of the company is regarded by many commentators as one of the greatest turnarounds in business history.[18][19][20]
In 2003, Jobs was diagnosed with a pancreas neuroendocrine tumor. Though it was initially treated, he reported a hormone imbalance, underwent a liver transplant in 2009, and appeared progressively thinner as his health declined.[21] On medical leave for most of 2011, Jobs resigned in August that year, and was elected Chairman of the Board. He died of respiratory arrest related to his metastatic tumor on October 5, 2011.
Jobs has received a number of honors and public recognition for his influence in the technology and music industries. He has widely been referred to as "legendary", a "futurist" or simply "visionary",[22][23][24][25] and has been described as the "Father of the Digital Revolution",[26] a "master of innovation",[27][28] and a "design perfectionist".[29][30]
Steven Paul Jobs was born in San Francisco on February 24, 1955 to two university students, Joanne Carole Schieble, of Swiss Catholic descent, and Syrian-born Abdulfattah "John" Jandali (Arabic:  ), who were both unmarried at the time.[31] Jandali, who was teaching in Wisconsin when Steve was born, said he had no choice but to put the baby up for adoption because his girlfriend's family objected to their relationship.[32]
The baby was adopted at birth by Paul Reinhold Jobs (19221993) and Clara Jobs (19241986), an Armenian American[33] whose maiden name was Hagopian.[34] According to Steve Jobs's commencement address at Stanford, Schieble wanted Jobs to be adopted only by a college-graduate couple. Schieble learned that Clara Jobs didn't graduate from college and Paul Jobs only attended high school, but signed final adoption papers after they promised her that the child would definitely be encouraged and supported to attend college. Later, when asked about his "adoptive parents", Jobs replied emphatically that Paul and Clara Jobs "were my parents."[35] He stated in his authorized biography that they "were my parents 1,000%."[36] Unknown to him, his biological parents would subsequently marry (December 1955), have a second child, novelist Mona Simpson, in 1957, and divorce in 1962.[36]
The Jobs family moved from San Francisco to Mountain View, California when Steve was five years old.[1][2] The parents later adopted a daughter, Patty. Paul worked as a mechanic and a carpenter, and taught his son rudimentary electronics and how to work with his hands.[1] The father showed Steve how to work on electronics in the family garage, demonstrating to his son how to take apart and rebuild electronics such as radios and televisions. As a result, Steve became interested in and developed a hobby of technical tinkering.[37]
Clara was an accountant[35] who taught him to read before he went to school.[1] Clara Jobs had been a payroll clerk for Varian Associates, one of the first high-tech firms in what became known as Silicon Valley.[38]
Jobs's youth was riddled with frustrations over formal schooling. At Monta Loma Elementary school in Mountain View, he was a prankster whose fourth-grade teacher needed to bribe him to study. Jobs tested so well, however, that administrators wanted him to skip two grades and enter high schoola proposal his parents declined, letting him skip only one grade.[36][39]
Jobs then attended Cupertino Junior High and Homestead High School in Cupertino, California.[2] At Homestead, Jobs became friends with Bill Fernandez, a neighbor who shared the same interests in electronics. Fernandez introduced Jobs to another, older computer whiz kid, Steve Wozniak (also known as "Woz"). In 1969 Woz started building a little computer board with Fernandez that they named "The Cream Soda Computer", which they showed to Jobs; he seemed really interested.[40]
Following high school graduation in 1972, Jobs enrolled at Reed College in Portland, Oregon. Reed was an expensive college which Paul and Clara could ill afford. They were spending much of their life savings on their son's higher education.[40] Jobs dropped out of college after six months and spent the next 18 months dropping in on creative classes, including a course on calligraphy.[41] He continued auditing classes at Reed while sleeping on the floor in friends' dorm rooms, returning Coke bottles for food money, and getting weekly free meals at the local Hare Krishna temple.[42] Jobs later said, "If I had never dropped in on that single calligraphy course in college, the Mac would have never had multiple typefaces or proportionally spaced fonts."[42]
In 1974, Jobs took a job as a technician at Atari, Inc. in Los Gatos, California.[43] He traveled to India in mid-1974[44] to visit Neem Karoli Baba[45] at his Kainchi Ashram with a Reed College friend (and, later, an early Apple employee), Daniel Kottke, in search of spiritual enlightenment. When they got to the Neem Karoli ashram, it was almost deserted as Neem Karoli Baba had died in September 1973.[43] Then they made a long trek up a dry riverbed to an ashram of Hariakhan Baba. In India, they spent a lot of time on bus rides from Delhi to Uttar Pradesh and back, then up to Himachal Pradesh and back.[43]
After staying for seven months, Jobs left India[46] and returned to the US ahead of Daniel Kottke.[43] Jobs had changed his appearance; his head was shaved and he wore traditional Indian clothing.[47][48] During this time, Jobs experimented with psychedelics, later calling his LSD experiences "one of the two or three most important things [he had] done in [his] life".[49][50] He also became a serious practitioner of Zen Buddhism, engaged in lengthy meditation retreats at the Tassajara Zen Mountain Center, the oldest St Zen monastery in the US.[51] He considered taking up monastic residence at Eihei-ji in Japan, and maintained a lifelong appreciation for Zen.[52] Jobs would later say that people around him who did not share his countercultural roots could not fully relate to his thinking.[49]
Jobs then returned to Atari, and was assigned to create a circuit board for the arcade video game Breakout. According to Atari co-founder Nolan Bushnell, Atari offered $100 for each chip that was eliminated in the machine. Jobs had little specialized knowledge of circuit board design and made a deal with Wozniak to split the fee evenly between them if Wozniak could minimize the number of chips. Much to the amazement of Atari engineers, Wozniak reduced the number of chips by 50, a design so tight that it was impossible to reproduce on an assembly line.[further explanation needed] According to Wozniak, Jobs told him that Atari gave them only $700 (instead of the offered $5,000), and that Wozniak's share was thus $350.[53] Wozniak did not learn about the actual bonus until ten years later, but said that if Jobs had told him about it and had said he needed the money, Wozniak would have given it to him.[54]
In the early 1970s, Jobs and Wozniak were drawn to technology like a magnet. Wozniak had designed a low-cost digital "blue box" to generate the necessary tones to manipulate the telephone network, allowing free long-distance calls. Jobs decided that they could make money selling it. The clandestine sales of the illegal "blue boxes" went well, and perhaps planted the seed in Jobs's mind that electronics could be fun and profitable.[55]
Jobs began attending meetings of the Homebrew Computer Club with Wozniak in 1975.[2] He greatly admired Edwin H. Land, the inventor of instant photography and founder of Polaroid Corporation, and would explicitly model his own career after that of Land's.[56][57]
In 1976, Jobs and Wozniak formed their own business, which they named "Apple Computer Company" in remembrance of a happy summer Jobs had spent picking apples. At first they started off selling circuit boards.[58]
Jobs and Steve Wozniak met in 1971, when their mutual friend, Bill Fernandez, introduced 21-year-old Wozniak to 16-year-old Jobs. In 1976, Wozniak single-handedly invented the Apple I computer. Wozniak showed it to Jobs, who suggested that they sell it. Jobs, Wozniak, and Ronald Wayne founded Apple computer in the garage of Jobs's parents in order to sell it.[59] They received funding from a then-semi-retired Intel product-marketing manager and engineer Mike Markkula.[60]
In 1978, Apple recruited Mike Scott from National Semiconductor to serve as CEO for what turned out to be several turbulent years. In 1983, Jobs lured John Sculley away from Pepsi-Cola to serve as Apple's CEO, asking, "Do you want to sell sugar water for the rest of your life, or do you want to come with me and change the world?"[61]
In the early 1980s, Jobs was among the first to see the commercial potential of Xerox PARC's mouse-driven graphical user interface, which led to the creation of the Apple Lisa. One year later, Apple employee Jef Raskin invented the Macintosh.[62][63]
The following year, Apple aired a Super Bowl television commercial titled "1984". At Apple's annual shareholders meeting on January 24, 1984, an emotional Jobs introduced the Macintosh to a wildly enthusiastic audience; Andy Hertzfeld described the scene as "pandemonium".[64]
While Jobs was a persuasive and charismatic director for Apple, some of his employees from that time described him as an erratic and temperamental manager. Disappointing sales caused a deterioration in Jobs's working relationship with Sculley and it eventually became a power struggle between Jobs and Sculley.[65] Jobs kept meetings running past midnight, sent out lengthy faxes, then called new meetings at 7:00am.[66]
Sculley learned that Jobswho believed Sculley to be "bad for Apple" and the wrong person to lead the companyhad been attempting to organize a boardroom coup, and on May 24, 1985, called a board meeting to resolve the matter.[65] Apple's board of directors sided with Sculley and removed Jobs from his managerial duties as head of the Macintosh division.[67][68] Jobs resigned from Apple five months later[65] and founded NeXT Inc. the same year.[66][69]
In a speech Jobs gave at Stanford University in 2005, he said being fired from Apple was the best thing that could have happened to him; "The heaviness of being successful was replaced by the lightness of being a beginner again, less sure about everything. It freed me to enter one of the most creative periods of my life." And he added, "I'm pretty sure none of this would have happened if I hadn't been fired from Apple. It was awful-tasting medicine, but I guess the patient needed it."[42][70][71]
After leaving Apple, Jobs founded NeXT Computer in 1985, with $7million. A year later, Jobs was running out of money, and with no product on the horizon, he appealed for venture capital. Eventually, he attracted the attention of billionaire Ross Perot who invested heavily in the company.[72] NeXT workstations were first released in 1990, priced at $9,999. Like the Apple Lisa, the NeXT workstation was technologically advanced, but was largely dismissed as cost-prohibitive by the educational sector for which it was designed.[73] The NeXT workstation was known for its technical strengths, chief among them its object-oriented software development system. Jobs marketed NeXT products to the financial, scientific, and academic community, highlighting its innovative, experimental new technologies, such as the Mach kernel, the digital signal processor chip, and the built-in Ethernet port. Tim Berners-Lee invented the World Wide Web on a NeXT computer at CERN.[74]
The revised, second-generation NeXTcube was released in 1990, also. Jobs touted it as the first "interpersonal" computer that would replace the personal computer. With its innovative NeXTMail multimedia email system, NeXTcube could share voice, image, graphics, and video in email for the first time. "Interpersonal computing is going to revolutionize human communications and groupwork", Jobs told reporters.[75] Jobs ran NeXT with an obsession for aesthetic perfection, as evidenced by the development of and attention to NeXTcube's magnesium case.[76] This put considerable strain on NeXT's hardware division, and in 1993, after having sold only 50,000 machines, NeXT transitioned fully to software development with the release of NeXTSTEP/Intel.[77] The company reported its first profit of $1.03million in 1994.[72] In 1996, NeXT Software, Inc. released WebObjects, a framework for Web application development. After NeXT was acquired by Apple Inc. in 1997, WebObjects was used to build and run the Apple Store,[77] MobileMe services, and the iTunes Store.
In 1986, Jobs bought The Graphics Group (later renamed Pixar) from Lucasfilm's computer graphics division for the price of $10million, $5million of which was given to the company as capital.[78]
The first film produced by the partnership, Toy Story (1995), with Jobs credited as executive producer,[79] brought fame and critical acclaim to the studio when it was released. Over the next 15 years, under Pixar's creative chief John Lasseter, the company produced box-office hits A Bug's Life (1998); Toy Story 2 (1999); Monsters, Inc. (2001); Finding Nemo (2003); The Incredibles (2004); Cars (2006); Ratatouille (2007); WALL-E (2008); Up (2009); and Toy Story 3 (2010). Finding Nemo, The Incredibles, Ratatouille, WALL-E, Up and Toy Story 3 each received the Academy Award for Best Animated Feature, an award introduced in 2001.[80]
In the years 2003 and 2004, as Pixar's contract with Disney was running out, Jobs and Disney chief executive Michael Eisner tried but failed to negotiate a new partnership,[82] and in early 2004, Jobs announced that Pixar would seek a new partner to distribute its films after its contract with Disney expired.
In October 2005, Bob Iger replaced Eisner at Disney, and Iger quickly worked to patch up relations with Jobs and Pixar. On January 24, 2006, Jobs and Iger announced that Disney had agreed to purchase Pixar in an all-stock transaction worth $7.4 billion. When the deal closed, Jobs became The Walt Disney Company's largest single shareholder with approximately seven percent of the company's stock.[83] Jobs's holdings in Disney far exceeded those of Eisner, who holds 1.7 percent, and of Disney family member Roy E. Disney, who until his 2009 death held about one percent of the company's stock and whose criticisms of Eisner  especially that he soured Disney's relationship with Pixar  accelerated Eisner's ousting. Upon completion of the merger, Jobs received 7% of Disney shares, and joined the Board of Directors as the largest individual shareholder.[83][84][85] Upon Jobs's death his shares in Disney were transferred to the Steven P. Jobs Trust led by Laurene Jobs.[86]
In 1996, Apple announced that it would buy NeXT for $427million. The deal was finalized in late 1996,[87] bringing Jobs back to the company he co-founded. Jobs became de facto chief after then-CEO Gil Amelio was ousted in July 1997. He was formally named interim chief executive in September.[88] In March 1998, to concentrate Apple's efforts on returning to profitability, Jobs terminated a number of projects, such as Newton, Cyberdog, and OpenDoc. In the coming months, many employees developed a fear of encountering Jobs while riding in the elevator, "afraid that they might not have a job when the doors opened. The reality was that Jobs's summary executions were rare, but a handful of victims was enough to terrorize a whole company."[89] Jobs also changed the licensing program for Macintosh clones, making it too costly for the manufacturers to continue making machines.
With the purchase of NeXT, much of the company's technology found its way into Apple products, most notably NeXTSTEP, which evolved into Mac OS X. Under Jobs's guidance, the company increased sales significantly with the introduction of the iMac and other new products; since then, appealing designs and powerful branding have worked well for Apple. At the 2000 Macworld Expo, Jobs officially dropped the "interim" modifier from his title at Apple and became permanent CEO.[90] Jobs quipped at the time that he would be using the title "iCEO".[91]
The company subsequently branched out, introducing and improving upon other digital appliances. With the introduction of the iPod portable music player, iTunes digital music software, and the iTunes Store, the company made forays into consumer electronics and music distribution. On June 29, 2007, Apple entered the cellular phone business with the introduction of the iPhone, a multi-touch display cell phone, which also included the features of an iPod and, with its own mobile browser, revolutionized the mobile browsing scene. While stimulating innovation, Jobs also reminded his employees that "real artists ship".[92]
Jobs was both admired and criticized for his consummate skill at persuasion and salesmanship, which has been dubbed the "reality distortion field" and was particularly evident during his keynote speeches (colloquially known as "Stevenotes") at Macworld Expos and at Apple Worldwide Developers Conferences. In 2005, Jobs responded to criticism of Apple's poor recycling programs for e-waste in the US by lashing out at environmental and other advocates at Apple's Annual Meeting in Cupertino in April. A few weeks later, Apple announced it would take back iPods for free at its retail stores. The Computer TakeBack Campaign responded by flying a banner from a plane over the Stanford University graduation at which Jobs was the commencement speaker.[42] The banner read "Steve, don't be a mini-playerrecycle all e-waste".
In 2006, he further expanded Apple's recycling programs to any US customer who buys a new Mac. This program includes shipping and "environmentally friendly disposal" of their old systems.[93]
In August 2011, Jobs resigned as CEO of Apple, but remained with the company as chairman of the company's board.[94][95] Hours after the announcement, Apple Inc. (AAPL) shares dropped fivepercent in after-hours trading.[96] This relatively small drop, when considering the importance of Jobs to Apple, was associated with the fact that his health had been in the news for several years, and he had been on medical leave since January 2011.[97] It was believed, according to Forbes, that the impact would be felt in a negative way beyond Apple, including at The Walt Disney Company where Jobs served as director.[98] In after-hours trading on the day of the announcement, Walt Disney Co. (DIS) shares dropped 1.5percent.[99]
Although Jobs earned only $1 a year as CEO of Apple,[100] Jobs held 5.426 million Apple shares worth $2.1 billion, as well as 138 million shares in Disney (which he received in exchange for Disney's acquisition of Pixar) worth $4.4 billion.[101][102] Jobs quipped that the $1 per annum he was paid by Apple was based on attending one meeting for 50 cents while the other 50 cents was based on his performance.[103] Forbes estimated his net wealth at $8.3billion in 2010, making him the 42nd-wealthiest American.[104]
In 2001, Jobs was granted stock options in the amount of 7.5million shares of Apple with an exercise price of $18.30. It was alleged that the options had been backdated, and that the exercise price should have been $21.10. It was further alleged that Jobs had thereby incurred taxable income of $20,000,000 that he did not report, and that Apple overstated its earnings by that same amount. As a result, Jobs potentially faced a number of criminal charges and civil penalties. The case was the subject of active criminal and civil government investigations,[105] though an independent internal Apple investigation completed on December 29, 2006, found that Jobs was unaware of these issues and that the options granted to him were returned without being exercised in 2003.[106]
On July 1, 2008, a $7-billion class action suit was filed against several members of the Apple Board of Directors for revenue lost due to the alleged securities fraud.[107][108]
Jobs was a demanding perfectionist[109][110] who always aspired to position his businesses and their products at the forefront of the information technology industry by foreseeing and setting trends, at least in innovation and style. He summed up that self-concept at the end of his keynote speech at the Macworld Conference and Expo in January 2007, by quoting ice hockey player Wayne Gretzky
There's an old Wayne Gretzky quote that I love. 'I skate to where the puck is going to be, not where it has been.' And we've always tried to do that at Apple. Since the very very beginning. And we always will.[111]
Much was made of Jobs's aggressive and demanding personality. Fortune wrote that he was "considered one of Silicon Valley's leading egomaniacs".[112] Commentaries on his temperamental style can be found in Michael Moritz's The Little Kingdom, The Second Coming of Steve Jobs, by Alan Deutschman; and iCon: Steve Jobs, by Jeffrey S. Young & William L. Simon. In 1993, Jobs made Fortune's list of America's Toughest Bosses in regard to his leadership of NeXT.
NeXT Cofounder Dan'l Lewin was quoted in Fortune as saying of that period, "The highs were unbelievable... But the lows were unimaginable", to which Jobs's office replied that his personality had changed since then.[113]
Tim Cook Apple CEO noted, "More so than any person I ever met in my life, [Jobs] had the ability to change his mind, much more so than anyone Ive ever met... Maybe the most underappreciated thing about Steve was that he had the courage to change his mind." [114]
In 2005, Jobs banned all books published by John Wiley & Sons from Apple Stores in response to their publishing an unauthorized biography, iCon: Steve Jobs.[115] In its 2010 annual earnings report, Wiley said it had "closed a deal... to make its titles available for the iPad."[116] Jef Raskin, a former colleague, once said that Jobs "would have made an excellent king of France", alluding to Jobs's compelling and larger-than-life persona.[117] Floyd Norman said that at Pixar, Jobs was a "mature, mellow individual" and never interfered with the creative process of the filmmakers.[118]
Jobs had a public war of words with Dell Computer CEO Michael Dell, starting in 1987 when Jobs first criticized Dell for making "un-innovative beige boxes".[119] On October 6, 1997, in a Gartner Symposium, when Michael Dell was asked what he would do if he ran then-troubled Apple Computer, he said "I'd shut it down and give the money back to the shareholders."[120] In 2006, Jobs sent an email to all employees when Apple's market capitalization rose above Dell's. The email read:
Team, it turned out that Michael Dell wasn't perfect at predicting the future. Based on today's stock market close, Apple is worth more than Dell. Stocks go up and down, and things may be different tomorrow, but I thought it was worth a moment of reflection today. Steve.[121]
Jobs was also a board member at Gap Inc. from 1999 to 2002.[122]
Apple's Bud Tribble coined the term "reality distortion field" in 1981, to describe Jobs's charisma and its effects on the developers working on the Macintosh project.[123] Tribble claimed that the term came from Star Trek.[123] Since then the term has also been used to refer to perceptions of Jobs's keynote speeches.[124]
The RDF was said by Andy Hertzfeld to be Steve Jobs's ability to convince himself and others to believe almost anything, using a mix of charm, charisma, bravado, hyperbole, marketing, appeasement, and persistence. Although the subject of criticism, Jobs's so-called reality distortion field was also recognized as creating a sense that the impossible was possible. Once the term became widely known, it was often used in the technology press to describe Jobs's sway over the public, particularly regarding new product announcements.[125][126]
Jobs's design aesthetic was influenced by the modernist architectural style of Joseph Eichler, and the industrial designs of Braun's Dieter Rams.[36] His design sense was also greatly influenced by the Buddhism which he experienced in India while on a seven-month spiritual journey.[127] His sense of intuition was also influenced by the spiritual people with whom he studied.[127]
According to Apple cofounder, Steve Wozniak, "Steve didn't ever code. He wasn't an engineer and he didn't do any original design..."[128][129] Daniel Kottke, one of Apple's earliest employees and a college friend of Jobs', stated that "Between Woz and Jobs, Woz was the innovator, the inventor. Steve Jobs was the marketing person."[130]
He is listed as either primary inventor or co-inventor in 342 United States patents or patent applications related to a range of technologies from actual computer and portable devices to user interfaces (including touch-based), speakers, keyboards, power adapters, staircases, clasps, sleeves, lanyards and packages. Jobs's contributions to most of his patents were to "the look and feel of the product".[131] Most of these are design patents (specific product designs; for example, Jobs listed as primary inventor in patents for both original and lamp-style iMacs, as well as PowerBook G4 Titanium) as opposed to utility patents (inventions).[132][133] He has 43 issued US patents on inventions.[134] The patent on the Mac OS X Dock user interface with "magnification" feature was issued the day before he died.[135] However, Jobs had little involvement in the engineering and technical side of the original Apple computers.[129]
Even while terminally ill in the hospital, Jobs sketched new devices that would hold the iPad in a hospital bed.[136] He also despised the oxygen monitor on his finger and suggested ways to revise the design for simplicity.[137]
The Macintosh was introduced in January 1984. The computer had no "Mac" name on the front, but rather just the Apple logo.[138] The Macintosh had a friendly appearance since it was meant to be easy to use. The disk drive is below the display, the Macintosh was taller, narrower, more symmetrical, and far more suggestive of a face. The Macintosh was identified as a computer that ordinary people could understand.[139]
After Jobs was forced out of Apple in 1985, he started a company that built workstation computers. The NeXT Computer was introduced in 1989. Sir Tim Berners-Lee created the world's first web browser on the NeXT Computer. The NeXT Computer was the basis for today's Macintosh OS X and iPhone operating system (iOS).[140]
Apple iMac was introduced in 1998 and its innovative design was directly the result of Jobs's return to Apple. Apple boasted "the back of our computer looks better than the front of anyone else's".[141] Described as "cartoonlike" the first iMac, clad in Bondi Blue plastic, was unlike any personal computer that came before. In 1999, Apple introduced Graphite gray Apple iMac and since has switched to all-white. Design ideas were intended to create a connection with the user such as the handle and a breathing light effect when the computer went to sleep.[142] The Apple iMac sold for $1,299 at that time. There were some technical revolutions for iMac too. The USB ports being the only device inputs on the iMac. So the iMac's success helped popularize the interface among third party peripheral makers, which is evidenced by the fact that many early USB peripherals were made of translucent plastic to match the iMac design.[143]
The first generation of iPod was released October 23, 2001. The major innovation of the iPod was its small size achieved by using a 1.8" hard drive compared to the 2.5" drives common to players at that time. The capacity of the first generation iPod ranged from 5G to 10 Gigabytes.[144] The iPod sold for US$399 and more than 100,000 iPods were sold before the end of 2001. The introduction of the iPod resulted in Apple becoming a major player in the music industry.[145] Also, the iPod's success prepared the way for the iTunes music store and the iPhone.[146] After the 1st generation of iPod, Apple released the hard drive-based iPod classic, the touchscreen iPod Touch, video-capable iPod Nano, screenless iPod Shuffle in the following years.[145]
Apple began work on the first iPhone in 2005 and the first iPhone was released on June 29, 2007. The iPhone created such a sensation that a survey indicated six out of ten Americans were aware of its release. Time magazine declared it "Invention of the Year" for 2007.[147] The Apple iPhone is a small device with multimedia capabilities and functions as a quad-band touch screen smartphone.[148] A year later, the iPhone 3G was released in July 2008 with the key feature was support for GPS, 3G data and tri-band UMTS/HSDPA. In June 2009, the iPhone 3GS, added voice control, a better camera, and a faster processor was introduced by Phil Schiller.[149] iPhone 4 was thinner than previous models, had a five megapixel camera which can record videos in 720p HD, and added a secondary front facing camera for video calls.[150] A major feature of the iPhone 4S, introduced in October 2011, was Siri, which is a virtual assistant that is capable of voice recognition.[147]
Arik Hesseldahl of BusinessWeek magazine stated that "Jobs isn't widely known for his association with philanthropic causes", compared to Bill Gates's efforts.[151] In contrast to Gates, Jobs did not sign the Giving Pledge of Warren Buffett which challenged the world's richest billionaires to give at least half their wealth to charity.[152] In an interview with Playboy in 1985, Jobs said in respect to money that "the challenges are to figure out how to live with it and to reinvest it back into the world which means either giving it away or using it to express your concerns or values."[153] Jobs also added that when he has some time we would start a public foundation but for now he does charitable acts privately.[154]
After resuming control of Apple in 1997, Jobs eliminated all corporate philanthropy programs initially.[155] Jobs's friends told The New York Times that he felt that expanding Apple would have done more good than giving money to charity.[156] Later, under Jobs, Apple signed to participate in Product Red program, producing red versions of devices to give profits from sales to charity. Apple has gone on to become the largest contributor to the charity since its initial involvement with it. The chief of the Product Red project, singer Bono cited Jobs saying there was "nothing better than the chance to save lives", when he initially approached Apple with the invitation to participate in the program.[157] Through its sales, Apple has been the largest contributor to Product Red's gift to the Global Fund, which fights AIDS, tuberculosis and malaria, according to Bono.[158][159]
Jobs's birth parents met at the University of Wisconsin. Abdulfattah "John" Jandali, from Syria,[160] taught there. Joanne Carole Schieble was his student; they were the same age because Jandali had "gotten his PhD really young." [161][162][163] Schieble had a career as a speech language pathologist. Jandali taught political science at the University of Nevada in the 1960s, and then made his career in the food and beverage industry, and since 2006, has been a vice president at a casino in Reno, Nevada.[164][165] In December 1955, ten months after giving up their baby boy, Schieble and Jandali married. In 1957 they had a daughter, Mona. They divorced in 1962, and Jandali lost touch with his daughter.[166] Her mother remarried and had Mona take the surname of her stepfather, so she became known as Mona Simpson.[162]
In the 1980s, Jobs found his birth mother, Joanne Schieble Simpson, who told him he had a biological sister, Mona Simpson. They met for the first time in 1985[166] and became close friends. The siblings kept their relationship secret until 1986, when Mona introduced him at a party for her first book.[35]
After deciding to search for their father, Simpson found Jandali managing a coffee shop. Without knowing who his son had become, Jandali told Mona that he had previously managed a popular restaurant in the Silicon Valley where "Even Steve Jobs used to eat there. Yeah, he was a great tipper." In a taped interview with his biographer Walter Isaacson, aired on 60 Minutes,[167] Jobs said: "When I was looking for my biological mother, obviously, you know, I was looking for my biological father at the same time, and I learned a little bit about him and I didn't like what I learned. I asked her to not tell him that we ever met...not tell him anything about me."[168] Jobs was in occasional touch with his mother Joanne Simpson,[155][169] who lives in a nursing home in Los Angeles.[162] When speaking about his biological parents, Jobs stated: "They were my sperm and egg bank. That's not harsh, it's just the way it was, a sperm bank thing, nothing more."[36] Jandali stated in an interview with the The Sun in August 2011, that his efforts to contact Jobs were unsuccessful. Jandali mailed in his medical history after Jobs's pancreatic disorder was made public that year.[170][171][172]
In her eulogy to Jobs at his memorial service, Mona Simpson stated:
Jobs's first child, Lisa Brennan-Jobs, was born in 1978, the daughter of his longtime partner Chris Ann Brennan, a Bay Area painter.[155] For two years, she raised their daughter on welfare while Jobs denied paternity by claiming he was sterile; he later acknowledged Lisa as his daughter.[155] Jobs later married Laurene Powell on March 18, 1991, in a ceremony at the Ahwahnee Hotel in Yosemite National Park. Presiding over the wedding was Kobun Chino Otogawa, a Zen Buddhist monk. Their son, Reed, was born September 1991, followed by daughters Erin in August 1995, and Eve in 1998.[173] The family lives in Palo Alto, California.[174]
Jobs once dated Joan Baez for a few years. Elizabeth Holmes, a friend of Jobs from his time at Reed College, believed that Jobs was interested in Baez because she had been the lover of Bob Dylan" (Dylan was the Apple icon's favorite musician).[175] Jobs confided in Joanna Hoffman his concerns about the relationship. She would later tell his official biographer "She was a strong woman, and he wanted to show he was in control. Plus, he always said he wanted to have a family, and with her he knew that he wouldn't. [176]
Jobs was also a fan of The Beatles. He referred to them on multiple occasions at Keynotes and also was interviewed on a showing of a Paul McCartney concert. When asked about his business model on 60 Minutes, he replied:
My model for business is The Beatles: They were four guys that kept each other's negative tendencies in check; they balanced each other. And the total was greater than the sum of the parts. Great things in business are never done by one person, they are done by a team of people.[177]
In 1982, Jobs bought an apartment in The San Remo, an apartment building in New York City with a politically progressive reputation, where Demi Moore, Steven Spielberg, Steve Martin, and Princess Yasmin Aga Khan, daughter of Rita Hayworth, also had apartments. With the help of I. M. Pei, Jobs spent years renovating his apartment in the top two floors of the building's north tower, only to sell it almost two decades later to U2 singer Bono. Jobs never moved in.[178][179]
In 1984, Jobs purchased the Jackling House, a 17,000-square-foot (1,600m2), 14-bedroom Spanish Colonial mansion designed by George Washington Smith in Woodside, California. Although it reportedly remained in an almost unfurnished state, Jobs lived in the mansion for almost ten years. According to reports, he kept a 1966 BMW R60/2 motorcycle in the living room, and let Bill Clinton use it in 1998. From the early 1990s, Jobs lived in a house in the Old Palo Alto neighborhood of Palo Alto. President Clinton dined with Jobs and 14 Silicon Valley CEOs there on August 7, 1996, at a meal catered by Greens Restaurant.[180][181] Clinton returned the favor and Jobs, who was a Democratic donor, slept in the Lincoln bedroom of the White House.[182]
Jobs allowed Jackling House to fall into a state of disrepair, planning to demolish the house and build a smaller home on the property; but he met with complaints from local preservationists over his plans. In June 2004, the Woodside Town Council gave Jobs approval to demolish the mansion, on the condition that he advertise the property for a year to see if someone would move it to another location and restore it. A number of people expressed interest, including several with experience in restoring old property, but no agreements to that effect were reached. Later that same year, a local preservationist group began seeking legal action to prevent demolition. In January 2007, Jobs was denied the right to demolish the property, by a court decision.[183] The court decision was overturned on appeal in March 2010, and the mansion was demolished beginning in February 2011.[184]
Jobs usually wore a black long-sleeved mock turtleneck made by Issey Miyake (that was sometimes reported to be made by St. Croix), Levi's 501 blue jeans, and New Balance 991 sneakers.[185][186] Jobs told Walter Isaacson "...he came to like the idea of having a uniform for himself, both because of its daily convenience (the rationale he claimed) and its ability to convey a signature style."[185] He was a pescetarian.[187]
Jobs's car was a silver Mercedes-Benz SL 55 AMG, which did not display its license plates, as he took advantage of a California law which gives a maximum of six months for new vehicles to receive plates; Jobs leased a new SL every six months.[188] Jobs involved himself with the details of designing his 78-metre luxury yacht Venus to keep thoughts of death at bay. It is also designed by Philippe Starck, and named after the deity.[189][190]
In a 2011 interview with biographer Walter Isaacson, Jobs revealed at one point he met with U.S. President Barack Obama, complained of the nation's shortage of software engineers, and told Mr. Obama that he was "headed for a one-term presidency." Jobs proposed that any foreign student who got an engineering degree at a U.S. university should automatically be offered a green card. After the meeting, Jobs commented, "The president is very smart, but he kept explaining to us reasons why things can't get done.... It infuriates me."[191]
Jobs contributed to a number of political candidates and causes during his life, giving $209,000 to Democrats, $45,700 to associated special interests and $1,000 to a Republican.[192]
In October 2003, Jobs was diagnosed with cancer,[193] and in mid-2004, he announced to his employees that he had a cancerous tumor in his pancreas.[194] The prognosis for pancreatic cancer is usually very poor;[195] Jobs stated that he had a rare, far less aggressive type known as islet cell neuroendocrine tumor.[194] Despite his diagnosis, Jobs resisted his doctors' recommendations for mainstream medical intervention for nine months,[155] instead consuming a special alternative medicine diet in an attempt to thwart the disease. According to Harvard researcher Ramzi Amri, his choice of alternative treatment "led to an unnecessarily early death."[193] According to Jobs's biographer, Walter Isaacson, "for nine months he refused to undergo surgery for his pancreatic cancer  a decision he later regretted as his health declined."[196] "Instead, he tried a vegan diet, acupuncture, herbal remedies and other treatments he found online, and even consulted a psychic. He also was influenced by a doctor who ran a clinic that advised juice fasts, bowel cleansings and other unproven approaches, before finally having surgery in July 2004."[197] He eventually underwent a pancreaticoduodenectomy (or "Whipple procedure") in July 2004, that appeared to successfully remove the tumor.[198][199][200] Jobs apparently did not receive chemotherapy or radiation therapy.[194][201] During Jobs's absence, Tim Cook, head of worldwide sales and operations at Apple, ran the company.[194]
In early August 2006, Jobs delivered the keynote for Apple's annual Worldwide Developers Conference. His "thin, almost gaunt" appearance and unusually "listless" delivery,[202][203] together with his choice to delegate significant portions of his keynote to other presenters, inspired a flurry of media and Internet speculation about his health.[204] In contrast, according to an Ars Technica journal report, Worldwide Developers Conference (WWDC) attendees who saw Jobs in person said he "looked fine".[205] Following the keynote, an Apple spokesperson said that "Steve's health is robust."[206]
Two years later, similar concerns followed Jobs's 2008 WWDC keynote address.[207] Apple officials stated Jobs was victim to a "common bug" and was taking antibiotics,[208] while others surmised his cachectic appearance was due to the Whipple procedure.[201] During a July conference call discussing Apple earnings, participants responded to repeated questions about Jobs's health by insisting that it was a "private matter". Others, however, voiced the opinion that shareholders had a right to know more, given Jobs's hands-on approach to running his company.[209][210] The New York Times published an article based on an off-the-record phone conversation with Jobs, noting that "While his health problems amounted to a good deal more than 'a common bug', they weren't life-threatening and he doesn't have a recurrence of cancer."[211]
On August 28, 2008, Bloomberg mistakenly published a 2500-word obituary of Jobs in its corporate news service, containing blank spaces for his age and cause of death. (News carriers customarily stockpile up-to-date obituaries to facilitate news delivery in the event of a well-known figure's death.) Although the error was promptly rectified, many news carriers and blogs reported on it,[212] intensifying rumors concerning Jobs's health.[213] Jobs responded at Apple's September 2008 Let's Rock keynote by essentially[214] quoting Mark Twain: "Reports of my death are greatly exaggerated."[215] At a subsequent media event, Jobs concluded his presentation with a slide reading "110/70", referring to his blood pressure, stating he would not address further questions about his health.[216]
On December 16, 2008, Apple announced that marketing vice-president Phil Schiller would deliver the company's final keynote address at the Macworld Conference and Expo 2009, again reviving questions about Jobs's health.[217][218] In a statement given on January 5, 2009, on Apple.com,[219] Jobs said that he had been suffering from a "hormone imbalance" for several months.[220]
On January 14, 2009, in an internal Apple memo, Jobs wrote that in the previous week he had "learned that my health-related issues are more complex than I originally thought", and announced a six-month leave of absence until the end of June 2009, to allow him to better focus on his health. Tim Cook, who previously acted as CEO in Jobs's 2004 absence, became acting CEO of Apple,[221] with Jobs still involved with "major strategic decisions."[221]
In April 2009, Jobs underwent a liver transplant at Methodist University Hospital Transplant Institute in Memphis, Tennessee.[222][223] Jobs's prognosis was described as "excellent".[222]
On January 17, 2011, a year and a half after Jobs returned from his liver transplant, Apple announced that he had been granted a medical leave of absence. Jobs announced his leave in a letter to employees, stating his decision was made "so he could focus on his health". As during his 2009 medical leave, Apple announced that Tim Cook would run day-to-day operations and that Jobs would continue to be involved in major strategic decisions at the company.[224][225] Despite the leave, he made appearances at the iPad 2 launch event (March 2), the WWDC keynote introducing iCloud (June 6), and before the Cupertino city council (June 7).[226]
Jobs announced his resignation as Apple's CEO on August 24, 2011, writing to the board, "I have always said if there ever came a day when I could no longer meet my duties and expectations as Apples CEO, I would be the first to let you know. Unfortunately, that day has come."[227] Jobs became chairman of the board thereafter, naming Tim Cook his successor as CEO,[228][229] and continued to work for Apple until the day before his death 7 weeks later.[230]

Jobs died at his California home around 3pm on October 5, 2011, due to complications from a relapse of his previously treated islet-cell neuroendocrine pancreatic cancer,[2][231][232] resulting in respiratory arrest.[233] He had lost consciousness the day before, and died with his wife, children and sister at his side.[234]
Both Apple and Microsoft flew their flags at half-staff throughout their respective headquarters and campuses.[235][236] Bob Iger ordered all Disney properties, including Walt Disney World and Disneyland, to fly their flags at half-staff, from October 6 to 12, 2011.[237]
His death was announced by Apple in a statement which read:
Steve's brilliance, passion and energy were the source of countless innovations that enrich and improve all of our lives. The world is immeasurably better because of Steve.
His greatest love was for his wife, Laurene, and his family. Our hearts go out to them and to all who were touched by his extraordinary gifts.[238]
For two weeks following his death, Apple's corporate Web site displayed a simple page, showing Jobs's name and lifespan next to his grayscale portrait.[239] Clicking on the image led to an obituary, which read:
An email address was also posted for the public to share their memories, condolences, and thoughts.[240][241] Over a million tributes were sent, which are now displayed on the Steve Jobs memorial page.
Also dedicating its homepage to Jobs was Pixar, with a photo of Jobs, John Lasseter and Edwin Catmull, and the eulogy they wrote:[242]
A small private funeral was held on October 7, 2011, of which details were not revealed out of respect to Jobs's family.[243] Apple announced on the same day that they had no plans for a public service, but were encouraging "well-wishers" to send their remembrance messages to an email address created to receive such messages.[244] Sunday, October 16, 2011, was declared "Steve Jobs Day" by Governor Jerry Brown of California.[245] On that day, an invitation-only memorial was held at Stanford University. Those in attendance included Apple and other tech company executives, members of the media, celebrities, close friends of Jobs, and politicians, along with Jobs's family. Bono, Yo Yo Ma, and Joan Baez performed at the service, which lasted longer than an hour. The service was highly secured, with guards at all of the university's gates, and a helicopter flying overhead from an area news station.[246][247]
A private memorial service for Apple employees was held on October 19, 2011, on the Apple Campus in Cupertino. Present were Cook, Bill Campbell, Norah Jones, Al Gore, and Coldplay, and Jobs's widow, Laurene, was in attendance. Some of Apple's retail stores closed briefly so employees could attend the memorial. A video of the service is available on Apple's website.[248]
Jobs is buried in an unmarked grave at Alta Mesa Memorial Park, the only non-denominational cemetery in Palo Alto.[249][250] He is survived by Laurene, his wife of 20 years, their three children, and Lisa Brennan-Jobs, his daughter from a previous relationship.[251] His family released a statement saying that he "died peacefully".[252][253] He "looked at his sister Patty, then for a long time at his children, then at his life's partner, Laurene, and then over their shoulders past them"; his last words, spoken hours before his death, were:
Steve Jobs's death broke news headlines on ABC, CBS, and NBC.[254] Numerous newspapers around the world carried news of his death on their front pages the next day. Several notable people, including US President Barack Obama,[255] British Prime Minister David Cameron,[256] Microsoft founder Bill Gates,[257] and The Walt Disney Company's Bob Iger commented on the death of Jobs. Wired News collected reactions and posted them in tribute on their homepage.[258] Other statements of condolence were made by many of Jobs's friends and colleagues, such as Steve Wozniak and George Lucas.[259][260] After Steve Jobs's death, Adult Swim aired a 15-second segment with the words "hello" in a script font fading in and then changing into "goodbye".
Major media published commemorative works. Time published a commemorative issue for Jobs on October 8, 2011. The issue's cover featured a portrait of Jobs, taken by Norman Seeff, in which he is sitting in the lotus position holding the original Macintosh computer, first published in Rolling Stone in January 1984. The issue marked the eighth time Jobs has been featured on the cover of Time.[261] The issue included a photographic essay by Diana Walker, a retrospective on Apple by Harry McCracken and Lev Grossman, and a six-page essay by Walter Isaacson. Isaacson's essay served as a preview of his biography, Steve Jobs.[262]
Bloomberg Businessweek also published a commemorative, ad-free issue, featuring extensive essays by Steve Jurvetson, John Sculley, Sean Wisely, William Gibson, and Walter Isaacson. On its cover, Steve Jobs is pictured in gray scale, along with his name and lifespan.
At the time of his resignation, and again after his death, Jobs was widely described as a visionary, pioneer and genius[263][264][265][266]perhaps one of the foremostin the field of business,[267][268] innovation,[269] and product design,[270] and a man who had profoundly changed the face of the modern world,[263][265][269] revolutionized at least six different industries,[264] and who was an "exemplar for all chief executives".[264] His death was widely mourned[269] and considered a loss to the world by commentators across the globe.[266]
After his resignation as Apple's CEO, Jobs was characterized as the Thomas Edison and Henry Ford of his time.[271][272] In his The Daily Show eulogy, Jon Stewart said that unlike others of Jobs's ilk, such as Thomas Edison or Henry Ford, Jobs died young. He felt that we had, in a sense, "wrung everything out of" these other men, but his feeling on Jobs was that "we're not done with you yet."[273] Malcolm Gladwell in The New Yorker asserted that "Jobs's sensibility was editorial, not inventive. His gift lay in taking what was in front of him ... and ruthlessly refining it."[274]
There was also a dissenting tone in some coverage of Jobs' life and works in the media, where attention focused on his near-fanatical control mindset and business ruthlessness. A Los Angeles Times media critic reported that the eulogies "came courtesy of reporters whoafter deadline and off the recordwould tell stories about a company obsessed with secrecy to the point of paranoia. They remind us how Apple shut down a youthful fanboy blogger, punished a publisher that dared to print an unauthorized Jobs biography and repeatedly ran afoul of the most basic tenets of a free press."[275] Free software pioneer Richard Stallman drew attention to Apple's strategy of tight corporate control over consumer computers and handheld devices, how Apple restricted news reporters, and persistently violated privacy: "Steve Jobs, the pioneer of the computer as a jail made cool, designed to sever fools from their freedom, has died".[276][277] Silicon Valley reporter Dan Gillmor stated that under Jobs, Apple had taken stances that in his view were "outright hostile to the practice of journalism"[275] - these included suing three "small fry" bloggers who reported tips about the company and its unreleased products including attempts to use the courts to force them to reveal their sources, suing teenager Nicholas Ciarelli, who wrote enthusiastic speculation about Apple products beginning at age 13[275] (Rainey wrote that Apple wanted to kill his 'ThinkSecret' blog as "It thought any leaks, even favorable ones, diluted the punch of its highly choreographed product launches with Jobs, in his iconic jeans and mock turtleneck outfit, as the star."[275]).
After Apple's founding, Jobs became a symbol of his company and industry. When Time named the computer as the 1982 "Machine of the Year", the magazine published a long profile of Jobs as "the most famous maestro of the micro".[278][279]
Jobs was awarded the National Medal of Technology by President Ronald Reagan in 1985, with Steve Wozniak (among the first people to ever receive the honor),[280] and a Jefferson Award for Public Service in the category "Greatest Public Service by an Individual 35 Years or Under" (also known as the Samuel S. Beard Award) in 1987.[281] On November 27, 2007, Jobs was named the most powerful person in business by Fortune magazine.[282] On December 5, 2007, California Governor Arnold Schwarzenegger and First Lady Maria Shriver inducted Jobs into the California Hall of Fame, located at The California Museum for History, Women and the Arts.[283]
In August 2009, Jobs was selected as the most admired entrepreneur among teenagers in a survey by Junior Achievement,[284] having previously been named Entrepreneur of the Decade 20 years earlier in 1989, by Inc. magazine.[285] On November 5, 2009, Jobs was named the CEO of the decade by Fortune magazine.[267]
In November 2010, Jobs was ranked No.17 on Forbes: The World's Most Powerful People.[286] In December 2010, the Financial Times named Jobs its person of the year for 2010, ending its essay[287] by stating, "In his autobiography, John Sculley, the former PepsiCo executive who once ran Apple, said this of the ambitions of the man he had pushed out: 'Apple was supposed to become a wonderful consumer products company. This was a lunatic plan. High-tech could not be designed and sold as a consumer product.'".[288] The Financial Times closed by rhetorically asking of this quote, "How wrong can you be."[287]
On December 21, 2011, Graphisoft company in Budapest presented the world's first bronze statue of Steve Jobs, calling him one of the greatest personalities of the modern age.[289]
In January 2012, when young adults (ages 16  25) were asked to identify the greatest innovator of all time, Steve Jobs placed second behind Thomas Edison.[290]
On February 12, 2012, Jobs was posthumously awarded the Grammy Trustees Award, an award for those who have influenced the music industry in areas unrelated to performance.[291]
In March 2012, global business magazine Fortune named Steve Jobs the "greatest entrepreneur of our time", describing him as "brilliant, visionary, inspiring", and "the quintessential entrepreneur of our generation".[292]
Two films, Disney's John Carter[293] and Pixar's Brave,[294] are dedicated to Jobs.
On October 5, 2012, Apple.com's homepage was changed to a video tribute to Jobs, because it was the first anniversary of his death, and it showed pictures with audio from some of his greatest keynotes. When the video ended, it showed a note from Tim Cook about the matter.
See also: Timeline of Steve Jobs media for a chronological list of his interviews, media appearances, and speaking engagements and their subject matter.
Steve Jobs (June 2005). Steve Jobs's 2005 Stanford Commencement Address. Stanford University. http://news.stanford.edu/news/2005/june15/videos/987.html.
   
1 Early life and education
2 Early career
3 Career

3.1 Apple Computer
3.2 NeXT Computer
3.3 Pixar and Disney
3.4 Return to Apple
3.5 Resignation


3.1 Apple Computer
3.2 NeXT Computer
3.3 Pixar and Disney
3.4 Return to Apple
3.5 Resignation
4 Business life

4.1 Wealth
4.2 Stock options backdating issue
4.3 Management style

4.3.1 Reality distortion field


4.4 Inventions and designs

4.4.1 The Macintosh Computer
4.4.2 The NeXT Computer
4.4.3 iMac
4.4.4 iPod
4.4.5 iPhone


4.5 Philanthropy


4.1 Wealth
4.2 Stock options backdating issue
4.3 Management style

4.3.1 Reality distortion field


4.3.1 Reality distortion field
4.4 Inventions and designs

4.4.1 The Macintosh Computer
4.4.2 The NeXT Computer
4.4.3 iMac
4.4.4 iPod
4.4.5 iPhone


4.4.1 The Macintosh Computer
4.4.2 The NeXT Computer
4.4.3 iMac
4.4.4 iPod
4.4.5 iPhone
4.5 Philanthropy
5 Personal life

5.1 Health issues


5.1 Health issues
6 Death

6.1 Media coverage


6.1 Media coverage
7 Honors and public recognition
8 Portrayals and coverage in books, film, and theater

8.1 Books
8.2 Documentaries
8.3 Films
8.4 Theater


8.1 Books
8.2 Documentaries
8.3 Films
8.4 Theater
9 References
10 Further reading
11 External links

11.1 Articles
11.2 Interviews


11.1 Articles
11.2 Interviews
3.1 Apple Computer
3.2 NeXT Computer
3.3 Pixar and Disney
3.4 Return to Apple
3.5 Resignation
4.1 Wealth
4.2 Stock options backdating issue
4.3 Management style

4.3.1 Reality distortion field


4.3.1 Reality distortion field
4.4 Inventions and designs

4.4.1 The Macintosh Computer
4.4.2 The NeXT Computer
4.4.3 iMac
4.4.4 iPod
4.4.5 iPhone


4.4.1 The Macintosh Computer
4.4.2 The NeXT Computer
4.4.3 iMac
4.4.4 iPod
4.4.5 iPhone
4.5 Philanthropy
4.3.1 Reality distortion field
4.4.1 The Macintosh Computer
4.4.2 The NeXT Computer
4.4.3 iMac
4.4.4 iPod
4.4.5 iPhone
5.1 Health issues
6.1 Media coverage
8.1 Books
8.2 Documentaries
8.3 Films
8.4 Theater
11.1 Articles
11.2 Interviews
The Little Kingdom (1984) by Michael Moritz, documenting the founding of (then) Apple Computer.
The Second Coming of Steve Jobs (2001), by Alan Deutschman
iCon: Steve Jobs (2005), by Jeffrey S. Young & William L. Simon
iWoz (2006), by Steve Wozniak, a co-founder of Apple. It is an autobiography of Steve Wozniak, but it covers much of Jobs's life and work at Apple.
Steve Jobs (2011), an authorized biography written by Walter Isaacson.
Inside Apple (2012), a book by Adam Lashinsky that reveals the secret systems, tactics, and leadership strategies that allowed Steve Jobs and his company to work.
The Zen of Steve Jobs (2012) written by Caleb Melby with artwork by Jess3, a graphic novel about the relationship of Jobs and Kobun Chino Otogawa and how the monk's mentorship influenced Jobs's business philosophy.
The Machine That Changed the World  Part 3 of this 1992 five-part documentary, called The Paperback Computer, prominently featured Jobs and his role in the early days of Apple.
Triumph of the Nerds  a 1996 three-part documentary for PBS, about the rise of the home computer/personal computer.
Nerds 2.0.1  a 1998 three-part documentary for PBS, (and sequel to Triumph of the Nerds) which chronicles the development of the Internet.
iGenius: How Steve Jobs Changed the World   a 2011 Discovery Channel documentary hosted by Adam Savage and Jamie Hyneman.[295]
Steve Jobs: One Last Thing  a 2011 PBS documentary produced by Pioneer Productions.[296] A slightly shortened and localized[297] version of the show was broadcast[298] in the United Kingdom the following day titled, Steve Jobs: iChanged the World  on Channel 4.[299]
Steve Jobs  A Sony Pictures film version of the biography by Walter Isaacson, with a screenplay and directed by Aaron Sorkin.
Jobs   an upcoming independent film by Joshua Michael Stern. Jobs will be portrayed by Ashton Kutcher.[300]
Pirates of Silicon Valley  a 1999 TNT film which chronicles the rise of Apple and Microsoft from the early 1970s to 1997. Jobs is portrayed by Noah Wyle.[301]
The Agony and Ecstasy of Steve Jobs  The Public Theater, New York City, 2012, starring Mike Daisey.[302]
Joan Ferrante (2012). Sociology: A Global Perspective. Cengage Learning. p.218. http://books.google.ca/books?id=JwvxLLrXGuMC&pg=PA218.
Shirin Sadeghi (October 6, 2011). "Steve Jobs' Arab-American background and the story of his adoption". New America Media. http://www.scpr.org/news/2011/10/06/29289/steve-jobs-arab-american-background-and-story-his-/.
"Steve Jobs Dies: He Was The Most Famous Arab in the World". International Business Times. October 5, 2011. http://www.ibtimes.com/articles/225891/20111005/steve-jobs-apple-dies-iphone-arab-syria.htm.
Butcher, Lee (1987). Accidental Millionaire: The rise and fall of Steve Jobs at Apple. Paragon House. ISBN978-0-913729-79-3.
Caddes, Carolyn (1986). Portraits of Success: Impressions of Silicon Valley Pioneers. Tioga Publishing Co.. ISBN0-935382-56-9.
Cringely, Robert X. (1996). Accidental Empires. HarperBusiness. ISBN0-88730-855-4.
Denning, Peter J.; Frenkel, Karen A. (1989). "A conversation with Steve Jobs". Communications of the ACM 32 (4): 436433. doi:10.1145/63334.63336.
Deutschman, Alan (2001). The Second Coming of Steve Jobs. Broadway. ISBN0-7679-0433-8.
Freiberger, Paul; Swaine, Michael (1999). Fire in the Valley: The Making of The Personal Computer. McGraw-Hill Trade. ISBN0-07-135892-7.
Hertzfeld, Andy (2004). Revolution in the Valley. O'Reilly Books. ISBN0-596-00719-1.
Kahney, Leander (2004). The Cult of Mac. No Starch Press. ISBN1-886411-83-2.
Levy, Steven (1984). Hackers: Heroes of the Computer Revolution. Anchor Press, Doubleday. ISBN0-385-19195-2.
Levy, Steven (1994). Insanely Great: The Life and Times of Macintosh, the Computer that Changed Everything. Penguin Books. ISBN0-670-85244-9.
Malone, Michael S. (1999). Infinite Loop. Aurum Press. ISBN1-85410-638-4. Bantam Doubleday Dell. ISBN 0-385-48684-7.
Markoff, John (2005). What the Dormouse Said: How the 60s Counterculture Shaped the Personal Computer Industry. New York: Viking. ISBN0-670-03382-0.
Schlender, Brent, "The Lost Steve Jobs Tapes", Fast Company magazine, May 2012 issue
Simon, William L.; Young, Jeffrey S. (2005). iCon: Steve Jobs, The Greatest Second Act in the History of Business. John Wiley & Sons. ISBN0-471-72083-6.
Stross, Randall E. (1993). Steve Jobs and The NeXT Big Thing. Atheneum Books. ISBN0-689-12135-0.
Slater, Robert (1987). Portraits in Silicon. MIT Press. ISBN0-262-19262-4. Chapter 28
Young, Jeffrey S. (1988). Steve Jobs: The Journey is the Reward. Scott, Foresman & Co.. ISBN0-673-18864-7.
Wozniak, Steve (2006). iWoz: Computer Geek to Cult Icon: How I invented the personal computer, co-founded Apple and had fun doing it. W. W. Norton & Co.. ISBN0-393-06143-4.
Book: Apple Inc.
Jobs's Macworld keynote in 1997 where he announced the partnership with Microsoft on YouTube
The template Cite video is being considered for deletion.
"Thoughts on Flash" by Steve Jobs, April 2010.
Appearances on C-SPAN
Steve Jobs at the Internet Movie Database
Works by or about Steve Jobs in libraries (WorldCat catalog)
Steve Jobs collected news and commentary at The Guardian
Steve Jobs collected news and commentary at The New York Times
Steve Jobs collected news and commentary at The Wall Street Journal
Bloomberg Game Changers: Steve Jobs A 48-minute video on Steve Jobs by Bloomberg
Steve Jobs Profile at Forbes
Cammeron, Brenna (October 5, 2011). "Steve Jobs Dies: A Timeline Of His Health". The Huffington Post. http://www.huffingtonpost.com/2011/10/05/steve-jobs-health-timeline_n_997313.html.
"Steve Jobs collected news and commentary at". AllThingsD. http://allthingsd.com/tag/steve-jobs/.
Steve Jobs remembrance notes from the community
Federal Bureau of Investigation dossier on Steven Paul Jobs.
"The Career of Apple and Steve Jobs". Time. 2007. http://www.time.com/time/photoessays/2007/steve_jobs/. Retrieved 2012-02-21.
Hertzfeld, Andy. "The Original Macintosh". folklore.org. http://folklore.org/ProjectView.py?project=Macintosh&characters=Steve%20Jobs&detail=medium. Retrieved 2012-02-21.
Lohr, Steve (January 12, 1997). "Creating Jobs". The New York Times. http://www.nytimes.com/1997/01/12/magazine/creating-jobs.html. Retrieved 2012-02-21.
Booth, Cathy; Jackson, David S.; Marchant, Valerie (October 6, 2011). "STEVE'S JOB: RESTART APPLE". Time. http://www.time.com/time/magazine/article/0,9171,986849,00.html. Retrieved 2012-02-21.
Elkind, Peter (March 5, 2008). "The trouble with Steve Jobs". Fortune. http://money.cnn.com/2008/03/02/news/companies/elkind_jobs.fortune/index.htm. Retrieved 2008-03-05.
McCracken, Harry (October 5, 2011). "Steve Jobs, 19552011: Mourning Technology's Great Reinventor". Time. http://www.time.com/time/business/article/0,8599,2096251,00.html. Retrieved 2012-02-21.
"The FBI File on Steve Jobs". The Wall Street Journal. http://online.wsj.com/article/SB10001424052970203646004577213042545376200.html. Retrieved 2012-02-21.
Steve Jobs in 1994: The Rolling Stone Interview, Rolling Stone  1994, republished January 17, 2011. Archived URL
Smithsonian Institution Oral History InterviewPDF(143KB)  April 20, 1995.
The Seed of Apple's Innovation, BusinessWeek  October 12, 2004.
How Big Can Apple Get?, Fortune  February 21, 2005.
'Good for the Soul' at the Wayback Machine (archived October 22, 2006) Newsweek, October 15, 2006.
Bill Gates and Steve Jobs (video and transcript of on stage interview), AllThingsD, May 30, 2007.
Videotaped Deposition of Steven P. Jobs in front of the Securities and Exchange Commission, March 18, 2008
Interview with Abdulfattah "John" Jandali, Jobs's biological father, by Mohannad Al-Haj Ali, published in Al Hayat and reprinted by Ya Libnan, February 28, 2011
Steve Jobs on Charlie Rose
"Steve Jobs's Appearances at D, the Full Video Sessions". AllThingsD. http://allthingsd.com/20111005/steve-jobs-appearances-at-d-the-full-sessions/.
.
#(`*KLA Tencor*`)#.
KLA-Tencor Corporation is a supplier of process control and yield management solutions for the semiconductor and related microelectronics industries. These technologies serve the semiconductor, data storage, LED, and other related nanoelectronics industries. The company's portfolio of products, software, analysis, services and expertise is designed to help IC manufacturers manage yield throughout the entire wafer fabrication process-from R & D to final yield analysis.
KLA-Tencor offers a broad spectrum of products and services that include in-line wafer defect monitoring; reticle and photomask defect inspection; wafer overlay; film and surface measurement; and overall yield- and fab-wide data analysis.
In the semiconductor industry, the fabrication of an integrated circuit (IC), or chip, is accomplished by depositing a series of film layers upon a silicon wafer, which forms the foundation of the chip. A single wafer may hold hundreds of chips, each of which will go on to power electronic devices such as MP3 players, cell phones, personal computers and more.
The chip is built layer by layer from patterned films of conducting, semiconducting or insulating materials. The role of KLA-Tencors products in this process is to monitor the defectivity and physical characteristics of each step, allowing the manufacturer to better control each process and maximize chip yield.
KLA-Tencor was formed in April 1997 through the merger of KLA Instruments (KLA) and Tencor Instruments (Tencor), two long-time leaders in the semiconductor equipment industry. Prior to the merger, both businesses served a segment of the inspection and metrology area; with KLA focused on defect inspection and Tencor placing its emphasis on metrology. Merging in a one-to-one stock swap valued at $1.3 billion, KLA-Tencor became the most important process control player in the industry, bringing to market a complete line of yield management products and services from a single company.
KLA was named after its founders, Ken Levy and Bob Anderson. The word Tencor came about because the founder of Tencor, Karel Urbanek, wanted a two syllable name that would be easy to remember.[citation needed]
KLA Instruments was first established in 1975, with its first product emerging on the market in 1978an automated inspection system that reduced photomask inspection time from eight hours to 15 minutes. Shortly thereafter, KLA Instruments went public and expanded its inspection product portfolio to include patterned wafer inspection systems. Two years later, KLA further broadened its offerings into the wafer metrology business through optical overlay and line-width measurement systems. During the subsequent few years, the company expanded its product base through the development of software tools to help integrate inspection and measurement data for analysisultimately forming the industrys first yield management group to provide customers with expertise in yield enhancement through engineering consulting services.
1977, Tencor Instruments established its name, and first introduced its productthe Alpha-Step stylus surface profilerjust seven months later. This tool provided significant improvement in step-height measurement, a critical parameter in measuring film layer thickness. In 1984, Tencor Instruments launched its first Surfscan producta particle and contamination defect system based on laser scanning technology, which soon became the production standard. By the late 1990s, Tencor had broadened its product offerings to also include defect review and data analysis tools. Following an initial public offering in 1993, Tencor then acquired Prometrix, a leading supplier of thin-film measurement tools, and further expanded its product offering. At the time of its merger with KLA, Tencor had revenues of approximately $403 million and 1,400 employees around the world.
Since the merger in 1997, KLA-Tencor has acquired the following companies: 1998  Amray Inc.  Nanopro GmbH  The Quantox product line from Keithley Instruments, Inc  VARS  The Ultrapointe subsidiary of Uniphase Corporation 1999  ACME Systems Inc. 2000  Fab Solutions, from ObjectSpace Inc.  FINLE Technologies, Inc. 2001  Phase Metrics 2004  Candela Instruments  Wafer Inspection Systems business of Inspex, Inc. 2006  ADE Corporation 2007  OnWafer Technologies  SensArray Corporation  Therma-Wave Corporation 2008  ICOS Vision Systems Corporation NV  Microelectronic Inspection Equipment (MIE) business unit of Vistec Semiconductor Systems 2010  Ambios Technology
KLA-Tencor continues to exploit the semiconductor equipment industry and also a number of other industries, including the light emitting diode (LED), data storage, as well as general materials research.
The semiconductor or chip industry is KLA-Tencors core focus. The semiconductor fabrication process begins with a bare silicon wafera round disk that is six, eight or twelve inches in diameter, about as thick as a credit card and gray in color. The process of manufacturing wafers is itself highly sophisticated, involving the creation of large ingots of silicon by pulling them out of a vat of molten silicon. The ingots are then sliced into wafers and polished to a mirror finish. The manufacturing cycle of an IC is grouped into three phases: design, fabrication and testing. IC design involves the architectural layout of the circuit, as well as design verification and reticle generation. The fabrication of a chip is accomplished by depositing a series of film layers that act as conductors, semiconductors or insulators. The deposition of these film layers is interspersed with numerous other process steps that create circuit patterns, remove portions of the film layers, and perform other functions such as heat treatment, measurement and inspection. Most advanced chip designs require hundreds of individual steps, many of which are performed multiple times. Most chips consist of two main structures: the lower structure, typically consisting of transistors or capacitors which perform the smart functions of the chip; and the upper interconnect structure, typically consisting of circuitry which connects the components in the lower structure. When all of the layers on the wafer have been fabricated, each chip on the wafer is tested for functionality. The wafer is then cut into individual devices, and those chips that passed functional testing are packaged. Final testing is performed on all packaged chips.
Richard P. (Rick) Wallace
Chief Executive Officer
Richard (Rick) P. Wallace currently serves as the President and Chief Executive Officer KLA-Tencor Corporation. Previously, Mr. Wallace served as President and Chief Operating Officer from July 2005 through December 2005. He began at KLA Instruments in 1988 as an applications engineer and has held various general management positions throughout his 22 years with the company. He currently serves on the Board of Directors of Semiconductor Equipment and Materials International (SEMI), an industry trade association and Beckman Coulter, an Orange-County based biomedical company. Earlier in his career, Mr. Wallace held positions with Ultratech Stepper and Cypress Semiconductor.
Ben Tsai
EVP, Chief Technology Officer and Corporate Alliances
Ben Tsai is executive vice president, CTO and Corporate Alliances for KLA-Tencor. In his post, Dr. Tsai is responsible for setting the technology direction for the company, business development, corporate alliances and venture investments. He is a key member of KLA-Tencors executive management team, and has held numerous positions at KLA-Tencor and KLA Instruments, prior to the KLA-Tencor merger.
At KLA-Tencor, Dr. Tsai has served as group vice president, CTO of systems and general manager for the Wafer Inspection Division. Prior to the KLA-Tencor merger, Dr. Tsai filled the post of CTO at KLA Instruments as well as numerous executive positions in the Wafer Inspection Division. Dr. Tsai also spent two years as general manager of engineering for KLA Acrotec, a joint venture between KLA Instruments and Japan Energy. In addition to working for KLA-Tencor and KLA Instruments, Dr. Tsai was senior vice president and executive officer, technology at Tokyo Electron Limited, a leading global supplier of semiconductor production equipment. Dr. Tsai currently serves on the board of Varian Semiconductor Associates Inc. Dr. Tsai is also a member of the Technical Advisory Board of Ultratech Inc.
Mark P. Dentinger
Executive Vice President and Chief Financial Officer
Mark P. Dentinger serves as KLA-Tencors Chief Financial Officer. In his position, he oversees and manages the companys Corporate Finance, Operational Finance, Investor Relations and corporate control processes. Furthermore, Dentinger ensures that KLA-Tencor continues to maintain a strong financial foundation to support its growth strategy and future business objectives.
Prior to working for KLA-Tencor, Dentinger held various senior finance roles during his nine years at BEA Systems, Inc. (now part of Oracle Corporation), including Chief Financial Officer for the last three years. In his former post, he managed all aspects of Finance, Investor Relations, Legal, Facilities, and Information Technology, among various other financial roles within the company. Previous to this, Dentinger worked at Compaq Computer Corporation (now part of Hewlett-Packard) for six years. There he served in various financial management positions until he was appointed Director of Finance, High Performance Systems Manufacturing, in 1996. Before joining Compaq, Dentinger spent nearly a decade at Ernst & Young where he worked as a Junior Staff Auditor and later a Senior Manager.
Kenneth Levy
Kenneth Levy is Chairman Emeritus of the KLA-Tencor Corporation. Levy has been associated with the semiconductor capital equipment industry for more than 30 years.
After receiving his Bachelor and Masters degrees in Electrical Engineering and spending six years in the aerospace simulation field, Levy joined the startup team at Computervision Corporation. He was responsible for Computervision's entry into the semiconductor capital equipment industry through their Cobilt Division.
Levy founded KLA Instruments Corporation in 1976.
KLA Instruments and Tencor Instruments merged in 1997.
In addition to his duties at KLA-Tencor, Mr. Levy serves as a director emeritus on the board of Semiconductor Equipment and Materials Institute (SEMI), and as a board member of Juniper Networks, Extreme Networks and Ultratech Stepper, Inc. He also serves on the boards of several privately held companies and acts as advisor to two venture capital partnerships.
In recognition of his many contributions to the industry, Mr. Levy has received numerous awards, including the "SEMMY" Semiconductor Equipment and Materials Institute award in the area of wafer fabrication, the SEMI Lifetime Achievement award, the Harvard Business School Association "Entrepreneur of the Year" award, and elected membership in the National Academy of Engineering, "for the development and commercialization of automated inspection systems for the semiconductor industry."[citation needed]
Karel Urbanek (19381991)
Founder of Tencor Instruments
Karel Urbanek founded Tencor Instruments, which merged with KLA Instruments in 1997.
After graduating from the Charles University Faculty of Mathematics and Physics in 1963, Urbanek immigrated to the United States from Prague, Czechslovakia. He spent a short time at MITs Lincoln Labs in Cambridge before moving to the San Francisco Bay Area, where, after five years at Varian Associates, he founded Randex, Inc.a maker of sputtering equipment for semiconductor manufacturing. In 1976, Urbanek founded Tencor Instruments, which provided measurement and process control equipment for the manufacture of semiconductor chips.
In 1977, seven months after the companys inception, Tencor Instruments introduced its first product, the Alpha-Step. Seven years later, Tencor Instruments launched Surfscan, a product that detects particles on wafers, which continues to be a part of KLA-Tencors leading defect detection product portfolio today. Urbanek continued his leadership of the thriving company as its CEO and Chairman of the Board until his untimely death in 1991.
In 1986, Urbanek became a member of the SEMI board of directors and later vice chairman and member of the executive committee of the board. Urbanek was a dedicated leader of the SEMI International Standards Program, which continues to be a cornerstone of the semiconductor industry. The Karel Urbanek Award, established in 1992, is the most prestigious honor for participants in the SEMI International Standards Program. It is presented regularly to outstanding program members who uphold and further Urbaneks contributions to the development of standards for the semiconductor and related microelectronics industries.
The manufacturing processes KLA-Tencors tools affect include: chip manufacturing, wafer manufacturing, reticle manufacturing, data storage/media head manufacturing, high-brightness light emitting diode (HBLED) manufacturing, compound semiconductor manufacturing, MEMS manufacturing, and general purpose/labs.
KLA-Tencors core product selection includes inspection and metrology (measurement) tools for reticle, substrate and IC manufacturing.
Reticle Inspection - Error-free reticles and photomasks are the first step in achieving high yields, since reticle defects can be replicated on production wafers. Mask shops and IC fabs rely on KLA-Tencors high-sensitivity reticle inspection systems to ensure that masks are defect-free when they are shipped to the fab, and that they remain defect-free during the fabrication process.[citation needed]
Wafer Inspection - KLA-Tencors wafer inspection product line can be further divided into patterned wafer, monitor wafer and bare substrate inspection. Patterned wafer defect inspection, review and analysis tools allow engineers to detect, count, classify and characterize yield failures caused by particles, pattern defects and electrical issues during all stages of the chip manufacturing process. Monitor-wafer and bare-substrate defect inspection include specialized tools used to detect, count and bin defects, then assess surface quality on blanket (unpatterned) films and bare substrate surfaces for process and tool qualification and monitoring.
Metrology - KLA-Tencors metrology solutions address integrated-circuit, substrate, medical device manufacturing, as well as scientific research and other applications. Precise metrology and control of surface topography and electro-optical properties are growing in importance in many industries as critical dimensions narrow, film thicknesses shrink to countable numbers of atomic layers and devices become more complex. KLA-Tencors metrology systems are manufactured and developed in the company's plant in Migdal HaEmek, Israel.
1 History
2 Industry
3 Current Management Team (Updated Apr. 2010)
4 Founding Executives
5 Product Portfolio
6 Customers
7 Competitors
8 External links
Freescale Semiconductor
IBM
Infineon
Intel
NEC
GlobalFoundries
Powerchip Semiconductor
ProMos Technologies
Qualcomm
Renesas
Samsung Electronics
Seagate Technologies
Sony
STMicroelectronics
Toshiba
Toppan
Tower Semiconductor Ltd.
TSMC
Texas Instruments
UMC
Winbond
Seagate
Q-Cells
Applied Materials
Hermes Microvision Inc
Hitachi Electronics Engineering Co., Ltd
Lasertec Corporation (Japan) - http://www.lasertec.co.jp/en/products/semiconductor/index.html
KLA-Tencor homepage
.
#(`*Dow Jones & Company*`)#.
Dow Jones & Company is an American publishing and financial information firm.
The company was founded in 1882 by three reporters: Charles Dow, Edward Jones, and Charles Bergstresser. Like The New York Times and the Washington Post, the company was in recent years publicly traded but privately controlled. The company was led by the Bancroft family, which effectively controlled 64% of all voting stock, before being acquired by News Corporation.
The company became a subsidiary of News Corporation after an extended takeover bid during 2007.[2] It was reported on August 1, 2007 that the bid had been successful[3][4] after an extended period of uncertainty about shareholder agreement.[5] The transaction was completed on December 13, 2007. It was worth US$5 billion or $60 a share, giving NewsCorp control of The Wall Street Journal and ending the Bancroft family's 105 years of ownership.[6]
In 2010, the company sold 90% of Dow Jones Indexes to the CME Group, including the Dow Jones Industrial Average.
Its flagship publication, The Wall Street Journal, is a daily newspaper in print and online covering business, financial national and international news and issues around the globe. It began publishing on July 8, 1889. Other editions of the Journal include:
Other consumer-oriented publications of Dow Jones include Barron's Magazine, a weekly overview of the world economy and markets; MarketWatch.com, the online financial news site; and the consumer magazine SmartMoney which was jointly owned with Hearst Corporation until they sold their stakes to Dow Jones in March 2010.
The monthly journal Far Eastern Economic Review closed in September 2009.
Dow Jones also owns Dow Jones Local Media Group, which publishes several community newspapers in the U.S.
The Dow Jones serves corporate markets and financial markets clients with financial news and information products and services. Its products combine content and technology tools to help drive decisions. Major brands include Dow Jones Newswires, Dow Jones Factiva, Dow Jones Indexes, Dow Jones Client Solutions and Dow Jones Financial Information Services.
In 2009 Dow Jones Ventures launched FINS.com, a standalone resource for financial professionals with information about finance careers and the finance industry.
In broadcasting, Dow Jones provides news content to CNBC in the U.S. It produces two shows for commercial radio, The Wall Street Journal Report on the Wall Street Journal Radio Network and The Dow Jones Report.
Dow Jones sold a 90% stake in its Index business for $607.5M to Chicago-based CME Group, which owns the Chicago Mercantile Exchange, in February 2010.[7] A few of the most widely used include:
The Bancroft family and heirs of Clarence W. Barron once effectively controlled the company class B shares, each with a voting power of ten regular shares, prior to its sale to News Corp. At one time, they controlled 64% of Dow Jones voting stock.[8]
On May 1, 2007, Dow Jones released a statement confirming that News Corporation, led by Rupert Murdoch, had made an unsolicited offer of $60 per share, or $5 billion, for Dow Jones.[9] Stock was briefly halted for pending press release. The halt lasted under 10 minutes while CNBC was receiving data. It has been suggested that the buyout offer is related to Murdoch's new cable business news channel Fox Business that launched in 2007. The Dow Jones brand brings instant credibility to the project.[10]
On June 6, 2007, CEO Brian Tierney of Philadelphia Media Holdings L.L.C., owning company of The Philadelphia Inquirer, Philadelphia Daily News, and Philly.com, went public in an article on Philly.com expressing interest in "joining with outside partners to buy Dow Jones." Tierney said, "We would participate as Philadelphia Media Holdings, along with other investors. We wouldn't do it alone."[11][dead link]
In June, MySpace founder Brad Greenspan put forth a bid to buy 25% of the Dow for $60 a share, the same price per share as News Corporation's bid. Greenspan's offer was for $1.25 billion for 25% of the company.[12]
On July 17, 2007, The Wall Street Journal, a unit of Dow Jones, reported that the company and News Corporation had agreed in principle on a US$5 billion takeover, that the offer would be put to the full Dow Jones board on the same evening in New York, and that the offer valued the company at 70% more than the company's market value.[13]
For too long, Dow Jones has limited its focus to the world of print media and allowed other, less established entities to generate millions of dollars in profits by developing financial reporting franchises on the Internet and cable television.
Upon investigating suspicious share price movements in the run-up to the announcement, the SEC alleged that board member Sir David Li, one of Hong Kong's most prominent businessmen, had informed his close friend and business associate Michael Leung of the impending offer. Leung had acted on this information by telling his daughter and son-in-law, who reaped a US$8.2 million profit from the transaction.[15]
Prior to its sale to News Corp, the last members of the board of directors of the company were: Christopher Bancroft, Lewis B. Campbell, Michael Elefante, John Engler, Harvey Golub, Leslie Hill, Irvine Hockaday, Peter Kann, David Li, M. Peter McPherson (Chairman), Frank Newman, James Ottaway, Elizabeth Steele, and William Steere.
1 Consumer media
2 Local media
3 Enterprise media
4 Ventures
5 Broadcasting
6 Indices
7 Ownership

7.1 Buyout offer

7.1.1 Insider trading scandal




7.1 Buyout offer

7.1.1 Insider trading scandal


7.1.1 Insider trading scandal
8 Corporate governance
9 See also
10 References
11 External links
7.1 Buyout offer

7.1.1 Insider trading scandal


7.1.1 Insider trading scandal
7.1.1 Insider trading scandal
The Wall Street Journal Asia covering news and business in Asia and around the world;
The Wall Street Journal Europe covering news and business in Europe and around the world;
The Wall Street Journal Special Editions, publishing translations of articles for inclusion in local newspapers, notably in Latin America;
The Daily the world's first iPad only newspaper;
Dow Jones Industrial Average (DJIA, "Dow 30", or often simply "The Dow")
Dow Jones Transportation Average
Dow Jones Utility Average
Dow Jones Composite Average
The Global Dow
Dow Jones Global Titans 50 Index
Dow Jones Total Stock Market Index
Dow Jones Sustainability Indexes
Dow Jones-UBS Commodity Indexes
Dow Jones Target Date Indexes
Closing milestones of the Dow Jones Industrial Average
List of assets owned by Dow Jones
Dow Jones corporate site
Dow Jones corporate history
Dow Jones Indexes corporate site
Dow Jones Stock Indexes averages research site
Dow Jones Indexes video on indexing using Latin America as an example
Wall Street Journal
Dow Jones Today
Dow Jones Newswires
Yahoo! Finance - Dow Jones & Company, Inc. Company Profile
MarketWatch news site
Dow Jones Sustainability Indexes
Industry Classification Benchmark - Dow Jones Indexes and FTSE
Overview of company history "Dow Jones Saga Reflects The Forces That Shaped The Wall Street Journal"
.
#(`*Stock market*`)#.
A stock market or equity market is a public entity (a loose network of economic transactions, not a physical facility or discrete entity) for the trading of company stock (shares) and derivatives at an agreed price; these are securities listed on a stock exchange as well as those only traded privately.
The size of the world stock market was estimated at about $36.6trillion at the beginning of October 2008.[1] The total world derivatives market has been estimated at about $791trillion face or nominal value,[2] 11 times the size of the entire world economy.[3] The value of the derivatives market, because it is stated in terms of notional values, cannot be directly compared to a stock or a fixed income security, which traditionally refers to an actual value. Moreover, the vast majority of derivatives 'cancel' each other out (i.e., a derivative 'bet' on an event occurring is offset by a comparable derivative 'bet' on the event not occurring). Many such relatively illiquid securities are valued as marked to model, rather than an actual market price.
The stocks are listed and traded on stock exchanges which are entities of a corporation or mutual organization specialized in the business of bringing buyers and sellers of the organizations to a listing of stocks and securities together. The largest stock market in the United States, by market capitalization, is the New York Stock Exchange (NYSE). In Canada, the largest stock market is the Toronto Stock Exchange. Major European examples of stock exchanges include the Amsterdam Stock Exchange, London Stock Exchange, Paris Bourse, and the Deutsche Brse (Frankfurt Stock Exchange). In Africa, examples include Nigerian Stock Exchange, JSE Limited, etc. Asian examples include the Singapore Exchange, the Tokyo Stock Exchange, the Hong Kong Stock Exchange, the Shanghai Stock Exchange, and the Bombay Stock Exchange. In Latin America, there are such exchanges as the BM&F Bovespa and the BMV. Australia has a national stock exchange, the Australian Securities Exchange, due to the size of its population.
Market participants include individual retail investors, institutional investors such as mutual funds, banks, insurance companies and hedge funds, and also publicly traded corporations trading in their own shares. Some studies have suggested that institutional investors and corporations trading in their own shares generally receive higher risk-adjusted returns than retail investors.[4]
Participants in the stock market range from small individual stock investors to large hedge fund traders, who can be based anywhere in the world. Their orders usually end up with a professional at a stock exchange, who executes the order of buying or selling.
Some exchanges are physical locations where transactions are carried out on a trading floor, by a method known as open outcry. This type of auction is used in stock exchanges and commodity exchanges where traders may enter "verbal" bids and offers simultaneously. The other type of stock exchange is a virtual kind, composed of a network of computers where trades are made electronically via traders.
Actual trades are based on an auction market model where a potential buyer bids a specific price for a stock and a potential seller asks a specific price for the stock. (Buying or selling at market means you will accept any ask price or bid price for the stock, respectively.) When the bid and ask prices match, a sale takes place, on a first-come-first-served basis if there are multiple bidders or askers at a given price.
The purpose of a stock exchange is to facilitate the exchange of securities between buyers and sellers, thus providing a marketplace (virtual or real). The exchanges provide real-time trading information on the listed securities, facilitating price discovery.
The New York Stock Exchange (NYSE) is a physical exchange, also referred to as a listed exchange  only stocks listed with the exchange may be traded, with a hybrid market for placing orders both electronically and manually on the trading floor. Orders executed on the trading floor enter by way of exchange members and flow down to a floor broker, who goes to the floor trading post specialist for that stock to trade the order. The specialist's job is to match buy and sell orders using open outcry. If a spread exists, no trade immediately takes placein this case the specialist should use his/her own resources (money or stock) to close the difference after his/her judged time. Once a trade has been made the details are reported on the "tape" and sent back to the brokerage firm, which then notifies the investor who placed the order. Although there is a significant amount of human contact in this process, computers play an important role, especially for so-called "program trading".
The NASDAQ is a virtual listed exchange, where all of the trading is done over a computer network. The process is similar to the New York Stock Exchange. However, buyers and sellers are electronically matched. One or more NASDAQ market makers will always provide a bid and ask price at which they will always purchase or sell 'their' stock.[5]
The Paris Bourse, now part of Euronext, is an order-driven, electronic stock exchange. It was automated in the late 1980s. Prior to the 1980s, it consisted of an open outcry exchange. Stockbrokers met on the trading floor or the Palais Brongniart. In 1986, the CATS trading system was introduced, and the order matching process was fully automated.
From time to time, active trading (especially in large blocks of securities) have moved away from the 'active' exchanges. Securities firms, led by UBS AG, Goldman Sachs Group Inc. and Credit Suisse Group, already steer 12 percent of U.S. security trades away from the exchanges to their internal systems. That share probably will increase to 18 percent by 2010 as more investment banks bypass the NYSE and NASDAQ and pair buyers and sellers of securities themselves, according to data compiled by Boston-based Aite Group LLC, a brokerage-industry consultant.[6]
Now that computers have eliminated the need for trading floors like the Big Board's, the balance of power in equity markets is shifting. By bringing more orders in-house, where clients can move big blocks of stock anonymously, brokers pay the exchanges less in fees and capture a bigger share of the $11billion a year that institutional investors pay in trading commissions.
Market participants include individual retail investors, institutional investors such as mutual funds, banks, insurance companies and hedge funds, and also publicly traded corporations trading in their own shares. Some studies have suggested that institutional investors and corporations trading in their own shares generally receive higher risk-adjusted returns than retail investors.[4]
A few decades ago, worldwide, buyers and sellers were individual investors, such as wealthy businessmen, usually with long family histories to particular corporations. Over time, markets have become more "institutionalized"; buyers and sellers are largely institutions (e.g., pension funds, insurance companies, mutual funds, index funds, exchange-traded funds, hedge funds, investor groups, banks and various other financial institutions).
The rise of the institutional investor has brought with it some improvements in market operations. There has been a gradual tendency for "fixed" (and exorbitant) fees being reduced for all investors, partly from falling administration costs but also assisted by large institutions challenging brokers' oligopolistic approach to setting standardised fees.
In 12th century France the courretiers de change were concerned with managing and regulating the debts of agricultural communities on behalf of the banks. Because these men also traded with debts, they could be called the first brokers. A common misbelief is that in late 13th century Bruges commodity traders gathered inside the house of a man called Van der Beurze, and in 1309 they became the "Brugse Beurse", institutionalizing what had been, until then, an informal meeting, but actually, the family Van der Beurze had a building in Antwerp where those gatherings occurred;[7] the Van der Beurze had Antwerp, as most of the merchants of that period, as their primary place for trading. The idea quickly spread around Flanders and neighboring counties and "Beurzen" soon opened in Ghent and Rotterdam.
In the middle of the 13th century, Venetian bankers began to trade in government securities. In 1351 the Venetian government outlawed spreading rumors intended to lower the price of government funds. Bankers in Pisa, Verona, Genoa and Florence also began trading in government securities during the 14th century. This was only possible because these were independent city states not ruled by a duke but a council of influential citizens. Italian companies were also the first to issue shares. Companies in England and the Low Countries followed in the 16th century.
The Dutch East India Company (founded in 1602) was the first joint-stock company to get a fixed capital stock and as a result, continuous trade in company stock occurred on the Amsterdam Exchange. Soon thereafter, a lively trade in various derivatives, among which options and repos, emerged on the Amsterdam market. Dutch traders also pioneered short selling - a practice which was banned by the Dutch authorities as early as 1610.[8]
There are now stock markets in virtually every developed and most developing economies, with the world's largest markets being in the United States, United Kingdom, Japan, India, China, Canada, Germany (Frankfurt Stock Exchange), France, South Korea and the Netherlands.[9]
The stock market is one of the most important sources for companies to raise money. This allows businesses to be publicly traded, or raise additional financial capital for expansion by selling shares of ownership of the company in a public market. The liquidity that an exchange affords the investors gives them the ability to quickly and easily sell securities. This is an attractive feature of investing in stocks, compared to other less liquid investments such as real estate.[citation needed] Some companies actively increase liquidity by trading in their own shares.[10][11]
History has shown that the price of shares and other assets is an important part of the dynamics of economic activity, and can influence or be an indicator of social mood. An economy where the stock market is on the rise is considered to be an up-and-coming economy. In fact, the stock market is often considered the primary indicator of a country's economic strength and development.[citation needed]
Rising share prices, for instance, tend to be associated with increased business investment and vice versa. Share prices also affect the wealth of households and their consumption. Therefore, central banks tend to keep an eye on the control and behavior of the stock market and, in general, on the smooth operation of financial system functions. Financial stability is the raison d'tre of central banks.[citation needed]
Exchanges also act as the clearinghouse for each transaction, meaning that they collect and deliver the shares, and guarantee payment to the seller of a security. This eliminates the risk to an individual buyer or seller that the counterparty could default on the transaction.[citation needed]
The smooth functioning of all these activities facilitates economic growth in that lower costs and enterprise risks promote the production of goods and services as well as possibly employment. In this way the financial system is assumed to contribute to increased prosperity.[citation needed]
The financial system in most western countries has undergone a remarkable transformation. One feature of this development is disintermediation. A portion of the funds involved in saving and financing, flows directly to the financial markets instead of being routed via the traditional bank lending and deposit operations. The general public interest in investing in the stock market, either directly or through mutual funds, has been an important component of this process.
Statistics show that in recent decades shares have made up an increasingly large proportion of households' financial assets in many countries. In the 1970s, in Sweden, deposit accounts and other very liquid assets with little risk made up almost 60 percent of households' financial wealth, compared to less than 20 percent in the 2000s. The major part of this adjustment is that financial portfolios have gone directly to shares but a good deal now takes the form of various kinds of institutional investment for groups of individuals, e.g., pension funds, mutual funds, hedge funds, insurance investment of premiums, etc.
The trend towards forms of saving with a higher risk has been accentuated by new rules for most funds and insurance, permitting a higher proportion of shares to bonds. Similar tendencies are to be found in other industrialized countries. In all developed economic systems, such as the European Union, the United States, Japan and other developed nations, the trend has been the same: saving has moved away from traditional (government insured) bank deposits to more risky securities of one sort or another.
(assumes 2% annual dividend)
[12]
From experience we know that investors may 'temporarily' move financial prices away from their long term aggregate price 'trends'. (Positive or up trends are referred to as bull markets; negative or down trends are referred to as bear markets). Over-reactions may occurso that excessive optimism (euphoria) may drive prices unduly high or excessive pessimism may drive prices unduly low. Economists continue to debate whether financial markets are 'generally' efficient.
According to one interpretation of the efficient-market hypothesis (EMH), only changes in fundamental factors, such as the outlook for margins, profits or dividends, ought to affect share prices beyond the short term, where random 'noise' in the system may prevail. (But this largely theoretic academic viewpointknown as 'hard' EMHalso predicts that little or no trading should take place, contrary to fact, since prices are already at or near equilibrium, having priced in all public knowledge.) The 'hard' efficient-market hypothesis is sorely tested by such events as the stock market crash in 1987, when the Dow Jones index plummeted 22.6 percentthe largest-ever one-day fall in the United States.[13]
This event demonstrated that share prices can fall dramatically even though, to this day, it is impossible to fix a generally agreed upon definite cause: a thorough search failed to detect any 'reasonable' development that might have accounted for the crash. (But note that such events are predicted to occur strictly by chance, although very rarely.) It seems also to be the case more generally that many price movements (beyond that which are predicted to occur 'randomly') are not occasioned by new information; a study of the fifty largest one-day share price movements in the United States in the post-war period seems to confirm this.[13]
, a 'soft' EMH has emerged which does not require that prices remain at or near equilibrium, but only that market participants not be able to systematically profit from any momentary market 'inefficiencies'. Moreover, while EMH predicts that all price movement (in the absence of change in fundamental information) is random (i.e., non-trending), many studies have shown a marked tendency for the stock market to trend over time periods of weeks or longer. Various explanations for such large and apparently non-random price movements have been promulgated. For instance, some research has shown that changes in estimated risk, and the use of certain strategies, such as stop-loss limits and Value at Risk limits, theoretically could cause financial markets to overreact. But the best explanation seems to be that the distribution of stock market prices is non-Gaussian (in which case EMH, in any of its current forms, would not be strictly applicable).[14][15]
Other research has shown that psychological factors may result in exaggerated (statistically anomalous) stock price movements (contrary to EMH which assumes such behaviors 'cancel out'). Psychological research has demonstrated that people are predisposed to 'seeing' patterns, and often will perceive a pattern in what is, in fact, just noise. (Something like seeing familiar shapes in clouds or ink blots.) In the present context this means that a succession of good news items about a company may lead investors to overreact positively (unjustifiably driving the price up). A period of good returns also boosts the investor's self-confidence, reducing his (psychological) risk threshold.[16]
Another phenomenonalso from psychologythat works against an objective assessment is group thinking. As social animals, it is not easy to stick to an opinion that differs markedly from that of a majority of the group. An example with which one may be familiar is the reluctance to enter a restaurant that is empty; people generally prefer to have their opinion validated by those of others in the group.
In one paper the authors draw an analogy with gambling.[17] In normal times the market behaves like a game of roulette; the probabilities are known and largely independent of the investment decisions of the different players. In times of market stress, however, the game becomes more like poker (herding behavior takes over). The players now must give heavy weight to the psychology of other investors and how they are likely to react psychologically.
The stock market, as with any other business, is quite unforgiving of amateurs. Inexperienced investors rarely get the assistance and support they need. In the period running up to the 1987 crash, less than 1 percent of the analyst's recommendations had been to sell (and even during the 20002002 bear market, the average did not rise above 5%). In the run up to 2000, the media amplified the general euphoria, with reports of rapidly rising share prices and the notion that large sums of money could be quickly earned in the so-called new economy stock market. (And later amplified the gloom which descended during the 20002002 bear market, so that by summer of 2002, predictions of a DOW average below 5000 were quite common.)
Sometimes, the market seems to react irrationally to economic or financial news, even if that news is likely to have no real effect on the fundamental value of securities itself. But, this may be more apparent than real, since often such news has been anticipated, and a counterreaction may occur if the news is better (or worse) than expected. Therefore, the stock market may be swayed in either direction by press releases, rumors, euphoria and mass panic; but generally only briefly, as more experienced investors (especially the hedge funds) quickly rally to take advantage of even the slightest, momentary hysteria.
Over the short-term, stocks and other securities can be battered or buoyed by any number of fast market-changing events, making the stock market behavior difficult to predict. Emotions can drive prices up and down, people are generally not as rational as they think, and the reasons for buying and selling are generally obscure. Behaviorists argue that investors often behave 'irrationally' when making investment decisions thereby incorrectly pricing securities, which causes market inefficiencies, which, in turn, are opportunities to make money.[18] However, the whole notion of EMH is that these non-rational reactions to information cancel out, leaving the prices of stocks rationally determined.
The Dow Jones Industrial Average biggest gain in one day was 936.42 points or 11 percent, this occurred on October13, 2008.[19]
A stock market crash is often defined as a sharp dip in share prices of equities listed on the stock exchanges. In parallel with various economic factors, a reason for stock market crashes is also due to panic and investing public's loss of confidence. Often, stock market crashes end speculative economic bubbles.
There have been famous stock market crashes that have ended in the loss of billions of dollars and wealth destruction on a massive scale. An increasing number of people are involved in the stock market, especially since the social security and retirement plans are being increasingly privatized and linked to stocks and bonds and other elements of the market. There have been a number of famous stock market crashes like the Wall Street Crash of 1929, the stock market crash of 19734, the Black Monday of 1987, the Dot-com bubble of 2000, and the Stock Market Crash of 2008.
One of the most famous stock market crashes started October 24, 1929 on Black Thursday. The Dow Jones Industrial lost 50% during this stock market crash. It was the beginning of the Great Depression. Another famous crash took place on October 19, 1987  Black Monday. The crash began in Hong Kong and quickly spread around the world.
By the end of October, stock markets in Hong Kong had fallen 45.5%, Australia 41.8%, Spain 31%, the United Kingdom 26.4%, the United States 22.68%, and Canada 22.5%. Black Monday itself was the largest one-day percentage decline in stock market history  the Dow Jones fell by 22.6% in a day. The names Black Monday and Black Tuesday are also used for October 2829, 1929, which followed Terrible Thursdaythe starting day of the stock market crash in 1929.
The crash in 1987 raised some puzzles-main news and events did not predict the catastrophe and visible reasons for the collapse were not identified. This event raised questions about many important assumptions of modern economics, namely, the theory of rational human conduct, the theory of market equilibrium and the efficient-market hypothesis. For some time after the crash, trading in stock exchanges worldwide was halted, since the exchange computers did not perform well owing to enormous quantity of trades being received at one time. This halt in trading allowed the Federal Reserve system and central banks of other countries to take measures to control the spreading of worldwide financial crisis. In the United States the SEC introduced several new measures of control into the stock market in an attempt to prevent a re-occurrence of the events of Black Monday.
Since the early 1990s, many of the largest exchanges have adopted electronic 'matching engines' to bring together buyers and sellers, replacing the open outcry system. Electronic trading now accounts for the majority of trading in many developed countries. Computer systems were upgraded in the stock exchanges to handle larger trading volumes in a more accurate and controlled manner. The SEC modified the margin requirements in an attempt to lower the volatility of common stocks, stock options and the futures market. The New York Stock Exchange and the Chicago Mercantile Exchange introduced the concept of a circuit breaker. The circuit breaker halts trading if the Dow declines a prescribed number of points for a prescribed amount of time. In February 2012, the Investment Industry Regulatory Organization of Canada (IIROC) introduced single-stock circuit breakers.[21]
The movements of the prices in a market or section of a market are captured in price indices called stock market indices, of which there are many, e.g., the S&P, the FTSE and the Euronext indices. Such indices are usually market capitalization weighted, with the weights reflecting the contribution of the stock to the index. The constituents of the index are reviewed frequently to include/exclude stocks in order to reflect the changing business environment.
Financial innovation has brought many new financial instruments whose pay-offs or values depend on the prices of stocks. Some examples are exchange-traded funds (ETFs), stock index and stock options, equity swaps, single-stock futures, and stock index futures. These last two may be traded on futures exchanges (which are distinct from stock exchangestheir history traces back to commodities futures exchanges), or traded over-the-counter. As all of these products are only derived from stocks, they are sometimes considered to be traded in a (hypothetical) derivatives market, rather than the (hypothetical) stock market.
Stock that a trader does not actually own may be traded using short selling; margin buying may be used to purchase stock with borrowed funds; or, derivatives may be used to control large blocks of stocks for a much smaller amount of money than would be required by outright purchase or sales.
In short selling, the trader borrows stock (usually from his brokerage which holds its clients' shares or its own shares on account to lend to short sellers) then sells it on the market, hoping for the price to fall. The trader eventually buys back the stock, making money if the price fell in the meantime and losing money if it rose. Exiting a short position by buying back the stock is called "covering a short position." This strategy may also be used by unscrupulous traders in illiquid or thinly traded markets to artificially lower the price of a stock. Hence most markets either prevent short selling or place restrictions on when and how a short sale can occur. The practice of naked shorting is illegal in most (but not all) stock markets.
In margin buying, the trader borrows money (at interest) to buy a stock and hopes for it to rise. Most industrialized countries have regulations that require that if the borrowing is based on collateral from other stocks the trader owns outright, it can be a maximum of a certain percentage of those other stocks' value. In the United States, the margin requirements have been 50% for many years (that is, if you want to make a $1000 investment, you need to put up $500, and there is often a maintenance margin below the $500).
A margin call is made if the total value of the investor's account cannot support the loss of the trade. (Upon a decline in the value of the margined securities additional funds may be required to maintain the account's equity, and with or without notice the margined security or any others within the account may be sold by the brokerage to protect its loan position. The investor is responsible for any shortfall following such forced sales.)
Regulation of margin requirements (by the Federal Reserve) was implemented after the Crash of 1929. Before that, speculators typically only needed to put up as little as 10 percent (or even less) of the total investment represented by the stocks purchased. Other rules may include the prohibition of free-riding: putting in an order to buy stocks without paying initially (there is normally a three-day grace period for delivery of the stock), but then selling them (before the three-days are up) and using part of the proceeds to make the original payment (assuming that the value of the stocks has not declined in the interim).
Global issuance of equity and equity-related instruments totaled $505billion in 2004, a 29.8% increase over the $389billion raised in 2003. Initial public offerings (IPOs) by US issuers increased 221% with 233 offerings that raised $45billion, and IPOs in Europe, Middle East and Africa (EMEA) increased by 333%, from $ 9billion to $39billion.
One of the many things people always want to know about the stock market is, "How do I make money investing?" There are many different approaches; two basic methods are classified by either fundamental analysis or technical analysis. Fundamental analysis refers to analyzing companies by their financial statements found in SEC Filings, business trends, general economic conditions, etc. Technical analysis studies price actions in markets through the use of charts and quantitative techniques to attempt to forecast price trends regardless of the company's financial prospects. One example of a technical strategy is the Trend following method, used by John W. Henry and Ed Seykota, which uses price patterns, utilizes strict money management and is also rooted in risk control and diversification.
Additionally, many choose to invest via the index method. In this method, one holds a weighted or unweighted portfolio consisting of the entire stock market or some segment of the stock market (such as the S&P 500 or Wilshire 5000). The principal aim of this strategy is to maximize diversification, minimize taxes from too frequent trading, and ride the general trend of the stock market (which, in the U.S., has averaged nearly 10% per year, compounded annually, since World War II).
According to much national or state legislation, a large array of fiscal obligations are taxed for capital gains. Taxes are charged by the state over the transactions, dividends and capital gains on the stock market, in particular in the stock exchanges. However, these fiscal obligations may vary from jurisdictions to jurisdictions because, among other reasons, it could be assumed that taxation is already incorporated into the stock price through the different taxes companies pay to the state, or that tax free stock market operations are useful to boost economic growth.[citation needed]
Public market
Exchange
Securities
Bond valuation
Corporate bond
Fixed income
Government bond
High-yield debt
Municipal bond
Common stock
Preferred stock
Registered share
Stock
Stock certificate
Stock exchange
Voting share
Credit derivative
Futures exchange
Hybrid security
Securitization
Forwards
Options
Spot market
Swaps
Currency
Exchange rate
Commodity market
Money market
Reinsurance market
Real estate market
Clearing house
Financial market participants
Financial regulation
Banks and banking
Corporate finance
Personal finance
Public finance
v
t
e
1 Trade
2 Market participants
3 History
4 Importance of stock market

4.1 Function and purpose
4.2 Relation of the stock market to the modern financial system
4.3 United States S&P stock market returns
4.4 Irrational behavior
4.5 Crashes


4.1 Function and purpose
4.2 Relation of the stock market to the modern financial system
4.3 United States S&P stock market returns
4.4 Irrational behavior
4.5 Crashes
5 Stock market index
6 Derivative instruments
7 Leveraged strategies

7.1 Short selling
7.2 Margin buying


7.1 Short selling
7.2 Margin buying
8 New issuance
9 Investment strategies
10 Taxation
11 See also
12 References
13 Further reading
14 External links
4.1 Function and purpose
4.2 Relation of the stock market to the modern financial system
4.3 United States S&P stock market returns
4.4 Irrational behavior
4.5 Crashes
7.1 Short selling
7.2 Margin buying
New York Stock Exchange (NYSE) circuit breakers[22]
Balance sheet
Dead cat bounce
List of market opening times
List of recessions
List of stock exchanges
List of stock market indices
Modeling and analysis of financial markets
NASDAQ-100
Securities regulation in the United States
Slippage (finance)
Stock market bubble
Stock market cycles
Stock market data systems
Hamilton, W. P. (1922). The Stock Market Baraometer. New York: John Wiley & Sons Inc (1998 reprint). ISBN0-471-24764-2.
Preda, Alex (2009). Framing Finance: The Boundaries of Markets and Modern Capitalism. University of Chicago Press. ISBN978-0-226-67932-7.
Stock exchanges at the Open Directory Project
Stocks investing at the Open Directory Project
.
#(`*NASDAQ*`)#.
The NASDAQ Stock Market, also known as simply the NASDAQ, is an American stock exchange. "NASDAQ" originally stood for National Association of Securities Dealers Automated Quotations.[3] It is the second-largest stock exchange by market capitalization in the world, after the New York Stock Exchange. The exchange is owned by NASDAQ OMX Group, which also owns the OMX stock exchange network.
NASDAQ was founded in 1971 by the National Association of Securities Dealers (NASD), who divested themselves of it in a series of sales in 2000 and 2001. It is owned and operated by the NASDAQ OMX Group, the stock of which was listed on its own stock exchange beginning July 2, 2002, under the ticker symbol NASDAQ:NDAQ. It is regulated by the Financial Industry Regulatory Authority (FINRA), the successor to the NASD.
When the NASDAQ stock exchange began trading on February 8, 1971, it was the world's first electronic stock market. At first, it was merely a computer bulletin board system and did not actually connect buyers and sellers.[4] The NASDAQ helped lower the spread (the difference between the bid price and the ask price of the stock) but somewhat paradoxically was unpopular among brokerages because they made much of their money on the spread.
NASDAQ was the successor to the over-the-counter (OTC) system of trading. As late as 1987, the NASDAQ exchange was still commonly referred to as the OTC in media and also in the monthly Stock Guides issued by Standard & Poor's Corporation.
Over the years, NASDAQ became more of a stock market by adding trade and volume reporting and automated trading systems. NASDAQ was also the first stock market in the United States to start trading online. Nobody before them had ever done this, highlighting NASDAQ-traded companies (usually in technology) and closing with the declaration that NASDAQ is "the stock market for the next hundred years." Its main index is the NASDAQ Composite, which has been published since its inception. However, its exchange-traded fund tracks the large-cap NASDAQ-100 index, which was introduced in 1985 alongside the NASDAQ 100 Financial Index.
Until 1987, most trading occurred via the telephone, but during the October 1987 stock market crash, market makers often didn't answer their phones. To counteract this, the Small Order Execution System (SOES) was established, which provides an electronic method for dealers to enter their trades. NASDAQ requires market makers to honor trades over SOES.
In 1992, it joined with the London Stock Exchange to form the first intercontinental linkage of securities markets. NASD spun off NASDAQ in 2000 to form a publicly traded company, the NASDAQ Stock Market, Inc.
In 2006 NASDAQ changed from stock market to licensed national exchange.
On November 8, 2007, NASDAQ bought the Philadelphia Stock Exchange (PHLX) for US$652 million. PHLX is the oldest stock exchange in Americahaving been in operation since 1790.
To qualify for listing on the exchange, a company must be registered with the United States Securities and Exchange Commission (SEC), have at least three market makers (financial firms that act as brokers or dealers for specific securities) and meet minimum requirements for assets, capital, public shares, and shareholders.
In February, 2011, in the wake of an announced merger of NYSE Euronext with Deutsche Brse, speculation developed that Nasdaq and IntercontinentalExchange (ICE) could mount a counter-bid of their own for NYSE. Nasdaq could be looking to acquire the American exchange's cash equities business, ICE the derivatives business. As of the time of the speculation, "NYSE Euronexts market value was $9.75 billion. Nasdaq was valued at $5.78 billion, while ICE was valued at $9.45 billion."[5] Late in the month, Nasdaq was reported to be considering asking either ICE or the Chicago Merc to join in what would probably have to be, if it proceeded, an $1112 billion counterbid.[6]
EASDAQ (European Association of Securities Dealers Automatic Quotation System) founded originally as a European equivalent to NASDAQ, it was purchased by NASDAQ in 2001 and became NASDAQ Eur03, it shut down operations as a result of the burst of the dot-com bubble. In 2007, NASDAQ Europe was revived as Equiduct and is currently operating under Brse Berlin.[7]
NASDAQ quotes are available at three levels:
NASDAQ has a pre-market session from 7:00am to 9:30am, a normal trading session from 9:30am to 4:00pm and a post-market session from 4:00pm to 8:00pm (all times in ET).[9]
1 History
2 Quote availability
3 Trading schedule
4 Indices
5 Market tiers
6 See also
7 References
8 External links
Level 1 shows the highest bid and lowest offerthe inside quote.
Level 2 shows all public quotes of market makers together with information of market dealers wishing to sell or buy stock and recently executed orders.[8]
Level 3 is used by the market makers and allows them to enter their quotes and execute orders.[citation needed]
NASDAQ-100
NASDAQ Bank
NASDAQ Biotechnology Index
NASDAQ Transportation Index
NASDAQ Composite
NASDAQ Capital Market  Small Cap
NASDAQ Global Market  Mid Cap
NASDAQ Global Select Market  Large Cap
ACT (NASDAQ)
NASDAQ futures
NASDAQ MarketSite
Economy of New York City
Official website
.
#(`*White House*`)#.
The White House is the official residence and principal workplace of the President of the United States. Located at 1600 Pennsylvania Avenue Northwest, Washington, D.C., the house was designed by Irish-born James Hoban,[1] and built between 1792 and 1800 of white-painted Aquia Creek sandstone in the Neoclassical style. It has been the residence of every U.S. president since John Adams. When Thomas Jefferson moved into the house in 1801, he (with architect Benjamin Henry Latrobe) expanded the building outward, creating two colonnades that were meant to conceal stables and storage.[2]
In 1814, during the War of 1812, the mansion was set ablaze by the British Army in the Burning of Washington, destroying the interior and charring much of the exterior. Reconstruction began almost immediately, and President James Monroe moved into the partially reconstructed house in October 1817. Construction continued with the addition of the South Portico in 1824 and the North in 1829. Because of crowding within the executive mansion itself, President Theodore Roosevelt had all work offices relocated to the newly constructed West Wing in 1901. Eight years later, President William Howard Taft expanded the West Wing and created the first Oval Office which was eventually moved as the section was expanded. The third-floor attic was converted to living quarters in 1927 by augmenting the existing hip roof with long shed dormers. A newly constructed East Wing was used as a reception area for social events; Jefferson's colonnades connected the new wings. East Wing alterations were completed in 1946, creating additional office space. By 1948, the house's load-bearing exterior walls and internal wood beams were found to be close to failure. Under Harry S. Truman, the interior rooms were completely dismantled and a new internal load-bearing steel frame constructed inside the walls. Once this work was completed, the interior rooms were rebuilt.
Today, the White House Complex includes the Executive Residence, West Wing, Cabinet Room, Roosevelt Room, East Wing, and the Eisenhower Executive Office Building, which houses the executive offices of the President and Vice President.
The White House is made up of six storiesthe Ground Floor, State Floor, Second Floor, and Third Floor, as well as a two-story basement. The term White House is regularly used as a metonym for the Executive Office of the President of the United States and for the president's administration and advisers in general. The property is a National Heritage Site owned by the National Park Service and is part of the President's Park. In 2007, it was ranked second on the American Institute of Architects list of "America's Favorite Architecture".
Following his April 1789 inauguration, President George Washington occupied two executive mansions in New York City: the Samuel Osgood House at 3 Cherry Street (April 1789 February 1790), and the Alexander Macomb House at 3941 Broadway (FebruaryAugust 1790).
The July 1790 Residence Act named Philadelphia, Pennsylvania, the temporary national capital for a 10-year period while the Federal City was under construction. The City of Philadelphia rented Robert Morris's city house at 190 High Street (now 524-30 Market Street) for Washington's presidential residence. The first president occupied the Market Street mansion from November 1790 to March 1797, and altered it in ways that may have influenced the design of the White House. As part of a futile effort to have Philadelphia named the permanent national capital, Pennsylvania built a presidential palace several blocks away, but Washington declined to move there.
President John Adams also occupied the Market Street mansion from March 1797 to May 1800. In November 1800, he became the first president to occupy the White House. The President's House in Philadelphia became a hotel and was demolished in 1832 while the unused presidential palace became home to the University of Pennsylvania.
The President's House was a major feature of Pierre (Peter) Charles L'Enfant's's plan for the newly established federal city, Washington, D.C.[3][4] The architect of the White House was chosen in a design competition which received nine proposals, including one submitted anonymously by Thomas Jefferson.[5]
President Washington visited Charleston, South Carolina, in May 1791 on his "Southern Tour", and saw the under-construction Charleston County Courthouse designed by Irish architect James Hoban. He is reputed to have met with Hoban then, and summoned the architect to Philadelphia and met with him there in June 1792.[6]
On July 16, 1792, the President met with the commissioners of the federal city to make his judgment in the architectural competition. His review is recorded as being brief, and he quickly selected Hoban's submission.[7]
Washington was not entirely pleased with the original submission, however; he found it too small, lacking ornament, and not monumental enough to house the nation's president. On his recommendation, the house was changed from three stories to two, and was widened from a nine-bay facade to an 11-bay facade. Hoban's competition drawings do not survive.
The building Hoban designed is verifiably influenced by the upper floors of Leinster House, in Dublin, Republic of Ireland, which later became the seat of the Oireachtas (the Irish parliament).[8] Several other Georgian-era Irish country houses have been suggested as sources of inspiration for the overall floor plan, details like the bow-fronted south front, and interior details like the former niches in the present Blue Room. These influences, though undocumented, are cited in the official White House guide, and in White House Historical Association publications. The first official White House guide, published in 1962, suggested a link between Hoban's design for the South Portico and Chteau de Rastignac, a neoclassical country house located in La Bachellerie in the Dordogne region of France and designed by Mathurin Salat. Construction on the French house was initially started before 1789, interrupted by the French Revolution for twenty years and then finally built 18121817 (based on Salat's pre-1789 design).[9] The theoretical link between the two houses has been criticized because Hoban did not visit France. Supporters of a connection posit that Thomas Jefferson, during his tour of Bordeaux in 1789, viewed Salat's architectural drawings (which were on-file at the College) at the cole Spciale d'Architecture (Bordeaux Architectural College).[10] On his return to the U.S. he then shared the influence with Washington, Hoban, Monroe, and Benjamin Henry Latrobe.[9]
Construction of the White House began with the laying of the cornerstone on October 13, 1792, although there was no formal ceremony.[11] The main residence, as well as foundations of the house, were built largely by enslaved and free African-American laborers, as well as employed Europeans.[12] Much of the other work on the house was performed by immigrants, many not yet with citizenship. The sandstone walls were erected by Scottish immigrants, employed by Hoban,[13] as were the high-relief rose and garland decorations above the north entrance and the "fish scale" pattern beneath the pediments of the window hoods. The initial construction took place over a period of eight years, at a reported cost of $232,371.83 (equal to $3,182,127 today). Although not yet completed, the White House was ready for occupancy on or circa November 1, 1800.[14]
Shortages, including material and labor, forced alterations to the earlier plan developed by French engineer Pierre Charles L'Enfant for a "palace" that was five times larger than the house that was eventually built.[13] The finished structure contained only two main floors instead of the planned three, and a less costly brick served as a lining for the stone faades. When construction was finished, the porous sandstone walls were coated with a mixture of lime, rice glue, casein, and lead, giving the house its familiar color and name.[13]
As it is a famed structure in America, several replicas of the White House have been constructed.
The principal facade of the White House, the north front, is of three floors and eleven bays. The ground floor is hidden by a raised carriage ramp and parapet, thus the facade appears to be of two floors. The central three bays are behind a prostyle portico (this was a later addition to the house, built circa 1830) serving, thanks to the carriage ramp, as a porte cochere. The windows of the four bays flanking the portico, at first-floor level, have alternating pointed and segmented pediments, while at second-floor level the pediments are flat. The principal entrance at the centre of the portico is surmounted by lunette fanlight. Above the entrance is a sculpted swag in relief. The roofline is hidden by a balustraded parapet.
The mansion's southern facade is a combination of the Palladian and neoclassical styles of architecture. It is of three floors, all visible. The ground floor is rusticated in the Palladian fashion. At the centre of the facade is a neoclassical projecting bow of three bays. The bow is flanked by 5 bays, the windows of which, as on the north facade, have alternating segmented and pointed pediments at first-floor level. The bow has a ground floor double staircase leading to an Ionic colonnaded loggia (with the Truman Balcony at second-floor level), known as the south portico. The more modern third floor is hidden by a balustraded parapet and plays no part in the composition of the facade.
The building was originally referred to variously as the "President's Palace", "Presidential Mansion", or "President's House".[15] The earliest evidence of the public calling it the "White House" was recorded in 1811.[16] A myth emerged that during the rebuilding of the structure after the Burning of Washington, white paint was applied to mask the burn damage it had suffered,[17] giving the building its namesake hue.[18] The name "Executive Mansion" was used in official contexts until President Theodore Roosevelt established the formal name by having "White HouseWashington" engraved on the stationery in 1901.[19][20] The current letterhead wording and arrangement "The White House" with the word "Washington" centered beneath goes back to the administration of Franklin D. Roosevelt.[20]
Although it was not completed until some years after the presidency of George Washington, it is also speculated that the name of the traditional residence of the President of the United States may have derived from Martha Washington's home, White House Plantation in Virginia, where the nation's first President had courted the First Lady in the mid-18th century.[21]
On Saturday, November 1, 1800, John Adams became the first president to take residence in the building.[13] During Adams' second day in the house, he wrote a letter to his wife Abigail, containing a prayer for the house. Adams wrote:
Theodore Roosevelt had Adams's blessing carved into the mantel in the State Dining Room.[22]
Adams lived in the house only briefly before Thomas Jefferson moved into the "pleasant country residence"[23] in 1801. Despite his complaints that the house was too big ("big enough for two emperors, one pope, and the grand lama in the bargain"[24]), Jefferson considered how the White House might be added to. With Benjamin Henry Latrobe, he helped lay out the design for the East and West Colonnades, small wings that help conceal the domestic operations of laundry, a stable and storage.[13] Today, Jefferson's colonnades link the residence with the East and West Wings.[13]
In 1814, during the War of 1812, the White House was set ablaze by British troops[25] during the Burning of Washington, in retaliation for burning Upper Canada's Parliament Buildings in the Battle of York; much of Washington was affected by these fires as well. Only the exterior walls remained, and they had to be torn down and mostly reconstructed because of weakening from the fire and subsequent exposure to the elements, except for portions of the south wall. Of the numerous objects taken from the White House when it was ransacked by British troops, only two have been recovered. First lady Dolley Madison rescued a painting of George Washington,[25] and in 1939, a Canadian man returned a jewelry box to President Franklin D. Roosevelt, claiming that his grandfather had taken it from Washington. Some observers allege that most of these spoils were lost when a convoy of British ships led by HMS Fantome sank en route to Halifax off Prospect during a storm on the night of November 24, 1814,[26][27] even though Fantome had no inolvement in that action.[28]
After the fire, President James Madison resided in The Octagon House from 1814 to 1815, and then the Seven Buildings from 1815 to the end of his term.[29] Meanwhile, both architect Benjamin Henry Latrobe and Hoban contributed to the design and oversight of the reconstruction, which lasted from 1815 until 1817. The south portico was constructed in 1824 during the James Monroe administration; the north portico was built six years later.[13] Though Latrobe proposed similar porticos before the fire in 1814, both porticos were built as designed by Hoban.[30] An elliptical portico at Chteau de Rastignac in La Bachellerie, France with nearly identical curved stairs is speculated as the source of inspiration due to its similarity with the South Portico,[31] although this matter is one of great debate.[32] Italian artisans, brought to Washington to help in constructing the U.S. Capitol, carved the decorative stonework on both porticos. Contrary to speculation, the North Portico was not modeled on a similar portico on another Dublin building, the Viceregal Lodge (now ras an Uachtarin, residence of the President of Ireland), for its portico postdates the White House porticos' design.[31] For the North Portico, a variation on the Ionic Order was devised incorporating a swag of roses between the volutes. This was done to link the new portico with the earlier carved roses above the entrance.
The White House as it looked following the conflagration of August 24, 1814
Jefferson and Latrobe's West Wing Colonnade in this nineteenth-century engraved view, is now the James S. Brady Press Briefing Room.
Principal story plan for the white house by Benjamin Henry Latrobe, 1807.
Earliest known photograph of the White House, taken c. 1846 by John Plumbe during the administration of James K. Polk.
By the time of the American Civil War, the White House had become overcrowded. The location of the White House was questioned, just north of a canal and swampy lands, which provided conditions ripe for malaria and other unhealthy conditions.[33] Brigadier General Nathaniel Michler was tasked to propose solutions to address these concerns. He proposed abandoning the use of the White House as a residence and designed a new estate for the first family at Meridian Hill in Washington, D.C., but Congress rejected the plan.[33]
The Panic of 1873 had led to an economic depression that persisted through much of the decade. The Statue of Liberty project was not the only undertaking that had difficulty raising money: construction of the obelisk later known as the Washington Monument sometimes stalled for years.[34]
When Chester Arthur took office in 1881, he ordered renovations to the White House to take place as soon as the recently widowed Lucretia Garfield moved out. Arthur inspected the work almost nightly and made several suggestions. Louis Comfort Tiffany was asked to send selected designers to assist. Over twenty wagons of furniture and household items were removed from the building and sold at a public auction.[35] All that was saved were bust portraits of John Adams and Martin Van Buren.[36] A proposal was made to build a new residence south of the White House, but it failed to gain support. In the fall of 1882 work was done on the main corridor, including tinting the walls pale olive and adding squares of gold leaf, and decorating the ceiling in gold and silver, and colorful traceries woven to spell "USA". The Red Room was painted a dull Pomeranian red, and its ceiling was decorated with gold, silver, and copper stars and stripes of red, white, and blue. A fifty-foot jeweled Tiffany glass screen, supported by imitation marble columns, replaced the glass doors that separated the main corridor from the north vestibule.[37]
In 1891, First Lady Caroline Harrison proposed major extensions to the White House, including a National Wing on the east for an historical art gallery, and a wing on the west for official functions.[33] A plan was devised by Colonel Theodore A. Bingham, which reflected the Harrison proposal.[33] These plans were ultimately rejected. However, in 1901 Theodore Roosevelt and his family moved in to the White House and hired McKim, Mead & White to carry out renovations and expansion, including the addition of a West Wing.[13] (McKim designed and managed the project.) President William Howard Taft enlisted the help of architect Nathan C. Wyeth to add additional space to the West Wing, which included the addition of the Oval Office.[33]
The West Wing was damaged by fire in 1929, but rebuilt during the remaining years of the Herbert Hoover presidency. In the 1930s, a second story was added, as well as a larger basement for White House staff, and President Franklin Roosevelt had the Oval Office moved to its present location: adjacent to the Rose Garden.[13]
Decades of poor maintenance, the construction of a fourth story attic during the Coolidge administration, and the addition of a second-floor balcony over the south portico for Harry S. Truman[38] took a great toll on the brick and sandstone structure built around a timber frame.[13] By 1948, the house was declared to be in imminent danger of collapse, forcing President Truman to commission a reconstruction and move across the street to Blair House from 1949 to 1951.[39] The work, done by the firm of Philadelphia contractor John McShain, required the complete dismantling of the interior spaces, construction of a new load-bearing internal steel frame and the reconstruction of the original rooms within the new structure.[38] The total cost of the renovations was about $5.7 million.[40] Some modifications to the floor plan were made, the largest being the repositioning of the grand staircase to open into the Entrance Hall, rather than the Cross Hall.[38] Central air conditioning was added, as well as two additional sub-basements providing space for workrooms, storage, and a bomb shelter.[13] The Trumans moved back into the White House on March 27, 1952.[13] While the house's structure was kept intact by the Truman reconstruction, much of the new interior finishes were generic, and of little historic value. Much of the original plasterwork, some dating back to the 18141816 rebuilding, was too damaged to reinstall, as was the original robust Beaux Arts paneling in the East Room. President Truman had the original timber frame sawn into paneling; the walls of the Vermeil Room, Library, China Room, and Map Room on the ground floor of the main residence were paneled in wood from the timbers.[41]
Jacqueline Kennedy, wife of President John F. Kennedy (196163), directed a very extensive and historic redecoration of the house. She enlisted the help of Henry Francis du Pont of the Winterthur Museum to assist in collecting artifacts for the mansion, many of which had once been housed there.[42] Other antiques, fine paintings, and improvements of the Kennedy period were donated to the White House by wealthy philanthropists, including the Crowninshield family, Jane Engelhard, Jayne Wrightsman, and the Oppenheimer family. Stphane Boudin of the House of Jansen, a Paris interior-design firm that had been recognized worldwide, was employed by Mrs. Kennedy to assist with the decoration.[42] Different periods of the early republic and world history were selected as a theme for each room: the Federal style for the Green Room, French Empire for the Blue Room, American Empire for the Red Room, Louis XVI for the Yellow Oval Room, and Victorian for the president's study, renamed the Treaty Room. Antique furniture was acquired, and decorative fabric and trim based on period documents was produced and installed. The Kennedy restoration resulted in a more authentic White House of grander stature, which recalled the French taste of Madison and Monroe.[42] In the Diplomatic Reception Room Mrs. Kennedy installed an antique "Vue de l'Amrique Nord" wall paper which Zuber & Cie had designed in 1834. The wallpaper had hung previously on the walls of another mansion until 1961 when that house was demolished for a grocery store. Just before the demolition, the wallpaper was salvaged and sold to the White House.
The first White House guidebook was produced under the direction of curator Lorraine Waxman Pearce with direct supervision from Mrs. Kennedy.[43] Sale of the guidebook helped finance the restoration.
Out of respect for the historic character of the White House, no substantive architectural changes have been made to the house since the Truman renovation.[44] Since the Kennedy restoration, every presidential family has made some changes to the private quarters of the White House, but the Committee for the Preservation of the White House must approve any modifications to the State Rooms. Charged with maintaining the historical integrity of the White House, the congressionally authorized committee works with each First Family  usually represented by the First Lady, the White House Curator, and the Chief Usher  to implement the family's proposals for altering the house.[45]
During the Nixon administration (196974), First Lady Pat Nixon refurbished the Green Room, Blue Room, and Red Room, working with Clement Conger, the curator appointed by President Richard Nixon.[46] Mrs. Nixon's efforts brought more than 600 artifacts to the house, the largest acquisition by any administration.[47] Her husband created the modern press briefing room over Franklin Roosevelt's old swimming pool.[48] Nixon also added a single-lane bowling alley to the White House basement.[49]
Computers and the first laser printer were added during the Carter administration, and the use of computer technology was expanded during the Reagan administration.[50] A Carter-era innovation, a set of solar water heating panels that were mounted on the roof of the White House, was removed during Reagan's presidency.[51][52] Redecorations were made to the private family quarters and maintenance was made to public areas during the Reagan years.[53] The house was accredited as a museum in 1988.[53]
In the 1990s, Bill and Hillary Clinton refurbished some rooms with the assistance of Arkansas decorator Kaki Hockersmith, including the Oval Office, the East Room, Blue Room, State Dining Room, Lincoln Bedroom, and Lincoln Sitting Room.[54] During the administration of George W. Bush, first lady Laura Bush refurbished the Lincoln Bedroom in a style contemporary to the Lincoln era; the Green Room, Cabinet Room, and theater were also refurbished.[54]
The White House became one of the first wheelchair-accessible government buildings in Washington when modifications were made during the presidency of Franklin D. Roosevelt, who used a wheelchair because of his paralytic illness. In the 1990s, Hillary Rodham Clinton  at the suggestion of Visitors Office Director Melinda N. Bates  approved the addition of a ramp in the East Wing corridor. It allowed easy wheelchair access for the public tours and special events that enter through the secure entrance building on the east side.
The president usually travels to and from the White House grounds via official motorcade or helicopter. In the 1950s, President Dwight D. Eisenhower was the first president to travel by helicopter to and from the White House grounds.[55]
Today the group of buildings housing the presidency is known as the White House Complex. It includes the central Executive Residence flanked by the East Wing and West Wing. The Chief Usher coordinates day to day household operations. The White House includes: six stories and 55,000ft (5,100 m) of floor space, 132 rooms and 35 bathrooms, 412 doors, 147 windows, twenty-eight fireplaces, eight staircases, three elevators, five full-time chefs, a tennis court, a (single-lane) bowling alley, a movie theater (officially called the White House Family Theater[56]), a jogging track, a swimming pool, and a putting green.[20] It receives up to 30,000 visitors each week.[57]
The original residence is in the center. Two colonnadesone on the east and one on the westdesigned by Jefferson, now serve to connect the East and West Wings, added later. The Executive Residence houses the president's dwelling, as well as rooms for ceremonies and official entertaining. The State Floor of the residence building includes the East Room, Green Room, Blue Room, Red Room, State Dining Room, Family Dining Room, Cross Hall, Entrance Hall, and Grand Staircase.[58] The Ground Floor is made up of the Diplomatic Reception Room, Map Room, China Room, Vermeil Room, Library, the main kitchen, and other offices.[59] The second floor family residence includes the Yellow Oval Room, East and West Sitting Halls, the White House Master Bedroom, President's Dining Room, the Treaty Room, Lincoln Bedroom and Queens' Bedroom, as well as two additional bedrooms, a smaller kitchen, and a private dressing room.[60] The third floor consists of the White House Solarium, Game Room, Linen Room, a Diet Kitchen, and another sitting room (previously used as President George W. Bush's workout room).[61]
The West Wing houses the President's office (the Oval Office) and offices of his senior staff, with room for about 50 employees. It also includes the Cabinet Room, where the president conducts business meetings and where the Cabinet meets,[62] as well as the White House Situation Room, James S. Brady Press Briefing Room, and Roosevelt Room.[63] In 2007, work was completed on renovations of the press briefing room, adding fiber optic cables and LCD screens for the display of charts and graphs.[64] The makeover took 11 months and cost $8 million, of which news outlets paid $2 million.[64] Some members of the President's staff are located in the adjacent Old Executive Office Building, formerly the State War and Navy building, and sometimes known as the Eisenhower Executive Office Building.[64]
This portion of the building was used as the setting for the popular television show The West Wing.
The East Wing, which contains additional office space, was added to the White House in 1942. Among its uses, the East Wing has intermittently housed the offices and staff of the First Lady, and the White House Social Office. Rosalynn Carter, in 1977, was the first to place her personal office in the East Wing and to formally call it the "Office of the First Lady." The East Wing was built during World War II in order to hide the construction of an underground bunker to be used in emergencies. The bunker has come to be known as the Presidential Emergency Operations Center.
The White House and grounds cover just over 18 acres (about 7.3 hectares). Before the construction of the North Portico, most public events were entered from the South Lawn, which was graded and planted by Thomas Jefferson. Jefferson also drafted a planting plan for the North Lawn that included large trees that would have mostly obscured the house from Pennsylvania Avenue. During the mid-to-late 19th century a series of ever larger green houses were built on the west side of the house, where the current West Wing is located. During this period, the North Lawn was planted with ornate carpet-style flowerbeds. Although the White House grounds have had many gardeners through their history, the general design, still largely used as master plan today, was designed in 1935 by Frederick Law Olmsted, Jr. of the Olmsted Brothers firm, under commission from President Franklin D. Roosevelt. During the Kennedy administration, the White House Rose Garden was redesigned by Rachel Lambert Mellon. The Rose garden borders the West Colonnade. Bordering the East Colonnade is the Jacqueline Kennedy Garden, which was begun by Jacqueline Kennedy but completed after her husband's assassination. On the weekend of June 23, 2006, a century-old American Elm (Ulmus americana L.) tree on the north side of the building, came down during one of the many storms amid intense flooding. Among the oldest trees on the grounds are several magnolias (Magnolia grandiflora) planted by Andrew Jackson. Michelle Obama planted the White Houses' first organic garden and installed beehives on the South Lawn of the White House, which will supply organic produce and honey to the First Family and for state dinners and other official gatherings.[65]
The Cross Hall, connecting the State Dining Room and the East Room on the State Floor.
Marine One prepares for landing on the South Lawn where State Arrival Ceremonies for visiting heads of state take place
The Yellow Oval Room in the private, second-floor family residence centered directly above the Blue Room.
The White House and surrounding grounds
Overhead view of the White House grounds
The White House with fountain and grounds.
Like the English and Irish country houses it was modeled on, the White House was, from the start, open to the public until the early part of the 20th century. President Thomas Jefferson held an open house for his second inaugural in 1805, and many of the people at his swearing-in ceremony at the Capitol followed him home, where he greeted them in the Blue Room. Those open houses sometimes became rowdy: in 1829, President Andrew Jackson had to leave for a hotel when roughly 20,000 citizens celebrated his inauguration inside the White House. His aides ultimately had to lure the mob outside with washtubs filled with a potent cocktail of orange juice and whiskey. Even so, the practice continued until 1885, when newly elected Grover Cleveland arranged for a presidential review of the troops from a grandstand in front of the White House instead of the traditional open house. Jefferson also permitted public tours of his house, which have continued ever since, except during wartime, and began the tradition of annual receptions on New Year's Day and on the Fourth of July. Those receptions ended in the early 1930s, although President Bill Clinton would briefly revive the New Year's Day open house in his first term.
The White House remained accessible in other ways; President Abraham Lincoln complained[citation needed] that he was constantly beleaguered by job seekers waiting to ask him for political appointments or other favors, or eccentric dispensers of advice like "General" Daniel Pratt, as he began the business day. Lincoln put up with the annoyance rather than risk alienating some associate or friend of a powerful politician or opinion maker.
In 1974, a stolen Army helicopter landed without authorization on the White House grounds. Twenty years later, in 1994, a light plane crashed on the White House grounds, and the pilot died instantly.[66] As a result of increased security regarding air traffic in the capital, the White House was evacuated in 2005 before an unauthorized aircraft could approach the grounds.[67]
On May 20, 1995, primarily as a response to the Oklahoma City bombing of April 19, 1995, the United States Secret Service closed off Pennsylvania Avenue to vehicular traffic in front of the White House from the eastern edge of Lafayette Park to 17th Street. Later, the closure was extended an additional block to the east to 15th Street, and East Executive Avenue, a small street between the White House and the Treasury Building.[68] The Pennsylvania Avenue closing, in particular, has been opposed by organized civic groups in Washington, D.C. They argue that the closing impedes traffic flow unnecessarily and is inconsistent with the well-conceived historic plan for the city. As for security considerations, they note that the White House is set much further back from the street than numerous other sensitive federal buildings are.[69]
Prior to its inclusion within the fenced compound that now includes the Old Executive Office Building to the west and the Treasury Building to the east, this sidewalk served as a queuing area for the daily public tours of the White House. These tours were suspended in the wake of the September 11 attacks. In September 2003, they resumed on a limited basis for groups making prior arrangements through their Congressional representatives or embassies in Washington for foreign nationals and submitting to background checks, but the White House remains closed to the public.[70] The White House Complex is protected by the United States Secret Service and the United States Park Police.
NASAMS (Norwegian Advanced Surface to Air Missile System) were used to guard air space over Washington, D.C. during the 2005 presidential inauguration. The same NASAMS units has since been used to protect the president and all air space around the White House, which is strictly prohibited to aircraft.[71][72]
For security reasons, the section of Pennsylvania Avenue in front of the White House is now closed to all vehicular traffic, except government officials.
White House at night
White House on the reverse (back) of the U.S. $20 note.
Coordinates: 385352N 770212W / 38.89767N 77.03655W / 38.89767; -77.03655
1 History

1.1 17891800
1.2 Architectural competition
1.3 Design influences
1.4 Construction
1.5 Architectural description
1.6 Naming conventions


1.1 17891800
1.2 Architectural competition
1.3 Design influences
1.4 Construction
1.5 Architectural description
1.6 Naming conventions
2 Evolution of the White House

2.1 Early use, the 1814 fire, and rebuilding
2.2 Overcrowding and building the West Wing
2.3 The Truman reconstruction
2.4 The Kennedy restoration


2.1 Early use, the 1814 fire, and rebuilding
2.2 Overcrowding and building the West Wing
2.3 The Truman reconstruction
2.4 The Kennedy restoration
3 The White House since the Kennedy restoration

3.1 Layout and amenities
3.2 Executive Residence
3.3 West Wing
3.4 East Wing
3.5 Grounds


3.1 Layout and amenities
3.2 Executive Residence
3.3 West Wing
3.4 East Wing
3.5 Grounds
4 Public access and security
5 See also
6 References
7 Further reading
8 External links
1.1 17891800
1.2 Architectural competition
1.3 Design influences
1.4 Construction
1.5 Architectural description
1.6 Naming conventions
2.1 Early use, the 1814 fire, and rebuilding
2.2 Overcrowding and building the West Wing
2.3 The Truman reconstruction
2.4 The Kennedy restoration
3.1 Layout and amenities
3.2 Executive Residence
3.3 West Wing
3.4 East Wing
3.5 Grounds






The White House as it looked following the conflagration of August 24, 1814









Jefferson and Latrobe's West Wing Colonnade in this nineteenth-century engraved view, is now the James S. Brady Press Briefing Room.









Principal story plan for the white house by Benjamin Henry Latrobe, 1807.









Earliest known photograph of the White House, taken c. 1846 by John Plumbe during the administration of James K. Polk.









The Cross Hall, connecting the State Dining Room and the East Room on the State Floor.









Marine One prepares for landing on the South Lawn where State Arrival Ceremonies for visiting heads of state take place









The Yellow Oval Room in the private, second-floor family residence centered directly above the Blue Room.









The White House and surrounding grounds









Overhead view of the White House grounds









The White House with fountain and grounds.









For security reasons, the section of Pennsylvania Avenue in front of the White House is now closed to all vehicular traffic, except government officials.









White House at night









White House on the reverse (back) of the U.S. $20 note.



List of largest historic houses in the United States
Blair House, official guest house of the President
Camp David
Curator of the White House
Germantown White House
Reported White House ghosts
List of National Historic Landmarks in the District of Columbia
List of U.S. Presidential residences
Number One Observatory Circle, residence of the Vice President
Rooms in the White House
Western White House
White House Acquisition Trust
White House Chief Calligrapher
White House Chief Floral Designer
White House Christmas tree
White House Communications Agency
White House Endowment Trust
White House Executive Chef
White House Fellows
White House Graphics and Calligraphy Office
White House History
White House Social Secretary
Abbott, James A. A Frenchman in Camelot: The Decoration of the Kennedy White House by Stphane Boudin. Boscobel Restoration Inc.: 1995. ISBN 0-9646659-0-5.
Abbott, James A. A Frenchman in Camelot: The Decoration of the Kennedy White House by Stphane Boudin. Boscobel Restoration Inc.: 1995. ISBN 0-9646659-0-5.
Abbott James A., and Elaine M. Rice. Designing Camelot: The Kennedy White House Restoration. Van Nostrand Reinhold: 1998. ISBN 0-442-02532-7.
Abbott, James A. Jansen. Acanthus Press: 2006. ISBN 0-926494-33-3.
Clinton, Hillary Rodham. An Invitation to the White House: At Home with History. Simon & Schuster: 2000. ISBN 0-684-85799-5.
Garrett, Wendell. Our Changing White House. Northeastern University Press: 1995. ISBN 1-55553-222-5.
Huchet de Qunetain, Christophe.De quelques bronzes dors franais conservs  la Maison-Blanche  Washington D.C.in La Revue, Pierre Berg & associs, n6, mars 2005 pp.545. OCLC 62701407.
Kenny, Peter M., Frances F. Bretter and Ulrich Leben. Honor Lannuier Cabinetmaker from Paris: The Life and Work of French biniste in Federal New York. The Metropolitan Museum of Art, New York and Harry Abrams: 1998. ISBN 0-87099-836-6.
Leish, Kenneth. The White House. Newsweek Book Division: 1972. ISBN 0-88225-020-5.
McKellar, Kenneth, Douglas W. Orr, Edward Martin, et al. Report of the Commission on the Renovation of the Executive Mansion. Commission on the Renovation of the Executive Mansion, Government Printing Office: 1952.
Monkman, Betty C. The White House: The Historic Furnishing & First Families. Abbeville Press: 2000. ISBN 0-7892-0624-2.
New York Life Insurance Company. The Presidents from 1789 to 1908 and the History of the White House. New York Life Insurance Company: 1908.
Penaud, Guy Dictionnaire des chteaux du Prigord. Editions Sud-Ouest: 1996. ISBN 2-87901-221-X.
Seale, William. The President's House. White House Historical Association and the National Geographic Society: 1986. ISBN 0-912308-28-1.
Seale, William, The White House: The History of an American Idea. White House Historical Association: 1992, 2001. ISBN 0-912308-85-0.
West, J.B. with Mary Lynn Kotz. Upstairs at the White House: My Life with the First Ladies. Coward, McCann & Geoghegan: 1973. ISBN 698-10546-X.
Wolff, Perry. A Tour of the White House with Mrs. John F. Kennedy. Doubleday & Company: 1962.
Exhibition Catalogue, Sale 6834: The Estate of Jacqueline Kennedy Onassis April 2326, 1996. Sothebys, Inc.: 1996.
The White House: An Historic Guide. White House Historical Association and the National Geographic Society: 2001. ISBN 0-912308-79-6.
The White House. The First Two Hundred Years, ed. by Frank Freidel/William Pencak, Boston 1994.
Official website
White House on Facebook
White House on Google+
The White House Historical Association, with historical photos, online tours and exhibits, timelines, and facts
National Park Service website for the President's Park
The White House Museum, a detailed online tour of the White House
Detailed 3D computer model of White House and grounds
Video of "White House Holiday Tour with Laura Bush", C-SPAN Dec 3, 2008
14 Video tours of different White House rooms, C-SPAN Dec 1, 2008
Video of "White House Tour", C-SPAN Jul 7, 1998
.
#(`*President of the United States*`)#.
The President of the United States of America (POTUS)[7] is the head of state and head of government of the United States. The president leads the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces.
Article II of the U.S. Constitution vests the executive power of the United States in the president and charges him with the execution of federal law, alongside the responsibility of appointing federal executive, diplomatic, regulatory, and judicial officers, and concluding treaties with foreign powers, with the advice and consent of the Senate. The president is further empowered to grant federal pardons and reprieves, and to convene and adjourn either or both houses of Congress under extraordinary circumstances.[8] Since the founding of the United States, the power of the president and the federal government have grown substantially[9] and each modern president, despite possessing no formal legislative powers beyond signing or vetoing congressionally passed bills, is largely responsible for dictating the legislative agenda of his party and the foreign and domestic policy of the United States.[10] The president is frequently described as the most powerful person in the world.[11][12][13][14][15]
The president is indirectly elected by the people through the Electoral College to a four-year term, and is one of only two nationally elected federal officers, the other being the Vice President of the United States.[16] The Twenty-second Amendment, adopted in 1951, prohibits anyone from ever being elected to the presidency for a third full term. It also prohibits a person from being elected to the presidency more than once if that person previously had served as president, or acting president, for more than two years of another person's term as president. In all, 43 individuals have served 55 four-year terms.[17] On January 20, 2009, Barack Obama became the 44th and current president. On November 6, 2012, he was re-elected and is scheduled to serve until January 20, 2017.
 Politics portal
In 1776, the Thirteen Colonies, acting through the Second Continental Congress, declared political independence from Great Britain during the American Revolution. The new states, though independent of each other as nation states,[18] recognized the necessity of closely coordinating their efforts against the British.[19] Desiring to avoid anything that remotely resembled a monarchy, Congress negotiated the Articles of Confederation to establish a weak alliance between the states.[18] As a central authority, Congress under the Articles was without any legislative power; it could make its own resolutions, determinations, and regulations, but not any laws, nor any taxes or local commercial regulations enforceable upon citizens.[19] This institutional design reflected the conception of how Americans believed the deposed British system of Crown and Parliament ought to have functioned with respect to the royal dominion: a superintending body for matters that concerned the entire empire.[19] Out from under any monarchy, the states assigned some formerly royal prerogatives (e.g., making war, receiving ambassadors, etc.) to Congress, while severally lodging the rest within their own respective state governments. Only after all the states agreed to a resolution settling competing western land claims did the Articles take effect on March 1, 1781, when Maryland became the final state to ratify them.
In 1783, the Treaty of Paris secured independence for each of the former colonies. With peace at hand, the states each turned toward their own internal affairs.[18] By 1786, Americans found their continental borders besieged and weak, their respective economies in crises as neighboring states agitated trade rivalries with one another, witnessed their hard currency pouring into foreign markets to pay for imports, their Mediterranean commerce preyed upon by North African pirates, and their foreign-financed Revolutionary War debts unpaid and accruing interest.[18] Civil and political unrest loomed. Aiming toward a first step of resolving interstate commercial antagonisms, Virginia called for a trade conference in Annapolis, Maryland, set for September 1786. When the convention failed for lack of attendance due to suspicions among most of the other states, the Annapolis delegates called for a convention to offer revisions to the Articles, to be held the next spring in Philadelphia. Prospects for the next convention appeared bleak until James Madison and Edmund Randolph succeeded in securing George Washington's attendance as a delegate to Philadelphia.[18][20]
When the Constitutional Convention convened in May 1787, the 12 state delegations in attendance (Rhode Island did not send delegates) brought with them an accumulated experience over a diverse set of institutional arrangements between legislative and executive branches from within their respective state governments. Most states maintained a weak executive without veto or appointment powers, elected annually by the legislature to a single term only, sharing power with an executive council, and countered by a strong legislature.[18] New York offered the greatest exception, having a strong, unitary governor with veto and appointment power elected to a three-year term, and eligible for reelection to an indefinite number of terms thereafter.[18] It was through the closed-door negotiations at Philadelphia that the presidency framed in the U.S. Constitution emerged.
The first power the Constitution confers upon the president is the veto. The Presentment Clause requires any bill passed by Congress to be presented to the president before it can become law. Once the legislation has been presented, the president has three options:
In 1996, Congress attempted to enhance the president's veto power with the Line Item Veto Act. The legislation empowered the president to sign any spending bill into law while simultaneously striking certain spending items within the bill, particularly any new spending, any amount of discretionary spending, or any new limited tax benefit. Once a president had stricken the item, Congress could pass that particular item again. If the president then vetoed the new legislation, Congress could override the veto by its ordinary means, a two-thirds vote in both houses. In Clinton v. City of New York, 524 U.S. 417 (1998), the U.S. Supreme Court ruled such a legislative alteration of the veto power to be unconstitutional.
Perhaps the most important of all presidential powers is command of the United States Armed Forces as commander-in-chief. While the power to declare war is constitutionally vested in Congress, the president commands and directs the military and is responsible for planning military strategy. The framers of the Constitution took care to limit the president's powers regarding the military; Alexander Hamilton explains this in Federalist No. 69:
Congress, pursuant to the War Powers Resolution, must authorize any troop deployments longer than 60 days, although that process relies on triggering mechanisms that have never been employed, rendering it ineffectual.[22] Additionally, Congress provides a check to presidential military power through its control over military spending and regulation. While historically presidents initiated the process for going to war,[23][24] critics have charged that there have been several conflicts in which presidents did not get official declarations, including Theodore Roosevelt's military move into Panama in 1903,[23] the Korean War,[23] the Vietnam War,[23] the invasions of Grenada in 1983[25] and Panama in 1990.[26]
Along with the armed forces, the president also directs U.S. foreign policy. Through the Department of State and the Department of Defense, the president is responsible for the protection of Americans abroad and of foreign nationals in the United States. The president decides whether to recognize new nations and new governments, and negotiates treaties with other nations, which become binding on the United States when approved by two-thirds vote of the Senate.
Although not constitutionally provided, presidents also sometimes employ "executive agreements" in foreign relations. These agreements frequently regard administrative policy choices germane to executive power; for example, the extent to which either country presents an armed presence in a given area, how each country will enforce copyright treaties, or how each country will process foreign mail. However, the 20th century witnessed a vast expansion of the use of executive agreements, and critics have challenged the extent of that use as supplanting the treaty process and removing constitutionally prescribed checks and balances over the executive in foreign relations. Supporters counter that the agreements offer a pragmatic solution when the need for swift, secret, and/or concerted action arises.
Suffice it to say that the President is made the sole repository of the executive powers of the United States, and the powers entrusted to him as well as the duties imposed upon him are awesome indeed.
The president is the head of the executive branch of the federal government and is constitutionally obligated to "take care that the laws be faithfully executed."[27] The executive branch has over four million employees, including members of the military.[28]
Presidents make numerous executive branch appointments: an incoming president may make up to 6,000 before he takes office and 8,000 more during his term. Ambassadors, members of the Cabinet, and other federal officers, are all appointed by a president with the "advice and consent" of a majority of the Senate. Appointments made while the Senate is in recess are temporary and expire at the end of the next session of the Senate.
The power of a president to fire executive officials has long been a contentious political issue. Generally, a president may remove purely executive officials at his discretion.[29] However, Congress can curtail and constrain a president's authority to fire commissioners of independent regulatory agencies and certain inferior executive officers by statute.[30]
The president possesses the ability to direct much of the executive branch through executive orders that are grounded in federal law or constitutionally granted executive power. Executive orders are reviewable by federal courts and can be repealed by federal legislation.
To manage the growing federal bureaucracy, Presidents have gradually surrounded themselves with many layers of staff, who were eventually organized into the Executive Office of the President of the United States. Within the Executive Office, the President's innermost layer of aides (and their assistants) are located in the White House Office.
The president also has the power to nominate federal judges, including members of the United States courts of appeals and the Supreme Court of the United States. However, these nominations do require Senate confirmation. Securing Senate approval can provide a major obstacle for presidents who wish to orient the federal judiciary toward a particular ideological stance. When nominating judges to U.S. district courts, presidents often respect the long-standing tradition of Senatorial courtesy. Presidents may also grant pardons and reprieves, as is often done just before the end of a presidential term, not without controversy.[31][32][33]
Historically, two doctrines concerning executive power have developed that enable the president to exercise executive power with a degree of autonomy. The first is executive privilege, which allows the president to withhold from disclosure any communications made directly to the president in the performance of executive duties. George Washington first claimed privilege when Congress requested to see Chief Justice John Jay's notes from an unpopular treaty negotiation with Great Britain. While not enshrined in the Constitution, or any other law, Washington's action created the precedent for the privilege. When Richard Nixon tried to use executive privilege as a reason for not turning over subpoenaed evidence to Congress during the Watergate scandal, the Supreme Court ruled in United States v. Nixon, 418 U.S. 683 (1974), that executive privilege did not apply in cases where a president was attempting to avoid criminal prosecution. When President Bill Clinton attempted to use executive privilege regarding the Lewinsky scandal, the Supreme Court ruled in Clinton v. Jones, 520 U.S. 681 (1997), that the privilege also could not be used in civil suits. These cases established the legal precedent that executive privilege is valid, although the exact extent of the privilege has yet to be clearly defined. Additionally, federal courts have allowed this privilege to radiate outward and protect other executive branch employees, but have weakened that protection for those executive branch communications that do not involve the president.[34]
The state secrets privilege allows the president and the executive branch to withhold information or documents from discovery in legal proceedings if such release would harm national security. Precedent for the privilege arose early in the 19th century when Thomas Jefferson refused to release military documents in the treason trial of Aaron Burr and again in 1876 in Totten v. United States, when the Supreme Court dismissed a case brought by a former Union spy.[35] However, the privilege was not formally recognized by the U.S. Supreme Court until United States v. Reynolds (1953) where it was held to be a common law evidentiary privilege.[36] Before the September 11 attacks, use of the privilege had been rare, but increasing in frequency.[37] Since 2001, the government has asserted the privilege in more cases and at earlier stages of the litigation, thus in some instances causing dismissal of the suits before reaching the merits of the claims, as in the Ninth Circuit's ruling in Mohamed v. Jeppesen Dataplan.[36][38][39] Critics of the privilege claim its use has become a tool for the government to cover up illegal or embarrassing government actions.[40][41]
The Constitution's Ineligibility Clause prevents the President (and all other executive officers) from simultaneously being a member of Congress. Therefore, the president cannot directly introduce legislative proposals for consideration in Congress. However, the president can take an indirect role in shaping legislation, especially if the president's political party has a majority in one or both houses of Congress. For example, the president or other officials of the executive branch may draft legislation and then ask senators or representatives to introduce these drafts into Congress. The president can further influence the legislative branch through constitutionally mandated, periodic reports to Congress. These reports may be either written or oral, but today are given as the State of the Union address, which often outlines the president's legislative proposals for the coming year.
In the 20th century critics began charging that too many legislative and budgetary powers have slid into the hands of presidents that should belong to Congress. As the head of the executive branch, presidents control a vast array of agencies that can issue regulations with little oversight from Congress. One critic charged that presidents could appoint a "virtual army of 'czars'  each wholly unaccountable to Congress yet tasked with spearheading major policy efforts for the White House."[42] Presidents have been criticized for making signing statements when signing congressional legislation about how they understand a bill or plan to execute it.[43] This practice has been criticized by the American Bar Association as unconstitutional.[44] Conservative commentator George Will wrote of an "increasingly swollen executive branch" and "the eclipse of Congress."[45]
According to Article II, Section 3, Clause 2 of the Constitution, the president may convene either or both houses of Congress. If both houses cannot agree on a date of adjournment, the president may appoint a date for Congress to adjourn.
As head of state, the president can fulfill traditions established by previous presidents. William Howard Taft started the tradition of throwing out the ceremonial first pitch in 1910 at Griffith Stadium, Washington, D.C., on the Washington Senators' Opening Day. Every president since Taft, except for Jimmy Carter, threw out at least one ceremonial first ball or pitch for Opening Day, the All-Star Game, or the World Series, usually with much fanfare.[46]
Other presidential traditions are associated with American holidays. Rutherford B. Hayes began in 1878 the first White House egg rolling for local children.[47] Beginning in 1947 during the Harry S. Truman administration, every Thanksgiving the president is presented with a live domestic turkey during the annual national thanksgiving turkey presentation held at the White House. Since 1989, when the custom of "pardoning" the turkey was formalized by George H. W. Bush, the turkey has been taken to a farm where it will live out the rest of its natural life.[48]
Presidential traditions also involve the president's role as head of government. Many outgoing presidents since James Buchanan traditionally give advice to their successor during the presidential transition.[49] Ronald Reagan and his successors have also left a private message on the desk of the Oval Office on Inauguration Day for the incoming president.[50]
During a state visit by a foreign head of state, the president typically hosts a State Arrival Ceremony held on the South Lawn, a custom begun by John F. Kennedy in 1961.[51] This is followed by a state dinner given by the president which is held in the State Dining Room later in the evening.[52]
The modern presidency holds the president as one of the nation's premier celebrities. Some argue that images of the presidency have a tendency to be manipulated by administration public relations officials as well as by presidents themselves. One critic described the presidency as "propagandized leadership" which has a "mesmerizing power surrounding the office."[53] Administration public relations managers staged carefully crafted photo-ops of smiling presidents with smiling crowds for television cameras.[54] One critic wrote the image of John F. Kennedy was described as carefully framed "in rich detail" which "drew on the power of myth" regarding the incident of PT 109[55] and wrote that Kennedy understood how to use images to further his presidential ambitions.[56] As a result, some political commentators have opined that American voters have unrealistic expectations of presidents: voters expect a president to "drive the economy, vanquish enemies, lead the free world, comfort tornado victims, heal the national soul and protect borrowers from hidden credit-card fees."[57]
Most of the nation's Founding Fathers expected the Congress, which was the first branch of government described in the Constitution, to be the dominant branch of government; they did not expect a strong executive.[58] However, presidential power has shifted over time, which has resulted in claims that the modern presidency has become too powerful,[59][60] unchecked, unbalanced,[61] and "monarchist" in nature.[62] Critic Dana D. Nelson believes presidents over the past thirty years have worked towards "undivided presidential control of the executive branch and its agencies."[63] She criticizes proponents of the unitary executive for expanding "the many existing uncheckable executive powers  such as executive orders, decrees, memorandums, proclamations, national security directives and legislative signing statements  that already allow presidents to enact a good deal of foreign and domestic policy without aid, interference or consent from Congress."[63] Activist Bill Wilson opined that the expanded presidency was "the greatest threat ever to individual freedom and democratic rule."[64]
Article II, Section 1, Clause 5 of the Constitution sets the principal qualifications one must meet to be eligible to the office of president. A president must:
A person who meets the above qualifications is still disqualified from holding the office of president under any of the following conditions:
The modern presidential campaign begins before the primary elections, which the two major political parties use to clear the field of candidates before their national nominating conventions, where the most successful candidate is made the party's nominee for president. Typically, the party's presidential candidate chooses a vice presidential nominee, and this choice is rubber-stamped by the convention.
Nominees participate in nationally televised debates, and while the debates are usually restricted to the Democratic and Republican nominees, third party candidates may be invited, such as Ross Perot in the 1992 debates. Nominees campaign across the country to explain their views, convince voters and solicit contributions. Much of the modern electoral process is concerned with winning swing states through frequent visits and mass media advertising drives.
Presidents are elected indirectly in the United States. A number of electors, collectively known as the Electoral College, officially select the president. On Election Day, voters in each of the states and the District of Columbia cast ballots for these electors. Each state is allocated a number of electors, equal to the size of its delegation in both Houses of Congress combined. Generally, the ticket that wins the most votes in a state wins all of that state's electoral votes and thus has its slate of electors chosen to vote in the Electoral College.
The winning slate of electors meet at its state's capital on the first Monday after the second Wednesday in December, about six weeks after the election, to vote. They then send a record of that vote to Congress. The vote of the electors is opened by the sitting vice presidentacting in his capacity as President of the Senateand read aloud to a joint session of the incoming Congress, which was elected at the same time as the president.
Pursuant to the Twentieth Amendment, the president's term of office begins at noon on January 20 of the year following the election. This date, known as Inauguration Day, marks the beginning of the four-year terms of both the president and the vice president. Before executing the powers of the office, a president is constitutionally required to take the presidential oath:
Although not required, presidents have traditionally palmed a Bible while swearing the oath and have added, "So help me God!" to the end of the oath.[68] Further, though no law requires that the oath of office be administered by any specific person, presidents are traditionally sworn in by the Chief Justice of the United States.[citation needed]
The term of office for president and vice president is four years. George Washington, the first president, set an unofficial precedent of serving only two terms, which subsequent presidents followed until 1940. Before Franklin D. Roosevelt, attempts at a third term were encouraged by supporters of Ulysses S. Grant and Theodore Roosevelt; neither of these attempts succeeded. In 1940, Franklin D. Roosevelt declined to seek a third term, but allowed his political party to "draft" him as its presidential candidate and was subsequently elected to a third term. In 1941, the United States entered World War II, leading voters to elect Roosevelt to a fourth term in 1944.
After the war, and in response to Roosevelt being elected to third and fourth terms, the Twenty-second Amendment was adopted. The amendment bars anyone from being elected president more than twice, or once if that person served more than half of another president's term. Harry S. Truman, who was president when the amendment was adopted, and so by the amendment's provisions exempt from its limitation, also briefly sought a third (a second full) term before withdrawing from the 1952 election.
Since the amendment's adoption, four presidents have served two full terms: Dwight D. Eisenhower, Ronald Reagan, Bill Clinton, and George W. Bush. Barack Obama has been elected to a second term. Jimmy Carter and George H. W. Bush sought a second term, but were defeated. Richard Nixon was elected to a second term, but resigned before completing it. Lyndon B. Johnson was the only president under the amendment to be eligible to serve more than two terms in total, having served for only fourteen months following John F. Kennedy's assassination. However, Johnson withdrew from the 1968 Democratic Primary, surprising many Americans. Gerald Ford sought a full term, after serving out the last two years and five months of Nixon's second term, but was not elected.
Vacancies in the office of president may arise under several possible circumstances: death, resignation and removal from office.
Article II, Section 4 of the Constitution allows the House of Representatives to impeach high federal officials, including the president, for "treason, bribery, or other high crimes and misdemeanors." Article I, Section 3, Clause 6 gives the Senate the power to remove impeached officials from office, given a two-thirds vote to convict. The House has thus far impeached two presidents: Andrew Johnson in 1868 and Bill Clinton in 1998. Neither was subsequently convicted by the Senate; however, Johnson was acquitted by just one vote.
Under Section 3 of the Twenty-fifth Amendment, the president may transfer the presidential powers and duties to the vice president, who then becomes acting president, by transmitting a statement to the Speaker of the House and the president pro tempore of the Senate stating the reasons for the transfer. The president resumes the discharge of the presidential powers and duties when he transmits, to those two officials, a written declaration stating that resumption. This transfer of power may occur for any reason the president considers appropriate; in 2002 and again in 2007, President George W. Bush briefly transferred presidential authority to Vice President Dick Cheney. In both cases, this was done to accommodate a medical procedure which required Bush to be sedated; both times, Bush returned to duty later the same day.[69]
Under Section 4 of the Twenty-fifth Amendment, the vice president, in conjunction with a majority of the Cabinet, may transfer the presidential powers and duties from the president to the vice president by transmitting a written declaration to the Speaker of the House and the president pro tempore of the Senate that the president is unable to discharge the presidential powers and duties. If this occurs, then the vice president will assume the presidential powers and duties as acting president; however, the president can declare that no such inability exists and resume the discharge of the presidential powers and duties. If the vice president and Cabinet contest this claim, it is up to Congress, which must meet within two days if not already in session, to decide the merit of the claim.
The United States Constitution mentions the resignation of the president but does not regulate the form of such a resignation or the conditions for its validity. Pursuant to federal law, the only valid evidence of the president's resignation is a written instrument to that effect, signed by the president and delivered to the office of the Secretary of State.[70] This has only occurred once, when Richard Nixon delivered a letter to Henry Kissinger to that effect.
The Constitution states that the vice president becomes president upon the removal from office, death or resignation of the preceding president. If the offices of president and vice president both are either vacant or have a disabled holder of that office, the next officer in the presidential line of succession, the Speaker of the House, becomes acting president. The line then extends to the president pro tempore of the Senate, followed by every member of the Cabinet in a set order.
dollars
The president earns a $400,000 annual salary, along with a $50,000 annual expense account, a $100,000 nontaxable travel account and $19,000 for entertainment.[74][75] The most recent raise in salary was approved by Congress and President Bill Clinton in 1999 and went into effect in 2001.
The White House in Washington, D.C. serves as the official place of residence for the president; he is entitled to use its staff and facilities, including medical care, recreation, housekeeping, and security services. The government pays for state dinners and other official functions, but the president pays for dry cleaning and food that he, his family, and personal guests consume; the high food bill often amazes new residents.[76] Naval Support Facility Thurmont, popularly known as Camp David, is a mountain-based military camp in Frederick County, Maryland used as a country retreat and for high alert protection of the president and his guests. Blair House, located next to the Old Executive Office Building at the White House Complex and Lafayette Park, is a complex of four connected townhouses exceeding 70,000 square feet (6,500m2) of floor space which serves as the president's official guest house and as a secondary residence for the president if needed.[77]
For ground travel, the president uses the presidential state car, which is an armored limousine built on a heavily modified Cadillac-based chassis.[78] One of two identical Boeing VC-25 aircraft, which are extensively modified versions of Boeing 747-200B airliners, serve as long distance travel for the president, and are referred to as Air Force One while the president is on board.[79][80] The president also uses a United States Marine Corps helicopter, designated Marine One when the president is aboard.
The United States Secret Service is charged with protecting the sitting president and the first family. As part of their protection, presidents, first ladies, their children and other immediate family members, and other prominent persons and locations are assigned Secret Service codenames.[81] The use of such names was originally for security purposes and dates to a time when sensitive electronic communications were not routinely encrypted; today, the names simply serve for purposes of brevity, clarity and tradition.[82]
The White House
Camp David
Blair House
Presidential State Car
Air Force One
Marine One
Beginning in 1959, all living former presidents were granted a pension, an office and a staff. The pension has increased numerous times with Congressional approval. Retired presidents now receive a pension based on the salary of the current administration's cabinet secretaries, which is $191,300 each year as of 2008.[83] Some former presidents have also collected congressional pensions.[84][dead link] The Former Presidents Act, as amended, also provides former presidents with travel funds and franking privileges.
Until 1997, all former presidents, and their families, were protected by the Secret Service until the president's death. The last president to have lifetime Secret Service protection was Bill Clinton; all subsequent presidents are protected by the Secret Service for a maximum of ten years after leaving office.[85]
Some presidents have had significant careers after leaving office. Prominent examples include William Howard Taft's tenure as Chief Justice of the United States and Herbert Hoover's work on government reorganization after World War II. Grover Cleveland, whose bid for reelection failed in 1888, was elected president again four years later in 1892. Two former presidents served in Congress after leaving the White House: John Quincy Adams was elected to the House of Representatives, serving there for seventeen years, and Andrew Johnson returned to the Senate in 1875. John Tyler served in the provisional Congress of the Confederate States during the Civil War and was elected to the Confederate House of Representatives.
Presidents may use their predecessors as emissaries to deliver private messages to other nations,[86] or as official representatives of the United States to state funerals and other important foreign events.[87] Richard Nixon made multiple foreign trips to countries including China and Russia, and was lauded as an elder statesman.[88] Jimmy Carter has become a global human rights campaigner, international arbiter and election monitor, and a recipient of the Nobel Peace Prize. Bill Clinton has also worked as an informal ambassador, most recently in the negotiations that led to the release of two American journalists, Laura Ling and Euna Lee from North Korea. Clinton has also been active politically since his presidential term ended, working with his wife Hillary on her presidential bid.
Currently there are four living former presidents:
Jimmy Carter (D),
served 19771981
George H. W. Bush (R),
served 19891993
Bill Clinton (D),
served 19932001
George W. Bush (R),
served 20012009
Each president since Herbert Hoover has created a repository known as a presidential library for preserving and making available his papers, records and other documents and materials. Completed libraries are deeded to and maintained by the National Archives and Records Administration (NARA); the initial funding for building and equipping each library must come from private, non-federal sources.[citation needed] There are currently thirteen presidential libraries in the NARA system. There are also a number of presidential libraries maintained by state governments and private foundations, such as the Abraham Lincoln Presidential Library and Museum, which is run by the State of Illinois.
As many presidents live for many years after leaving office, several of them have personally overseen the building and opening of their own presidential libraries, some even making arrangements for their own burial at the site. Several presidential libraries therefore contain the graves of the president they document, such as the Richard Nixon Presidential Library in Yorba Linda, California and the Ronald Reagan Presidential Library in Simi Valley, California. The graves are viewable by the general public visiting these libraries.
List of Presidents of the United States
   
1 Origin
2 Powers and duties

2.1 Article I legislative role
2.2 Article II executive powers

2.2.1 War and foreign affairs powers
2.2.2 Administrative powers
2.2.3 Juridical powers
2.2.4 Legislative facilitator


2.3 Ceremonial roles
2.4 Critics of presidency's evolution


2.1 Article I legislative role
2.2 Article II executive powers

2.2.1 War and foreign affairs powers
2.2.2 Administrative powers
2.2.3 Juridical powers
2.2.4 Legislative facilitator


2.2.1 War and foreign affairs powers
2.2.2 Administrative powers
2.2.3 Juridical powers
2.2.4 Legislative facilitator
2.3 Ceremonial roles
2.4 Critics of presidency's evolution
3 Selection process

3.1 Eligibility
3.2 Campaigns and nomination
3.3 Election and oath
3.4 Tenure and term limits
3.5 Vacancy or disability


3.1 Eligibility
3.2 Campaigns and nomination
3.3 Election and oath
3.4 Tenure and term limits
3.5 Vacancy or disability
4 Compensation
5 Post-presidency

5.1 Presidential libraries


5.1 Presidential libraries
6 See also

6.1 Lists relating to the United States presidency
6.2 Categories
6.3 Articles


6.1 Lists relating to the United States presidency
6.2 Categories
6.3 Articles
7 Notes
8 References
9 Further reading
10 External links
2.1 Article I legislative role
2.2 Article II executive powers

2.2.1 War and foreign affairs powers
2.2.2 Administrative powers
2.2.3 Juridical powers
2.2.4 Legislative facilitator


2.2.1 War and foreign affairs powers
2.2.2 Administrative powers
2.2.3 Juridical powers
2.2.4 Legislative facilitator
2.3 Ceremonial roles
2.4 Critics of presidency's evolution
2.2.1 War and foreign affairs powers
2.2.2 Administrative powers
2.2.3 Juridical powers
2.2.4 Legislative facilitator
3.1 Eligibility
3.2 Campaigns and nomination
3.3 Election and oath
3.4 Tenure and term limits
3.5 Vacancy or disability
5.1 Presidential libraries
6.1 Lists relating to the United States presidency
6.2 Categories
6.3 Articles
Constitution
Taxation
Congress

House

Speaker
Party leaders
Congressional districts


Senate

President pro tempore
Party leaders




House

Speaker
Party leaders
Congressional districts


Speaker
Party leaders
Congressional districts
Senate

President pro tempore
Party leaders


President pro tempore
Party leaders
House

Speaker
Party leaders
Congressional districts


Speaker
Party leaders
Congressional districts
Senate

President pro tempore
Party leaders


President pro tempore
Party leaders
Speaker
Party leaders
Congressional districts
President pro tempore
Party leaders
President
Vice President
Cabinet
Federal agencies
Federal courts

Supreme Court
Courts of Appeals
District Courts


Supreme Court
Courts of Appeals
District Courts
Supreme Court
Courts of Appeals
District Courts
Presidential elections
Midterm elections
Off-year elections
Democratic
Republican
Third parties
State government

Governors
Legislatures (List)
State courts


Governors
Legislatures (List)
State courts
Local government
Governors
Legislatures (List)
State courts
Other countries
Atlas
v
t
e
If Congress is still convened, the bill becomes law.
If Congress has adjourned, thus preventing the return of the legislation, the bill does not become law. This latter outcome is known as the pocket veto.
be a natural-born citizen of the United States;[note 1]
be at least thirty-five years old;
have been a permanent resident in the United States for at least fourteen years.
Under the Twenty-second Amendment, no person can be elected president more than twice. The amendment also specifies that if any eligible person who serves as president or acting president for more than two years of a term for which some other eligible person was elected president, the former can only be elected president once. Scholars disagree whether anyone no longer eligible to be elected president could be elected vice president, pursuant to the qualifications set out under the Twelfth Amendment.[65]
Under Article I, Section 3, Clause 7, upon conviction in impeachment cases, the Senate has the option of disqualifying convicted individuals from holding other federal offices, including the presidency.[66]
Under Section 3 of the Fourteenth Amendment, no person who swore an oath to support the Constitution, and later rebelled against the United States, can become president. However, this disqualification can be lifted by a two-thirds vote of each house of Congress.
Presidential Amenities






The White House









Camp David









Blair House









Presidential State Car









Air Force One









Marine One



Living former presidents






Jimmy Carter (D),
served 19771981









George H. W. Bush (R),
served 19891993









Bill Clinton (D),
served 19932001









George W. Bush (R),
served 20012009



Category:United States presidential history
Curse of Tippecanoe
Second-term curse
Fiction regarding United States presidential succession
Imperial Presidency
The Imperial Presidency
Imperiled presidency
Presidential $1 Coin Program
Vice President of the United States
President of the Continental Congress
Bumiller, Elisabeth (January 2009). "Inside the Presidency". National Geographic 215 (1): 130149. http://ngm.nationalgeographic.com/2009/01/president/bumiller-text.
Couch, Ernie. Presidential Trivia. Rutledge Hill Press. March 1, 1996. ISBN 1-55853-412-1
Lang, J. Stephen. The Complete Book of Presidential Trivia. Pelican Publishing. September 2001. ISBN 1-56554-877-9
Leonard Leo, James Taranto, and William J. Bennett. Presidential Leadership: Rating the Best and the Worst in the White House. Simon and Schuster, June 2004, hardcover, 304 pages, ISBN 0-7432-5433-3
Presidential Studies Quarterly, published by Blackwell Synergy, is a quarterly academic journal on the presidency.
Waldman, Michael, and George Stephanopoulos. My Fellow Americans: The Most Important Speeches of America's Presidents, from George Washington to George W. Bush. Sourcebooks Trade. September 2003. ISBN 1-4022-0027-7
Winder, Michael K. Presidents and Prophets: The Story of America's Presidents and the LDS Church. Covenant Communications. September 2007. ISBN 1-59811-452-2
"Executive Office of the President". http://www.whitehouse.gov/administration/eop/. Retrieved January 21, 2009.
"White House". http://www.whitehouse.gov/president/. Retrieved October 7, 2005.
A New Nation Votes: American Election Returns, 17871825 Presidential election returns including town and county breakdowns
"Life Portraits of the American Presidents". C-SPAN. http://www.americanpresidents.org/. Retrieved October 7, 2005. Companion website for the C-SPAN television series: American Presidents: Life Portraits
"Presidential Documents from the National Archives". http://www.footnote.com/us-presidents.php. Retrieved March 21, 2007. Collection of letters, portraits, photos, and other documents from the National Archives
"The American Presidency Project". UC Santa Barbara. http://www.presidency.ucsb.edu/. Retrieved October 7, 2005. Collection of over 67,000 presidential documents
The History Channel: US Presidents
"All the President's Roles". Ask Gleaves. http://www.gvsu.edu/hauenstein/index.cfm?id=600041AC-93E7-4378-305E5A2BF6EC3C57. Retrieved October 20, 2006. Article analyzing a president's many hats
Hauenstein Center for Presidential Studies Educational site on the American presidency
"Presidents' Occupations". http://www.factmonster.com/ipka/A0768854.html. Retrieved August 20, 2007. Listing of every President's occupations before and after becoming the Commander in Chief
"The Masonic Presidents Tour". The Masonic Library and Museum of Pennsylvania. http://www.pagrandlodge.org/mlam/presidents/index.html. Retrieved October 7, 2005. Brief histories of the Masonic careers of Presidents who were members of the Freemasons
"The Presidents". American Experience. http://www.pbs.org/wgbh/amex/presidents/. Retrieved March 4, 2007. PBS site on the American presidency
Presidents of the United States: Resource Guides from the Library of Congress
Shapell Manuscript Foundation Images of documents written by U.S. presidents
.
#(`*Minecraft*`)#.
See here
Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang. It was publicly released for the PC on May 17, 2009, as a developmental alpha version and, after gradual updates, was published as a full release version on November 18, 2011. A version for Android was released a month earlier on October 7, and an iOS version was released on November 17, 2011. On May 9, 2012, the game was released on Xbox 360 as an Xbox Live Arcade game, co-developed by 4J Studios. All versions of Minecraft receive periodic updates.
The creative and building aspects of Minecraft allow players to build constructions out of textured cubes in a 3D procedurally generated world. Other activities in the game include exploration, gathering resources, crafting and combat. Gameplay in its commercial release has two principal modes: survival, which requires players to acquire resources and maintain their health and hunger; and creative, where players have an unlimited supply of resources, the ability to fly, and no health or hunger. A third gameplay mode named hardcore is the same as survival, differing only in difficulty; it is set to hardest setting and respawning is disabled, forcing players to delete their worlds upon death.
Minecraft received five awards from the 2011 Game Developers Conference: it was awarded the Innovation Award, Best Downloadable Game Award, and the Best Debut Game Award from the Game Developers Choice Awards; and the Audience Award, as well as the Seumas McNally Grand Prize, from the Independent Games Festival in 2011. In 2012, Minecraft was awarded a Golden Joystick Award in the category Best Downloadable Game.[5] As of December 13, 2012, the game has sold over 8.3 million copies on PC and over 17.5 million copies across all platforms.[6]
Minecraft is an open world game that has no specific goals for the player to accomplish,[7] allowing players a large amount of freedom in choosing how to play the game.[8] However, there is an optional achievement system.[9] The gameplay by default is first person, but players have the option to play in third person mode. The core gameplay revolves around breaking and placing blocks. The game world is essentially composed of rough 3D objectsmainly cubesthat are arranged in a fixed grid pattern and represent different materials, such as dirt, stone, various ores, water, and tree trunks. While players can move freely across the world, objects and items can only be placed at fixed locations relative to the grid. Players can gather these material blocks and place them elsewhere, thus allowing for various constructions. The game primarily consists of two game modes: survival and creative. Unlike in survival mode, in creative mode, players have access to unlimited blocks, regenerate health when damaged, and can fly freely around the world. The game also has a changeable difficulty system of four levels; the easiest difficulty (peaceful) removes any hostile creatures that spawn.[10]
At the start of the game, the player is placed on the surface of a procedurally generated and virtually infinite game world.[11] Players can walk across the terrain consisting of plains, mountains, forests, caves, and various water bodies.[11] The world is divided into biomes ranging from deserts to jungles to snowfields. The in-game time system follows a day and night cycle, with one full cycle lasting 20 real time minutes. Throughout the course of the game, players encounter various non-player characters known as mobs, including animals, villagers and hostile creatures. During the daytime, non-hostile animals, such as cows, pigs, and chickens, spawn. They may be hunted for food and crafting materials. During nighttime and in dark areas, hostile mobs, such as large spiders, skeletons, and zombies spawn.[11] Some Minecraft-unique creatures have been noted by reviewers, such as the Creeper, an exploding creature that sneaks up on the player, and the Enderman, a creature with the ability to teleport and pick up blocks.[12]
The game world is procedurally generated as players explore it, using a seed which is obtained from the system clock at the time of world creation unless manually specified by the player.[13][14] Although limits exist on vertical movement both up and down, Minecraft allows for an infinitely large game world to be generated on the horizontal plane, only running into technical problems when extremely distant locations are reached. The game achieves this by splitting the game world data into smaller sections called "chunks", which are only created or loaded into memory when players are nearby.
Complex systems can be built using the in-game physics engine with the use of primitive mechanical devices, electrical circuits, and logic gates built with an in-game material known as redstone. For example, a door can be opened or closed by pressing a connected button or stepping on a pressure plate. Similarly, larger and more complex systems can be produced, such as a working arithmetic logic unitas used in CPUs.[15]
Minecraft features two alternate dimensions  the Nether and The End.[12] The Nether is a hell-like dimension accessed via player-built portals that contain many unique resources and can be used to travel great distances in the overworld.[16] The End is a barren land in which a boss dragon called Enderdragon dwells.[17] Killing the dragon cues the game's ending credits, written by Irish author Julian Gough.[18] Players are then allowed to teleport back to their original spawn point in the overworld, and will receive "The End" achievement.
In this mode, players have to gather natural resources found in the environment in order to craft certain blocks and items.[11] Depending on the difficulty, monsters spawn at night and other dark places, necessitating that the player builds a shelter.[11] The mode also features a health bar which is depleted by attacks from monsters, falls, drowning, falling into lava, suffocation, starvation, and other events. Players also have a hunger bar, which must be periodically refilled by eating various food (pork chops, bread, etc.) in-game. Health replenishes when players have a full hunger bar, and also regenerates regardless of fullness if players play on the easiest difficulty.
There are a wide variety of items that players can craft in Minecraft.[19] Players can craft armor, which can help mitigate damage from attacks, while weapons such as swords can be crafted to kill enemies and other animals. Players may acquire different resources to craft tools, such as weapons, armor, food, and various other items. By acquiring better resources, players can craft more effective items. For example, tools such as axes, shovels, or pickaxes, can be used to chop down trees, dig soil, and mine ores, respectively; and tools made out of better resources (such as iron in place of stone) perform their tasks more quickly and can be used more heavily before breaking. Players may also trade goods with villager mobs through a bartering system.[20] Emerald ores are often the currency of the villagers, although some trade with wheat or other materials.
The game has an inventory system and players are limited to the number of items they can carry, specifically, 36 spaces. Upon dying, items in the players' inventories are dropped, and players respawn at the current spawn point, which is set by default where players begin the game, but can be reset if players sleep in beds in-game. Dropped items can be recovered if players can reach them before they despawn. Players may acquire experience points by killing mobs and other players, mining, smelting ores, and cooking food. Experience can then be spent on enchanting tools, armor and weapons. Enchanted items are generally more powerful, last longer, or have other special effects.
Players may also play in hardcore mode, a variant of survival mode that differs primarily in the game being locked to the hardest gameplay setting as well as featuring permadeath; upon players' death, their world is deleted.[21] Introduced as a feature in 1.3, if players die in hardcore while playing multiplayer, they are kicked from the server and denied entry.
In creative mode, players have access to unlimited resources or items through the inventory menu, and can place or remove them instantly.[22] Players do not take environmental or mob damage, and are not affected by hunger.[23] They can fly freely[24] around the game world and only die by breaking through bedrock and falling into the void. The game mode helps players focus on building, and is useful for creating large projects.[22]
In adventure mode, the gameplay is similar to survival mode except players are unable to break blocks without appropriate tools. The players may still interact with items (such as chests) and mechanics (such as buttons, levers, and repeaters). This mode was developed for playing player-made custom maps,[25] which often involve rules that prohibit breaking blocks. Another addition designed for custom maps is the command block, a block that allows mapmakers to expand interactions with players through server commands.[26]
Multiplayer on Minecraft is available through player-hosted servers and enables multiple players to interact and communicate with each other on a single world.[27] Players can run their own servers or use a hosting provider. Single player worlds allow LAN connection, so players on the same network can join locally without a server setup.[28] Minecraft multiplayer servers are guided by server operators, who have access to server commands such as setting the time of day and teleporting players around. Operators can also set up restrictions concerning which usernames or IP addresses are allowed to enter the server.[27]
Multiplayer servers offer players a wide range of activities, with some servers having their own unique rules and customs. Competitions are available in some servers, in which players can participate in a variety of games, including racing, battle, and capture the flag objective-based games. A gamemode, PvP (player vs. player), may be enabled to allow fighting between players.[29]
The developer of Minecraft, Markus Persson aka Notch, had previously worked on games such as Wurm Online, and as a game developer for King.com for over four years.[31][32] Persson quit his job at King.com in order to independently develop Minecraft.[31][33] Persson was inspired to create Minecraft by several other games such as Dwarf Fortress, Infiniminer by Zachary Barth, and Dungeon Keeper by Bullfrog Productions. He was still working out the basics of gameplay when he discovered Infiniminer and played with others on the TIGSource.com forums.[32][33] At the time, he had visualized an isometric 3D building game that would be a cross between his inspirations and had made some early prototypes.[31] After discovering Infiniminer, Persson declared, "My god, I realized that that was the game I wanted to do."[34] Infiniminer heavily influenced the style of gameplay that eventually resulted in Minecraft, including the first-person aspect of the game and the "blocky" visual style.[33]
Minecraft was released to the public on May 17, 2009, as a developmental "alpha" release,[35] with a beta release on December 20, 2010.[36][37] Although Persson maintained a day job with Jalbum.net at first, he later moved to working part-time and has since quit in order to work on Minecraft full-time as sales of the beta version of the game have expanded.[32] Persson continues to update the game with releases distributed to users automatically. Persson plans to continue these updates after the release of the full game as long as there is still an active userbase.[33] These updates have included features such as new items, new blocks, an alternate "Hell" dimension (accessible through construction of a portal) that Persson terms "The Nether", tameable wolves that assist the player, and changes to the game's behaviour (e.g., how water flows). Persson plans to eventually release the game as open-source after sales have dropped off and when he wants to move onto other projects.[32]
In September 2010, Persson announced that he and a friend were starting a video game company, Mojang, with the money earned from Minecraft. This company was intended to back the development of Minecraft and an unrelated game, Scrolls, which his friend was to primarily work on. As part of creating the company, Persson hired "an artist, a web site developer, and a business guy", additional programmers, and established an office in Stockholm.[38][39] The four additional employees hired in 2010 were Jens Bergensten, a programmer; Daniel Kaplan, the "business guy"; Jakob Porser, who will be working on the other game for Mojang; and Markus "Junkboy" Toivonen, a pixel artist.[40][41][42] The plans for Persson's new company were delayed by weeks when his account with PayPal, containing over US$763,000 in proceeds from Minecraft sales, was frozen due to a "suspicious withdrawal or deposit".[43]
On December 11, 2010, Persson announced that Minecraft was entering its beta testing phase on December 20, 2010.[44] He further stated that users who bought the game after this date would no longer be guaranteed to receive all future content free of charge as it "scared both the lawyers and the board." However, bug fixes and all updates leading up to and including the release would still be free. At the start of 2011 Mojang expanded to include Carl Manneh as a "managing director" and Tobias Mllstam as a programmer.[45] Mojang moved the game out of beta and released the full version on November 18, 2011.[46] The game has been continuously updated since the release.
Initially, Minecraft.net provided online systems to authenticate logins and host the player's profile including the modifiable character skin pattern and purchased gift codes. On January 18, 2011, Persson announced in a blog post that Minecrafts web servers would be switching to being hosted solely on the Amazon Web Services (AWS) content delivery network. Persson stated in his personal blog that their old web host was having trouble and that Mojang would be switching to using AWS as their host for both Minecraft.net and Minecrafts web functions such as logging in.[47] On February 21, Mojang hired Dan Frisk to oversee the servers and back end for both Minecraft and Scrolls.
In December 1, 2011, Jens Bergensten (also known by his pseudonym Jeb) took full creative control over Minecraft. On December 2, 2011, Persson announced that he would be stepping down as the lead developer of Minecraft, with Bergensten becoming lead developer.[48] Persson would remain as a developer of Minecraft but would be taking time away from the game in order to work on an unannounced project.[49] On February 28, 2012, Bergensten announced that the main developers of Bukkit, a community-based project that works on Minecraft server implementation, joined the ranks of the Mojang team to work on "improving both the server and the client to offer better official support for larger servers and server modifications".[50]
All of Minecraft's soundtrack is non-lyrical ambient music by German composer Daniel "C418" Rosenfeld. On March 4, 2011, Rosenfeld released a soundtrack, titled Minecraft Volume Alpha, which includes most of the songs in Minecraft, as well as other music not in Minecraft.[51]
The PC was the original platform for Minecraft and currently is the most popular version. The game runs on Windows, Mac OS X, and Linux.
An older version of Minecraft, called Minecraft Classic is available online for players.[53] Unlike newer versions of Minecraft, the classic version is free to play, though it is no longer updated. It functions much the same as creative mode, allowing players to build and destroy any and all parts of the world either alone or in a multiplayer server. There are no computer creatures in this mode, and environmental hazards such as lava will not damage players. Some blocks function differently since their behavior was later changed during development.
Minecraft 4k is a simplified version of Minecraft similar to the classic version that was developed for the Java 4K game programming contest "in way less than 4 kilobytes".[54] The map itself is finite - composed of 64x64x64 blocks - and the same world is generated every time. Players are restricted to placing or destroying blocks, which are randomly located and consist of grass, dirt, stone, wood, leaves, and brick.[55]
On August 16, 2011, Minecraft Pocket Edition was released for the Xperia Play on the Android Market as an early alpha version. It was then released for all other compatible devices on October 8, 2011.[56][57] The current version of the software concentrates on the creative building and the primitive survival aspect of the game, and does not contain all the features of the PC and Xbox 360 releases. The current release allows for multiplayer across a local wireless network.[58] An iOS version of Minecraft was released on November 17, 2011.[59] On his Twitter account, Jens Bergensten noted that the Pocket Edition of Minecraft is written in C++ and not Java, due to iOS not being able to support Java.[60] A substantial update (alpha version 0.5.0) was released on November 15, 2012, with feature additions including a Nether Reactor bringing the Pocket Edition closer to the PC version.[61]
The Xbox 360 version of the game, developed by 4J Studios, was released on May 9, 2012.[62][63] It is planned to support Kinect play and cross-platform playability with the PC version.[62] On March 22, 2012, it was announced that Minecraft would be the flagship game in a new Xbox Live promotion called Arcade NEXT.[63]
The game has some features that are exclusive to the Xbox 360 version, including the newly designed crafting system, the control interface, and the ability to play with friends via Xbox Live.[64] On October 16, 2012, update 1.8.2 was made available on Xbox 360 for Minecraft players.[65] This update included many features, such as more mobs, different food, the hunger bar and creative mode.[66]
A port of Minecraft for the Raspberry Pi was officially revealed at MineCon 2012. Mojang stated that the Pi Edition is similar to the Pocket Edition except that is downgraded to an older version, and with the added ability of using text commands to edit the game world. Players can open the game code and use programming language to manipulate things in the game world.[67]
A wide variety of user-generated content for Minecraft, such as modifications, texture packs and custom maps, is available for download from the Internet, especially from fan sites. Modifications of the Minecraft code, called mods, add a variety of gameplay changes, ranging from new blocks, new items, new mobs to entire arrays of mechanisms to craft.[68][69] Examples of these are the Technic Pack and its multiplayer variant Tekkit, which incorporate many other mods that focus on industrial and electrical machines.[70] Texture packs that customize the game's graphics are also available.[71] Custom maps have become popular as well. Players can create their own maps, which often contain challenges, puzzles and quests, and share with others to play.[25] In version 1.4, Mojang added content specifically designed for playing custom maps, such as adventure mode,[25] and command blocks.[26] In 2012, Mojang announced that they plan to add an official modding API.[72]
The Xbox 360 version supports DLC via the Xbox Live Marketplace. Currently, the only content available for purchase are additional character costumes.[73] Unlike the PC version, however, this version does not support player-made mods, texture packs or custom maps.[74]
In September 2010, after an impromptu free-to-play weekend, Minecraft sales spiked with over 25,000 purchases in 24 hours.[75][76] On January 12, 2011, Minecraft passed 1,000,000 purchases,[77][78] less than a month after reaching Beta. At the same time, the game had no publisher backing and has never been commercially advertised except through word of mouth,[79] and apparently unpaid mention in popular media like Penny Arcade.[80] By April 2011, Persson estimated that Minecraft had made 23million (US$33million) in revenue, with 800,000 sales of the alpha version of the game, and over 1million sales of the beta version.[81] On July 1, 2011 Minecraft passed the 10 million registered users mark.[82] As of November 7, 2011, Minecraft had over 16 million registered users, and over 4 million purchases.[83] As of December 13, 2012, the game has sold over 8.3 million copies on PC.[6]
The Xbox 360 version of Minecraft became profitable within the first 24 hours of the game's release when the game broke the Xbox Live sales records with 400,000 players online.[84] Within a week of being on the Xbox Live Marketplace, Minecraft sold upwards of one million copies.[85] It was announced in December 2012 that Minecraft has sold over 4.48 million copies since the game debuted on Xbox LIVE Arcade in May 2012.[6] In addition, Minecraft: Pocket Edition has reached a figure of 5 million in sales bringing the total sales for Minecraft across all platforms to over 17.5 million.[6]
Minecraft generally received favorable responses from critics. The game has been praised for the creative freedom it grants players in-game, as well as the ease of enabling emergent gameplay.[97][98][99] PC Gamer listed Minecraft as the fourth-best game to play at work.[100]
A review of the alpha version, by Scott Munro of the Daily Record, called it "already something special" and urged readers to buy it.[101] Jim Rossignol of Rock, Paper, Shotgun also recommended the alpha of the game, calling it "a kind of generative 8-bit Lego Stalker".[102] On September 17, 2010, gaming webcomic Penny Arcade began a series of comics and news posts about the addictiveness of the game.[103] Video game talk show Good Game gave it a 7.5 and 9 out of 10, praising its creativity and customization, though they criticized its lack of a tutorial.[104]
In December 2010, Good Game selected Minecraft as their choice for "Best Downloadable Game of 2010" title,[105] Gamasutra named it the eighth best game of the year as well as the eighth best indie game of the year,[106][107] and Rock, Paper, Shotgun named it the game of the year.[108] Indie DB awarded the game the 2010 "Indie of the Year" award as chosen by voters, in addition to two out of five Editor's Choice awards for "Most Innovative" and "Best Singleplayer Indie".[109] It was also awarded "Game of the Year" by PC Gamer UK.[110] The game was nominated for the "Seumas McNally Grand Prize", "Technical Excellence", and "Excellence in Design" awards at the March 2011 Independent Games Festival[111] and won the Grand Prize along with community-voted "Audience Award".[112] At Game Developers Choice Awards 2011, Minecraft won the award for Best debut game, Best downloadable game and Most Innovative game award, winning every award for which it was nominated.[113][114][115] It has also won GameCity's videogame arts prize[116]
On May 5, 2011, Minecraft was selected as one of the 80 games that would be displayed at the Smithsonian American Art Museum as part of "The Art of Video Games" exhibit that opened on March 16, 2012.[117][118]
The Xbox 360 version was generally received positively by critics, but did not receive as much praise compared to the PC version. Although reviewers noted the lack of features such as mod support and Creative Mode as disappointing, they acclaimed the version's addition of a tutorial and in-game tips and crafting recipes, saying that they make the game more user-friendly.[74]
In 2012, Minecraft XBLA was awarded a Golden Joystick Award in the Best Downloadable Game category,[5] and a TIGA Games Industry Award in the Best Arcade Game category.[119]
On May 11, 2011, Persson announced that an official Minecraft convention titled "MineCon 2011" would be held on November 1819 at Mandalay Bay Hotel and Casino in Las Vegas, Nevada, and on August 11, the MineCon website was launched.[120] The event included the official launch of Minecraft; keynote speeches, including one by Persson; building and costume contests; Minecraft-themed breakout classes; exhibits by leading gaming and Minecraft-related companies; commemorative merchandise; and autograph and picture times with Mojang employees and well-known contributors from the Minecraft community.[121] After MineCon, there was an Into The Nether after-party with electronic musician deadmau5.[122] Free codes were given to every attendee of MineCon that unlocked alpha versions of Mojang's other upcoming game, Scrolls, as well as an additional non-Mojang game, Cobalt, developed by Oxeye Game Studios.[123] All 4,500 tickets were sold out in MineCon 2011,[124] and nearly 5,000 fans went to the event. On August 2, 2012, Mojang announced that MineCon 2012 would take place in Disneyland Paris from November 2425.[125] The tickets were quickly sold out with each released batch.
A Lego set based on Minecraft called Lego Minecraft was released on June 6, 2012.[126] The set, called "Micro World", centers around the game's player character, Steve, and the antagonist, the Creeper.[127] In December 2011, Mojang submitted the concept of Minecraft merchandise to Lego for the Lego Cuusoo program, from which it quickly received 10,000 votes by users, prompting Lego to review the concept.[128] On January 24, 2012, Lego Cuusoo announced the concept was approved and they would develop sets based around Minecraft.[128] In February 2012, the first Lego Minecraft set of Micro World was showcased and made available for pre-orders, with a release set for the summer of 2012.[129]
Mojang collaborates with Jinx, an online game merchandise store, to sell Minecraft (and eventually Scrolls and 0x10c) products, like T-shirts, foam pickaxes, toys of creatures in the game, and other items of clothing including hats and hoodies.[130] By May 2012, over one million dollars was made from Minecraft merchandise sales, with T-shirts and socks the most popular products.[131]
In early 2011, Minecraft-related videos began to gain popularity on YouTube, often made by commentators and containing screen-capture videos of the game and voice-overs.[132] Common coverage in the videos includes creations made by players, walkthroughs of various tasks, and popular culture parodies. By May 2012, over four million Minecraft-related YouTube videos were uploaded.[131] Some of the popular commentators have received employment at Machinima, a gaming video company that owns the most-viewed entertainment channel on YouTube.[132] A popular group that regularly produces Minecraft videos, attained millions of views, and had the most attended panel at MineCon 2011 is The Yogscast.[132][133] Other famous YouTube personals include Jordan Maron, who has created many Minecraft parodies, including "Minecraft Style", a parody of the international hit single "Gangnam Style.[134]
In 2012 Mojang received offers from Hollywood producers who want to produce Minecraft-related TV shows; however, Mojang stated that they would engage in such projects when "the right idea comes along."[131]
Minecraft has been referenced by other video games, such as RuneScape, Torchlight II, Borderlands 2, Choplifter HD, Super Meat Boy, The Elder Scrolls V: Skyrim, The Binding of Isaac, Team Fortress 2, and FTL: Faster Than Light.[135] It was also referenced by musician deadmau5 in his performances.[136]
After the release of Minecraft, some video games were released with various similarities with Minecraft, and some have been called "clones" of the game. There have been a few Minecraft-like and Minecraft-inspired games across various gaming platforms since the game became popular.[137] Examples include Ace of Spades, CastleMiner, CraftWorld, FortressCraft, Terraria,[138] and Total Miner.
The possible applications of Minecraft have been discussed extensively, especially in the fields of computer-aided design and education. In a panel at MineCon 2011, a Swedish developer discussed the possibility of using the game to redesign public buildings and parks, stating that rendering using Minecraft was much more user-friendly for the community, making it easier to envision the functionality of new buildings and parks.[132]
In 2012, a member of the Human Dynamics group at the MIT Media Lab, Cody Sumter, said that Notch hasn't just built a game. He's tricked 40 million people into learning to use a CAD program. Various software has been developed to allow virtual designs to be printed using professional 3D printers or personal printers such as MakerBot and RepRap.[139]
In September 2012, Mojang began the Block By Block project in cooperation with UN Habitat to create real-world environments in Minecraft.[140] The project allows young people who live in those environments participate in designing the changes they would like to see. Using Minecraft, the community has helped reconstruct the areas of concern, and citizens are invited to enter the Minecraft servers and modify their own neighborhood. Minecraft has turned out to be the perfect tool to facilitate this process, Manneh writes. The three-year partnership will support UN-Habitats Sustainable Urban Development Network to upgrade 300 public spaces by 2016. Mojang signed Minecraft building community, FyreUK, to help render the environments into Minecraft. The first pilot project began in Kibera, one of Nairobis informal settlements, and is already in the planning phase.
The Block By Block project is based on an earlier initiative started in October 2011, Mina Kvarter (My Block), which gave young people in Swedish communities a tool to visualize how they wanted to change their part of town. The project was a helpful way to visualize urban planning ideas without necessarily having a training in architecture. The ideas presented by the citizens were a template for political decisions.[141]
Minecraft has been used in educational settings to enhance students' learning.[142] In 2011, an educational organization named MinecraftEdu was formed with the goal of introducing the Minecraft into schools. The group works with Mojang to make the game affordable and accessible to schools.[143] Educational activities involving the game include labeling objects with vocabulary from foreign languages, building cellular structures, and creating settings that resemble those found in assigned novels.
In a blog post,[13] Persson explains:
PC Release: 1.4.5
Pocket Edition: 0.5.0 (Alpha)
Xbox 360: 1.2 (1.8.1 Beta (Called 1.8.2))
WW November 18, 2011[1]
WW October 7, 2011[2]
WW November 17, 2011[3]
WW May 9, 2012[4]
1 Gameplay

1.1 Survival mode
1.2 Creative mode
1.3 Adventure mode
1.4 Multiplayer


1.1 Survival mode
1.2 Creative mode
1.3 Adventure mode
1.4 Multiplayer
2 Development

2.1 Soundtrack


2.1 Soundtrack
3 Platforms

3.1 Personal computer
3.2 Minecraft Pocket Edition
3.3 Minecraft: Xbox 360 Edition
3.4 Minecraft: Pi Edition


3.1 Personal computer
3.2 Minecraft Pocket Edition
3.3 Minecraft: Xbox 360 Edition
3.4 Minecraft: Pi Edition
4 User-generated content and DLC
5 Reception

5.1 Commercial
5.2 Critical


5.1 Commercial
5.2 Critical
6 MineCon
7 Merchandise
8 Popular culture
9 Applications
10 Footnotes
11 See also
12 References
13 External links
1.1 Survival mode
1.2 Creative mode
1.3 Adventure mode
1.4 Multiplayer
2.1 Soundtrack
3.1 Personal computer
3.2 Minecraft Pocket Edition
3.3 Minecraft: Xbox 360 Edition
3.4 Minecraft: Pi Edition
5.1 Commercial
5.2 Critical
Lightweight Java Game Library, a Java library used by Minecraft.
Minicraft, a 2D action/Survival game made by Markus Persson
Official website
Minecraft Wiki, a Minecraft community wiki
.
#(`*Cross country running*`)#.
Cross country running is a sport in which teams and individuals run a race on open-air courses over natural terrain. The course, typically 412 kilometres (2.57.5 mi) long, may include surfaces of grass and earth, pass through woodlands and open country, and include hills, flat ground and sometimes gravel road. It is both an individual and a team sport, runners are judged on individual times and a points scoring method for teams. Both men and women of all ages compete in cross country, which usually takes place during autumn and winter, and can include weather conditions of rain, sleet, snow or hail, and a wide range of temperatures.
Cross country running is one of the disciplines under the umbrella sport of athletics, long-distance track and road running. Although open-air running competitions are pre-historic, the rules and traditions of cross country racing emerged in Britain. The English championship became the first national competition in 1876 and the International Cross Country Championships was held for the first time in 1903. Since 1973 the foremost elite competition has been the IAAF World Cross Country Championships.[1]
Cross country courses generally are laid out on an open or woodland area. The IAAF recommends that courses be grass-covered, and have rolling terrain with frequent but smooth turns. Courses consist of one or more loops, with a long straight at the start and another leading to the finish line.
Because of variations in conditions, international standardization of cross country courses is impossible, and not necessarily desirable. Part of cross country running's appeal is the natural and distinct characteristics of each venue's terrain and weather. Terrain can vary from open fields to forest hills and even across rivers.
According to the IAAF, an ideal cross country course has a loop of 1,750 to 2,000 metres (1,910 to 2,200 yd) laid out on an open or wooded land. It should be covered by grass, as much as possible, and include rolling hills "with smooth curves and short straights". While it is perfectly acceptable for local conditions to make dirt or snow the primary surface, courses should minimize running on roads or other macadamised paths. Parks and golf courses often provide good locations. While a course may include natural or artificial obstacles, cross country courses support continuous running, and generally do not require climbing over high barriers, through deep ditches, or fighting through underbrush.[2]
A course at least 5 metres (5.5yd) wide allows competitors to pass during the race. Clear markings keep competitors from making wrong turns, and spectators from interfering with the competition. Markings may include tape or ribbon on both sides of the course, chalk or paint on the ground, or cones. Some courses use colored flags to indicate directions: red flags for left turns, yellow flags for right turns and blue flags for continuing straight ahead. Courses also commonly include distance markings, usually at each kilometer or each mile.[3]
The course should have 400 to 1,200 m (440 to 1,300 yd) of straight terrain before the first turn, to reduce contact and congestion at the start. However, many courses at smaller competitions have their first turn after a much shorter distance.[4]
Courses for international competitions consist of a loop between 1750 and 2000 meters. Athletes complete three to six loops, depending on the race. Senior men compete on a 12-kilometre course. Senior women and junior men compete on an 8-kilometre course. Junior women compete on a 6-kilometre course.[2]
In the United States, college men typically compete on 8km (5.0mi) or 10km (6.2mi) courses, while college women race for 5km (3.1mi) or 6km (3.7mi).[4] High school courses may be as short as 2.5km (1.6mi), but the most common distance is 5 kilometers (3.1mi) (although a few states, such as California and Louisiana, race 3 miles (4.8km)).[5]
All runners start at the same time, from a starting arc (or line) marked with lanes or boxes for each team or individual. An official, 50 meters or more in front of the starting line, fires a pistol to indicate the start. If runners collide and fall within the first 100 meters, officials can call the runners back and restart the race. Crossing the line or starting before the starting pistol is fired most often results in disqualification of the runner.
The course ends at a finish line located at the beginning of a funnel or chute (a long walkway marked with flags) that keeps athletes single-file in order of finish and facilitates accurate scoring.
Depending on the timing and scoring system, finish officials may collect a small slip from each runner's bib, to keep track of finishing positions. An alternative method (common in the UK) is to have four officials in two pairs. In the first pair, one official reads out numbers of finishers and the other records them. In the second pair, one official reads out times for the other to record. At the end of the race the two lists are joined along with information from the entry information. The major disadvantage of this system is that distractions can easily upset the results, particularly when large numbers of runners finish close together.
Chip timing has grown in popularity to increase accuracy and decrease the number of officials required at the finish line. Each runner attaches a transponder with RFID to his or her shoe. When the runner crosses the finish line an electronic pad records the chip number and matches the runner to a database. Chip timing allows officials to use checkpoint mats throughout the race to calculate split times, and to ensure runners cover the entire course. This is by far the most accurate method, although it is the most expensive.[3]
Scores are determined by summing the top four or five individual finishing places on each team. In international competition, a team typically consists of six runners, with the top four scoring. In the United States, the most common scoring system is seven runners, with the top five scoring. Points are awarded to the individual runners of eligible teams, equal to the position in which they cross the finish line (first place gets 1 point, second place gets 2 points, etc.). The points for these runners are summed, and the low score wins. Individual athletes, and athletes from incomplete teams are excluded from scoring. Ties can be broken in several ways. In international competition, ties are resolved in favour of the team whose last scoring member finishes nearer to first place. In high school competition, ties are resolved in favor of the team whose next non-scoring member finishes first. In U.S. college competition, ties are not resolved.[4]
The lowest possible score in a five-to-score match is 15 (1+2+3+4+5), achieved by a team's runners finishing in each of the top five positions. If there is a single opposing team then they would have a score of 40 (6+7+8+9+10), which can be considered a "sweep" for the winning team. In some competitions a team's sixth and seventh runner are scored in the overall field and are known as "pushers" or "displacers" as their place can count ahead of other runners. In the above match, if there are two non-scoring runners and they came 6th and 7th overall, the opponent's score would be 50 (8+9+10+11+12). Accordingly, the official score of a forfeited dual meet is 15-50.
Because of differences between courses in running surface, frequency and tightness of turns, and amount of up and downhill, cross country strategy does not necessarily simplify to running a steady pace from start to finish. Coaches and cross country runners debate the relative merits of fast starts to get clear of the field, versus steady pacing to maximize physiological efficiency. Some teams emphasize running in a group in order to provide encouragement to others on the team, while others hold that every individual should run his or her own race. In addition,whether you run ahead 'of the pack' or behind it and pull ahead in the end is important, but can vary according to the runner's individual skill, endurance, and the length of the race. Most important, however, is the training beforehand.[6][7][8]
Cross country running involves very little specialized equipment. Most races are run in shorts and vests or singlets, usually in club or school colors. In particularly cold conditions, long-sleeved shirts and tights can be worn to retain warmth without losing mobility. The most common footwear are cross country spikes, lightweight racing shoes with a rubber sole and approximately six metal spikes screwed into the forefoot part of the sole. Spike length depends on race conditions, with a muddy course appropriate for spikes as long as 25 millimetres (0.98in). If a course has a harder surface, spikes as short as 6 millimetres (0.24in) may be most effective. While spikes are suitable for grassy, muddy, or other slippery conditions, runners may choose to wear racing flats, rubber-soled racing shoes without spikes, if the course includes significant portions of paved surfaces or dirt road.[5]
While humans have raced each other over natural terrain since before recorded history, formal cross country competition traces its history to the 19th century and an English game called "hare and hounds" or "the paper chase". English schools started competing in cross country races in 1837, and established a national championship on December 7, 1867. It was held on the Wimbledon Common in England, the same location of the tennis competition. It was the first cross country race that was considered "open", or could be run by anyone. Its original purpose was to imitate steeplechase for off-season training, and was considered a bit of a joke. The race was about 3.5 miles long, and went through very boggy and hilly terrain. The course was not well marked, and many competitors got lost. Matters were not helped by the fact that the race was run in the dark, as it began at 5 pm. [9]
Cross country was contested as a team and individual event at the 1904, 1912, 1920 and 1924 Summer Olympics. A United States team won the gold medal for cross country in the 1904 Olympics, Sweden took gold in 1912, and Finland, led by Paavo Nurmi, captured the gold in 1920 and 1924. During the 1924 race in the Paris heat wave, only 15 of the 38 competitors reached the finish.[10] Eight of those were taken away on stretchers.[10] One athlete began to run in tight circles after reaching the stadium and later knocked himself unconscious,[11] while another fainted 50 metres from the finish.[12] Jos Anda and Edvin Wide were reported dead,[13] and medics spent hours trying to find all the competitors who had blacked out along the course.[12] Although the reports of deaths were unfounded, spectators were shocked by the attrition rate and Olympic officials decided to ban cross country running from future Games.[13] Since 1928, cross country has been contested only as the fifth discipline of the modern pentathlon, and is the only discipline where the Olympic competition is only part of the modern pentathlon.[14]
Europeans dominated early International Cross Country Championships, first held at the Hamilton Park Racecourse in Scotland on 28 March 1903. England won the first 14 titles, and 43 of 59 until the IAAF took over the competition in 1973. France was the next most successful country in the early years, winning 12 championships between 1922 and 1956. Belgium is the only other country to win at the International Cross Country Championship, capturing titles in 1948, 1957, 1961 and 1963. The English also dominated the individual competition, with an Englishman winning the individual title 35 times, including three wins by Jack Holden (19331935).
The first international cross country championship for women was held in 1931, and thirteen more times through 1972. England won 12 of these early championships, losing only in 1968 and 1969 (to the United States). American Doris Brown won five consecutive individual titles between 1967 and 1971.
Beginning in 1973, the IAAF began hosting the renamed World Cross Country Championships each year. In 1975, the New Zealand men and United States women won, marking the first championships by non-European countries. In 1981 an African nation (Ethiopia) won the men's race for the first time, and a decade later an African nation (Kenya) won the women's race for the first time. Ethiopia or Kenya has captured every men's title since 1981 and every women's title since 2001. Through 2010, Kenya has won 40 World Cross Country Championships and Ethiopia has won 23.[15]
In addition to the World Cross Country Championships, the IAAF sponsors six annual area-level competitions: the African Cross Country Championships, Asian Cross Country Championships, European Cross Country Championships, NACAC Cross Country Championships, Oceania Cross Country Championships and South American Cross Country Championships.
Beyond championships, IAAF world cross country meetings include the Great Edinburgh International Cross Country, Cross Internacional de Itlica, Antrim International Cross Country, Cinque Mulini, Nairobi Cross, Chiba International Cross Country, Fukuoka International Cross Country meet, Eurocross and Almond Blossom Cross Country.[17]
Cross country running is organised at the state level by the athletics association for each state. In Queensland this Queensland Athletics.[18] In the Masters category (over 30), this is organised by Australian Masters Athletics. Brisbane will host the Australian Masters Nationals Championships,[19] April 2124, 2011 with the Cross Country hosted by Thompson Estate and Eastern Suburbs Athletics [20] at Minnippi Parklands.
The cross country season in Brisbane is usually March - September. During the season there is usually one race each week in a different park, normally organised and hosted by one of the participating clubs. Photos of such events can be found here.[21]
Cross country running is a far reaching sport in Canada. Starting in elementary school, most children have had some form of exposure to cross country running, usually in the form of an annual all-school event. In middle school, races are more serious and are divided by grade and gender. In high school the races are very serious and tend to be the main talent pool (especially at the senior level) for university or national-level runners. At the university level, the sport is administered by Canadian Interuniversity Sport.[22]
The organization of cross country running in the United Kingdom has continued to be mostly devolved to the four national associations: England, Wales, Scotland, and Northern Ireland. The sport is based around the clubs, which usually are mixed cross country and road running clubs. The current position (which is changing) is that in England, the English Cross Country Association is part of the Amateur Athletic Association.
Cross country running takes place from roughly September until March. Most matches are parts of different cross country leagues, which are organised on an ad hoc basis. These vary from large, high quality leagues, such as the London Metropolitan Cross Country League, Birmingham League and Surrey League (which is unusual in requiring ten runners to score) to small, local leagues (such as the Gloucestershire AA league), and individual clubs can be a member of several leagues.
Typically there will be four or five fixtures a season. In addition there are county championships, area championships (north, south, and midlands), the national championship (whose location rotates around the three areas), and the Inter-Counties Championship (which is often the best quality race owing to its restricted entry and its role as the trial for the World Championships).
In addition there can be many inter-club matches, particularly among the older clubs. Most league matches are around 10km (6.2mi) long, and most championships 12 to 15km (c. 7 to 9 miles) long. Most clubs are mixed, though men's and women's races tend to be run separately.
Secondary school aged students are also to compete at local schools races, with a set number of students qualifying for county level, at which there is a further race to qualify for the English Schools Cross Country race. There is also quite a lot of racing between universities, with larger fixtures organised through BUCS.
Primary schools, although more often the juniors, also participate in cross country events and in some areas of England have done so since the late 1960s. An example would be schools near Ouston, County Durham which compete as part of Chester-le-Street & District Primary Cross Country Association.[23]
USA Track & Field hosts four annual national cross country championships. The USA Cross Country Championships, first held in 1890, include six races: masters women (8km), masters men (8km), junior women (6km), junior men (8km), open women (8km) and open men (12km). In addition to crowning national champions, the championships serve as the trials race to select the Team USA squad for the IAAF World Cross Country Championships. The USA Masters 5km Cross Country Championships, first held in 2002, include a men's race and a women's race. The USATF National Club Cross Country Championships, first held in 1998, feature the top clubs from across the United States as they vie for honors and bragging rights as the nation's top cross country team. The USATF National Junior Olympic Cross Country Championships, first held in 2001, has races for boys and girls in five different two-year age divisions.[24]
Most American universities and colleges field men's and women's cross country teams as part of their athletic program. Over 900 men's cross country teams and over 1000 women's cross country teams compete in the three divisions of the National Collegiate Athletic Association.[25] Men usually race 10km (6.2mi) or 8km (5.0mi), and women usually race 6km (3.7mi) or 5km (3.1mi).[4] The season culminates in men's and women's championships.
Every state offers cross country as a high school sport for boys and girls. Over 440,000 high school students compete in cross country each year, making it the sixth most popular sport for girls, and seventh most popular for boys.[26] The standard high school cross country race distance is 5km (3.1mi), though some states run a shorter, 3- to 4-kilometer course for girls. Beginning in 1979, the Foot Locker Cross Country Championships have offered a national championship for high school cross country runners. Since 2004, the Nike Cross Nationals have offered an alternative national championship, focused on teams rather than individuals. A 2008 film, The Long Green Line, documented the success of Joe Newton, cross country coach at York Community High School in Elmhurst, Illinois.[27]
While many middle schools (grades 6-8) in the U.S. offer cross country as a school sport, youth running clubs dominate in this age group. A typical middle school course is 3km (1.9mi) or 2mi (3.2km), and races may not split up boys and girls. Few elementary schools in the U.S. have school teams, but many running clubs exist for youth runners. Youth running clubs compete in local, regional, and national championships sanctioned by the AAU or USATF. Course distances for this age group vary depending on the age of the athlete. Common championship distances are:
Mt. San Antonio College in Walnut, California hosts the largest cross country invitational in the United States, with over 22,000 runners from community colleges, high schools and elementary schools competing. The meet started in 1948 and continues today.[28]
Outstanding American cross country runners include Don Lash, who won seven consecutive national championships from 1934 to 1940 and Pat Porter, who won eight titles from 1982 to 1989. Only two American athletes have won the IAAF World Cross Country Championships: Craig Virgin, who won in 1980 and again in 1981 and Lynn Jennings from 19901992.
One variation on traditional cross country is mountain running, which incorporates significant uphill and/or downhill sections as an additional challenge to the course. Orienteering is another competitive sport similar to cross country, although it features an element of navigation absent from the set and marked courses of cross country.[29]
1 Race course

1.1 Course design
1.2 Distances
1.3 Start
1.4 Finish


1.1 Course design
1.2 Distances
1.3 Start
1.4 Finish
2 Scoring
3 Strategy
4 Equipment
5 History

5.1 Olympic Games
5.2 World championships
5.3 Notable athletes

5.3.1 Men
5.3.2 Women




5.1 Olympic Games
5.2 World championships
5.3 Notable athletes

5.3.1 Men
5.3.2 Women


5.3.1 Men
5.3.2 Women
6 Regional organization

6.1 Australia
6.2 Canada
6.3 United Kingdom
6.4 United States


6.1 Australia
6.2 Canada
6.3 United Kingdom
6.4 United States
7 Variations
8 References
1.1 Course design
1.2 Distances
1.3 Start
1.4 Finish
5.1 Olympic Games
5.2 World championships
5.3 Notable athletes

5.3.1 Men
5.3.2 Women


5.3.1 Men
5.3.2 Women
5.3.1 Men
5.3.2 Women
6.1 Australia
6.2 Canada
6.3 United Kingdom
6.4 United States
Kenenisa Bekele won both short and long World Cross Country course titles in the same year five times (20022006), after a junior men victory and senior long course silver in 2001. The IAAF calls him the "greatest ever male cross country runner to have graced the sport."[16]
Carlos Lopes  first man to win World Cross Country title three times.
John Ngugi  first man to win five World Cross Country titles, including four consecutively in the late 1980s.
Paavo Nurmi was a four-time Olympic gold medalist and undefeated throughout his 19-year career in cross country running
Steve Prefontaine  three-time NCAA cross country champion and subject of the films Prefontaine and Without Limits.
Gaston Roelants  four-time champion at the International Cross Country Championship between 1962 and 1972.
Paul Tergat  long course champion five years in a row (19951999), plus a bronze medal finish in 2000.
Doris Brown  won the International Cross Country Championship for five consecutive years (19671971).
Zola Budd  young prodigy who twice won women's World championship (19851986), known for running barefooted.
Tirunesh Dibaba  won three times at the World long course and once at the short.
Lynn Jennings  won World title three times.
Edith Masai  won the World short race three times.
Derartu Tulu  won World titles three times in six years (1995, 1997, 2000).
Grete Waitz  first athlete to win five IAAF World Cross Country titles.
Gete Wami  won twice at the World long course and once at the short.
Sonia O'Sullivan - was the first person who achieved wins in both the short and long races in the World Cross-Country Championships (1998)
.
#(`*Track and field*`)#.
Track and field is a sport comprising various competitive athletic contests based on running, jumping, and throwing. The name of the sport derives from the competition venue: a stadium with an oval running track around a grass field. The throwing and jumping events generally take place in the central enclosed area.
Track and field falls under the umbrella sport of athletics(which includes road running, cross-country running, and race walking). The two most prestigious international track and field competitions are held under the banner of athletics: the athletics competition at the Olympic Games and the IAAF World Championships in Athletics. The International Association of Athletics Federations is the international governing body for track and field.
Track and field events are generally individual sports with athletes challenging each other to decide a single victor. The racing events are won by the athlete with the fastest time, while the jumping and throwing events are won by the athlete who has achieved the greatest distance or height in the contest. The running events are categorised as sprints, middle and long-distance events, relays, and hurdling. Regular jumping events include long jump, triple jump, high jump and pole vault, while the most common throwing events are shot put, javelin, discus and hammer. There are also "combined events", such as heptathlon and decathlon, in which athletes compete in a number of the above events.
Records are kept of the best performances in specific events, at world and national levels, right down to a personal level. However, if athletes are deemed to have violated the event's rules or regulations, they are disqualified from the competition and their marks are erased.
The sport of track and field has its roots in human prehistory. Track and field-style events are among the oldest of all sporting competitions, as running, jumping and throwing are natural and universal forms of human physical expression. The first recorded examples of organized track and field events at a sports festival are the Ancient Olympic Games. At the first Games in 776 BC in Olympia, Greece, only one event was contested: the stadion footrace.[1] The scope of the Games expanded in later years to include further running competitions, but the introduction of the Ancient Olympic pentathlon marked a step towards track and field as it is recognised todayit comprised a five-event competition of the long jump, javelin throw, discus throw, the stadion foot race, and wrestling.[2][3]
Track and field events were also present at the Panhellenic Games in Greece around this period, and they spread to Rome in Italy around 200 BC.[4][5] After the period of Classical antiquity (in which the sport was largely Greco-Roman influenced) new track and field events began developing in parts of Northern Europe in the Middle Ages. The stone put and weight throw competitions popular among Celtic societies in Ireland and Scotland were precursors to the modern shot put and hammer throw events. One of the last track and field events to develop was the pole vault, which stemmed from competitions such as the Fierljeppen contests in the Northern European Lowlands in the 18th century.
Discrete modern track and field competitions, separate from general sporting festivals, were first recorded in the late 19th century. These were typically organised by educational institutions, military organisations and sports clubs as competitions between rival establishments.[6] Competitive hurdling first came into being around this point, with the advent of the steeplechase in England around 1850.[7] The Amateur Athletic Association was established in England in 1880 as the first national body for the sport of athletics and, under this grouping, track and field became the focus of the annual AAA Championships. The United States also began holding an annual national competitionthe USA Outdoor Track and Field Championshipsfirst held in 1876 by the New York Athletic Club.[8] Following the establishment of general sports governing bodies for the United States (the Amateur Athletic Union in 1888) and France (the Union des socits franaises de sports athltiques in 1889), track and field events began to be promoted and codified.
The establishment of the modern Olympic Games at the end of the 19th century marked a new high for track and field. The Olympic athletics programme, comprising track and field events plus a marathon race, contained many of the foremost sporting competitions of the 1896 Summer Olympics. The Olympics also consolidated the use of metric measurements in international track and field events, both for race distances and for measuring jumps and throws. The Olympic athletics programme greatly expanded over the next decades, and track and field contests remained among the Games' most prominent. The Olympics was the elite competition for track and field, and only amateur sportsmen could compete. Track and field continued to be a largely amateur sport, as this rule was strictly enforced: Jim Thorpe was stripped of his track and field medals from the 1912 Olympics after it was revealed that he had played baseball professionally.[9]
That same year, the International Amateur Athletic Federation (IAAF) was established, becoming the international governing body for track and field, and it enshrined amateurism as one of its founding principles for the sport. The National Collegiate Athletic Association held their first Men's Outdoor Track and Field Championship in 1921, making it one of the most prestigious competitions for students, and this was soon followed by the introduction of track and field at the inaugural World Student Games in 1923.[10] The first continental track and field competition was the 1919 South American Championships, which was followed by the European Athletics Championships in 1934.[11] Up until the early 1920s, track and field had been almost exclusively a male-only pursuit. The women's sports movement led to the introduction of five track and field events for women in the athletics at the 1928 Summer Olympics and more women's events were gradually introduced as years progressed (although it was only towards the end of the century that the men's and women's programmes approached parity of events). Furthermore, major track and field competitions for disabled athletes were first introduced at the 1960 Summer Paralympics.
With the rise of numerous regional championships, as well as the growth in Olympic-style multi-sport events (such as the Commonwealth Games and the Pan-American Games), competitions between international track and field athletes became widespread. From the 1960s onwards, the sport gained more exposure and commercial appeal through television coverage and the increasing wealth of nations. After over half a century of amateurism, the amateur status of the sport began to be displaced by growing professionalism in the late 1970s.[6] As a result, the Amateur Athletic Union was dissolved in the United States and it was replaced with a non-amateur body solely focused on the sport of athletics: The Athletics Congress (later USA Track and Field).[12] The IAAF soon followed suit in 1982, abandoning amateurism, and later removing all references to it from its name by rebranding itself as the International Association of Athletics Federations.[6] The following year saw the establishment of the IAAF World Championships in Athleticsthe first ever global competition just for athleticswhich, with the Olympics, became one of track and field's most prestigious competitions.
The profile of the sport reached a new high in the 1980s, with a number of athletes becoming household names (such as Carl Lewis, Sergey Bubka, Sebastian Coe, Zola Budd and Florence Griffith-Joyner). Many world records were broken in this period, and the added political element between competitors of the United States, East Germany, and the Soviet Union, in reaction to the Cold War, only served to stoke the sport's popularity. The increase in the commercial capacity of track and field was also met with developments in the application of sports science, and there were many changes to coaching methods, athlete's diet regimes, training facilities and sports equipment. This was also accompanied by an increase in the use of performance-enhancing drugs, and prominent cases, such as those of Olympic gold medallists Ben Johnson and Marion Jones, damaged the public image and marketability of the sport.
From the 1990s onwards, track and field became increasingly more professional and international, as the IAAF gained over two hundred member nations. The IAAF World Championships in Athletics became a fully professional competition with the introduction of prize money in 1997,[6] and in 1998 the IAAF Golden Leaguean annual series of major track and field meetings in Europeprovided a higher level of economic incentive in the form of a US$1 million jackpot. In 2010, the series was replaced by the more lucrative IAAF Diamond League, a fourteen-meeting series held in Europe, Asia, North America and the Middle Eastthe first ever worldwide annual series of track and field meetings.[13]
Track and field events are divided into three broad categories: track events, field events, and combined events. The majority of athletes tend to specialise in just one event (or event type) with the aim of perfecting their performances, although the aim of combined events athletes is to become proficient in a number of disciplines. Track events involve running on a track over a specified distances andin the case of the hurdling and steeplechase eventsobstacles may be placed on the track. There are also relay races in which teams of athletes run and pass on a baton to their team member at the end of a certain distance.
There are two types of field events: jumps, and throws. In jumping competitions, athletes are judged on either the length or height of their jumps. The performances of jumping events for distance are measured from a board or marker, and any athlete overstepping this mark is judged to have fouled. In the jumps for height, an athlete must clear his or her body over a crossbar without knocking the bar off the supporting standards. The majority of jumping events are unaided, although athletes propel themselves vertically with purpose-built sticks in the pole vault.
The throwing events involve hurling an implement (such as a heavy weight, javelin or discus) from a set point, with athletes being judged on the distance that the object is thrown. Combined events involve the same group of athletes contesting a number of different track and field events. Points are given for their performance in each event and the athlete with the greatest points total at the end of all events is the winner.

Races over short distances, or sprints, are among the oldest running competitions. The first 13 editions of the Ancient Olympic Games featured only one event, the stadion race, which was literally a race from one end of the stadium to the other.[1] Sprinting events are focused around athletes reaching and sustaining their quickest possible running speed. Three sprinting events are currently held at the Olympics and outdoor World Championships: the 100 metres, 200 metres, and 400 metres. These events have their roots in races of imperial measurements that later changed to metric: the 100m evolved from the 100 yard dash,[14] the 200m distances came from the furlong (or 1/8 of a mile),[15] and the 400m was the successor to the 440 yard dash or quarter-mile race.[16]
At the professional level, sprinters begin the race by assuming a crouching position in the starting blocks before leaning forward and gradually moving into an upright position as the race progresses and momentum is gained.[17] Athletes remain in the same lane on the running track throughout all sprinting events,[16] with the sole exception of the 400m indoors. Races up to 100m are largely focused upon acceleration to an athlete's maximum speed.[17] All sprints beyond this distance increasingly incorporate an element of endurance.[18] Human physiology dictates that a runner's near-top speed cannot be maintained for more than thirty seconds or so because lactic acid builds up once leg muscles begin suffer oxygen deprivation.[16]
The 60 metres is a common indoor event and indoor world championship event. Less-common events include the 50 metres, 55 metres, 300 metres and 500 metres which are run in some high school and collegiate competitions in the United States. The 150 metres, though rarely competed, has a star-studded history: Pietro Mennea set a world best in 1983,[19] Olympic champions Michael Johnson and Donovan Bailey went head-to-head over the distance in 1997,[20] and Usain Bolt improved Mennea's record in 2009.[19]
The most common middle distance track events are the 800 metres, 1500 metres and mile run, although the 3000 metres may also be classified as a middle distance event.[21] The 880 yard run, or half mile, was the forebear of the 800m distance and it has its roots in competitions in the United Kingdom in the 1830s.[22] The 1500m came about as a result of running three laps of a 500m track, which was commonplace in continental Europe in the 20th century.[23]
Runners start the race from a standing position along a curved starting line and after hearing the starter's pistol they head towards the innermost track to follow the quickest route to the finish. In 800m races athletes begin at a staggered starting point before the turn in the track and they must remain in their lanes for the first 100m of the race.[24] This rule was introduced to reduce the amount of physical jostling between runners in the early stages of the race.[22] In the 800m race, runners have the option of starting in blocks; doing so may reduce time spent accelerating off the starting line. Physiologically, these middle distance events demand that athletes have good aerobic and anaerobic energy producing systems, and also that they have strong speed endurance.[25]
The 1500m and mile run events have historically been some of the most prestigious track and field events. Swedish rivals Gunder Hgg and Arne Andersson broke each other's 1500m and mile world records on a number of occasions in the 1940s.[26][27] The prominence of the distances were maintained by Roger Bannister, who (in 1954) was the first to run the long-elusive four-minute mile,[28][29] and Jim Ryun's exploits served to popularise interval training.[23] Races between British rivals Sebastian Coe, Steve Ovett and Steve Cram characterised middle distance running in 1980s.[30] From the 1990s onwards, North Africans such as Noureddine Morceli of Algeria and Hicham El Guerrouj of Morocco came to dominate the 1500 and mile events.[23]
Beyond the short distances of sprinting events, factors such as an athlete's reactions and top speed becomes less important, while qualities such as pace, race tactics and endurance become more so.[22][23]
There are three common long distance running events in track and field competitions: 3000 metres, 5000 metres and 10,000 metres. The latter two races are both Olympic and World Championship events outdoors, while the 3000m is held at the IAAF World Indoor Championships. The 5000m and 10,000m events have their historical roots in the 3-mile and 6-mile races. The 3000m was historically used as a women's long distance event, entering the World Championship programme in 1983 and Olympic programme in 1984, but this was abandoned in favour of a women's 5000m event in 1995.[31]
In terms of competition rules and physical demands, long distance track races have much in common with middle distance races, except that pacing, stamina, and race tactics become much greater factors in performances.[32][33] However, a number of athletes have achieved success in both middle and long distance events, including Sad Aouita who set world records from 1500m to 5000m.[34] The use of pace-setters in long distance events is very common at the elite level, although they are not present at championship level competitions as all qualified competitors want to win.[33][35]
The long distance track events gained popularity in the 1920s by the achievements of the "Flying Finns", such as multiple Olympic champion Paavo Nurmi. The successes of Emil Ztopek in the 1950s promoted intense interval training methods, but Ron Clarke's world record-breaking feats established the importance of natural training and even-paced running. The 1990s saw the rise of North and East African runners in long distance events. Kenyan and Ethiopian athletes, in particular, have since remained dominant in these events.[31]
Relay races are the only track and field event in which a team of runners directly compete against other teams.[36] Typically, a team is made up of four runners of the same sex. Each runner completes their specified distance (referred to as a leg) before handing over a baton to a team mate, who then begins their leg upon receiving the baton. There is usually a designated area where athletes must exchange the baton. Teams may be disqualified if they fail to complete the change within the area, or if the baton is dropped during the race. A team may also be disqualified if its runners are deemed to have wilfully impeded other competitors.
Relay races emerged in the United States in the 1880s as a variation on charity races between firemen, who would hand a red pennant on to team mates every 300yards. There are two very common relay events: the 4100 metres relay and the 4400 metres relay. Both events entered the Olympic programme at the 1912 Summer Games after a one-off men's medley relay featured in 1908 Olympics.[37] The 4100 m event is run strictly within the same lane on the track, meaning that the team collectively runs one complete circuit of the track. Teams in a 4400 m event remain in their own lane until the runner of the second leg passes the first bend, at which point runners can leave their lanes and head towards the inner-most part of the circuit. For the second and third baton change overs, team mates must align themselves in respect of their team position  leading teams take the inner lanes while team mates of the slower teams must await the baton on outer lanes.[36][38]
The IAAF keeps world records for five different types of track relays. As with 4100m and 4400m events, all races comprise teams of four athletes running the same distances, with the less commonly contested distances being the 4200m, 4800m and 41500m relays.[39] Other events include the distance medley relay (comprising legs of 1200m, 400m, 800m, and 1600m), which is frequently held in the United States, and a sprint relay, known as the Swedish medley relay, which is popular in Scandinavia and held at the World Youth Championships in Athletics programme.[40] Relay events have significant participation in the United States, where a number of large meetings (or relay carnivals) are focused almost exclusively on relay events.[41]
Races with hurdles as obstacles were first popularised in the 19th century in England.[42] The first known event, held in 1830, was a variation of the 100-yard dash that included heavy wooden barriers as obstacles. A competition between the Oxford and Cambridge Athletic Clubs in 1864 refined this, holding a 120-yard race (109.72m) with ten hurdles of 3-foot and 6inches (1.06m) in height (each placed 10 yards (9.14m) apart), with the first and final hurdles 15 yards from the start and finish, respectively. French organisers adapted the race into metric (adding 28cm) and the basics of this race, the men's 110 metres hurdles, has remained largely unchanged.[43] The origin of the 400 metres hurdles also lies in Oxford, where (around 1860) a competition was held over 440yards and twelve 1.06m high wooden barriers were placed along the course. The modern regulations stem from the 1900 Summer Olympics: the distance was fixed to 400m while ten 3-foot (91.44cm) hurdles were placed 35m apart on the track, with the first and final hurdles being 45m and 40m away from the start and finish, respectively.[44] Women's hurdles are slightly lower at 84cm for the 100m event and 76cm (2ft 6in) for the 400m event.[43][44]
By far the most common events are the 100 metres hurdles for women, 110m hurdles for men and 400m hurdles for both sexes. The men's 110m has been featured at every modern Summer Olympics while the men's 400m was introduced in the second edition of the Games.[43][44] Women's initially competed in the 80 metres hurdles event, which entered the Olympic programme in 1932. This was extended to the 100m hurdles at the 1972 Olympics,[43] but it was not until 1984 that a women's 400m hurdles event took place at the Olympics (having been introduced at the 1983 World Championships in Athletics the previous year).[44]
Outside of the hurdles events, the steeplechase race is the other track and field event with obstacles. Just as the hurdling events, the steeplechase finds its origin in student competition in Oxford, England. However, this event was born as a human variation on the original steeplechase competition found in horse racing. A steeplechase event was held on a track for the 1879 English championships and the 1900 Summer Olympics featured men's 2500m and 4000m steeplechase races. The event was held over various distances until the 1920 Summer Olympics marked the rise of the 3000 metres steeplechase as the standard event.[45] The IAAF set the standards of the event in 1954, and the event is held on a 400m circuit that includes a water jump on each lap.[46] Despite the long history of men's steeplechase in track and field, the women's steeplechase only gained World Championship status in 2005, with its first Olympic appearance coming in 2008.
The long jump is one of the oldest track and field events, having its roots as one of the events within the ancient Greek pentathlon contest. The athletes would take a short run up and jump into an area of dug up earth, with the winner being the one who jumped furthest.[47] Small weights (Halteres) were held in each hand during the jump then swung back and dropped near the end to gain extra momentum and distance.[48] The modern long jump, standardised in England and the United States around 1860, bears resemblance to the ancient event although no weights are used. Athletes sprint along a length of track that leads to a jumping board and a sandpit.[49] The athletes must jump before a marked line and their achieved distance is measured from the nearest point of sand disturbed by the athlete's body.[50]
The athletics competition at the first Olympics featured a men's long jump competition and a women's competition was introduced at the 1948 Summer Olympics.[49] Professional long jumpers typically have strong acceleration and sprinting abilities. However, athletes must also have a consistent stride to allow them to take off near the board while still maintaining their maximum speed.[50][51] In addition to the traditional long jump, a standing long jump contest exists requires that athletes leap from a static position without a run-up. A men's version of this event featured on the Olympic programme from 1900 to 1912.[52]
Similar to the long jump, the triple jump takes place on a track heading towards a sandpit. Originally, athletes would hop on the same leg twice before jumping into the pit, but this was changed to the current "hop, step and jump" pattern from 1900 onwards.[53] There is some dispute over whether the triple jump was contested in ancient Greece: while some historians claim that a contest of three jumps occurred at Ancient Games,[53] others such as Stephen G. Miller believe this is incorrect, suggesting that the belief stems from a mythologised account of Phayllus of Croton having jumped 55 ancient feet (around 16.3m).[48][54] The Book of Leinster, a 12th century Irish manuscript, records the existence of geal-ruith (triple jump) contests at the ancient Tailteann Games.[55]
The men's triple jump competition has been ever-present at the modern Olympics, but it was not until 1993 that a women's version gained World Championship status and went on to have its first Olympic appearance three years later.[53] A men's standing triple jump event featured at the 1900 and 1904 Olympics but such competitions have since become very uncommon, although it is still used as a non-competitive exercise drill.[56]
The first recorded instances of high jumping competitions were in Scotland in the 19th century.[57] Further competitions were organised in 1840 in England and in 1865 the basic rules of the modern event were standardised there.[58] Athletes have a short run up and then take off from one foot to jump over a horizontal bar and fall back onto a cushioned landing area.[59] The men's high jump was included in the 1896 Olympics and a women's competition soon followed in 1928.
Jumping technique has played a significant part in the history of the event. High jumpers typically cleared the bar feet first in the late 19th century, using either the Scissors, Eastern cut-off or Western roll technique. The straddle technique became prominent in the mid-20th century, but Dick Fosbury overturned tradition by pioneering a backwards and head-first technique in the late 1960s  the Fosbury Flop  which won him the gold at the 1968 Olympics. This technique has become the overwhelming standard for the sport from the 1980s onwards.[58][60] The standing high jump was contested at the Olympics from 1900 to 1912, but is now relatively uncommon outside of its use as an exercise drill.
In terms of sport, the use of poles for vaulting distances was recorded in Fierljeppen contests in the Frisian area of Europe, and vaulting for height was seen at gymnastics competitions in Germany in the 1770s.[61] One of the earliest recorded pole vault competitions was in Cumbria, England in 1843.[62] The basic rules and technique of the event originated in the United States. The rules required that athletes do not move their hands along the pole and athletes began clearing the bar with their feet first and twisting so that the stomach faces the bar. Bamboo poles were introduced in the 20th century and a metal box in the runway for planting the pole became standard. Landing matresses were introduced in the mid-20th century to protect the athletes who were clearing increasingly greater heights.[61]
The modern event sees athletes run down a strip of track, plant the pole in the metal box, and vault over the horizontal bar before letting go of the pole and falling backwards onto the landing matress.[63] While earlier versions used wooden, metal or bamboo, modern poles are generally made from artificial materials such as fibreglass or carbon fibre.[64] The pole vault has been an Olympic event since 1896 for men, but it was over 100 years later that the first women's world championship competition was held at the 1997 IAAF World Indoor Championships. The first women's Olympic pole vaulting competition occurred in 2000.[61]
The genesis of the shot put can be traced to pre-historic competitions with rocks:[65] in the middle ages the stone put was known in Scotland and the steinstossen was recorded in Switzerland. In the 17th century, cannonball throwing competitions within the English military provided a precursor to the modern sport.[66][67] The modern rules were first laid out in 1860 and required that competitors take legal throws within a square throwing area of seven feet (2.13m) on each side. This was amended to a circle area with a seven foot diameter in 1906, and the weight of the shot was standardised to 16pounds (7.26kg). Throwing technique was also refined over this period, with bent arm throws being banned as they were deemed too dangerous and the side-step and throw technique arising in the United States in 1876.[66] Shot Putters are generally the largest and most explosive athletes on a team.[citation needed]
The shot put has been an Olympic sport for men since 1896 and a women's competition using a 4kg (8.82lb) shot was added in 1948. Further throwing techniques have arisen since the post-war era: in the 1950s Parry O'Brien popularised the 180 degree turn and throw technique commonly known as the "glide," breaking the world record 17 times along the way, while Aleksandr Baryshnikov and Brian Oldfield introduced the "spin" or rotational technique in 1976.[66][68]
As one of the events within the ancient pentathlon, the history of the discus throw dates back to 708 BC.[69] In ancient times a heavy circular disc was thrown from a set standing position on a small pedestal, and it was this style that was revived for the 1896 Olympics.[70] This continued until the 1906 Intercalated Games in Athens, which featured both the ancient style and the increasingly popular modern style of turning and throwing. By the 1912 Olympics, the ancient standing throw style had fallen into disuse and contests starting within a 2.5m squared throwing area became the standard.[71] The discus implement was standardised to 2kg (4.4pounds) in weight and 22cm (8inches) in diameter in 1907.[70] The women's discus was among the first women's events on the Olympic programme, being introduced in 1928.[72]
As an implement of war and hunting, javelin throwing began in prehistoric times.[73] Along with the discus, the javelin was the second throwing event in the ancient Olympic pentathlon. Records from 708 BC show two javelin competition types co-existing: throwing at a target and throwing the javelin for distance. It was the latter type from which the modern event derives.[74] In ancient competitions, athletes would wrap an ankyle (thin leather strip) around the javelin that acted as a sling to facilitate extra distance.[75] The javelin throw gained much popularity in Scandinavia in the late 18th century and athletes from the region are still among the most dominant throwers in men's competitions.[74] The modern event features a short run up on a track and then the thrower releases the javelin before the foul line.
The first Olympic men's javelin throw contest was held in 1908 and a women's competition was introduced in 1932.[76][77] The first javelins were made of various types of wood, but in the 1950s, former athlete Bud Held introduced a hollow javelin, then a metal javelin, both of which increased throwers performances.[74] Another former athlete, Mikls Nmeth invented the rough-tailed javelin and throws reached in excess of 100m  edging towards the limits of stadia.[78] The distances and the increasing number of horizontal landings led the IAAF to redesign the men's javelin to reduce distance and increase the implement's downward pitching moment to allow for easier measurement. Rough-tailed designs were banned in 1991 and all marks achieved with such javelins were removed from the record books. The women's javelin underwent a similar redesign in 1999.[74] The current javelin specifications are 2.6 to 2.7m in length and 800grams in weight for men, and between 2.2 to 2.3m and 600g for women.[79]
The earliest recorded precursors to the modern hammer throw stem from the Tailteann Games around 1800 BC, which featured events such as throwing either a weight attached to a rope, a large rock on a wooden handle, or even a chariot wheel on a wooden axle.[80] Other ancient competitions included throwing a cast iron ball attached to a wooden handle  the root of the term "hammer throw" due to their resemblance to the tools.[81] In 16th century England, contests involving the throwing of actual blacksmith's Sledgehammers were recorded.[80] The hammer implement was standardised in 1887 and the competitions began to resemble the modern event. The weight of the metal ball was set at 16pounds (7.26kg) while the attached wire had to measure between 1.175m and 1.215m.[81]
The men's hammer throw became an Olympic event in 1900 but the women's event  using a 4kg (8.82lb) weight  was not widely competed until much later, finally featuring on the women's Olympic programme a century later.[82] The distance's thrown by male athletes became greater from the 1950s onwards as a result of improved equipment using the denser metals, a switch to concrete throwing areas, and more advanced training techniques.[83] Professional hammer throwers as historically large, strong, sturdy athletes. However, qualities such as refined technique, speed and flexibility have become increasingly important in the modern era as the legal throwing area has been reduced from 90 to 34.92 degrees and throwing technique involves three to four controlled rotations.[81][84][85]
Combined (or multi-discipline) events are competitions in which athletes participate in a number of track and field events, earning points for their performance in each event, which adds to a total points score. Outdoors, the most common combined events are the men's decathlon and the women's heptathlon. Due to stadium limitations, indoor combined events competition have a reduced number of events, resulting in the men's heptathlon and the women's pentathlon. Athletes are allocated points based on an international-standard points scoring system, such as the decathlon scoring table.
The Ancient Olympic pentathlon (comprising long jump, javelin, discus, the stadion race and wrestling) was a precursor to the track and field combined events and this ancient event was restored at the 1906 Summer Olympics (Intercalated Games). A men's decathlon was held at the 1904 Summer Olympics, albeit contested between five American and two British athletes.
The term track and field is intertwined with the stadiums that first hosted such competitions. The two basic features of a track and field stadium are the outer oval-shaped running track and an area of turf within this trackthe field. In earlier competitions, track lengths varied: the Panathinaiko Stadium measured 333.33metres at the 1896 Summer Olympics, while at the 1904 Olympics the distance was a third of a mile (536.45m) at Francis Field. As the sport developed, the IAAF standardised the length to 400m and stated that the tracks must be split into six to eight running lanes. Precise widths for the lanes were established, as were regulations regarding the curvature of the track. Tracks made of flattened cinders were popular in the early 20th century but synthetic tracks became standard in the late 1960s. 3M's Tartan track (an all-weather running track of polyurethane) gained popularity after its use at the 1968 US Olympic Trials and the 1968 Summer Olympics and it began the process in which synthetic tracks became the standard for the sport. Many track and field stadiums are multi-purpose stadiums, with the running track surrounding a field built for other sports, such as the various types of football.
The field of the stadium combines a number of elements for use in the jumping and throwing events. The long jump and triple jump areas comprise a straight, narrow 40-metre running track with a sandpit at one or both ends. Jumps are measured from a take off boardtypically a small strip of wood with a plasticine marker attachedwhich ensures athletes jump from behind the measurement line. The pole vault area is also a 40-metre running track and has an indentation in the ground (the box) where vaulters plant their poles to propel themselves over a crossbar before falling onto cushioned landing mats. The high jump is a stripped down version of this, with an open area of track or field that leads to a crossbar with a square area of landing mats behind it.
The four throwing events generally all begin on one side of the stadium. The javelin throw typically takes place on a piece of track that is central and parallel to the straights of the main running track. The javelin throwing area is a sector shape frequently across the Pitch (sports field) in the middle of the stadium, ensuring that the javelin has a minimal chance of causing damage or injury. The discus throw and hammer throw contests begin in a tall metal cage usually situated in one of the corners of the field. The cage reduces the danger of implements being thrown out of the field of play and throws travel diagonally across the field in the centre of the stadium. The shot put features a circular throwing area with a toe board at one end. The throwing area is a sector. Some stadia also have a water jump area on one side of the field specifically for steeplechase races.
Basic indoor venues may be adapted gymnasiums, which can easily accommodate high jump competitions and short track events. Full-size indoor arenas (i.e. those fully equipped to host all events for the World Indoor Championships) bear similarities with their outdoor equivalents. Typically, a central area is surrounded by a 200-metre oval track with four to eight lanes. The track can be banked at the turns to allow athletes to run around the radius more comfortably. There is also a second running track going straight across the field area, parallel to the straights of the main circuit. This track is used for the 60 metres and 60 metres hurdles events, which are held almost exclusively indoors. Another common adaptation is a 160 yard track (11 laps to a mile) that fits into a common basketball court sized arena. This was quite popular when races were held at imperial distances, which gradually was phased out by different organizations in the 1970s and 1980s. Examples of this configuration include the Millrose Games at Madison Square Garden,[86] and the Sunkist Invitational formerly held in the Los Angeles Sports Arena.[87]
All four of the common jumping events are held at indoor venues. The long and triple jump areas run alongside the central 60m track and are mostly identical in form to their outdoor counterparts. The pole vault track and landing area are also alongside the central running track. Shot put (or weight throw) is the only throwing event held indoors due to size restrictions. The throwing area is similar to the outdoor event, but the landing sector is a rectangular section surrounded by netting or a stop barrier.[88]
The rules of track athletics or of track events in athletics as observed in most international athletics competitions are set by the Competition Rules of the International Association of Athletics Federations (IAAF). The most recent complete set of rules is the 2009 rules that relate only to competitions in 2009.[89] Key rules of track events are those regarding starting, running and finishing.
The start of a race is marked by a white line 5cm wide. In all races that are not run in lanes the start line must be curved, so that all the athletes start the same distance from the finish.[90] Starting blocks may be used for all races up to and including 400 m (including the first leg of the 4 x 100 m and 4 x 400 m) and may not be used for any other race. No part of the starting block may overlap the start line or extend into another lane.[91] All races must be started by the report of the starter's gun or approved starting apparatus fired upwards after he or she has ascertained that athletes are steady and in the correct starting position.[92] An athlete may not touch either the start line or the ground in front of it with his or her hands or feet when on his or her marks.[93] At most international competitions the commands of the starter in his or her own language, in English or in French, shall, in races up to and including 400 m, be "On your marks" and "Set". When all athletes are "set", the gun must be fired, or an approved starting apparatus must be activated.[93] However, if the starter is not satisfied that all is ready to proceed, the athletes may be called out of the blocks and the process started over.
False start: An athlete, after assuming a final set position, may not commence his starting motion until after receiving the report of the gun, or approved starting apparatus. If, in the judgment of the starter or recallers, he does so any earlier, it is considered a false start. It is deemed a false start if, in the judgment of the starter an athlete fails to comply with the commands "on your marks" or "set" as appropriate after a reasonable time; or an athlete after the command "on your marks" disturbs other athletes in the race through sound or otherwise. If the runner is in the "set" position and moves, then the runner is also disqualified.[94] As of 2010[update], any athlete making a false start is disqualified.[95]
In International elite competition, electronically tethered starting blocks sense the reaction time of the athletes. If the athlete reacts in less than 0.1 second, an alert sounds for a recall starter and the offending athlete is guilty of a false start.[92]
In all races run in lanes, each athlete must keep within his allocated lane from start to finish. This also applies to any portion of a race run in lanes. If an athlete leaves the track or steps on the line demarking the track, he/she should be disqualified.[96] Also, any athlete who jostles or obstructs another athlete, in a way that impedes his progress, should be disqualified from that event.[97] However, if an athlete is pushed or forced by another person to run outside his lane, and if no material advantage is gained, the athlete should not be disqualified.
There are races that start in lanes and then at a "break" line, the competitors merge. Examples of this are the 800 metres, 4x400 relay and the indoor 400 metres. Variations on this, with alleys made up of multiple lanes on the track, are used to start large fields of distance runners.
The finish of a race is marked by a white line 5cm wide.[98] The finishing position of athletes is determined by the order in which any part of their torso (as distinguished from the head, neck, arms, legs, hands or feet) reaches the vertical plane of the nearer edge of the finish line.[99] Fully automatic timing systems (photo timing) are becoming more and more common at increasingly lower levels of track meets, improving the accuracy, while eliminating the need for eagle-eyed officials on the finish line. Fully automatic timing (FAT) is required for high level meets and any time a (sprint) record is set (though distance records can be accepted if timed by three independent stopwatches).
With the accuracy of the timing systems, ties are rare. Ties between different athletes are resolved as follows: In determining whether there has been a tie in any round for a qualifying position for the next round based on time, a judge (called the chief photo finish judge) must consider the actual time recorded by the athletes to one thousandth of a second. If the judge decides that there has been a tie, the tying athletes must be placed in the next round or, if that is not practicable, lots must be drawn to determine who must be placed in the next round. In the case of a tie for first place in any final, the referee decides whether it is practicable to arrange for the athletes so tying to compete again. If he decides it is not, the result stands. Ties in other placings remain.
In general, most field events allow a competitor to take their attempt individually, under theoretically the same conditions as the other competitors in the competition. Each attempt is measured to determine who achieved the longest distance.
Vertical jumps (High Jump and Pole Vault) set a bar at a particular height. The competitor must clear the bar without knocking it off the standards that are holding the bar (flat). Three failures in a row ends the competitor's participation in the event. The competitor has the option to PASS their attempt, which can be used to strategic advantage (of course that advantage is lost if the competitor misses). A pass could be used to save energy and avoid taking a jump that would not improve their position in the standings. After all competitors have either cleared, passed or failed their attempts at a height, the bar goes up. The amount the bar goes up is predetermined before the competition, though when one competitor remains, that competitor may choose their own heights for the remaining attempts. A record is kept of each attempt by each competitor. After all competitors have taken their attempts, the one jumping the highest is the winner, and so on down the other competitors in the event. Ties are broken by first, the number of attempts taken at the highest height (fewest wins), and then if still tied, by the total number of misses in the competition as a whole. The bar does not go back to a lower height except to break a tie for first place or a qualifying position. If those critical positions are still tied after applying the tiebreakers, all tied competitors take a fourth jump at the last height. If they still miss, the bar goes down one increment where they again jump. This process continues until the tie is broken.
Horizontal jumps (Long Jump and Triple Jump) and all throws must be initiated behind a line. In the case of horizontal jumps, that line is a straight line perpendicular to the runway. In the case of throws, that line is an arc or a circle. Crossing the line while initiating the attempt invalidates the attemptit becomes a foul. All landings must occur in a sector. For the jumps, that is a sand filled pit, for throws it is a defined sector. A throw landing on the line on the edge of sector is a foul (the inside edge of the line is the outside edge of the sector). Assuming a proper attempt, officials measure the distance from the closest landing point back to the line. The measuring tape is carefully straightened to the shortest distance between the point and the line. To accomplish this, the tape must be perfectly perpendicular to the take off line in jumps, or is pulled through the center point of the arc for throws. The officials at the landing end of the tape have the zero, while the officials at the point of initiation measure and record the length. Whenever a record (or potential record) occurs, that measurement is taken (again) with a steel tape, and observed by at least three officials (plus usually the meet referee). Steel tapes are easily bent and damaged, so are not used to measure everyday competitions. For major competitions, each competitor gets three tries. The top competitors (usually 8 or 9 depending on that competition's rules or the number of lanes on the track) gets three more tries. At that level of competition, the order of competitors for those final three attempts are setso the competitor in first place at the end of the third round is last, while the last competitor to qualify goes first. Some meets rearrange the competition order again for the final round, so the final attempt is taken by the leader at that point. At other competitions, meet management may choose to limit all competitors to four or three attempts. Whatever the format, all competitors get an equal number of attempts.
The international governance of track and field falls under the jurisdiction of athletics organisations. The International Association of Athletics Federations is the global governing body for track and field, and athletics as a whole. The governance of track and field at continental and national level is also done by athletics bodies. Some national federations are named after the sport, including USA Track & Field and the Philippine Amateur Track & Field Association, but these organisations govern more than just track and field and are in fact athletics governing bodies.[100][101] These national federations regulate sub-national and local track and field clubs, as well as other types of running clubs.[102]
The major global track and field competitions are both held under the scope of athletics. Track and field contests make up the majority of events on the Olympic and Paralympic athletics programmes, which occur every four years. Track and field events have held a prominent position at the Summer Olympics since its inception in 1896,[103] and the events are typically held in the main stadium of the Olympic and Paralympic Games. Events such as the 100 metres receive some of the highest levels of media coverage of any Olympic or Paralympic sporting event.
The other two major international competition for track and field are organised by the IAAF. The IAAF had selected the Olympic competition as its world championship event in 1913, but a separate world championships for athletics alone was first held in 1983  the IAAF World Championships in Athletics. The championships comprised track and field competitions plus the marathon and racewalking competitions. Initially, this worked on a quadrennial basis but, after 1991, it changed to a biennial format. In terms of indoor track and field, the IAAF World Indoor Championships in Athletics has been held every two years since 1985 and this is the only world championships that consists of solely track and field events.
Similar to the event programmes at the Olympics, Paralympics and World Championships, track and field forms a significant part of continental championships. The South American Championships in Athletics, created in 1919, was the first continental championships and the European Athletics Championships became the second championships of this type in 1934. The Asian Athletics Championships and African Championships in Athletics were created in the 1970s and Oceania started its championships in 1990.
There are also indoor continental competitions in Europe (European Athletics Indoor Championships) and Asia (Asian Indoor Athletics Championships). There has not been a consistent championships for all of North America, which may be (in part) due to the success of both the Central American and Caribbean Championships and the USA Outdoor Track and Field Championships. Most countries have a national championship in track and field and, for athletes, these often play a role in gaining selection into major competitions. Some countries hold many track and field championships at high school and college-level, which help develop younger athletes. Some of these have gained significant exposure and prestige, such as the NCAA Track and Field Championship in the United States and the Jamaican High School Championships.[104] However, the number and status of such competitions significantly vary from country to country.
Mirroring the role that track and field events have at the Summer Olympics and Paralympics, the sport is featured within the athletics programmes of many major multi-sport events. Among some of the first of these events to follow the Olympic-style model were the World University Games in 1923, the Commonwealth Games in 1930, and the Maccabiah Games in 1932.[105] The number of major multi-sport events greatly increased during the 20th century and thus did the number of track and field events held within them. Typically, track and field events are hosted at the main stadium of the games.
After the Olympic and Paralympic Games, the most prominent events for track and field athletes include the three IOC-sanctioned continental games: the All-Africa Games, Asian Games, and the Pan American Games. Other games such as the Commonwealth Games and Summer Universiade, and World Masters Games have significant participation from track and field athletes. Track and field is also present at the national games level, with competitions such as the Chinese National Games serving as the most prestigious national competition for domestic track and field athletes.
One-day track and field meetings form the most common and seasonal aspect of the sport  they are the most basic level of track and field competition. Meetings are generally organised annually either under the patronage of an educational institution or sports club, or by a group or business that serves as the meeting promoter. In the case of the former, athletes are selected to represent their club or institution. In the case of privately run or independent meetings, athletes participate on an invitation-only basis.[106]
The most basic type of meetings are all-comers track meets, which are largely small, local, informal competitions that allow people of all ages and abilities to compete.[107] As meetings become more organized they can gain official sanctioning by the local or national association for the sport.[108]
At the professional level, meetings began to offer significant financial incentives for all athletes in the 1990s in Europe with the creation of the "Golden Four" competition, comprising meetings in Zrich, Brussels, Berlin and Oslo. This expanded and received IAAF backing as the IAAF Golden League in 1998,[109] which was later supplemented by the branding of selected meetings worldwide as the IAAF World Athletics Tour. In 2010, the Golden League idea was expanded globally as the IAAF Diamond League series and this now forms the top tier of professional one-day track and field meetings.[110]
Athletes performances are timed or measured at virtually all track and field competitions. Doing so can not only serve as a way of determining the winner in an event, but it can also be used for historical comparison (i.e. a record). A large variety of record types exist and men's and women's performances are recorded separately. The foremost types of records organise athlete's performances by the region they representbeginning with national records, then continental records, up to the global or world record level. National governing bodies control the national record lists, the area associations organise their respective continental lists, and the IAAF ratifies world records.
The IAAF ratifies track and field world records if they meet their set criteria. The IAAF first published a world records list in 1914, initially for men's events only. There were 53 recognised records in running, hurdling and relay, and 12 field records. World records in women's events began in 1936 as more events were gradually added to the list, but significant changes were made in the late 1970s. First, all records in imperial measurements were abandoned in 1976, with the sole exceptional being the mile run due to the prestige and history of the event. The following year, all world records in sprint events would only be recognised if fully automatic electronic timing was used (as opposed to the traditional hand-timing stopwatch method). In 1981, electronic timing was made compulsory for all world record runs in track and field, with times being recorded to within one hundredth of a second. Two additional types of world record were introduced in 1987: world records for indoor competitions, and world records for junior athletes under 20 years old.[111]
The next most important record type are those achieved at a specific competition. For example the Olympic records represent the best performances by athletes at the Summer Olympics. All major championships and games have their relevant competition records and a large number of track and field meetings keep a note of their meet records. Other record types include: stadium records, records by age range, records by disability, and records by institution or organisation. Cash bonuses are usually offered to athletes if they break significant records, as doing so can generate greater interest and public attendance in track and field competitions.
Track and field athletes are banned from ingesting or using certain substances by governing bodies for the sport, from the national to the global level. The IAAF's constitution incorporates the World Anti-Doping Code among other anti-doping measures.[112] Practices such as blood doping and the use of anabolic steroids, peptide hormones, stimulants, or diuretics can give athletes a physical competitive advantage in track and field.[113] The use of such substances in track and field is opposed on both ethical and medical grounds. Given that the sport functions by measuring and comparing athletes' performances, performance-enhancing substances create an uneven playing field  athletes who do not use doping substances have a disadvantage over rivals who do. Medically, the use of banned substances may have an adverse effect upon athletes' health. However, some exemptions are made for athletes who take banned substances for therapeutic use, and athletes are not sanctioned for usage in these cases,[114] such as Kim Collins' failed drug test due to asthma medication.[115]
To prevent use of performance-enhancing substances, athletes must submit to drug tests that are conducted both in and out of competition by anti-doping officials or accredited medical staff.[114] Athletes found to have taken substances on the World Anti-Doping Agency's banned list receive sanctions and may be banned from competition for a period of time,[116] reflecting the seriousness of the infraction. However, the use of substances not on the prohibited list may also result in sanctions if the substance is deemed similar to a banned substance in either composition or effect. Athletes may also be sanctioned for missing tests, seeking to avoid testing or tampering with results, refusing to submit to testing, through circumstantial evidence, or confession of use.[114]
Doping has played a significant part in the modern history of track and field. State-sponsored doping in East Germany with hormones and anabolic steroids marked the rise of women from the German Democratic Republic in track and field from the late 1960s to the 1980s. A number of these women, such as Marita Koch, broke world records and were highly successful at international competitions. Some athletes, who were following a doping plan from their teenage years, suffered significant health problems as a result of the regime.[117][118] Ben Johnson ran a new world record in the 100m at the 1988 Seoul Olympics but was later banned for using anabolic steroids.[119] In the mid-first decade of the 21st century, the BALCO Scandal eventually resulted in the downfall of prominent sprinters such as Marion Jones and Tim Montgomery, among others, through their usage of banned substances.[120]
CONSUDATLE  South American Championships: Outdoor
EAA  European Championships: Outdoor
1 History
2 Events

2.1 Running

2.1.1 Sprints
2.1.2 Middle distance
2.1.3 Long distance
2.1.4 Relays
2.1.5 Hurdling


2.2 Jumping

2.2.1 Long jump
2.2.2 Triple jump
2.2.3 High jump
2.2.4 Pole vault


2.3 Throwing

2.3.1 Shot put
2.3.2 Discus throw
2.3.3 Javelin throw
2.3.4 Hammer throw


2.4 Combined events


2.1 Running

2.1.1 Sprints
2.1.2 Middle distance
2.1.3 Long distance
2.1.4 Relays
2.1.5 Hurdling


2.1.1 Sprints
2.1.2 Middle distance
2.1.3 Long distance
2.1.4 Relays
2.1.5 Hurdling
2.2 Jumping

2.2.1 Long jump
2.2.2 Triple jump
2.2.3 High jump
2.2.4 Pole vault


2.2.1 Long jump
2.2.2 Triple jump
2.2.3 High jump
2.2.4 Pole vault
2.3 Throwing

2.3.1 Shot put
2.3.2 Discus throw
2.3.3 Javelin throw
2.3.4 Hammer throw


2.3.1 Shot put
2.3.2 Discus throw
2.3.3 Javelin throw
2.3.4 Hammer throw
2.4 Combined events
3 Stadium

3.1 Outdoor
3.2 Indoor


3.1 Outdoor
3.2 Indoor
4 Rules

4.1 Track rules

4.1.1 Starting
4.1.2 Running the race
4.1.3 The finish


4.2 Field rules


4.1 Track rules

4.1.1 Starting
4.1.2 Running the race
4.1.3 The finish


4.1.1 Starting
4.1.2 Running the race
4.1.3 The finish
4.2 Field rules
5 Organizations
6 Competitions

6.1 Olympics, Paralympics and world championships
6.2 Other championships
6.3 Multi-sport events
6.4 Meetings


6.1 Olympics, Paralympics and world championships
6.2 Other championships
6.3 Multi-sport events
6.4 Meetings
7 Records
8 Doping
9 References
10 External links
2.1 Running

2.1.1 Sprints
2.1.2 Middle distance
2.1.3 Long distance
2.1.4 Relays
2.1.5 Hurdling


2.1.1 Sprints
2.1.2 Middle distance
2.1.3 Long distance
2.1.4 Relays
2.1.5 Hurdling
2.2 Jumping

2.2.1 Long jump
2.2.2 Triple jump
2.2.3 High jump
2.2.4 Pole vault


2.2.1 Long jump
2.2.2 Triple jump
2.2.3 High jump
2.2.4 Pole vault
2.3 Throwing

2.3.1 Shot put
2.3.2 Discus throw
2.3.3 Javelin throw
2.3.4 Hammer throw


2.3.1 Shot put
2.3.2 Discus throw
2.3.3 Javelin throw
2.3.4 Hammer throw
2.4 Combined events
2.1.1 Sprints
2.1.2 Middle distance
2.1.3 Long distance
2.1.4 Relays
2.1.5 Hurdling
2.2.1 Long jump
2.2.2 Triple jump
2.2.3 High jump
2.2.4 Pole vault
2.3.1 Shot put
2.3.2 Discus throw
2.3.3 Javelin throw
2.3.4 Hammer throw
3.1 Outdoor
3.2 Indoor
4.1 Track rules

4.1.1 Starting
4.1.2 Running the race
4.1.3 The finish


4.1.1 Starting
4.1.2 Running the race
4.1.3 The finish
4.2 Field rules
4.1.1 Starting
4.1.2 Running the race
4.1.3 The finish
6.1 Olympics, Paralympics and world championships
6.2 Other championships
6.3 Multi-sport events
6.4 Meetings
Note: Events in italics are competed at indoor world championships only
International Association of Athletics Federations website
USA Track & Field website
Track and field at About.com
Results & Statistics for Collegiate, High School, Middle School, and Club teams
Masters T&F World Rankings
.
#(`*Tesla (company)*`)#.
TESLA (originally named after Nikola Tesla, later explained as abbreviation from "TEchnika SLAboproud" - which means "Low voltage technology") was a large, state-owned electrotechnical conglomerate in the former Czechoslovakia.
The company was established as Elektra on 18 January 1921 and renamed TESLA on 7 March 1946. TESLA had a state-sponsored monopoly on electronics production in communist Czechoslovakia, and produced nearly all electronic products in the country until 1989. Many subsidiaries were created, including those at Liptovsk Hrdok, Hradec Krlov, Pardubice, r nad Szavou, Bratislava, and Nin. Later, some of them were transformed into independent state-owned companies.
While the company's wide range of production was impressive, quantity usually did not meet the needs of industrial customers. Many products gradually became obsolete simply because they were not updated; e.g. one particular type of diode was manufactured for over 30 years without modifications.
Other products, e.g. some kinds of SCRs or Power-Transistors were competitive with the world market and so TESLA was the supplier for all eastern Europe Countries. Some high quality products were even exported to western countries for example turntables NC 470 or NC 500 under mark Lenco.
TESLA had to contend with both foreign and new domestic competition after the fall of communism in Czechoslovakia, and had difficulty competing effectively, which resulted in dramatic downsizing and privatization of the majority of its stores and production facilities. TESLA's logo is a rare sight in the present-day Czech Republic and Slovakia, as only a few of its subsidiaries have survived. One of its former subsidiaries, JJ Electronic is known for its manufacture of vacuum tubes and TESLA Litovel is known for its manufacture of high end turntables known as Pro-Ject.
Former TESLA companies with new name:
Some of the present TESLA companies:
JJ Electronic
SEV Litovel
Tesla Liptovsk Hrdok
Tesla Stropkov
Tesla Pardubice
Tesla Jihlava
Tesla Karln (in Prague)
Tesla Blatn
Tesla Hloubtn (in Prague)
.
#(`*SpaceX*`)#.
Elon Musk (CEO and CTO)
Gwynne Shotwell (President)[1]
Space Exploration Technologies Corporation, or SpaceX, is an American space transport company headquartered in Hawthorne, California. It was founded in 2002 by former PayPal entrepreneur Elon Musk. It has developed the Falcon 1 and Falcon 9 launch vehicles, both of which were designed from conception to eventually become reusable. SpaceX also developed the Dragon spacecraft to be flown into orbit by the Falcon 9 launch vehicle, initially transporting cargo and later planned to carry humans. On 25 May 2012, SpaceX made history as the world's first privately held company to send a cargo payload, carried on the Dragon spacecraft, to the International Space Station.[4]
In order to control quality and costs, SpaceX designs, tests and fabricates the majority of its components in-house, including the Merlin, Kestrel, and Draco rocket engines used on the Falcon launch vehicles and the Dragon spacecraft. In 2006, NASA awarded the company a Commercial Orbital Transportation Services (COTS) contract to design and demonstrate a launch system to resupply cargo to the International Space Station (ISS). On 9 December 2010, the launch of the COTS Demo Flight 1 mission, SpaceX became the first privately funded company to successfully launch, orbit and recover a spacecraft. On 22 May 2012, SpaceX's Falcon 9 rocket carried the unmanned Dragon capsule into space, marking the first time a private company has sent a spacecraft to the space station. The unmanned, cone-shaped capsule became the first privately built and operated vehicle to ever dock with the orbiting outpost.
NASA has also awarded SpaceX a contract to develop and demonstrate a human-rated Dragon as part of its Commercial Crew Development (CCDev) program to transport crew to the ISS. SpaceX is planning its first crewed Dragon/Falcon9 flight in 2015, when it expects to have a fully certified, human-rated launch escape system incorporated into the spacecraft.
Besides NASA contracts, SpaceX has signed contracts with private sector companies, non-American government agencies and the American military for its launch services. It has already launched, for a paying customer, a low earth orbiting satellite with its Falcon 1 booster in 2009.[5] The company plans to launch its first commercial geostationary satellite in 2013 from a Falcon 9.
Future projects that are in the planning stages or in development include the Falcon Heavy launch system, as well as a NASA robotic mission to Mars in 2018. The Heavy is based on Falcon 9 technology, and if construction goes as planned, it will be the most powerful rocket in the American inventory since the Apollo-era Saturn V. Falcon Heavy can be used to send a crewed Dragon spacecraft on lunar orbiting missions  such as the Apollo 8 mission; or be used to send a modified unpiloted Dragon on a Mars landing mission. Musk has stated that his intention for the company is to help in the creation of a permanent human presence on Mars.
SpaceX was founded in June 2002 by PayPal and Tesla Motors co-founder Elon Musk who had invested US$100 million of his own money by March 2006.[6] In January 2005, SpaceX bought a 10% stake in Surrey Satellite Technology Ltd.[7] On 4 August 2008, SpaceX accepted a further $20 million investment from the Founders Fund.[8] The company has grown rapidly since it was founded in 2002, growing from 160 employees in November 2005 to more than 500 by July 2008, to over 1,100 in 2010.[9][10] Two-thirds of the company is owned by its founder[11] and his 70 million shares are worth $875 million on private markets,[12] which roughly values SpaceX at $1.3 billion as of February 2012.[13] An initial public offering may happen by the end of 2013.[14] After the COTS 2+ flight the company valuation nearly doubled at $2.4 billion.[15][16]
Musk believes the high prices of other space-launch services are driven in part by unnecessary bureaucracy. He has stated that one of his goals is to improve the cost and reliability of access to space, ultimately by a factor of ten.[17] SpaceX became the first private company to successfully launch and return a spacecraft from orbit on 8 December 2010, after its Dragon capsule returned from a two-orbit flight.[18] Space Foundation recognized SpaceX for its successful Dragon launch and recovery with the Space Achievement Award in 2011.[19]
At various conferences, SpaceX has revealed concept slides for future engine, stage, and launch vehicle designs. Development of these designs would be predicated on demand for increased performance. Company plans in 2004 called for "development of a heavy lift product and even a super-heavy, if there is customer demand" with each size increase resulting in a significant decrease in cost per pound to orbit. CEO Elon Musk said: "I believe $500 per pound ($1,100/kg) or less is very achievable."[20]
Elon Musk has stated the personal goal of eventually enabling human exploration and settlement of Mars.[21] He stated in a 2011 interview that he hopes to send humans to Mars' surface within 1020 years.[21]
As of May 2012[update], SpaceX has operated on total funding of approximately $1 billion in its first ten years of operation. Of this, private equity has provided about $200M, with Musk investing approximately $100M and other investors having put in about $100M (Founders Fund, Draper Fisher Jurvetson, ...).[22] The remainder has come from progress payments on long-term launch contracts and development contracts. NASA has put in about $400500M of this amount, with most of that as progress payments on launch contracts. SpaceX currently has contracts for 40 launch missions, and each of those contracts provide down payments at contract signing, plus many are paying progress payments as launch vehicle components are built in advance of mission launch, driven in part by US accounting rules for recognizing long-term revenue.[23] On August 3, 2012, NASA announced new agreements with SpaceX and two other companies to design and develop the next generation of U.S. human spaceflight capabilities, enabling a launch of astronauts from U.S. soil in the next five years. Advances made by these companies under newly signed Space Act Agreements through the agency's Commercial Crew Integrated Capability (CCiCap) initiative are intended to ultimately lead to the availability of commercial human spaceflight services for both government and commercial customers. As part of this agreement, SpaceX was awarded a contract worth up to $440 million for contract deliverables between 2012 and May 2014.[24][25]
SpaceX Headquarters is located at 1 Rocket Road Hawthorne, California. The large facility, formerly used to build Boeing 747 fuselages, houses SpaceX's office space, mission control, and vehicle factory. [26]
SpaceX has used a number of launch sites, including:
All Falcon 1 launches have taken place at Omelek Island. Falcon 9 launches on the SpaceX manifest are planned for Cape Canaveral SLC-40 and Vandenberg AFB SLC-4E (Polar Launches);[27] As of November 2012, all of the first four Falcon 9 flights used the Cape Canaveral SLC-40.
The company purchased the McGregor, Texas, testing facilities of defunct Beal Aerospace, where it refitted the largest test stand at the facilities for Falcon9 testing. SpaceX plans to upgrade the facility for launch testing a VTVL rocket in 2012.[28]
On 16 June 2009, SpaceX announced the opening of its Astronaut Safety and Mission Assurance Department. It hired former NASA astronaut Ken Bowersox to oversee the department as a vice president of the company.[29] However, it has since been reported that the former astronaut subsequently left SpaceX in late 2011. No reason was given and no replacement in that position has been announced.[30]
SpaceX broke ground on their own launch site, located at Vandenberg Air Force Base, on 13 July 2011.[31] The launch site is intended for use by the Falcon Heavy launch vehicle, which is scheduled to be brought on site in 2012 with a test flight to follow soon after.[32] The project is expected to cost between $20 to $30 million for the first 24 months of construction and operation; thereafter, operational costs are expected to be $510 million per year. The site is said to be a natural fit for SpaceX, as they attempted to schedule a Falcon 1 launch from there previously but had to move the launch to the Reagan site due to scheduling conflicts.[31] SpaceX plans to launch up to 16 flights per year by 2015 from Vandenberg.[31]
As of September 2012[update], SpaceX is considering seven potential locations around the country for a new commercial launch pad. In April 2012, potential locations initially included "sites in Alaska, California, Florida, Texas and Virginia."[33] In September 2012, it became clear that Georgia and Puerto Rico were also interested in pursuing the new SpaceX commercial spaceport facility.[34] The Camden County, Georgia Joint Development Authority voted unanimously in November 2012 to "explore developing an aero-spaceport facility" at an Atlantic coastal site to support both horizontal and vertical launch operations.[35]
One of the proposed locations for the new commercial-mission-only spaceport is south Texas, which was revealed in April 2012, via preliminary regulatory documentation. The FAA's Office of Commercial Space Transportation initiated a Notice of Intent to conduct an Environmental Impact Statement[36] and public hearings on the new launch site, which would be located in Cameron County, Texas. The site would support up to 12 commercial launches per year, including two Falcon Heavy launches.[37] "Before anything could be done on the project, an environmental impact statement, a public scoping period and a public scoping meeting would need to be held."[38] Although preliminary environmental assessment regulatory documents have been filed for this location, that site has not yet been selected as the site for construction.[33] The first public meeting is scheduled for 15 May 2012.[38] As of May 2012[update], the state of Texas is considering a package of incentives to encourage SpaceX to locate at the Brownsville, Texas location.[39]
On 18 August 2006, NASA announced that SpaceX had won a NASA Commercial Orbital Transportation Services (COTS) contract to demonstrate cargo delivery to the International Space Station (ISS) with a possible option for crew transport. [40] This contract, designed by NASA to provide "seed money" for development of new boosters, paid SpaceX $278 million to develop the Falcon 9 launch vehicle, with incentive payments paid at milestones culminating in three demonstration launches. [41] In December 2008 SpaceX and Orbital Sciences Corporation each won a Commercial Resupply Services (CRS) contract. That of SpaceX is for at least 12 missions for $1.6 billion to carry supplies and cargo to and from the ISS. [42] On 9 December 2010, the launch of the COTS Demo Flight 1 mission, SpaceX became the first privately funded company to successfully launch, orbit and recover a spacecraft.
The original NASA contract called for the COTS Demo Flight 1 to occur the second quarter of 2008;[43] this flight was delayed several times, occurring at 1543 UTC on 8 December 2010.[44] Dragon was successfully deployed into orbit, circling the Earth twice, and then made a controlled reentry burn that put it on target for a splashdown in the Pacific Ocean off the coast of Mexico.[45] With Dragon's safe recovery, SpaceX become the first private company to launch, orbit, and recover a spacecraft; prior to this mission, only government agencies had been able to recover orbital spacecraft.[45]
According to the original schedule, in COTS Demo Flight 2 the Dragon spacecraft would make its second flight and would rendezvous with the ISS but not be berthed. The third flight would see Dragon being berthed to the ISS.[43] However, after the success of the first mission, NASA conditionally agreed on 15 July 2011 that the two flights would be combined, and the next Dragon mission was to have Dragon being berthed with the ISS.[46] On 9 December 2011, NASA formally approved the merger of the COTS 2 and 3 missions into the COTS 2 flight, but yet again delayed the tentative launch date by another month to 7 February 2012.[47] However, on 16 January 2012, SpaceX announced it needed more time for engineering tests, and postponed the launch date again, with no replacement date initially announced.[48] On 19 May at approximately 4:55AM EDT the launch for the COTS 2+ mission was automatically aborted when the pressure in one of the engine chambers was higher than expected. The COTS Demo Flight 2 launch was postponed to 22 May 2012, at which point it succeeded in putting the Dragon spacecraft into orbit. Several days later, the Dragon capsule successfully berthed with the International Space Station, marking the first time that a private spacecraft had accomplished this feat.[49][50]
SpaceX is planning a crewed Dragon/Falcon9 flight in future years when it expects to have a fully certified, human-rated launch escape system incorporated into the spacecraft.[51] NASA's Commercial Crew Development (CCDev) program intends to develop commercially operated manned spacecrafts that are capable of delivering crew to the ISS. SpaceX did not participate during the first round, however during the second round of the program NASA awarded SpaceX with a contract worth $75 million to further develop their launch escape system, test a crew accommodations mock-up and to further progress the Falcon 9/Dragon crew transportation design.[52][53] SpaceX later submitted a proposal for the third round of the CCDev program which became Commercial Crew Integrated Capability (CCiCap).[54]
On August 3, 2012, NASA announced new agreements with SpaceX and two other companies to design and develop the next generation of U.S. human spaceflight capabilities, enabling a launch of astronauts from U.S. soil in the next five years. Advances made by these companies under newly signed Space Act Agreements through the agency's CCiCap initiative are intended to ultimately lead to the availability of commercial human spaceflight services for government and commercial customers. As part of this agreement, SpaceX was awarded $440 million, ostensibly to continue development and testing of its DragonRider spacecraft.[55]
In addition to SpaceX's privately funded plans for an eventual Mars mission, as of July 2011[update] NASA Ames Research Center had developed a concept for a low-cost Mars mission that would utilize Falcon Heavy as the launch vehicle and trans-Martian injection vehicle, and the Dragon capsule to enter the Martian atmosphere. The concept, called 'Red Dragon', would be proposed for funding in 2012/2013 as a NASA Discovery mission, for launch in 2018 and arrival at Mars several months later. The science objectives of the mission would be to look for evidence of life  detecting "molecules that are proof of life, like DNA or perchlorate reductase ... proof of life through biomolecules. ... Red Dragon would drill 3.3 feet (1.0m) or so underground, in an effort to sample reservoirs of water ice known to lurk under the red dirt." The mission cost is projected to be less than $425,000,000, not including the launch cost.[56]
On 2 May 2005, SpaceX announced that it had been awarded an Indefinite Delivery/Indefinite Quantity (IDIQ) contract for Responsive Small Spacelift (RSS) launch services by the United States Air Force, which could allow the Air Force to purchase up to $100 million worth of launches from the company.[57] On 22 April 2008, NASA announced that it had awarded an IDIQ Launch Services contract to SpaceX for Falcon 1 and Falcon 9 launches. The contract will be worth up to $1 billion, depending on the number of missions awarded. The contract covers launch services ordered by 30 June 2010, for launches through December 2012.[58] Musk stated in the same 2008 announcement that SpaceX has sold 14 contracts for flights on the various Falcon vehicles.[58]
In December 2012, SpaceX announced its first two launch contracts with the United States Department of Defense. "The United States Air Force Space and Missile Systems Center awarded SpaceX two Evolved Expendable Launch Vehicle (EELV)-class missions:" Deep Space Climate Observatory (DSCOVR) and Space Test Program 2 (STP-2), to be launched in 2014 and 2015, respectively. DSCOVR will be launched on a Falcon 9 launch vehicle while STP-2 will be launched on a Falcon Heavy.[59]
SpaceX announced on 15 March 2010 that it will launch SES-8, a medium-sized communications satellite for SES, on a Falcon 9 vehicle in 2013.[60] SES is SpaceX's first contract for a geostationary communications satellite launch.[60] In June 2010, SpaceX was awarded the largest-ever commercial space launch contract, worth $492 million, to launch Iridium satellites using Falcon 9 rockets.[61]
SpaceX is manufacturing two main space launch vehicles: the Falcon 1, which made its first successful flight on 28 September 2008,[62][63] and the large Evolved Expendable Launch Vehicle (EELV)-class Falcon 9, which flew successfully into orbit on its maiden launch on 4 June 2010.[63] A Falcon 5 launcher was also planned, but its development was stopped in favor of the Falcon 9.[64] SpaceX also developed the Dragon, a pressurized orbital spacecraft that is launched on top of a Falcon 9 booster, that can carry cargo, and is in the process of being human-rated.[65]
SpaceX has flown, or is in development on, several orbital launch vehicles: the Falcon 1, Falcon 9, and Falcon Heavy. As of 2012, the Falcon 9 is currently in active usage and the Falcon Heavy is under development with a large manifest of flights after 2013.
The Falcon 1 was a small rocket capable of placing several hundred kilograms into low earth orbit.[63] It also functioned as an early testbed for developing concepts and components for the larger Falcon 9.[63] Falcon 1 made five flights in 2006-2009. Initial Falcon 1 flights were launched from the US government's Reagan Test Center on the island atoll of Kwajalein in the Pacific Ocean, and represented the first attempt to fly a ground-launched rocket to orbit from that site.[66] On 28 September 2008, the Falcon 1 succeeded in reaching orbit on its fourth attempt, becoming the first privately funded, liquid-fueled rocket to do so.[67] The Falcon 1 carried its first successful commercial payload, RazakSAT, into orbit on 13 July 2009, on its fifth launch.[5]
The Falcon 9 is an EELV-class medium-lift vehicle capable of delivering up to 10,450 kilograms (23,000lb) to orbit, and is intended to compete with the Delta IV and the Atlas V rockets. It has nine Merlin engines in its first stage.[68] The Falcon 9 rocket successfully reached orbit on its first attempt in June 2010. The second flight for the Falcon 9 vehicle was the COTS Demo Flight 1 on 8 December 2010, the first launch under the NASA Commercial Orbital Transportation Services (COTS) contract, and was similarly successful.[58] Its third flight, COTS Demo Flight 2, launched on 22 May 2012, and was the first commercial spacecraft to reach and dock with the International Space Station.[69]
The Falcon Heavy is currently under development as a heavy-lift configuration using a cluster of three Falcon 9 first stage cores with a total 27 uprated Merlin 1D engines and propellant crossfeed.[70] [71] SpaceX is aiming for the first demonstration flight of the Falcon Heavy in 2013.[70]
In 2005, SpaceX announced plans to pursue a human-rated commercial space program through the end of the decade.[72] The Dragon spacecraft is intended to carry up to seven astronauts into orbit and beyond.[73] It is a conventional blunt-cone ballistic capsule, which is capable of carrying 7 people or a mixture of personnel and cargo to and from low Earth orbit.[73] It is launched atop a Falcon 9 launch vehicle, the spacecraft's nosecone is jettisoned shortly after launch. For NASA cargo missions, Dragon will be equipped with a Common Berthing Mechanism, and will be berthed to the U.S. segment of the ISS by the Canadarm2.[74] For NASA manned missions, Dragon will be equipped with the NASA Docking System and dock to the U.S. segment.[75]
In 2006, NASA announced that the company was one of two selected to provide crew and cargo resupply demonstration contracts to the ISS under the COTS program.[76] SpaceX will demonstrate cargo resupply and eventually crew transportation services using the Dragon. NASA's original plan called for COTS demonstration flights between 2008 and 2010.[77][78] SpaceX was not able to meet that schedule, but eventually began test-flights in 2010.
The first flight of a Dragon structural test article took place 4 June 2010, from Launch Complex 40 at Cape Canaveral Air Force Station during the maiden flight of the Falcon 9 launch vehicle; the mock-up Dragon lacked avionics, heat shield, and other key elements normally required of a fully operational spacecraft but contained all the necessary characteristics to validate the flight performance of the launch vehicle.[79] An operational Dragon spacecraft was launched on 8 December 2010 aboard COTS Demo Flight 1, the Falcon 9's second flight, and safely returned to Earth after two orbits, completing all its mission objectives.[65]
In 2009 and 2010, Musk suggested on several occasions that plans for a human-rated variant of Dragon were proceeding and had a 2- to 3-year time line to completion.[80][81] On 18 April 2011, NASA issued a $75 million contract, as part of its second-round commercial crew development (CCDev) program, for SpaceX to develop an integrated launch escape system for Dragon in preparation for human-rating it as a crew transport vehicle to the ISS.[82] This Space Act Agreement runs from April 2011 until May 2012, when the next round of contracts are to be awarded.[82] NASA approved the technical plans for the system in October 2011, and SpaceX began building prototype hardware.[51][dated info]
Several modifications or additions to the Falcon rocket family are currently being developed by SpaceX. These include two vehicles that further technology development objectives toward reusable launch systems: the Grasshopper test vehicle and the commercial launch vehicle reusable Falcon 9
A major goal of SpaceX has been to develop a rapidly reusable launch system. Their current work in this development effort involves testing the Grasshopper launch vehicle. Grasshopper is a vertical takeoff, vertical landing (VTVL) technology demonstrator rocket[83] built in 2011 for low-altitude testing which began in 2012.[28][84][85][86]
Since the founding of SpaceX in 2002, the company has developed three families of rocket engines  Merlin and Kestrel for launch vehicle propulsion, and the Draco RCS control thrusters. SpaceX is currently under development of two more rocket engines: SuperDraco and Raptor.
In a June 2009 AIAA presentation, a conceptual plan for the Raptor project was unveiled.[87] As of 2012[update], information was released to the public that Raptor is a higher performance Methane/LOX rocket engine, not an upper stage, and that SpaceX is undertaking a substantial new rocket engine development program for Methane-based engines.[88] When the project originally surfaced, and very little public information was released, "Raptor" was at that time a LOX/liquid hydrogen second stage concept for Falcon 9.[87]
Longer term projects under study include the much larger Merlin 2 engine. The Merlin 2 may be used on conceptual heavy-lift launch vehicles Falcon X, Falcon X Heavy, and Falcon XX.[89]
In October 2012, SpaceX CEO Elon Musk publicly announced concept work on a rocket engine that would be "several times as powerful as the 1 Merlin series, and won't use Merlin's RP-1 fuel".[90] Elon Musk declined to provide details at that time, but did indicate details would be forthcoming "sometime next year"(2013).[91] The engine is intended for a new SpaceX rocket, using multiple of those large engines, and would notionally launch payload masses of the order of 150 to 200 tonnes (150,000 to 200,000 kg) to low-Earth orbit, exceeding the payload mass capability of the NASA Space Launch System.[90]
In November 2012, SpaceX CEO Elon Musk announced a new direction for propulsion side of the company: developing methane/LOX rocket engines for launch vehicle main and upper stages.[88]
While this may result in a family of methane-based engines, what has been released to date indicates that one of these will be an upper stage engine designated Raptor.[88]
1 Background

1.1 Funding


1.1 Funding
2 Facilities

2.1 Headquarters
2.2 Flight operations

2.2.1 New commercial-only launch site




2.1 Headquarters
2.2 Flight operations

2.2.1 New commercial-only launch site


2.2.1 New commercial-only launch site
3 NASA collaborations

3.1 COTS program and CRS
3.2 CCDev and CCiCap programs
3.3 "Red Dragon" Mars mission concept


3.1 COTS program and CRS
3.2 CCDev and CCiCap programs
3.3 "Red Dragon" Mars mission concept
4 Other Contracts
5 Space vehicles

5.1 Falcon launch vehicles
5.2 Dragon
5.3 Other concepts under development


5.1 Falcon launch vehicles
5.2 Dragon
5.3 Other concepts under development
6 Rocket engines
7 See also
8 References
9 External links
1.1 Funding
2.1 Headquarters
2.2 Flight operations

2.2.1 New commercial-only launch site


2.2.1 New commercial-only launch site
2.2.1 New commercial-only launch site
3.1 COTS program and CRS
3.2 CCDev and CCiCap programs
3.3 "Red Dragon" Mars mission concept
5.1 Falcon launch vehicles
5.2 Dragon
5.3 Other concepts under development
Ronald Reagan Ballistic Missile Defense Test Site, Omelek Island, Kwajalein Atoll, Marshall Islands
Cape Canaveral Air Force Station Space Launch Complex 40
Vandenberg Air Force Base, Space Launch Complex 4
List of Falcon 9 missions
Orbital Sciences Corporation
Space exploration
Private spaceflight
SpaceX official company website
Maiden flight slide presentationPDF
Michael Belfiore's notes from SpaceX pre-launch conference in 2005
Kwajalein Atoll and Rockets (Candid and highly unofficial blog by Elon Musk's brother Kimbal, with on-site pictures and reporting.)
Video of Elon Musk discussing his interest in enabling human missions to Mars on YouTube
SpaceX on Twitter
SpaceX's channel on YouTube
SpaceX on Facebook
.
#(`*Google*`)#.
Google Inc. (NASDAQ:GOOG) is an American multinational corporation which provides Internet-related products and services, including internet search, cloud computing, software and advertising technologies.[6] Advertising revenues from AdWords generate almost all of the company's profits.[7][8]
The company was founded by Larry Page and Sergey Brin while both attended Stanford University. Together, Brin and Page own about 16 percent of the company's stake. Google was first incorporated as a privately held company on September4, 1998, and its initial public offering followed on August19, 2004. The company's mission statement from the outset was "to organize the world's information and make it universally accessible and useful"[9] and the company's unofficial slogan is "Don't be evil".[10][11] In 2006, the company moved to its current headquarters in Mountain View, California.
Rapid growth since incorporation has triggered a chain of products, acquisitions, and partnerships beyond the company's core web search engine. The company offers online productivity software including email, an office suite, and social networking. Google's products extend to the desktop as well, with applications for web browsing, organizing and editing photos, and instant messaging. Google leads the development of the Android mobile operating system, as well as the Google Chrome OS browser-only operating system,[12] found on specialized netbooks called Chromebooks. Google has increasingly become a hardware company with its partnerships with major electronics manufacturers on its high-end Nexus series of devices and its acquisition of Motorola Mobility in May 2012,[13] as well as the construction of fiber-optic infrastructure in Kansas City as part of the Google Fiber broadband Internet service project.[14]
Google has been estimated to run over one million servers in data centers around the world,[15] and process over one billion search requests[16] and about twenty-four petabytes of user-generated data every day.[17][18][19][20]
As of November 2012, Alexa listed the main U.S.-focused google.com site as the Internet's second most visited website and numerous international Google sites as being in the top hundred, as well as several other Google-owned sites such as YouTube and Blogger.[21] Google also ranks number two in the BrandZ brand equity database.[22] The dominant market position of Google's services has led to criticism of the company over issues including privacy, copyright, and censorship.[23][24]
Google began in January 1996 as a research project by Larry Page and Sergey Brin when they were both PhD students at Stanford University in California.[26]
While conventional search engines ranked results by counting how many times the search terms appeared on the page, the two theorized about a better system that analyzed the relationships between websites.[27] They called this new technology PageRank, where a website's relevance was determined by the number of pages, and the importance of those pages, that linked back to the original site.[28][29]
A small search engine called "RankDex" from IDD Information Services designed by Robin Li was, since 1996, already exploring a similar strategy for site-scoring and page ranking.[30] The technology in RankDex would be patented[31] and used later when Li founded Baidu in China.[32][33]
Page and Brin originally nicknamed their new search engine "BackRub", because the system checked backlinks to estimate the importance of a site.[34][35][36]
Eventually, they changed the name to Google, originating from a misspelling of the word "googol",[37][38] the number one followed by one hundred zeros, which was picked to signify that the search engine wants to provide large quantities of information for people.[39] Originally, Google ran under the Stanford University website, with the domains google.stanford.edu and z.stanford.edu.[40][41]
The domain name for Google was registered on September 15, 1997,[42] and the company was incorporated on September 4, 1998. It was based in a friend's (Susan Wojcicki[26]) garage in Menlo Park, California. Craig Silverstein, a fellow PhD student at Stanford, was hired as the first employee.[26][43][44]
In May 2011, the number of monthly unique visitors to Google surpassed 1 billion for the first time, an 8.4 percent increase from May 2010 (931 million).[45]
The first funding for Google was an August 1998 contribution of US$100,000 from Andy Bechtolsheim, co-founder of Sun Microsystems, given before Google was even incorporated.[47] Early in 1999, while still graduate students, Brin and Page decided that the search engine they had developed was taking up too much of their time from academic pursuits. They went to Excite CEO George Bell and offered to sell it to him for $1million. He rejected the offer, and later criticized Vinod Khosla, one of Excite's venture capitalists, after he had negotiated Brin and Page down to $750,000. On June 7, 1999, a $25million round of funding was announced,[48] with major investors including the venture capital firms Kleiner Perkins Caufield & Byers and Sequoia Capital.[47]
Google's initial public offering (IPO) took place five years later on August 19, 2004. At that time Larry Page, Sergey Brin, and Eric Schmidt agreed to work together at Google for 20 years, until the year 2024.[49] The company offered 19,605,052 shares at a price of $85 per share.[50][51] Shares were sold in a unique online auction format using a system built by Morgan Stanley and Credit Suisse, underwriters for the deal.[52][53] The sale of $1.67billion gave Google a market capitalization of more than $23billion.[54] The vast majority of the 271million shares remained under the control of Google, and many Google employees became instant paper millionaires. Yahoo!, a competitor of Google, also benefited because it owned 8.4million shares of Google before the IPO took place.[55]
Some people speculated that Google's IPO would inevitably lead to changes in company culture. Reasons ranged from shareholder pressure for employee benefit reductions to the fact that many company executives would become instant paper millionaires.[56] As a reply to this concern, co-founders Sergey Brin and Larry Page promised in a report to potential investors that the IPO would not change the company's culture.[57] In 2005, however, articles in The New York Times and other sources began suggesting that Google had lost its anti-corporate, no evil philosophy.[58][59][60] In an effort to maintain the company's unique culture, Google designated a Chief Culture Officer, who also serves as the Director of Human Resources. The purpose of the Chief Culture Officer is to develop and maintain the culture and work on ways to keep true to the core values that the company was founded on: a flat organization with a collaborative environment.[61] Google has also faced allegations of sexism and ageism from former employees.[62][63]
The stock's performance after the IPO went well, with shares hitting $700 for the first time on October 31, 2007,[64] primarily because of strong sales and earnings in the online advertising market.[65] The surge in stock price was fueled mainly by individual investors, as opposed to large institutional investors and mutual funds.[65] The company is now listed on the NASDAQ stock exchange under the ticker symbol GOOG and under the Frankfurt Stock Exchange under the ticker symbol GGQ1.
In March 1999, the company moved its offices to Palo Alto, California, home to several other noted Silicon Valley technology startups.[66] The next year, against Page and Brin's initial opposition toward an advertising-funded search engine,[67] Google began selling advertisements associated with search keywords.[26] In order to maintain an uncluttered page design and increase speed, advertisements were solely text-based. Keywords were sold based on a combination of price bids and click-throughs, with bidding starting at five cents per click.[26] This model of selling keyword advertising was first pioneered by Goto.com, an Idealab spin-off created by Bill Gross.[68][69] When the company changed names to Overture Services, it sued Google over alleged infringements of the company's pay-per-click and bidding patents. Overture Services would later be bought by Yahoo! and renamed Yahoo! Search Marketing. The case was then settled out of court, with Google agreeing to issue shares of common stock to Yahoo! in exchange for a perpetual license.[70]
During this time, Google was granted a patent describing its PageRank mechanism.[71] The patent was officially assigned to Stanford University and lists Lawrence Page as the inventor. In 2003, after outgrowing two other locations, the company leased its current office complex from Silicon Graphics at 1600 Amphitheatre Parkway in Mountain View, California.[72] The complex has since come to be known as the Googleplex, a play on the word googolplex, the number one followed by a googol zeroes. The Googleplex interiors were designed by Clive Wilkinson Architects. Three years later, Google would buy the property from SGI for $319million.[73] By that time, the name "Google" had found its way into everyday language, causing the verb "google" to be added to the Merriam Webster Collegiate Dictionary and the Oxford English Dictionary, denoted as "to use the Google search engine to obtain information on the Internet."[74][75]
Since 2001, Google has acquired many companies, mainly focusing on small venture capital companies. In 2004, Google acquired Keyhole, Inc.[76] The start-up company developed a product called Earth Viewer that gave a three-dimensional view of the Earth. Google renamed the service to Google Earth in 2005. Two years later, Google bought the online video site YouTube for $1.65billion in stock.[77] On April 13, 2007, Google reached an agreement to acquire DoubleClick for $3.1billion, giving Google valuable relationships that DoubleClick had with Web publishers and advertising agencies.[78] Later that same year, Google purchased GrandCentral for $50million.[79] The site would later be changed over to Google Voice. On August 5, 2009, Google bought out its first public company, purchasing video software maker On2 Technologies for $106.5million.[80] Google also acquired Aardvark, a social network search engine, for $50million, and commented on its internal blog, "we're looking forward to collaborating to see where we can take it".[81] In April 2010, Google announced it had acquired a hardware startup, Agnilux.[82]
In addition to the many companies Google has purchased, the company has partnered with other organizations for everything from research to advertising. In 2005, Google partnered with NASA Ames Research Center to build 1,000,000 square feet (93,000m2) of offices.[83] The offices would be used for research projects involving large-scale data management, nanotechnology, distributed computing, and the entrepreneurial space industry. Google entered into a partnership with Sun Microsystems in October 2005 to help share and distribute each other's technologies.[84] The company also partnered with AOL of Time Warner,[85] to enhance each other's video search services. Google's 2005 partnerships also included financing the new .mobi top-level domain for mobile devices, along with other companies including Microsoft, Nokia, and Ericsson.[86] Google would later launch "AdSense for Mobile", taking advantage of the emerging mobile advertising market.[87] Increasing its advertising reach even further, Google and Fox Interactive Media of News Corporation entered into a $900million agreement to provide search and advertising on popular social networking site MySpace.[88]
In October 2006, Google announced that it had acquired the video-sharing site YouTube for US$1.65billion in Google stock, and the deal was finalized on November 13, 2006.[89] Google does not provide detailed figures for YouTube's running costs, and YouTube's revenues in 2007 were noted as "not material" in a regulatory filing.[90] In June 2008, a Forbes magazine article projected the 2008 YouTube revenue at US$200million, noting progress in advertising sales.[91] In 2007, Google began sponsoring NORAD Tracks Santa, a service that follows Santa Claus' progress on Christmas Eve,[92] using Google Earth to "track Santa" in 3-D for the first time,[93] and displacing former sponsor AOL. Google-owned YouTube gave NORAD Tracks Santa its own channel.[94]
In 2008, Google developed a partnership with GeoEye to launch a satellite providing Google with high-resolution (0.41m monochrome, 1.65m color) imagery for Google Earth. The satellite was launched from Vandenberg Air Force Base on September 6, 2008.[95] Google also announced in 2008 that it was hosting an archive of Life Magazine's photographs as part of its latest partnership. Some of the images in the archive were never published in the magazine.[96] The photos were watermarked and originally had copyright notices posted on all photos, regardless of public domain status.[97]
In 2010, Google Energy made its first investment in a renewable energy project, putting $38.8million into two wind farms in North Dakota. The company announced the two locations will generate 169.5megawatts of power, or enough to supply 55,000 homes. The farms, which were developed by NextEra Energy Resources, will reduce fossil fuel use in the region and return profits. NextEra Energy Resources sold Google a twenty percent stake in the project to get funding for its development.[98] Also in 2010, Google purchased Global IP Solutions, a Norway-based company that provides web-based teleconferencing and other related services. This acquisition will enable Google to add telephone-style services to its list of products.[99] On May 27, 2010, Google announced it had also closed the acquisition of the mobile ad network AdMob. This purchase occurred days after the Federal Trade Commission closed its investigation into the purchase.[100] Google acquired the company for an undisclosed amount.[101] In July 2010, Google signed an agreement with an Iowa wind farm to buy 114 megawatts of energy for 20 years.[102]
On April 4, 2011, The Globe and Mail reported that Google bid $900 million for six thousand Nortel Networks patents.[103]
On August 15, 2011, Google announced that it would acquire Motorola Mobility for $12.5 billion[104][105] subject to approval from regulators in the United States and Europe. In a post on Google's blog, Google Chief Executive and co-founder Larry Page revealed that Google's acquisition of Motorola Mobility is a strategic move to strengthen Google's patent portfolio. The company's Android operating system has come under fire in an industry-wide patent battle, as Apple and Microsoft have taken to court Android device makers such as HTC, Samsung and Motorola.[106] The merger was completed on the 22 May 2012, after the approval of People's Republic of China.[107] This purchase was made in part to help Google gain Motorola's considerable patent portfolio on mobile phones and wireless technologies to help protect it in its ongoing patent disputes with other companies,[108] mainly Apple and Microsoft[106] and to allow it to continue to freely offer Android.[109] In order to expand its social networking services, Google plans to purchase Silicon Valley start up Meebo.[110]
On June 5, 2012 Google announced it acquired Quickoffice, a company widely known for their mobile productivity suite for both iOS and Android. Google plans to integrate Quickoffice's technology into its own product suite.[111]
Google Inc. currently owns and operates 6 data centers across the U.S., plus one in Finland and another in Belgium. On September 28, 2011 the company has announced to build three data centers at a cost of more than $200 million in Asia (Singapore, Hong Kong and Taiwan) and has already purchased the land for them. Google said they will be operational in one to two years.[112]
In 2011, 96% of Google's revenue was derived from its advertising programs.[113] For the 2006 fiscal year, the company reported $10.492billion in total advertising revenues and only $112million in licensing and other revenues.[114] Google has implemented various innovations in the online advertising market that helped make it one of the biggest brokers in the market. Using technology from the company DoubleClick, Google can determine user interests and target advertisements so they are relevant to their context and the user that is viewing them.[115][116] Google Analytics allows website owners to track where and how people use their website, for example by examining click rates for all the links on a page.[117] Google advertisements can be placed on third-party websites in a two-part program. Google's AdWords allows advertisers to display their advertisements in the Google content network, through either a cost-per-click or cost-per-view scheme. The sister service, Google AdSense, allows website owners to display these advertisements on their website, and earn money every time ads are clicked.[118]
One of the disadvantages and criticisms of this program is Google's inability to combat click fraud, when a person or automated script "clicks" on advertisements without being interested in the product, which causes that advertiser to pay money to Google unduly. Industry reports in 2006 claim that approximately 14 to 20 percent of clicks were in fact fraudulent or invalid.[119] Furthermore, there has been controversy over Google's "search within a search", where a secondary search box enables the user to find what they are looking for within a particular website. It was soon reported that when performing a search within a search for a specific company, advertisements from competing and rival companies often showed up along with those results, drawing users away from the site they were originally searching.[120] Another complaint against Google's advertising is its censorship of advertisers, though many cases concern compliance with the Digital Millennium Copyright Act. For example, in February 2003, Google stopped showing the advertisements of Oceana, a non-profit organization protesting a major cruise ship's sewage treatment practices. Google cited its editorial policy at the time, stating "Google does not accept advertising if the ad or site advocates against other individuals, groups, or organizations."[121] The policy was later changed.[122] In June 2008, Google reached an advertising agreement with Yahoo!, which would have allowed Yahoo! to feature Google advertisements on its web pages. The alliance between the two companies was never completely realized due to antitrust concerns by the U.S. Department of Justice. As a result, Google pulled out of the deal in November 2008.[123][124]
In an attempt to advertise its own products, Google launched a website called Demo Slam, developed to demonstrate technology demos of Google Products.[125] Each week, two teams compete at putting Google's technology into new contexts. Search Engine Journal said Demo Slam is "a place where creative and tech-savvy people can create videos to help the rest of the world understand all the newest and greatest technology out there."[126]
Google Search, a web search engine, is the company's most popular service. According to market research published by comScore in November 2009, Google is the dominant search engine in the United States market, with a market share of 65.6%.[127] Google indexes billions[128] of web pages, so that users can search for the information they desire, through the use of keywords and operators. Despite its popularity, it has received criticism from a number of organizations. In 2003, The New York Times complained about Google's indexing, claiming that Google's caching of content on its site infringed its copyright for the content.[129] In this case, the United States District Court of Nevada ruled in favor of Google in Field v. Google and Parker v. Google.[130][131] Furthermore, the publication 2600: The Hacker Quarterly has compiled a list of words that the web giant's new instant search feature will not search.[132] Google Watch has also criticized Google's PageRank algorithms, saying that they discriminate against new websites and favor established sites,[133] and has made allegations about connections between Google and the NSA and the CIA.[134] Despite criticism, the basic search engine has spread to specific services as well, including an image search engine, the Google News search site, Google Maps, and more. In early 2006, the company launched Google Video, which allowed users to upload, search, and watch videos from the Internet.[135] In 2009, however, uploads to Google Video were discontinued so that Google could focus more on the search aspect of the service.[136] The company even developed Google Desktop, a desktop search application used to search for files local to one's computer (discontinued in 2011). Google's most recent development in search is its partnership with the United States Patent and Trademark Office to create Google Patents, which enables free access to information about patents and trademarks.
One of the more controversial search services Google hosts is Google Books. The company began scanning books and uploading limited previews, and full books where allowed, into its new book search engine. The Authors Guild, a group that represents 8,000 U.S. authors, filed a class action suit in a New York City federal court against Google in 2005 over this new service. Google replied that it is in compliance with all existing and historical applications of copyright laws regarding books.[137] Google eventually reached a revised settlement in 2009 to limit its scans to books from the U.S., the UK, Australia and Canada.[138] Furthermore, the Paris Civil Court ruled against Google in late 2009, asking it to remove the works of La Martinire (ditions du Seuil) from its database.[139] In competition with Amazon.com, Google plans to sell digital versions of new books.[140] On July 21, 2010, in response to newcomer Bing, Google updated its image search to display a streaming sequence of thumbnails that enlarge when pointed at. Though web searches still appear in a batch per page format, on July 23, 2010, dictionary definitions for certain English words began appearing above the linked results for web searches.[141] Google's algorithm was changed in March 2011, giving more weight to high-quality content[142] possibly by the use of n-grams to remove spun content.[143]
In addition to its standard web search services, Google has released over the years a number of online productivity tools. Gmail, a free webmail service provided by Google, was launched as an invitation-only beta program on April 1, 2004,[144] and became available to the general public on February 7, 2007.[145] The service was upgraded from beta status on July 7, 2009,[146] at which time it had 146million users monthly.[147] The service would be the first online email service with one gigabyte of storage, and the first to keep emails from the same conversation together in one thread, similar to an Internet forum.[144] The service currently offers over 7600 MB of free storage with additional storage ranging from 20 GB to 16 TB available for US$0.25 per 1 GB per year.[148] Furthermore, software developers know Gmail for its pioneering use of AJAX, a programming technique that allows web pages to be interactive without refreshing the browser.[149] One criticism of Gmail has been the potential for data disclosure, a risk associated with many online web applications. Steve Ballmer (Microsoft's CEO),[150] Liz Figueroa,[151] Mark Rasch,[152] and the editors of Google Watch[153] believe the processing of email message content goes beyond proper use, but Google claims that mail sent to or from Gmail is never read by a human being beyond the account holder, and is only used to improve relevance of advertisements.[154]
Google Docs, another part of Google's productivity suite, allows users to create, edit, and collaborate on documents in an online environment, not dissimilar to Microsoft Word. The service was originally called Writely, but was obtained by Google on March 9, 2006, where it was released as an invitation-only preview.[155] On June 6 after the acquisition, Google created an experimental spreadsheet editing program,[156] which would be combined with Google Docs on October 10.[157] A program to edit presentations would complete the set on September 17, 2007,[158] before all three services were taken out of beta along with Gmail, Google Calendar and all products from the Google Apps Suite on July 7, 2009.[146]
Google entered the enterprise market in February 2002 with the launch of its Google Search Appliance, targeted toward providing search technology for larger organizations.[26] Google launched the Mini three years later, which was targeted at smaller organizations. Late in 2006, Google began to sell Custom Search Business Edition, providing customers with an advertising-free window into Google.com's index. The service was renamed Google Site Search in 2008.[159]
Google Apps is another primary Google enterprise service offering. The service allows organizations to bring Google's web application offerings, such as Gmail and Google Docs, into its own domain. The service is available in several editions: a basic free edition (formerly known as Google Apps Standard edition), Google Apps for Business, Google Apps for Education, and Google Apps for Government. Special editions include extras such as more disk space, API access, a service level agreement (SLA), premium support, and additional apps. In the same year Google Apps was launched, Google acquired Postini[160] and proceeded to integrate the company's security technologies into Google Apps[161] under the name Google Postini Services.[162]
Additional Google enterprise offerings include geospatial solutions (e.g., Google Earth and Google Maps); security and archival solutions (e.g., Postini); and Chromebooks for business and education (i.e., personal computing run on browser-centric operating systems).
Google Translate is a server-side machine translation service, which can translate between 35 different languages. Browser extensions allow for easy access to Google Translate from the browser. The software uses corpus linguistics techniques, where the program "learns" from professionally translated documents, specifically UN and European Parliament proceedings.[163] Furthermore, a "suggest a better translation" feature accompanies the translated text, allowing users to indicate where the current translation is incorrect or otherwise inferior to another translation.
Google launched its Google News service in 2002. The site proclaimed that the company had created a "highly unusual" site that "offers a news service compiled solely by computer algorithms without human intervention. Google employs no editors, managing editors, or executive editors."[164] The site hosted less licensed news content than Yahoo! News, and instead presented topically selected links to news and opinion pieces along with reproductions of their headlines, story leads, and photographs.[165] The photographs are typically reduced to thumbnail size and placed next to headlines from other news sources on the same topic in order to minimize copyright infringement claims. Nevertheless, Agence France Presse sued Google for copyright infringement in federal court in the District of Columbia, a case which Google settled for an undisclosed amount in a pact that included a license of the full text of AFP articles for use on Google News.[166]
In 2006, Google made a bid to offer free wireless broadband access throughout the city of San Francisco along with Internet service provider EarthLink. Large telecommunications companies such as Comcast and Verizon opposed such efforts, claiming it was "unfair competition" and that cities would be violating their commitments to offer local monopolies to these companies. In his testimony before Congress on network neutrality in 2006, Google's Chief Internet Evangelist Vint Cerf blamed such tactics on the fact that nearly half of all consumers lack meaningful choice in broadband providers.[167] Google currently offers free wi-fi access in its hometown of Mountain View, California.[168]
In 2010, Google announced the Google Fiber project with plans to build an ultra-high-speed broadband network for 50,000 to 500,000 customers in one or more American cities.[169] On March 30, 2011, Google announced that Kansas City, Kansas would be the first community where the new network would be deployed.[170] In July 2012, Google completed the construction of a fiber-optic broadband internet network infrastructure in Kansas City, and after building an infrastructure, Google announced pricing for Google Fiber. The service will offer three options including a free broadband internet option, a 1Gbit/s internet option for $70 per month and a version that includes television service for $120 per month.[14]
In 2007, reports surfaced that Google was planning the release of its own mobile phone, possibly a competitor to Apple's iPhone.[171][172][173] The project, called Android, turned out not to be a phone but an operating system for mobile devices, which Google acquired and then released as an open source project under the Apache 2.0 license.[174] Google provides a software development kit for developers so applications can be created to be run on Android-based phones. In September 2008, T-Mobile released the G1, the first Android-based phone.[175] More than a year later on January 5, 2010, Google released an Android phone under its own company name called the Nexus One.[176]
Other projects Google has worked on include a new collaborative communication service, a web browser, and even a mobile operating system. The first of these was first announced on May 27, 2009. Google Wave was described as a product that helps users communicate and collaborate on the web. The service is Google's "email redesigned", with realtime editing, the ability to embed audio, video, and other media, and extensions that further enhance the communication experience. Google Wave was previously in a developer's preview, where interested users had to be invited to test the service, but was released to the general public on May 19, 2010, at Google's I/O keynote. On September 1, 2008, Google pre-announced the upcoming availability of Google Chrome, an open source web browser,[177] which was then released on September 2, 2008. The next year, on July 7, 2009, Google announced Google Chrome OS, an open source Linux-based operating system that includes only a web browser and is designed to log users into their Google account.[178][179]
Google Goggles is a mobile application available on Android and iOS used for image recognition and non-text-based search. In addition to scanning QR codes, the app can recognize historic landmarks, import business cards, and solve Sudoku puzzles.[180] While Goggles could originally identify people as well, Google has limited that functionality as a privacy protection.[181]
In 2011, Google announced that it will unveil Google Wallet, a mobile application for wireless payments.[182]
In late June 2011, Google soft-launched a social networking service called Google+.[183] On July 14, 2011, Google announced that Google+ had reached 10 million users just two weeks after it was launched in this "limited" trial phase.[184] After four weeks in operation, it had reached 25 million users.[185]
Google is known for having an informal corporate culture. On Fortune magazine's list of best companies to work for, Google ranked first in 2007, 2008 and 2012[186][187][188] and fourth in 2009 and 2010.[189][190] Google was also nominated in 2010 to be the worlds most attractive employer to graduating students in the Universum Communications talent attraction index.[191] Google's corporate philosophy embodies such casual principles as "you can make money without doing evil," "you can be serious without a suit," and "work should be challenging and the challenge should be fun."[192]
Google's stock performance following its initial public offering has enabled many early employees to be competitively compensated.[194] After the company's IPO, founders Sergey Brin and Larry Page and CEO Eric Schmidt requested that their base salary be cut to $1. Subsequent offers by the company to increase their salaries have been turned down, primarily because their main compensation continues to come from owning stock in Google. Before 2004, Schmidt was making $250,000 per year, and Page and Brin each earned a salary of $150,000.[195]
In 2007 and through early 2008, several top executives left Google. In October 2007, former chief financial officer of YouTube Gideon Yu joined Facebook[196] along with Benjamin Ling, a high-ranking engineer.[197] In March 2008, Sheryl Sandberg, then vice-president of global online sales and operations, began her position as chief operating officer of Facebook[198] while Ash ElDifrawi, formerly head of brand advertising, left to become chief marketing officer of Netshops, an online retail company that was renamed Hayneedle in 2009.[199] On April 4, 2011 Larry Page became CEO and Eric Schmidt became Executive Chairman of Google.[200] In July 2012 Google's first female employee, Marissa Mayer left Google to become Yahoo's CEO.[201]
As a motivation technique, Google uses a policy often called Innovation Time Off, where Google engineers are encouraged to spend 20% of their work time on projects that interest them. Some of Google's newer services, such as Gmail, Google News, Orkut, and AdSense originated from these independent endeavors.[202] In a talk at Stanford University, Marissa Mayer, Google's Vice President of Search Products and User Experience until July 2012, showed that half of all new product launches at the time had originated from the Innovation Time Off.[203]
In March 2011, consulting firm Universum released data that Google ranks first on the list of ideal employers by nearly 25 percent chosen from more than 10,000 young professionals asked.[204] Fortune magazine ranked Google as number one on its 100 Best Companies To Work For list for 2012.[205]
Google's headquarters in Mountain View, California is referred to as "the Googleplex", a play on words on the number googolplex and the headquarters itself being a complex of buildings. The lobby is decorated with a piano, lava lamps, old server clusters, and a projection of search queries on the wall. The hallways are full of exercise balls and bicycles. Each employee has access to the corporate recreation center. Recreational amenities are scattered throughout the campus and include a workout room with weights and rowing machines, locker rooms, washers and dryers, a massage room, assorted video games, table football, a baby grand piano, a billiard table, and ping pong. In addition to the rec room, there are snack rooms stocked with various foods and drinks, with special emphasis placed on nutrition.[206] Free food is available to employees 24/7, with paid vending machines prorated favoring nutritional value.[207]
In 2006, Google moved into 311,000 square feet (28,900m2) of office space in New York City, at 111 Eighth Avenue in Manhattan.[208] The office was specially designed and built for Google, and it now houses its largest advertising sales team, which has been instrumental in securing large partnerships.[208] In 2003, they added an engineering staff in New York City, which has been responsible for more than 100 engineering projects, including Google Maps, Google Spreadsheets, and others. It is estimated that the building costs Google $10million per year to rent and is similar in design and functionality to its Mountain View headquarters, including table football, air hockey, and ping-pong tables, as well as a video game area. In November 2006, Google opened offices on Carnegie Mellon's campus in Pittsburgh, focusing on shopping related advertisement coding and smartphone applications and programs.[209][210] By late 2006, Google also established a new headquarters for its AdWords division in Ann Arbor, Michigan.[211] Furthermore, Google has offices all around the world, and in the United States, including Ann Arbor, Michigan; Atlanta, Georgia; Austin, Texas; Boulder, Colorado; Cambridge, Massachusetts; New York City; San Francisco, California; Seattle, Washington; Reston, Virginia, and Washington, D.C.
Google is taking steps to ensure that its operations are environmentally sound. In October 2006, the company announced plans to install thousands of solar panels to provide up to 1.6megawatts of electricity, enough to satisfy approximately 30% of the campus' energy needs.[212] The system will be the largest solar power system constructed on a U.S. corporate campus and one of the largest on any corporate site in the world.[212] In addition, Google announced in 2009 that it was deploying herds of goats to keep grassland around the Googleplex short, helping to prevent the threat from seasonal bush fires while also reducing the carbon footprint of mowing the extensive grounds.[213][214] The idea of trimming lawns using goats originated from R. J. Widlar, an engineer who worked for National Semiconductor.[215] Despite this, Google has faced accusations in Harper's Magazine of being an "energy glutton", and was accused of employing its "Don't be evil" motto as well as its very public energy-saving campaigns as an attempt to cover up or make up for the massive amounts of energy its servers actually require.[216]
Google has a tradition of creating April Fools' Day jokes. For example, Google MentalPlex allegedly featured the use of mental power to search the web.[217] In 2007, Google announced a free Internet service called TiSP, or Toilet Internet Service Provider, where one obtained a connection by flushing one end of a fiber-optic cable down their toilet.[218] Also in 2007, Google's Gmail page displayed an announcement for Gmail Paper, allowing users to have email messages printed and shipped to them.[219] In 2008 Google announced Gmail Custom time where users could change the time that the email was sent.[220] In 2010, Google jokingly changed its company name to Topeka in honor of Topeka, Kansas, whose mayor actually changed the city's name to Google for a short amount of time in an attempt to sway Google's decision in its new Google Fiber Project.[221][222] In 2011, Google announced Gmail Motion, an interactive way of controlling Gmail and the computer with body movements via the user's webcam.[223]
In addition to April Fools' Day jokes, Google's services contain a number of easter eggs. For instance, Google included the Swedish Chef's "Bork bork bork," Pig Latin, "Hacker" or leetspeak, Elmer Fudd, Pirate, and Klingon as language selections for its search engine.[224] In addition, the search engine calculator provides the Answer to the Ultimate Question of Life, the Universe, and Everything from Douglas Adams' The Hitchhiker's Guide to the Galaxy.[225] Furthermore, when searching the word "recursion", the spell-checker's result for the properly spelled word is exactly the same word, creating a recursive link.[226] Likewise, when searching for the word "anagram," meaning a rearrangement of letters from one word to form other valid words, Google's suggestion feature displays "Did you mean: nag a ram?"[227] In Google Maps, searching for directions between places separated by large bodies of water, such as Los Angeles and Tokyo, results in instructions to "kayak across the Pacific Ocean." During FIFA World Cup 2010, search queries like "World Cup", "FIFA", etc. caused the "Goooo...gle" page indicator at the bottom of every result page to read "Goooo...al!" instead.[228] Typing in 'Do a barrel roll' in the search engine will make the page do a 360 rotation.
In 2004, Google formed the not-for-profit philanthropic Google.org, with a start-up fund of $1billion.[229] The mission of the organization is to create awareness about climate change, global public health, and global poverty. One of its first projects was to develop a viable plug-in hybrid electric vehicle that can attain 100 miles per gallon. Google hired Dr. Larry Brilliant as the program's executive director in 2004[230] and the current director is Megan Smith.[231]
In 2008 Google announced its "project 10100" which accepted ideas for how to help the community and then allowed Google users to vote on their favorites.[232] After two years of silence, during which many wondered what had happened to the program,[233] Google revealed the winners of the project, giving a total of ten million dollars to various ideas ranging from non-profit organizations that promote education to a website that intends to make all legal documents public and online.[234]
In 2011, Google donated 1million euros to International Mathematical Olympiad to support the next five annual International Mathematical Olympiads (20112015).[235] On July 2012, Google launched a "Legalize Love" campaign in support of gay rights worldwide.[236]
Google uses various tax avoidance strategies. Consequently, out of the five largest American technology companies it pays the lowest taxes to the countries of origin of its revenues. This is accomplished partly by licensing technology through subsidiaries in Ireland, Bermuda, the Bahamas and the Netherlands.[237] This has reportedly sparked a French investigation into Google's transfer pricing practices.[238]
Following criticism of the ammount of corporation tax paid in the UK Mr Eric Schmitt stated on 12 Dec 2012 that the corporation had no intention of paying more[239]. "It's called capitalism" he said.
Google is a noted supporter of network neutrality. According to Google's Guide to Net Neutrality:
On February 7, 2006, Vint Cerf, a co-inventor of the Internet Protocol (IP), and current Vice President and "Chief Internet Evangelist" at Google, in testimony before Congress, said, "allowing broadband carriers to control what people see and do online would fundamentally undermine the principles that have made the Internet such a success."[241]
     
1 History

1.1 Financing and initial public offering
1.2 Growth
1.3 Acquisitions and partnerships
1.4 Google data centers


1.1 Financing and initial public offering
1.2 Growth
1.3 Acquisitions and partnerships
1.4 Google data centers
2 Products and services

2.1 Advertising
2.2 Search engine
2.3 Productivity tools
2.4 Enterprise products
2.5 Other products


2.1 Advertising
2.2 Search engine
2.3 Productivity tools
2.4 Enterprise products
2.5 Other products
3 Corporate affairs and culture

3.1 Employees
3.2 Googleplex
3.3 Easter eggs and April Fools' Day jokes
3.4 Philanthropy
3.5 Tax Avoidance
3.6 Network neutrality


3.1 Employees
3.2 Googleplex
3.3 Easter eggs and April Fools' Day jokes
3.4 Philanthropy
3.5 Tax Avoidance
3.6 Network neutrality
4 See also
5 References
6 External links
1.1 Financing and initial public offering
1.2 Growth
1.3 Acquisitions and partnerships
1.4 Google data centers
2.1 Advertising
2.2 Search engine
2.3 Productivity tools
2.4 Enterprise products
2.5 Other products
3.1 Employees
3.2 Googleplex
3.3 Easter eggs and April Fools' Day jokes
3.4 Philanthropy
3.5 Tax Avoidance
3.6 Network neutrality
Comparison of web search engines
Criticism of Google
Gayglers LGBT employee group
Google Catalogs
Google China
Google Doodle
Google logo
Google platform
Google Ventures venture capital fund
Googlebot web crawler
History of Google
List of Google domains
Official website (Mobile)
Corporate homepage
Corporate history and timeline
Google on Blogger
Google's channel on YouTube
Google at CrunchBase
Google Research
Google website from November 11, 1998 at the Internet Archive
Google at the Open Directory Project
Google companies grouped at OpenCorporates
Google, Inc. at Google Finance
Google, Inc. at Yahoo! Finance
Google, Inc. at Reuters
Google, Inc. SEC filings at EDGAR Online
Google, Inc. SEC filings at the Securities and Exchange Commission
Google Inc. at Hoovers
.
#(`*United Nations*`)#.
The United Nations (abbreviated UN in English, and ONU in French and Spanish), is an international organization whose stated aims are facilitating cooperation in international law, international security, economic development, social progress, human rights, and achievement of world peace. The UN was founded in 1945 after World War II to replace the League of Nations, to stop wars between countries, and to provide a platform for dialogue. It contains multiple subsidiary organizations to carry out its missions.
There are 193 member states, including every internationally recognized sovereign state in the world but the Vatican City. From its offices around the world, the UN and its specialized agencies decide on substantive and administrative issues in regular meetings held throughout the year. The organization has six principal organs: the General Assembly (the main deliberative assembly); the Security Council (for deciding certain resolutions for peace and security); the Economic and Social Council (for assisting in promoting international economic and social cooperation and development); the Secretariat (for providing studies, information, and facilities needed by the UN); the International Court of Justice (the primary judicial organ); and the United Nations Trusteeship Council (which is currently inactive). Other prominent UN System agencies include the World Health Organization (WHO), the World Food Programme (WFP) and United Nations Children's Fund (UNICEF). The UN's most prominent position is Secretary-General which has been held by Ban Ki-moon of South Korea since 2007.
The United Nations Headquarters resides in international territory in New York City, with further main offices at Geneva, Nairobi, and Vienna. The organization is financed from assessed and voluntary contributions from its member states, and has six official languages: Arabic, Chinese, English, French, Russian, and Spanish.[2]
The League of Nations failed to prevent World War II (19391945). Because of the widespread recognition that humankind could not afford a third world war, the United Nations was established to replace the flawed League of Nations in 1945 in order to maintain international peace and promote cooperation in solving international economic, social and humanitarian problems. The earliest concrete plan for a new world organization was begun under the aegis of the U.S. State Department in 1939. Franklin D. Roosevelt first coined the term 'United Nations' as a term to describe the Allied countries. The term was first officially used on 1 January 1942, when 26 governments signed the Atlantic Charter, pledging to continue the war effort.[3] On 25 April 1945, the UN Conference on International Organization began in San Francisco, attended by 50 governments and a number of non-governmental organizations involved in drafting the United Nations Charter. The UN officially came into existence on 24 October 1945 upon ratification of the Charter by the five then-permanent members of the Security CouncilFrance, the Republic of China, the Soviet Union, the United Kingdom and the United Statesand by a majority of the other 46 signatories. The first meetings of the General Assembly, with 51 nations represented, and the Security Council, took place in Methodist Central Hall Westminster in London in January 1946.[4]
The organization was based at the Sperry Gyroscope Corporation's facility in Lake Success, New York, from 19461952, before moving to the United Nations Headquarters building in Manhattan upon its completion.
Since its creation, there has been controversy and criticism of the United Nations. In the United States, an early opponent of the UN was the John Birch Society, which began a "get US out of the UN" campaign in 1959, charging that the UN's aim was to establish a "One World Government." After the Second World War, the French Committee of National Liberation was late to be recognized by the US as the government of France, and so the country was initially excluded from the conferences that aimed at creating the new organization. Charles de Gaulle criticized the UN, famously calling it a machin ("contraption"), and was not convinced that a global security alliance would help maintain world peace, preferring direct defence treaties between countries.[5]
Shortly after its establishment the UN sought recognition as an international legal person due to the case of Reparations for Injuries Suffered in the Service of the United Nations[6] with the advisory opinion delivered by the International Court of Justice (ICJ). The question arose whether the United Nations, as an organisation, had "the capacity to bring an international claim against a government regarding injuries that the organisation alleged had been caused by that state."[7]
The Court stated: the Organization was intended to exercise and enjoy, and is in fact exercising and enjoying functions and rights, which can only be explained on the basis of the possession of a large measure of international personality and the capacity to operate upon an international plane ... Accordingly, the Court has come to the conclusion that the Organization is an international person. That is not the same thing as saying that it is a State, which it certainly is not, or that its legal personality and rights and duties are the same as those of a State ... What it does mean is that it is a subject of international law and capable of possessing international rights and duties, and that it has capacity to maintain its rights by bringing international claims.[8]
The United Nations' system is based on five principal organs (formerly six  the Trusteeship Council suspended operations in 1994, upon the independence of Palau, the last remaining UN trustee territory);[9] the General Assembly, the Security Council, the Economic and Social Council (ECOSOC), the Secretariat, and the International Court of Justice.
Four of the five principal organs are located at the main United Nations Headquarters located on international territory in New York City.[10] The International Court of Justice is located in The Hague, while other major agencies are based in the UN offices at Geneva,[11] Vienna,[12] and Nairobi.[13] Other UN institutions are located throughout the world.
The six official languages of the United Nations, used in intergovernmental meetings and documents, are Arabic, Chinese, English, French, Russian, and Spanish.[2] The Secretariat uses two working languages, English and French. Four of the official languages are the national languages of the permanent members of the Security Council (the United Kingdom and the United States share English as a de facto official language); Spanish and Arabic are the languages of the two largest blocs of official languages outside of the permanent members (Spanish being official in 20 countries, Arabic in 26). Five of the official languages were chosen when the UN was founded; Arabic was added later in 1973. The United Nations Editorial Manual states that the standard for English language documents is British usage and Oxford spelling, the Chinese writing standard is Simplified Chinese. This replaced Traditional Chinese in 1971 when the UN representation of China was changed from the Republic of China to the People's Republic of China (see China and the United Nations for details).
The General Assembly is the main deliberative assembly of the United Nations. Composed of all United Nations member states, the assembly meets in regular yearly sessions under a president elected from among the member states. Over a two-week period at the start of each session, all members have the opportunity to address the assembly. Traditionally, the Secretary-General makes the first statement, followed by the president of the assembly. The first session was convened on 10 January 1946 in the Methodist Central Hall Westminster in London and included representatives of 51 nations.
When the General Assembly votes on important questions, a two-thirds majority of those present and voting is required. Examples of important questions include: recommendations on peace and security; election of members to organs; admission, suspension, and expulsion of members; and, budgetary matters. All other questions are decided by majority vote. Each member country has one vote. Apart from approval of budgetary matters, resolutions are not binding on the members. The Assembly may make recommendations on any matters within the scope of the UN, except matters of peace and security that are under Security Council consideration.
Conceivably, the one state, one vote power structure could enable states comprising just eight percent of the world population to pass a resolution by a two-thirds vote (see List of countries by population). However, as no more than recommendations, it is difficult to imagine a situation in which a recommendation by member states constituting just eight percent of the world's population, would be adhered to by the remaining ninety-two percent of the population, should they object.[citation needed] Likewise, countries representing a 3.4% of the world population could form a majority on any of the questions that are decided by majority vote.
The Security Council is charged with maintaining peace and security among countries. While other organs of the United Nations can only make 'recommendations' to member governments, the Security Council has the power to make binding decisions that member governments have agreed to carry out, under the terms of Charter Article 25.[15] The decisions of the Council are known as United Nations Security Council resolutions.
The Security Council is made up of 15 member states, consisting of 5 permanent membersChina, France, Russia, the United Kingdom and the United Statesand 10 non-permanent members, currently Azerbaijan, India, South Africa, Colombia, Morocco, Togo, Germany, Pakistan, Guatemala, and Portugal.[16] The five permanent members hold veto power over substantive but not procedural resolutions allowing a permanent member to block adoption but not to block the debate of a resolution unacceptable to it. The ten temporary seats are held for two-year terms with member states voted in by the General Assembly on a regional basis. The presidency of the Security Council is rotated alphabetically each month.[17]
The United Nations Secretariat is headed by the Secretary-General, assisted by a staff of international civil servants worldwide. It provides studies, information, and facilities needed by United Nations bodies for their meetings. It also carries out tasks as directed by the UN Security Council, the UN General Assembly, the UN Economic and Social Council, and other UN bodies. The United Nations Charter provides that the staff be chosen by application of the "highest standards of efficiency, competence, and integrity," with due regard for the importance of recruiting on a wide geographical basis.
The Charter provides that the staff shall not seek or receive instructions from any authority other than the UN. Each UN member country is enjoined to respect the international character of the Secretariat and not seek to influence its staff. The Secretary-General alone is responsible for staff selection.
The Secretary-General's duties include helping resolve international disputes, administering peacekeeping operations, organizing international conferences, gathering information on the implementation of Security Council decisions, and consulting with member governments regarding various initiatives. Key Secretariat offices in this area include the Office of the Coordinator of Humanitarian Affairs and the Department of Peacekeeping Operations. The Secretary-General may bring to the attention of the Security Council any matter that, in his or her opinion, may threaten international peace and security.[citation needed]
The Secretariat is headed by the Secretary-General, who acts as the de facto spokesperson and leader of the UN. The current Secretary-General is Ban Ki-moon, who took over from Kofi Annan in 2007 and has been elected for a second term to conclude at the end of 2016.[18]
Envisioned by Franklin D. Roosevelt as a "world moderator", the position is defined in the UN Charter as the organization's "chief administrative officer",[19] but the Charter also states that the Secretary-General can bring to the Security Council's attention "any matter which in his opinion may threaten the maintenance of international peace and security",[20] giving the position greater scope for action on the world stage. The position has evolved into a dual role of an administrator of the UN organization, and a diplomat and mediator addressing disputes between member states and finding consensus to global issues.[21]
The Secretary-General is appointed by the General Assembly, after being recommended by the Security Council, where the permanent members have veto power.[22] The General Assembly can theoretically override the Security Council's recommendation if a majority vote is not achieved, although this has not happened so far.[23] There are no specific criteria for the post, but over the years, it has become accepted that the post shall be held for one or two terms of five years, that the post shall be appointed on the basis of geographical rotation, and that the Secretary-General shall not originate from one of the five permanent Security Council member states.[23]
The International Court of Justice (ICJ), located in The Hague, Netherlands, is the primary judicial organ of the United Nations. Established in 1945 by the United Nations Charter, the Court began work in 1946 as the successor to the Permanent Court of International Justice. The Statute of the International Court of Justice, similar to that of its predecessor, is the main constitutional document constituting and regulating the Court.[25]
It is based in the Peace Palace in The Hague, Netherlands, sharing the building with the Hague Academy of International Law, a private centre for the study of international law. Several of the Court's current judges are either alumni or former faculty members of the Academy. Its purpose is to adjudicate disputes among states. The court has heard cases related to war crimes, illegal state interference and ethnic cleansing, among others, and continues to hear cases.[26]
The Economic and Social Council (ECOSOC) assists the General Assembly in promoting international economic and social cooperation and development. ECOSOC has 54 members, all of which are elected by the General Assembly for a three-year term. The president is elected for a one-year term and chosen amongst the small or middle powers represented on ECOSOC. ECOSOC meets once a year in July for a four-week session. Since 1998, it has held another meeting each April with finance ministers heading key committees of the World Bank and the International Monetary Fund (IMF). Viewed separate from the specialized bodies it coordinates, ECOSOC's functions include information gathering, advising member nations, and making recommendations. In addition, ECOSOC is well-positioned to provide policy coherence and coordinate the overlapping functions of the UNs subsidiary bodies and it is in these roles that it is most active.
The United Nations Permanent Forum on Indigenous Issues (UNPFII or PFII) is the UN's central coordinating body for matters relating to the concerns and rights of the world's indigenous peoples. The forum, which evolved from the United Nations Working Group on Indigenous Populations, is an advisory body within the framework of the United Nations System that reports to the UN's Economic and Social Council (ECOSOC); however, it performs an advisory function in relation to other branches of the United Nations system. It also works with other U.N. bodies as they address indigenous rights through Conventions such as the International Labour Organizations Convention No.169 and the Convention on Biological Diversity (Article 8j).[27]
The Forum's mandate is to:
Since the passage of the Declaration on the Rights of Indigenous Peoples in 2007, much of the work of UNPFII has surrounded the compliance of U.N. member states to the standards of that declaration.[27] However, it performs many other international functions as well.[28]
Many UN organizations and agencies exist to work on particular issues. Some of the most well-known agencies are the International Atomic Energy Agency, the Food and Agriculture Organization, UNESCO (United Nations Educational, Scientific and Cultural Organization), the World Bank and the World Health Organization.
It is through these agencies that the UN performs most of its humanitarian work. Examples include mass vaccination programmes (through the WHO), the avoidance of famine and malnutrition (through the work of the WFP) and the protection of vulnerable and displaced people (for example, by the UNHCR).
The United Nations Charter stipulates that each primary organ of the UN can establish various specialized agencies to fulfil its duties.
With the addition of South Sudan on 14 July 2011,[29] there are currently 193 United Nations member states, including all fully recognized independent states[30] apart from Vatican City (the Holy See, which holds sovereignty over the state of Vatican City, is a permanent observer, for details see Holy See and the United Nations).[31]
The United Nations Charter outlines the rules for membership:
The Group of 77 at the UN is a loose coalition of developing nations, designed to promote its members' collective economic interests and create an enhanced joint negotiating capacity in the United Nations. There were 77 founding members of the organization, but the organization has since expanded to 130 member countries. The group was founded on 15 June 1964 by the "Joint Declaration of the Seventy-Seven Countries" issued at the United Nations Conference on Trade and Development (UNCTAD). The first major meeting was in Algiers in 1967, where the Charter of Algiers was adopted and the basis for permanent institutional structures was begun.[32]
The UN, after approval by the Security Council, sends peacekeepers to regions where armed conflict has recently ceased or paused to enforce the terms of peace agreements and to discourage combatants from resuming hostilities. Since the UN does not maintain its own military, peacekeeping forces are voluntarily provided by member states of the UN. The forces, also called the "Blue Helmets", who enforce UN accords, are awarded United Nations Medals, which are considered international decorations instead of military decorations. The peacekeeping force as a whole received the Nobel Peace Prize in 1988.[33]
The founders of the UN had envisaged that the organization would act to prevent conflicts between nations and make future wars impossible, however the outbreak of the Cold War made peacekeeping agreements extremely difficult because of the division of the world into hostile camps. Following the end of the Cold War, there were renewed calls for the UN to become the agency for achieving world peace, as several dozen ongoing conflicts continue to rage around the globe.
A 2005 RAND Corp study found the UN to be successful in two out of three peacekeeping efforts. It compared UN nation-building efforts to those of the United States, and found that seven out of eight UN cases are at peace, as compared with four out of eight US cases at peace.[34] Also in 2005, the Human Security Report documented a decline in the number of wars, genocides and human rights abuses since the end of the Cold War, and presented evidence, albeit circumstantial, that international activismmostly spearheaded by the UNhas been the main cause of the decline in armed conflict since the end of the Cold War.[35] Situations where the UN has not only acted to keep the peace but also occasionally intervened include the Korean War (19501953), and the authorization of intervention in Iraq after the Persian Gulf War in 1990.
The UN has also drawn criticism for perceived failures. In many cases, member states have shown reluctance to achieve or enforce Security Council resolutions, an issue that stems from the UN's intergovernmental natureseen by some as simply an association of 193 member states who must reach consensus, not an independent organization. Disagreements in the Security Council about military action and intervention are seen as having failed to prevent the 1971 Bangladesh atrocities,[36] the 1994 Rwandan Genocide,[37] failed to provide humanitarian aid and intervene in the Second Congo War, failed to intervene in the 1995 Srebrenica massacre and protect a refugee haven by authorizing peacekeepers to use force, failure to deliver food to starving people in Somalia, failure to implement provisions of Security Council resolutions related to the Israeli-Palestinian conflict, and continuing failure to prevent genocide or provide assistance in Darfur. UN peacekeepers have also been accused of child rape, sexual abuse or soliciting prostitutes during various peacekeeping missions, starting in 2003, in the Congo,[38] Haiti,[39][40] Liberia,[41] Sudan and what is now South Sudan,[42] Burundi and Cte d'Ivoire.[43] In 2004, former Israeli ambassador to the UN Dore Gold criticized what it called the organization's moral relativism in the face of (and occasional support of)[44] genocide and terrorism that occurred between the moral clarity of its founding period and the present day. Gold specifically mentions Yasser Arafat's 1988 invitation to address the General Assembly as a low point in the UN's history.[45]
In addition to peacekeeping, the UN is also active in encouraging disarmament. Regulation of armaments was included in the writing of the United Nations Charter in 1945 and was envisioned as a way of limiting the use of human and economic resources for the creation of them.[46] However, the advent of nuclear weapons came only weeks after the signing of the charter and immediately halted concepts of arms limitation and disarmament, resulting in the first resolution of the first ever General Assembly meeting calling for specific proposals for "the elimination from national armaments of atomic weapons and of all other major weapons adaptable to mass destruction".[47] The principal forums for disarmament issues are the General Assembly First Committee, the UN Disarmament Commission, and the Conference on Disarmament, and considerations have been made of the merits of a ban on testing nuclear weapons, outer space arms control, the banning of chemical weapons and land mines, nuclear and conventional disarmament, nuclear-weapon-free zones, the reduction of military budgets, and measures to strengthen international security.
The UN is one of the official supporters of the World Security Forum, a major international conference on the effects of global catastrophes and disasters, which took place in the United Arab Emirates in October 2008.
The pursuit of human rights was a central reason for creating the UN. World War II atrocities and genocide led to a ready consensus that the new organization must work to prevent any similar tragedies in the future. An early objective was creating a legal framework for considering and acting on complaints about human rights violations. The UN Charter obliges all member nations to promote "universal respect for, and observance of, human rights" and to take "joint and separate action" to that end. The Universal Declaration of Human Rights, though not legally binding, was adopted by the General Assembly in 1948 as a common standard of achievement for all. The Assembly regularly takes up human rights issues.
The UN and its agencies are central in upholding and implementing the principles enshrined in the Universal Declaration of Human Rights. A case in point is support by the UN for countries in transition to democracy. Technical assistance in providing free and fair elections, improving judicial structures, drafting constitutions, training human rights officials, and transforming armed movements into political parties have contributed significantly to democratization worldwide. The UN has helped run elections in countries with little or no democratic history, including recently in Afghanistan and East Timor. The UN is also a forum to support the right of women to participate fully in the political, economic, and social life of their countries. The UN contributes to raising consciousness of the concept of human rights through its covenants and its attention to specific abuses through its General Assembly, Security Council resolutions, or International Court of Justice rulings.
The purpose of the United Nations Human Rights Council, established in 2006,[48] is to address human rights violations. The Council is the successor to the United Nations Commission on Human Rights, which was often criticized for the high-profile positions it gave to member states that did not guarantee the human rights of their own citizens.[49] The council has 47 members distributed by region, which each serve three-year terms, and may not serve three consecutive terms.[50] A candidate to the body must be approved by a majority of the General Assembly. In addition, the council has strict rules for membership, including a universal human rights review. While some members with questionable human rights records have been elected, it is fewer than before with the increased focus on each member state's human rights record.[51]
The rights of some 370million indigenous peoples around the world are also a focus for the UN, with a Declaration on the Rights of Indigenous Peoples being approved by the General Assembly in 2007.[52] The declaration outlines the individual and collective rights to culture, language, education, identity, employment and health, thereby addressing post-colonial issues that had confronted indigenous peoples for centuries. The declaration aims to maintain, strengthen and encourage the growth of indigenous institutions, cultures and traditions. It also prohibits discrimination against indigenous peoples and promotes their active participation in matters that concern their past, present and future.[52] The United Nations Permanent Forum on Indigenous Issues is the UN's central coordinating body for matters relating to the concerns and rights of the world's indigenous peoples. The forum is an advisory body within the framework of the United Nations System that reports to the UN's Economic and Social Council.
In conjunction with other organizations such as the Red Cross, the UN provides food, drinking water, shelter and other humanitarian services to populaces suffering from famine, displaced by war, or afflicted by other disasters. Major humanitarian branches of the UN are the World Food Programme (which helps feed more than 100million people a year in 80 countries), the office of the High Commissioner for Refugees with projects in over 116 countries, as well as peacekeeping projects in over 24 countries.
In 2011, the United Nations passed its first resolution recognizing LGBT rights, and followed up with a report documenting violations of the rights of LGBT people, including hate crime, criminalization of homosexuality, and discrimination.[53][54] In July 2012, the South African and Brazilian ambassadors demanded further discussion on discrimination against LGBT people. The joint statement said that sexual orientation and gender identity is a human rights issue. This was rejected by a group lead by the Organisation of Islamic Cooperation.[55]
Millennium Development Goals
The UN is involved in supporting development, e.g. by the formulation of the Millennium Development Goals. The UN Development Programme (UNDP) is the largest multilateral source of grant technical assistance in the world. Organizations like the World Health Organization (WHO), UNAIDS, and The Global Fund to Fight AIDS, Tuberculosis and Malaria are leading institutions in the battle against diseases around the world, especially in poor countries. The UN Population Fund is a major provider of reproductive services. 32 UN agencies performing tasks on development are coordinating their efforts through the United Nations Development Group or UNDG.[56]
The UN also promotes human development through some related agencies, particularly the UNDP.[57] The World Bank Group and International Monetary Fund (IMF), for example, are independent, specialized agencies and observers within the UN framework, according to a 1947 agreement. They were initially formed as separate from the UN through the Bretton Woods Agreement in 1944.[58]
The UNDP annually publishes the Human Development Index (HDI), a comparative measure ranking countries by poverty, literacy, education, life expectancy, and other factors.[59]
The Millennium Development Goals (declared in the United Nations Millennium Declaration, signed in September 2000) are eight goals that all of the then 192 United Nations member states have agreed to try to achieve by the year 2015.[60]
From time to time, the different bodies of the United Nations pass resolutions that contain operating paragraphs that begin with the words "requests", "calls upon", or "encourages", which the Secretary-General interprets as a mandate to set up a temporary organization or do something. These mandates can be as little as researching and publishing a written report, or mounting a full-scale peacekeeping operation (usually the exclusive domain of the Security Council).
Although the specialized institutions, such as the WHO, were originally set up by this means, they are not the same as mandates because they are permanent organizations that exist independently of the UN with their own membership structure. One could say that original mandate was simply to cover the process of setting up the institution, and has therefore long expired. Most mandates expire after a limited period and require renewal from the body, which set them up.
One of the outcomes of the 2005 World Summit was a mandate (labelled id 17171) for the Secretary-General to "review all mandates older than five years originating from resolutions of the General Assembly and other organs". To facilitate this review and to finally bring coherence to the organization, the Secretariat has produced an on-line registry of mandates to draw together the reports relating to each one and create an overall picture.[61]
On 5 June 2007, World Environment Day, Secretary-General Ban Ki-moon made public his ambition to make the United Nations more efficient in its operations: I would like to see our renovated Headquarters complex eventually become a globally acclaimed model of efficient use of energy and resources. Beyond New York, the initiative should include the other United Nations headquarters and offices around the globe. The UN's progress towards achieving this goal is communicated through the initiative Greening the Blue (see external links below).
Over the lifetime of the UN, over 80 colonies have attained independence.[62] The General Assembly adopted the Declaration on the Granting of Independence to Colonial Countries and Peoples in 1960 with no votes against but abstentions from all major colonial powers. Through the UN Committee on Decolonization,[63] created in 1962, the UN has focused considerable attention on decolonization. It has also supported the new states that have arisen as a result of self-determination initiatives. The committee has overseen the decolonization of every country larger than 20,000km and removed them from the United Nations list of Non-Self-Governing Territories, besides Western Sahara, a country larger than the UK only relinquished by Spain in 1975.
The UN declares and coordinates international observances, periods of time to observe some issue of international interest or concern. Using the symbolism of the UN, a specially designed logo for the year, and the infrastructure of the United Nations System, various days and years have become catalysts to advancing key issues of concern on a global scale. For example, World Tuberculosis Day, Earth Day and International Year of Deserts and Desertification.
The UN is financed from assessed and voluntary contributions from member states. The General Assembly approves the regular budget and determines the assessment for each member. This is broadly based on the relative capacity of each country to pay, as measured by their gross national income (GNI), with adjustments for external debt and low per capita income.[65]
The Assembly has established the principle that the UN should not be overly dependent on any one member to finance its operations. Thus, there is a 'ceiling' rate, setting the maximum amount any member is assessed for the regular budget. In December 2000, the Assembly revised the scale of assessments to reflect current global circumstances. As part of that revision, the regular budget ceiling was reduced from 25% to 22%. For the least developed countries (LDCs), a ceiling rate of 0.01% is applied.[65] In addition to the ceiling rates, the minimum amount assessed to any member nation (or 'floor' rate) is set at 0.001% of the UN budget. Refer to the table for major contributors.
A large share of UN expenditures addresses the core UN mission of peace and security. The peacekeeping budget for the 20052006 fiscal year was approximately US$5billion, 2.5billion (compared to approximately US$1.5billion, 995million for the UN core budget over the same period), with some 70,000 troops deployed in 17 missions around the world.[66] UN peace operations are funded by assessments, using a formula derived from the regular funding scale, but including a weighted surcharge for the five permanent Security Council members, who must approve all peacekeeping operations. This surcharge serves to offset discounted peacekeeping assessment rates for less developed countries. As of 1 January 2011, the top 10 providers of assessed financial contributions to United Nations peacekeeping operations were: the United States, Japan, the United Kingdom, Germany, France, Italy, China, Canada, Spain and the Republic of Korea.[67]
Special UN programmes not included in the regular budget (such as UNICEF, the WFP and UNDP) are financed by voluntary contributions from other member governments. Most of this is financial contributions, but some is in the form of agricultural commodities donated for afflicted populations. Since their funding is voluntary, many of these agencies suffer severe shortages during economic recessions. In July 2009, the World Food Programme reported that it has been forced to cut services because of insufficient funding.[68] It has received barely a quarter of the total it needed for the 09/10 financial year.
The UN and its agencies are immune to the laws of the countries where they operate, safeguarding UN's impartiality with regard to the host and member countries.[69]
Despite their independence in matters of human resources policy, the UN and its agencies voluntarily apply the laws of member states regarding same-sex marriages, allowing decisions about the status of employees in a same-sex partnership to be based on nationality. The UN and its agencies recognize same-sex marriages only if the employees are citizens of countries that recognize the marriage. This practice is not specific to the recognition of same-sex marriage but reflects a common practice of the UN for a number of human resources matters. It has to be noted though that some agencies provide limited benefits to domestic partners of their staff and that some agencies do not recognise same-sex marriage or domestic partnership of their staff.[citation needed]
Since its founding, there have been many calls for reform of the United Nations, although little consensus on how to do so. Some want the UN to play a greater or more effective role in world affairs, while others want its role reduced to humanitarian work.[70] There have also been numerous calls for the UN Security Council's membership to be increased, for different ways of electing the UN's Secretary-General, and for a United Nations Parliamentary Assembly.
The UN has also been accused of bureaucratic inefficiency and waste. During the 1990s, the United States withheld dues citing inefficiency, and only started repayment on the condition that a major reforms initiative was introduced. In 1994, the Office of Internal Oversight Services (OIOS) was established by the General Assembly to serve as an efficiency watchdog.[71]
An official reform programme was begun by Kofi Annan in 1997. Reforms mentioned include changing the permanent membership of the Security Council (which currently reflects the power relations of 1945), making the bureaucracy more transparent, accountable and efficient, making the UN more democratic, and imposing an international tariff on arms manufacturers worldwide.[citation needed]
In September 2005, the UN convened a World Summit that brought together the heads of most member states, calling the summit "a once-in-a-generation opportunity to take bold decisions in the areas of development, security, human rights and reform of the United Nations."[72] Kofi Annan had proposed that the summit agree on a global "grand bargain" to reform the UN, renewing the organization's focus on peace, security, human rights and development, and to make it better equipped at facing 21st century issues. The World Summit Outcome Document delineated the conclusions of the meeting, including: the creation of a Peacebuilding Commission, to help countries emerging from conflict; a Human Rights Council and a democracy fund; a clear and unambiguous condemnation of terrorism "in all its forms and manifestations"; agreements to devote more resources to the Office of Internal Oversight Services; agreements to spend billions more on achieving the Millennium Development Goals; the dissolution of the Trusteeship Council, because of the completion of its mission; and, the agreement that individual states, with the assistance of the international community, have the "responsibility to protect" populations from genocide, war crimes, ethnic cleansing and crimes against humanity- with the understanding that the international community is prepared to act "collectively" in a timely and decisive manner to protect vulnerable civilians should a state "manifestly fail" in fulfilling its responsibility.[73]
The Office of Internal Oversight Services is being restructured to better define its scope and mandate, and will receive more resources. In addition, to improve the oversight and auditing capabilities of the General Assembly, an Independent Audit Advisory Committee (IAAC) is being created. In June 2007, the Fifth Committee created a draft resolution for the terms of reference of this committee.[74][75] An ethics office was established in 2006, responsible for administering new financial disclosure and whistleblower protection policies. Working with the OIOS, the ethics office also plans to implement a policy to avoid fraud and corruption.[76] The Secretariat is in the process of reviewing all UN mandates that are more than five years old. The review is intended to determine which duplicative or unnecessary programmes should be eliminated. Not all member states agree on which of the over 7000 mandates should be reviewed. The dispute centres on whether mandates that have been renewed should be examined.[77] Indeed, the obstacles identified  in particular, the lack of information on the resource implications of each mandate  constituted sufficient justification for the General Assembly to discontinue the mandate review in September 2008.
In the meantime, the General Assembly launched a number of new loosely related reform initiatives in April 2007, covering international environmental governance, Delivering as One at the country level to enhance the consolidation of UN programme activities and a unified gender organization. Whereas little was achieved on the first two issues, the General Assembly approved in September 2010 the establishment of UN Women as the new UN organization for gender equality and the empowerment of women. UN Women was established by unifying the resources and mandates of four small entities for greater impact and its first head is Ms. Michelle Bachelet, former President of Chile.[citation needed]
A review of UN action during the final months of the Sri Lanka's civil war in 2009, in which tens of thousands of people were killed, criticized the UN leadership, UN Security Council and top UN officials in Sri Lanka.[78]
    
1 History
2 Legal basis of establishment
3 Organization

3.1 General Assembly
3.2 Security Council
3.3 Secretariat

3.3.1 Secretary-General


3.4 International Court of Justice
3.5 Economic and Social Council

3.5.1 Permanent Forum on Indigenous Issues


3.6 Specialized institutions


3.1 General Assembly
3.2 Security Council
3.3 Secretariat

3.3.1 Secretary-General


3.3.1 Secretary-General
3.4 International Court of Justice
3.5 Economic and Social Council

3.5.1 Permanent Forum on Indigenous Issues


3.5.1 Permanent Forum on Indigenous Issues
3.6 Specialized institutions
4 Membership

4.1 Group of 77


4.1 Group of 77
5 Objectives

5.1 Peacekeeping and security
5.2 Human rights and humanitarian assistance
5.3 Social and economic development
5.4 Mandates

5.4.1 Greening the Blue


5.5 Other


5.1 Peacekeeping and security
5.2 Human rights and humanitarian assistance
5.3 Social and economic development
5.4 Mandates

5.4.1 Greening the Blue


5.4.1 Greening the Blue
5.5 Other
6 Funding
7 Personnel policy
8 Reform & Criticism
9 See also
10 References
11 Further reading
12 External links
3.1 General Assembly
3.2 Security Council
3.3 Secretariat

3.3.1 Secretary-General


3.3.1 Secretary-General
3.4 International Court of Justice
3.5 Economic and Social Council

3.5.1 Permanent Forum on Indigenous Issues


3.5.1 Permanent Forum on Indigenous Issues
3.6 Specialized institutions
3.3.1 Secretary-General
3.5.1 Permanent Forum on Indigenous Issues
4.1 Group of 77
5.1 Peacekeeping and security
5.2 Human rights and humanitarian assistance
5.3 Social and economic development
5.4 Mandates

5.4.1 Greening the Blue


5.4.1 Greening the Blue
5.5 Other
5.4.1 Greening the Blue
v
t
e
may resolve non-compulsory recommendations to states, or suggestions to the UNSC (not a Parliament)
decides on the admission of new members, on proposal of the UNSC
adopts the budget
elects the non-permanent members of the UNSC, all members of ECOSOC, on the proposal of the UNSC the UN Secretary General, and the 15 judges of the ICJ
supports the other UN bodies administratively, e.g. in the organization of conferences, writing reports and studies, and the preparation of the budget-plan
its chairman - the UN Secretary General - is elected by the UN General Assembly for a five-year mandate and is the most important representative of the UN
beside its headquarters in New York City it has three main offices in Geneva, Nairobi and Vienna
decides disputes between states that recognize its jurisdiction and creates legal opinions
the 15 judges are elected by the UN General Assembly for nine years. It renders judgement with relative majority
parties on the ICJ can only be countries, however no international organizations and other subjects of international law (not to be confused with the ICC)
responsible for the maintenance of international peace and security
the most powerful organ of the UN, as it may adopt compulsory resolutions
its decisions include peacekeeping- and peace enforcement-missions, as well as non-military pressure mediums, such as trade embargos
has 15 members: five permanent members with veto power, and ten elected members
responsible for cooperation between states on economic and social fields (raising the general standard of living, solve economic, social and health problems, promotion of human rights, culture and education, as well as humanitarian aid)
therefore it has established numerous functional and regional commissions
also coordinates the cooperation with the numerous specialized agencies of the United Nations
has 54 members, who are elected by the UN General Assembly to serve staggered three-year mandates
was originally designed to manage colonial possessions that were earlier League of Nations mandates
is inactive since 1994, with the last trust territory (Namibia) attaining independence in 1990
Provide expert advice and recommendations to the Economic and Social Council and to the various programmes, funds and agencies of the United Nations System through the Council;
Raise awareness and promote the integration and coordination of activities related to indigenous issues within the UN system;
Prepare and disseminate information on these issues.
Collective security
High-level Panel on United Nations Systemwide Coherence
International relations
List of current Permanent Representatives to the United Nations
Model United Nations
Official statistics
UNESCO Goodwill Ambassador
Office of the United Nations High Commissioner for Refugees Goodwill Ambassador UNHCR Goodwill Ambassador
Official languages of the United Nations
United Nations Association
United Nations Convention on the Law of the Sea
United Nations Group of Experts on Geographical Names (UNGEGN)
United Nations in popular culture
United Nations International School
United Nations Peace Messenger Cities
United Nations Postal Administration
United Nations Security Council
United Nations University
University for Peace
World Heritage Site
United Nations Foundation
Yearbook of the United Nations
Model United Nations
United Nations Intellectual History Project Book Series. Indiana University Press.
"Think Again: The United Nations", Madeleine K. Albright, Foreign Policy, September/October 2004.
Hans Kchler, Quo Vadis, United Nations?, in: Law Review, Polytechnic University of the Philippines, College of Law, May 2005 Online version.
An Insider's Guide to the UN, Linda Fasulo, Yale University Press (1 November 2003), hardcover, 272 pages, ISBN 0-300-10155-4.
United Nations: The First Fifty Years, Stanley Mesler, Atlantic Monthly Press (1 March 1997), hardcover, 416 pages, ISBN 0-87113-656-2.
Act of Creation: The Founding of the United Nations, Stephen Schlesinger, Westview Press (1 September 2003), softcover, 374 pages, ISBN 978-0-8133-3324-3.
The United Nations Security Council and War: The Evolution of Thought and Practice since 1945, edited by Vaughan Lowe, Adam Roberts, Jennifer Welsh and Dominik Zaum, Oxford University Press, Oxford, 2010, paperback, 794 pages. ISBN 978-0-19-958330-0. For US edition, click here.
United Nations, Divided World: The UN's Roles in International Relations, edited by Adam Roberts and Benedict Kingsbury, Oxford University Press, Oxford, 2nd edition, 1993, paperback, 589 pages. ISBN 0-19-827926-4. For US edition, click here.
A Guide to Delegate Preparation: A Model United Nations Handbook, edited by Scott A. Leslie, The United Nations Association of the United States of America, 2004 edition (October 2004), softcover, 296 pages, ISBN 1-880632-71-3.
"U.S. At WarInternational." Time Magazine XLV.19 7 May 1945: 2528.
The Oxford Handbook on the United Nations, edited by Thomas G. Weiss and Sam Daws, Oxford University Press, July 2007, hardcover, 896 pages, ISBN 978-0-19-927951-7, ISBN 0-19-927951-9.
Gold, Dore. Tower of Babble: How the United Nations Has Fueled Global Chaos. New York: Three Rivers Press, 2004.
Official website
United Nations Systems of Organizations
About the United Nations
Global Issues on the UN Agenda
High-level Panel on United Nations Systemwide Coherence
Journal of the United Nations: Programme of meetings and agenda.
The United Nations Regional Information Centre (UNRIC)
Greening the Blue: The United Nations' approach to managing its own sustainability performance
UN Chronicle Magazine
UN Organisation Chart
UN Works
United Nations CharterCharter text
United Nations Security Council Resolutions
United Nations Volunteers
United Nations Webcasts
Universal Declaration of Human Rights
World Map of UN websites and locations
United Nations Rule of Law, the United Nations' centralized website on the rule of law.
Documents and Resources on UN, War, War Crimes and Genocide
Eye on the UNA Project of the Hudson Institute New York and the Touro Law Center Institute for Human Rights
United Nations LibGuide resources from the University of Illinois at UrbanaChampaign Library
History of the United NationsUK Government site
Inner City PressUN related news.
Outcomes of the 2005 World SummitPDF(82.9 KB)
Permanent Missions To The United Nations
Searchable archive of UN discussions and votes
List of UN datasets on CKAN, a registry of open data
Task Force on United NationsU.S. Institute of Peace
UN watchnon-governmental organization based in Geneva whose mandate is to monitor the performance of the United Nations by the yardstick of its own Charter.
United Nations Association of the UK: independent policy authority on the UN
United Nations: Inside the Glass HouseIndependent news reports by the news agency Inter Press Service
United Nations eLearning Unit created by ISRGUniversity of Innsbruck
United Nations Research Guide from the Mississippi State University Libraries
Website of the Global Policy Forum, an independent think-tank on the UN
United Nations Offices Worldwide
.
